<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-Á•ûÁªèÁΩëÁªú]]></title>
    <url>%2F2018%2F08%2F25%2FMachine%20learning%20Artificial%20Neural%20Network%2F</url>
    <content type="text"><![CDATA[‰ªÄ‰πàÊòØ‰∫∫Â∑•Á•ûÁªèÁΩëÁªúÊ®°Âûã‰∫∫Â∑•Á•ûÁªèÁΩëÁªú(Artificial Neural Network, ANN)Ê≤°Êúâ‰∏Ä‰∏™‰∏•Ê†ºÁöÑÊ≠£ÂºèÂÆö‰πâ„ÄÇÂÆÉÁöÑÂü∫Êú¨ÁâπÁÇπÔºåÊòØËØïÂõæÊ®°‰ªøÂ§ßËÑëÁöÑÁ•ûÁªèÂÖÉ‰πãÈó¥‰º†ÈÄíÔºåÂ§ÑÁêÜ‰ø°ÊÅØÁöÑÊ®°Âºè„ÄÇ ‰∏Ä‰∏™ËÆ°ÁÆóÊ®°ÂûãÔºåË¶ÅË¢´Áß∞‰∏∫‰∏∫Á•ûÁªèÁΩëÁªúÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÂΩºÊ≠§ËøûÊé•ÁöÑËäÇÁÇπ Ôºà‰πüÁß∞ ‚ÄòÁ•ûÁªèÂÖÉ‚ÄôÔºâÔºåÂπ∂‰∏îÂÖ∑Â§á‰∏§‰∏™ÁâπÊÄßÔºöÊØè‰∏™Á•ûÁªèÂÖÉÔºåÈÄöËøáÊüêÁßçÁâπÂÆöÁöÑËæìÂá∫ÂáΩÊï∞ Ôºà‰πüÂè´ÊøÄÂä±ÂáΩÊï∞ activation functionÔºâÔºåËÆ°ÁÆóÂ§ÑÁêÜÊù•Ëá™ÂÖ∂ÂÆÉÁõ∏ÈÇªÁ•ûÁªèÂÖÉÁöÑÂä†ÊùÉËæìÂÖ•ÂÄºÁ•ûÁªèÂÖÉ‰πãÈó¥ÁöÑ‰ø°ÊÅØ‰º†ÈÄíÁöÑÂº∫Â∫¶ÔºåÁî®ÊâÄË∞ìÂä†ÊùÉÂÄºÊù•ÂÆö‰πâÔºåÁÆóÊ≥ï‰ºö‰∏çÊñ≠Ëá™ÊàëÂ≠¶‰π†ÔºåË∞ÉÊï¥Ëøô‰∏™Âä†ÊùÉÂÄºÊÄªÁªìÔºöÁ•ûÁªèÁΩëÁªúÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÂ∞±ÊòØÔºöËÆ°ÁÆó„ÄÅËøûÊé•„ÄÅËØÑ‰º∞„ÄÅÁ∫†Èîô„ÄÅÂ≠¶‰π† Á•ûÁªèÁΩëÁªúÊ®°ÂûãÂèØ‰ª•ÂàÜ‰∏∫ÔºöÂâçÂêëÁΩëÁªúÁΩëÁªú‰∏≠ÂêÑ‰∏™Á•ûÁªèÂÖÉÊé•ÂèóÂâç‰∏ÄÁ∫ßÁöÑËæìÂÖ•ÔºåÂπ∂ËæìÂá∫Âà∞‰∏ã‰∏ÄÁ∫ßÔºåÁΩëÁªú‰∏≠Ê≤°ÊúâÂèçÈ¶àÔºåÂèØ‰ª•Áî®‰∏Ä‰∏™ÊúâÂêëÊó†ÁéØË∑ØÂõæË°®Á§∫„ÄÇËøôÁßçÁΩëÁªúÂÆûÁé∞‰ø°Âè∑‰ªéËæìÂÖ•Á©∫Èó¥Âà∞ËæìÂá∫Á©∫Èó¥ÁöÑÂèòÊç¢ÔºåÂÆÉÁöÑ‰ø°ÊÅØÂ§ÑÁêÜËÉΩÂäõÊù•Ëá™‰∫éÁÆÄÂçïÈùûÁ∫øÊÄßÂáΩÊï∞ÁöÑÂ§öÊ¨°Â§çÂêà„ÄÇÁΩëÁªúÁªìÊûÑÁÆÄÂçïÔºåÊòì‰∫éÂÆûÁé∞„ÄÇÂèç‰º†ÁΩëÁªúÊòØ‰∏ÄÁßçÂÖ∏ÂûãÁöÑÂâçÂêëÁΩëÁªú„ÄÇ ÂèçÈ¶àÁΩëÁªúÁΩëÁªúÂÜÖÁ•ûÁªèÂÖÉÈó¥ÊúâÂèçÈ¶àÔºåÂèØ‰ª•Áî®‰∏Ä‰∏™Êó†ÂêëÁöÑÂÆåÂ§áÂõæË°®Á§∫„ÄÇËøôÁßçÁ•ûÁªèÁΩëÁªúÁöÑ‰ø°ÊÅØÂ§ÑÁêÜÊòØÁä∂ÊÄÅÁöÑÂèòÊç¢ÔºåÂèØ‰ª•Áî®Âä®ÂäõÂ≠¶Á≥ªÁªüÁêÜËÆ∫Â§ÑÁêÜ„ÄÇÁ≥ªÁªüÁöÑÁ®≥ÂÆöÊÄß‰∏éËÅîÊÉ≥ËÆ∞ÂøÜÂäüËÉΩÊúâÂØÜÂàáÂÖ≥Á≥ª„ÄÇHopfieldÁΩëÁªú„ÄÅÊ≥¢ËÄ≥ÂÖπÊõºÊú∫ÂùáÂ±û‰∫éËøôÁßçÁ±ªÂûã„ÄÇ ÊøÄÊ¥ªÂáΩÊï∞Áî®‰∫éÂ§ÑÁêÜÂ§çÊùÇÁöÑÈùûÁ∫øÊÄßÂàÜÁ±ªÊÉÖÂÜµ„ÄÇÊØîÁ∫øÊÄßÂõûÂΩí„ÄÅlogisticÂõûÂΩíÁÅµÊ¥ª„ÄÇËÆ≠ÁªÉÁöÑÊó∂ÂÄôÊ≥®ÊÑèËøáÊãüÂêà„ÄÇÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞Sigmoidùëì(ùë•)=1/(1+exp‚Å°(‚àíùë•))ÁâπÁÇπÔºöÂΩìxË∂ãËøëË¥üÊó†Á©∑Êó∂ÔºåyË∂ãËøë‰∫é0ÔºõË∂ãËøë‰∫éÊ≠£Êó†Á©∑Êó∂ÔºåyË∂ãËøë‰∫é1ÔºõxË∂ÖÂá∫[-6,6]ÁöÑËåÉÂõ¥ÂêéÔºåÂáΩÊï∞ÂÄºÂü∫Êú¨‰∏äÊ≤°ÊúâÂèòÂåñÔºåÂÄºÈùûÂ∏∏Êé•Ëøë0ÊàñËÄÖ1ËØ•ÂáΩÊï∞ÁöÑÂÄºÂüüËåÉÂõ¥ÈôêÂà∂Âú®(0,1)‰πãÈó¥ÔºåËøôÊ†∑sigmoidÂáΩÊï∞Â∞±ËÉΩ‰∏é‰∏Ä‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉËÅîÁ≥ªËµ∑Êù•‰∫Ü„ÄÇùëì^‚Ä≤ (ùë•)=ùëì(ùë•)(1‚àíùëì(ùë•)) ÂèåÊõ≤Ê≠£Âàátanh‚Å°(ùë•)=(ùëí^ùë•‚àíùëí^(‚àíùë•))/(ùëí^ùë•+ùëí^(‚àíùë•) )ÁâπÁÇπÔºöÂΩìxË∂ãËøëË¥üÊó†Á©∑Êó∂ÔºåyË∂ãËøë‰∫é-1ÔºõË∂ãËøë‰∫éÊ≠£Êó†Á©∑Êó∂ÔºåyË∂ãËøë‰∫é1ÔºõxË∂ÖÂá∫[-3,3]ÁöÑËåÉÂõ¥ÂêéÔºåÂáΩÊï∞ÂÄºÂü∫Êú¨‰∏äÊ≤°ÊúâÂèòÂåñÔºåÂÄºÈùûÂ∏∏Êé•Ëøë-1ÊàñËÄÖ1ËØ•ÂáΩÊï∞ÁöÑÂÄºÂüüËåÉÂõ¥ÈôêÂà∂Âú®(-1,1)‰πãÈó¥tanh^‚Ä≤ (ùë•)=1‚àítanh(x)^2 ‰øÆÊ≠£Á∫øÊÄßÂçïÂÖÉRectifier Linear UnitsÔºàReLUÔºâùëì(ùë•)=max‚Å°(0,ùë•)ÁâπÁÇπÔºöÂè™ÊúâÊúâ‰∏ÄÂçäÈöêÂê´Â±ÇÊòØÂ§Ñ‰∫éÊøÄÊ¥ªÁä∂ÊÄÅÔºåÂÖ∂‰ΩôÈÉΩÊòØËæìÂá∫‰∏∫0‰∏ç‰ºöÂá∫Áé∞Ê¢ØÂ∫¶Ê∂àÂ§±ÁöÑÈóÆÈ¢òÔºàÂç≥Âú®sigmoidÊé•ËøëÈ•±ÂíåÂå∫Êó∂ÔºåÂØºÊï∞Ë∂ã‰∫é0ÔºåËøôÁßçÊÉÖÂÜµ‰ºöÈÄ†Êàê‰ø°ÊÅØ‰∏¢Â§±ÔºâÂè™ÈúÄÊØîËæÉ„ÄÅ‰πòÂä†ËøêÁÆóÔºåÂõ†Ê≠§ËÆ°ÁÆóÊñπ‰æøÔºåËÆ°ÁÆóÈÄüÂ∫¶Âø´ÔºåÂä†ÈÄü‰∫ÜÁΩëÁªúÁöÑËÆ≠ÁªÉReLUÊØîsigmoidÊõ¥Êé•ËøëÁîüÁâ©Â≠¶ÁöÑÊøÄÊ¥ªÊ®°ÂûãËøòÊúâ‰∏Ä‰∫õÊîπËøõÊàñÁöÑÂèò‰Ωì Softplusùëì(ùë•)=log‚Å°(1+ùëí^ùë• )ÁâπÁÇπÔºöxË∂ã‰∫éË¥üÊó†Á©∑Êó∂ÔºåsoftplusË∂ã‰∫é0ÔºõxË∂ã‰∫éÊ≠£Êó†Á©∑Êó∂Ôºå softplusË∂ã‰∫éxÂÆÉÊòØReLUÁöÑÂπ≥ÊªëÁâàÂÆÉÊòØsigmoidÁöÑÂéüÂáΩÊï∞ ÊçüÂ§±ÂáΩÊï∞Áî®‰∫éÂõûÂΩí‰∏≠ÁöÑÂùáÊñπÊçüÂ§±Ôºöùê∏=1/2 (ùë¶‚àíùë¶ ÃÇ )^2Áî®‰∫éÂàÜÁ±ª‰∏≠ÁöÑ‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Ôºöùê∏=‚àí‚àë‚ñí„Äñùë¶ùëò ùëôùëúùëî((ùë¶ùëò ) ÃÇ )‚Äù ‚Äú „Äók=1,2,‚Ä¶,mË°®Á§∫mÁßçÁ±ªÂà´„ÄÇÂú®ËøùÁ∫¶È¢ÑÊµã‰∏≠m=2 Âü∫‰∫é Anaconda ÁöÑÂÆâË£ÖÂÆâË£ÖtensorflowÂª∫Á´ã‰∏Ä‰∏™ conda ËÆ°ÁÆóÁéØÂ¢ÉÂêçÂ≠óÂè´tensorflow: Python 2.7$ conda create -n tensorflow python=2.7 Python 3.4$ conda create -n tensorflow python=3.4 activate tensorflow ÂÆâË£Ötensorflowconda install ‚Äìchannel https://conda.anaconda.org/conda-forge tensorflow import tensorflow as tfÈÄÄÂá∫python3ÁéØÂ¢ÉÊàñÂΩì‰Ω†‰∏çÁî® TensorFlow ÁöÑÊó∂ÂÄô,ÂÖ≥Èó≠ÁéØÂ¢É:(tensorflow)$ deactivate$ # Your prompt should change back windows‰∏ãÂÆâË£ÖÂçáÁ∫ßpippython -m pip install ‚Äìupgrade pipÂÆâË£Ötensorflowpip3 install ‚Äìupgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0-cp35-cp35m-win_amd64.whl]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow ÂÆâË£Ö‰ΩøÁî®]]></title>
    <url>%2F2018%2F08%2F18%2FInstall%20TensorFlow%2F</url>
    <content type="text"><![CDATA[Âü∫‰∫éAnacondaÁöÑTensorflowÂÆâË£ÖAnacondaÊ†πÊçÆÂÆòÁΩëÈÄâÊã©Âü∫‰∫é‰∏çÂêåÁöÑpythonÁâàÊú¨ÂÆâË£ÖÔºöhttps://www.anaconda.com/download/#windows Âçö‰∏ªÈÄâÊã©Python 3.6 versionÔºåwindows 64bit ÂÆâË£ÖÂÆåÊàêÂêéÈúÄË¶ÅÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºåÊ†πÁõÆÂΩïÂíåScriptsÁõÆÂΩïÂä†ÂÖ•Âà∞Path‰∏ãÈù¢G:\ProgramData\Anaconda3;G:\ProgramData\Anaconda3\Scripts 1.Ê£ÄÊµãanacondaÁéØÂ¢ÉÊòØÂê¶ÂÆâË£ÖÊàêÂäüÔºöconda ‚Äìversion2.Ê£ÄÊµãÁõÆÂâçÂÆâË£Ö‰∫ÜÂì™‰∫õÁéØÂ¢ÉÂèòÈáèÔºöconda info ‚Äìenvs 3.ÂÆâË£ÖpythonÁâàÊú¨ÔºàÂçö‰∏ªÈÄâÊã©3.5ÔºâÔºöconda create ‚Äìname tensorflow python=3.5ÂÆâË£ÖÂêéÊòØ3.5.64.ÊøÄÊ¥ªtensflowÁöÑÁéØÂ¢ÉÔºöactivate tensorflow5.Ê£ÄÊµãtensflowÁöÑÁéØÂ¢ÉÊ∑ªÂä†Âà∞‰∫ÜAnacondaÈáåÈù¢Ôºöconda info ‚Äìenvs6.ÂÆâË£Ötensorflow gruÁâàÊú¨pip install ‚Äìignore-installed ‚Äìupgrade tensorflow-gpu ÂÆâË£ÖÂÖ∂‰ªñÁªÑ‰ª∂Ôºöpip install pandasconda install scikit-learnconda install matplotlib IDEÊÉ≥Ë¶Å‰ΩøÁî®tensorflow ÈúÄË¶ÅÂà∂ÂÆötensorflowÁöÑpythonÁâàÊú¨]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-ÈöèÊú∫Ê£ÆÊûó]]></title>
    <url>%2F2018%2F08%2F16%2FMathematical%20%20Random%20Forest%2F</url>
    <content type="text"><![CDATA[sklearn.ensemble.RandomForestClassifier n_estimators : integer, optional (default=10) Ê£ÆÊûóÈáåÔºàÂÜ≥Á≠ñÔºâÊ†ëÁöÑÊï∞ÁõÆ criterion : string, optional (default=‚Äùgini‚Äù) Ë°°ÈáèÂàÜË£ÇË¥®ÈáèÁöÑÊÄßËÉΩÔºàÂáΩÊï∞Ôºâ„ÄÇ ÂèóÊîØÊåÅÁöÑÊ†áÂáÜÊòØÂü∫Â∞º‰∏çÁ∫ØÂ∫¶ÁöÑ‚Äùgini‚Äù,Âíå‰ø°ÊÅØÂ¢ûÁõäÁöÑ‚Äùentropy‚ÄùÔºàÁÜµÔºâ„ÄÇÊ≥®ÊÑèÔºöËøô‰∏™ÂèÇÊï∞ÊòØÁâπÂÆöÊ†ëÁöÑ max_features : int, float, string or None, optional (default=‚Äùauto‚Äù) ÈÄâÊã©ÊúÄÈÄÇÂ±ûÊÄßÊó∂ÂàíÂàÜÁöÑÁâπÂæÅ‰∏çËÉΩË∂ÖËøáÊ≠§ÂÄº: Â¶ÇÊûúÊòØintÔºåÂ∞±Ë¶ÅËÄÉËôëÊØè‰∏ÄÊ¨°ÂàÜÂâ≤Â§ÑÁöÑmax_featureÁâπÂæÅ Â¶ÇÊûúÊòØfloatÔºåÈÇ£‰πàmax_featuresÂ∞±ÊòØ‰∏Ä‰∏™ÁôæÂàÜÊØîÔºåÈÇ£‰πàÔºàmax_feature*n_featuresÔºâÁâπÂæÅÊï¥Êï∞ÂÄºÊòØÂú®ÊØè‰∏™ÂàÜÂâ≤Â§ÑËÄÉËôëÁöÑ„ÄÇ Â¶ÇÊûúÊòØautoÔºåÈÇ£‰πàmax_features=sqrt(n_features)ÔºåÂç≥n_featuresÁöÑÂπ≥ÊñπÊ†πÂÄº„ÄÇ Â¶ÇÊûúÊòØlog2ÔºåÈÇ£‰πàmax_features=log2(n_features) Â¶ÇÊûúÊòØNone,ÈÇ£‰πàmax_features=n_features Ê≥®ÊÑèÔºöÂØªÊâæÂàÜÂâ≤ÁÇπ‰∏ç‰ºöÂÅúÊ≠¢ÔºåÁõ¥Âà∞ÊâæÂà∞ÊúÄÂ∞ë‰∏Ä‰∏™ÊúâÊïàÁöÑËäÇÁÇπÂàíÂàÜÂå∫ÔºåÂç≥‰ΩøÂÆÉÈúÄË¶ÅÊúâÊïàÊ£ÄÊü•Ë∂ÖËøámax_featuresÁöÑÁâπÂæÅ„ÄÇ max_depth : integer or None, optional (default=None)ÔºàÂÜ≥Á≠ñÔºâÊ†ëÁöÑÊúÄÂ§ßÊ∑±Â∫¶„ÄÇÂ¶ÇÊûúÂÄº‰∏∫NoneÔºåÈÇ£‰πà‰ºöÊâ©Â±ïËäÇÁÇπÔºåÁõ¥Âà∞ÊâÄÊúâÁöÑÂè∂Â≠êÊòØÁ∫ØÂáÄÁöÑÔºåÊàñËÄÖÁõ¥Âà∞ÊâÄÊúâÂè∂Â≠êÂåÖÂê´Â∞ë‰∫émin_sample_splitÁöÑÊ†∑Êú¨„ÄÇ min_samples_split : int, float, optional (default=2) Ê†πÊçÆÂ±ûÊÄßÂàíÂàÜËäÇÁÇπÊó∂ÔºåÊØè‰∏™ÂàíÂàÜÊúÄÂ∞ëÁöÑÊ†∑Êú¨Êï∞„ÄÇ Â¶ÇÊûú‰∏∫intÔºåÈÇ£‰πàËÄÉËôëmin_samples_split‰Ωú‰∏∫ÊúÄÂ∞èÁöÑÊï∞Â≠ó„ÄÇ Â¶ÇÊûú‰∏∫floatÔºåÈÇ£‰πàmin_samples_splitÊòØ‰∏Ä‰∏™ÁôæÂàÜÊØîÔºåÂπ∂‰∏îÊääceil(min_samples_split*n_samples)ÊòØÊØè‰∏Ä‰∏™ÂàÜÂâ≤ÊúÄÂ∞èÁöÑÊ†∑Êú¨Êï∞Èáè„ÄÇ Âú®ÁâàÊú¨0.18‰∏≠Êõ¥ÊîπÔºö‰∏∫ÁôæÂàÜÊØîÊ∑ªÂä†ÊµÆÁÇπÂÄº„ÄÇ Âè∂Â≠êËäÇÁÇπÊúÄÂ∞ëÁöÑÊ†∑Êú¨Êï∞„ÄÇ Â¶ÇÊûú‰∏∫intÔºåÈÇ£‰πàËÄÉËôëmin_samples_leaf‰Ωú‰∏∫ÊúÄÂ∞èÁöÑÊï∞Â≠ó„ÄÇ Â¶ÇÊûú‰∏∫floatÔºåÈÇ£‰πàmin_samples_leaf‰∏∫‰∏Ä‰∏™ÁôæÂàÜÊØîÔºåÂπ∂‰∏îceil(min_samples_leaf*n_samples)ÊòØÊØè‰∏Ä‰∏™ËäÇÁÇπÁöÑÊúÄÂ∞èÊ†∑Êú¨Êï∞Èáè„ÄÇ Âú®ÁâàÊú¨0.18‰∏≠Êõ¥ÊîπÔºö‰∏∫ÁôæÂàÜÊØîÊ∑ªÂä†ÊµÆÁÇπÂÄº„ÄÇ min_weight_fraction_leaf : float, optional (default=0.) ‰∏Ä‰∏™Âè∂Â≠êËäÇÁÇπÊâÄÈúÄË¶ÅÁöÑÊùÉÈáçÊÄªÂíåÔºàÊâÄÊúâÁöÑËæìÂÖ•Ê†∑Êú¨ÔºâÁöÑÊúÄÂ∞èÂä†ÊùÉÂàÜÊï∞„ÄÇÂΩìsample_weightÊ≤°ÊúâÊèê‰æõÊó∂ÔºåÊ†∑Êú¨ÂÖ∑ÊúâÁõ∏ÂêåÁöÑÊùÉÈáç max_leaf_nodes : int or None, optional (default=None) Âè∂Â≠êÊ†ëÁöÑÊúÄÂ§ßÊ†∑Êú¨Êï∞„ÄÇ ‰ª•ÊúÄ‰ºòÁöÑÊñπÊ≥ï‰ΩøÁî®max_leaf_nodesÊù•ÁîüÈïøÊ†ë„ÄÇÊúÄÂ•ΩÁöÑËäÇÁÇπË¢´ÂÆö‰πâ‰∏∫‰∏çÁ∫ØÂ∫¶‰∏äÁöÑÁõ∏ÂØπÂáèÂ∞ë„ÄÇÂ¶ÇÊûú‰∏∫None,ÈÇ£‰πà‰∏çÈôêÂà∂Âè∂Â≠êËäÇÁÇπÁöÑÊï∞Èáè„ÄÇ min_impurity_split : float, Ê†ëÊó©ÊúüÁîüÈïøÁöÑÈòàÂÄº„ÄÇÂ¶ÇÊûú‰∏Ä‰∏™ËäÇÁÇπÁöÑ‰∏çÁ∫ØÂ∫¶Ë∂ÖËøáÈòàÂÄºÈÇ£‰πàËøô‰∏™ËäÇÁÇπÂ∞Ü‰ºöÂàÜË£ÇÔºåÂê¶ÂàôÂÆÉËøòÊòØ‰∏ÄÁâáÂè∂Â≠ê„ÄÇ Ëá™0.19Áâà‰ª•Âêé‰∏çÊé®Ëçê‰ΩøÁî®Ôºömin_impurity_splitÂ∑≤Ë¢´ÂºÉÁî®ÔºåÂèñËÄå‰ª£‰πãÁöÑÊòØ0.19‰∏≠ÁöÑmin_impurity_decrease„ÄÇmin_impurity_splitÂ∞ÜÂú®0.21‰∏≠Ë¢´Âà†Èô§„ÄÇ ‰ΩøÁî®min_impurity_decrease min_impurity_decrease : float, optional (default=0.) Â¶ÇÊûúËäÇÁÇπÁöÑÂàÜË£ÇÂØºËá¥ÁöÑ‰∏çÁ∫ØÂ∫¶ÁöÑ‰∏ãÈôçÁ®ãÂ∫¶Â§ß‰∫éÊàñËÄÖÁ≠â‰∫éËøô‰∏™ËäÇÁÇπÁöÑÂÄºÔºåÈÇ£‰πàËøô‰∏™ËäÇÁÇπÂ∞Ü‰ºöË¢´ÂàÜË£Ç„ÄÇ ‰∏çÁ∫ØÂ∫¶Âä†ÊùÉÂáèÂ∞ëÊñπÁ®ãÂºèÂ¶Ç‰∏ãÔºö N_t / N (impurity - N_t_R / N_t right_impurity- N_t_L / N_t * left_impurity) NÊòØÊ†∑Êú¨ÊÄªÁöÑÊï∞ÈáèÔºåN_tÊòØÂΩìÂâçËäÇÁÇπÂ§ÑÁöÑÊ†∑Êú¨Êï∞ÈáèÔºåN_t_LÊòØÂ∑¶Â≠©Â≠êËäÇÁÇπÊ†∑Êú¨ÁöÑÊï∞Èáè,ËøòÊúâN_t_RÊòØÂè≥Â≠©Â≠êËäÇÁÇπÁöÑÊ†∑Êú¨Êï∞Èáè„ÄÇ NÔºåN_tÔºåN_t_RÂíåN_t_LÂÖ®ÈÉ®ÊòØÊåáÂä†ÊùÉÊÄªÂíåÔºåÂ¶ÇÊûúsample_weightÈÄöËøáÁöÑËØù„ÄÇ 0.19ÁâàÊú¨Êñ∞Âä†ÁöÑÂèÇÊï∞„ÄÇ bootstrap : boolean, optional (default=True) Âª∫Á´ãÂÜ≥Á≠ñÊ†ëÊó∂ÔºåÊòØÂê¶‰ΩøÁî®ÊúâÊîæÂõûÊäΩÊ†∑„ÄÇ oob_score : bool (default=False) ÊòØÂê¶‰ΩøÁî®Ë¢ãÂ§ñÊ†∑Êú¨Êù•‰º∞ËÆ°Ê≥õÂåñÁ≤æÂ∫¶„ÄÇ n_jobs : integer, optional (default=1) Áî®‰∫éÊãüÂêàÂíåÈ¢ÑÊµãÁöÑÂπ∂Ë°åËøêË°åÁöÑÂ∑•‰ΩúÔºà‰Ωú‰∏öÔºâÊï∞Èáè„ÄÇÂ¶ÇÊûúÂÄº‰∏∫-1ÔºåÈÇ£‰πàÂ∑•‰ΩúÊï∞ÈáèË¢´ËÆæÁΩÆ‰∏∫Ê†∏ÁöÑÊï∞Èáè„ÄÇ random_state : int, RandomState instance or None, optional (default=None) RandomStateIf intÔºårandom_stateÊòØÈöèÊú∫Êï∞ÁîüÊàêÂô®‰ΩøÁî®ÁöÑÁßçÂ≠ê; Â¶ÇÊûúÊòØRandomStateÂÆû‰æãÔºårandom_stateÂ∞±ÊòØÈöèÊú∫Êï∞ÁîüÊàêÂô®; Â¶ÇÊûú‰∏∫NoneÔºåÂàôÈöèÊú∫Êï∞ÁîüÊàêÂô®ÊòØnp.random‰ΩøÁî®ÁöÑRandomStateÂÆû‰æã„ÄÇ verbose : int, optional (default=0) ÊéßÂà∂ÂÜ≥Á≠ñÊ†ëÂª∫Á´ãËøáÁ®ãÁöÑÂÜó‰ΩôÂ∫¶„ÄÇ warm_start : bool, optional (default=False) ÂΩìË¢´ËÆæÁΩÆ‰∏∫TrueÊó∂ÔºåÈáçÊñ∞‰ΩøÁî®‰πãÂâçÂëºÂè´ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÁî®Êù•ÁªôÂÖ®‰ΩìÊãüÂêàÂíåÊ∑ªÂä†Êõ¥Â§öÁöÑ‰º∞ËÆ°Âô®ÔºåÂèç‰πãÔºå‰ªÖ‰ªÖÂè™ÊòØ‰∏∫‰∫ÜÊãüÂêà‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑÊ£ÆÊûó„ÄÇ class_weight : dict, list of dicts, ‚Äúbalanced‚Äù, ‚Äúbalanced_subsample‚Äù ÊàñËÄÖNone,ÔºàÈªòËÆ§ÂÄº‰∏∫NoneÔºâ,‰∏éÊ†ºÂºè{class_label: weight}Áõ∏ÂÖ≥ËÅîÁöÑÁ±ªÁöÑÂèØÈÄâÁöÑÊùÉÂÄº„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÁªôÂÄºÔºåÊâÄÊúâÁöÑÁ±ªÂà∞ÈÉΩÂ∫îËØ•Êúâ‰∏Ä‰∏™ÊùÉÂÄº„ÄÇÂØπ‰∫éÂ§öËæìÂá∫ÈóÆÈ¢òÔºå‰∏Ä‰∏™Â≠óÂÖ∏Â∫è ÂàóÂèØ‰ª•ÊåâÁÖßyÁöÑÂàóÁöÑÈ°∫Âà©Ë¢´Êèê‰æõ„ÄÇ ËØ∑Ê≥®ÊÑèÔºåÂØπ‰∫éÂ§öËæìÂá∫ÔºàÂåÖÊã¨Â§öÊ†áÁ≠æÔºâÔºåÂÖ∂ÊùÉÂÄºÂ∫îËØ•Ë¢´ÂÆö‰πâ‰∏∫ÂÆÉËá™Â∑±Â≠óÂÖ∏ÁöÑÊØè‰∏ÄÂàóÁöÑÊØè‰∏Ä‰∏™Á±ª„ÄÇ‰æãÂ¶ÇÔºåÂØπ‰∫éÂõõÁ±ªÂ§öÊ†áÁ≠æÂàÜÁ±ªÔºåÊùÉÂÄºÂ∫îËØ•Â¶Ç[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] ËøôÊ†∑ÔºåËÄå‰∏çÊòØ[{1:1}, {2:5}, {3:1}, {4:1}].ËøôÊ†∑„ÄÇ ‚Äúbalanced‚ÄùÊ®°Âºè‰ΩøÁî®yÁöÑÂÄºÊù•Ëá™Âä®ÁöÑË∞ÉÊï¥ÊùÉÂÄºÔºå‰∏éËæìÂÖ•Êï∞ÊçÆ‰∏≠Á±ªÂà´È¢ëÁéáÊàêÂèçÊØîÔºåÂ¶ÇÔºön_samples / (n_classes * np.bincount(y)) ‚Äúbalanced_subsample‚ÄùÊ®°ÂºèÂíå‚Äùbalanced‚ÄùÁõ∏ÂêåÔºåÈô§‰∫ÜÊùÉÂÄºÊòØÂü∫‰∫éÊØèÊ£µÊàêÈïøÊ†ëÊúâÊîæÂõûÊäΩÊ†∑ËÆ°ÁÆóÁöÑ„ÄÇ ÂØπ‰∫éÂ§öËæìÂá∫ÔºåyÁöÑÊØèÂàóÊùÉÂÄºÂ∞ÜÁõ∏‰πò„ÄÇ ËØ∑Ê≥®ÊÑèÔºåÂ¶ÇÊûúÊåáÂÆö‰∫Üsample_weight,Ëøô‰∫õÊùÉÂÄºÂ∞Ü‰ºöÂíåsample_weightÁõ∏‰πòÔºàÈÄöËøáÊãüÂêàÊñπÊ≥ï‰º†ÈÄíÔºâ„ÄÇ Attributes: Â±ûÊÄß estimators_ : ÂÜ≥Á≠ñÊ†ëÂàÜÁ±ªÂô®ÁöÑÂ∫èÂàó ÊãüÂêàÁöÑÂ≠ê‰º∞ËÆ°Âô®ÁöÑÈõÜÂêà„ÄÇ classes_ : Êï∞ÁªÑÁª¥Â∫¶=[n_classes]ÁöÑÊï∞ÁªÑÊàñËÄÖ‰∏Ä‰∏™ËøôÊ†∑Êï∞ÁªÑÁöÑÂ∫èÂàó„ÄÇ Á±ªÂà´Ê†áÁ≠æÔºàÂçï‰∏ÄËæìÂá∫ÈóÆÈ¢òÔºâÔºåÊàñËÄÖÁ±ªÂà´Ê†áÁ≠æÁöÑÊï∞ÁªÑÂ∫èÂàóÔºàÂ§öËæìÂá∫ÈóÆÈ¢òÔºâ„ÄÇ nclasses : int or list Á±ªÂà´ÁöÑÊï∞ÈáèÔºàÂçïËæìÂá∫ÈóÆÈ¢òÔºâÔºåÊàñËÄÖ‰∏Ä‰∏™Â∫èÂàóÔºåÂåÖÂê´ÊØè‰∏Ä‰∏™ËæìÂá∫ÁöÑÁ±ªÂà´Êï∞ÈáèÔºàÂ§öËæìÂá∫ÈóÆÈ¢òÔºâ nfeatures : int ÊâßË°åÊãüÂêàÊó∂ÁöÑÁâπÂæÅÊï∞Èáè„ÄÇ noutputs : int ÊâßË°åÊãüÂêàÊó∂ÁöÑËæìÂá∫Êï∞Èáè„ÄÇ featureimportances : array of shape = [n_features] ÁâπÂæÅÁöÑÈáçË¶ÅÊÄßÔºàÂÄºË∂äÈ´òÔºåÁâπÂæÅË∂äÈáçË¶ÅÔºâ oobscore : float‰ΩøÁî®Ë¢ãÂ§ñ‰º∞ËÆ°Ëé∑ÂæóÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂæóÂàÜ„ÄÇ oob_decisionfunction :Áª¥Â∫¶=[n_samples,n_classes]ÁöÑÊï∞ÁªÑ Âú®ËÆ≠ÁªÉÈõÜ‰∏äÁî®Ë¢ãÂ§ñ‰º∞ËÆ°ËÆ°ÁÆóÁöÑÂÜ≥Á≠ñÂáΩÊï∞„ÄÇÂ¶ÇÊûún_estimatorsÂæàÂ∞èÁöÑËØùÔºåÈÇ£‰πàÂú®ÊúâÊîæÂõûÊäΩÊ†∑‰∏≠Ôºå‰∏Ä‰∏™Êï∞ÊçÆÁÇπ‰πü‰∏ç‰ºöË¢´ÂøΩÁï•ÊòØÂèØËÉΩÁöÑ„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåoob_decisionfunction ÂèØËÉΩÂåÖÊã¨NaN„ÄÇ ÂèÇÊï∞ÁöÑÈªòËÆ§ÂÄºÊéßÂà∂ÂÜ≥Á≠ñÊ†ëÁöÑÂ§ßÂ∞èÔºà‰æãÂ¶ÇÔºåmax_depthÔºåÔºåmin_samples_leafÁ≠âÁ≠âÔºâÔºåÂØºËá¥ÂÆåÂÖ®ÁöÑÁîüÈïøÂíåÂú®Êüê‰∫õÊï∞ÊçÆÈõÜ‰∏äÂèØËÉΩÈùûÂ∏∏Â§ßÁöÑÊú™‰øÆÂâ™ÁöÑÊ†ë„ÄÇ‰∏∫‰∫ÜÈôç‰ΩéÂÜÖÂÆπÊ∂àËÄóÔºåÂÜ≥Á≠ñÊ†ëÁöÑÂ§çÊùÇÂ∫¶ÂíåÂ§ßÂ∞èÂ∫îËØ•ÈÄöËøáËÆæÁΩÆËøô‰∫õÂèÇÊï∞ÂÄºÊù•ÊéßÂà∂„ÄÇËøô‰∫õÁâπÂæÅÊÄªÊòØÂú®ÊØè‰∏™ÂàÜÂâ≤‰∏≠ÈöèÊú∫ÊéíÂàó„ÄÇ Âõ†Ê≠§ÔºåÂç≥‰Ωø‰ΩøÁî®Áõ∏ÂêåÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåmax_features = n_featuresÂíåbootstrap = FalseÔºåÂ¶ÇÊûúÂú®ÊêúÁ¥¢ÊúÄ‰Ω≥ÂàÜÂâ≤ÊúüÈó¥ÊâÄÂàó‰∏æÁöÑËã•Âπ≤ÂàÜÂâ≤ÁöÑÂáÜÂàôÁöÑÊîπËøõÊòØÁõ∏ÂêåÁöÑÔºåÈÇ£‰πàÊâæÂà∞ÁöÑÊúÄ‰Ω≥ÂàÜÂâ≤ÁÇπÂèØËÉΩ‰ºö‰∏çÂêå„ÄÇ ‰∏∫‰∫ÜÂú®ÊãüÂêàËøáÁ®ã‰∏≠Ëé∑Âæó‰∏Ä‰∏™Á°ÆÂÆöÁöÑË°å‰∏∫Ôºårandom_stateÂ∞Ü‰∏çÂæó‰∏çË¢´‰øÆÊ≠£„ÄÇ]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-Ê¢ØÂ∫¶‰∏ãÈôç]]></title>
    <url>%2F2018%2F04%2F27%2FMathematical%20regression%20gradient%20descent%2F</url>
    <content type="text"><![CDATA[Ê¢ØÂ∫¶‰∏ãÈôçÔºöÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºàBatch Gradient DescentÔºåÁÆÄÁß∞BGDÔºâ ‰ºòÁÇπÔºöÂÖ®Â±ÄÊúÄ‰ºòËß£ÔºõÊòì‰∫éÂπ∂Ë°åÂÆûÁé∞Ôºõ Áº∫ÁÇπÔºöÂΩìÊ†∑Êú¨Êï∞ÁõÆÂæàÂ§öÊó∂ÔºåËÆ≠ÁªÉËøáÁ®ã‰ºöÂæàÊÖ¢„ÄÇÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºàStochastic Gradient DescentÔºåÁÆÄÁß∞SGDÔºâ ‰ºòÁÇπÔºöËÆ≠ÁªÉÈÄüÂ∫¶Âø´ÔºõËø≠‰ª£Ê¨°Êï∞Â∞ë Áº∫ÁÇπÔºöÂáÜÁ°ÆÂ∫¶‰∏ãÈôçÔºåÂπ∂‰∏çÊòØÂÖ®Â±ÄÊúÄ‰ºòÔºõ‰∏çÊòì‰∫éÂπ∂Ë°åÂÆûÁé∞„ÄÇÂ∞èÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÔºàMBGDÔºâÂ¶ÇÊûúÊçüÂ§±ÂáΩÊï∞ÊòØÂá∏ÂáΩÊï∞ÔºåÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂæóÂà∞ÁöÑËß£Â∞±‰∏ÄÂÆöÊòØÂÖ®Â±ÄÊúÄ‰ºòËß£„ÄÇ Âá∏ÂáπÂáΩÊï∞ÔºöËÆæf(x)Âú®Âå∫Èó¥D‰∏äËøûÁª≠ÔºåÂ¶ÇÊûúÂØπD‰∏ä‰ªªÊÑè‰∏§ÁÇπa„ÄÅbÊÅíÊúâfÔºàÔºàa+bÔºâ/2Ôºâ&lt;(f(a)+f(b))/2ÈÇ£‰πàÁß∞f(x)Âú®D‰∏äÁöÑÂõæÂΩ¢ÊòØÔºàÂêë‰∏äÔºâÂáπÁöÑÔºàÊàñÂáπÂºßÔºâÔºõÂ¶ÇÊûúÊÅíÊúâfÔºàÔºàa+bÔºâ/2Ôºâ&gt;(f(a)+f(b))/2ÈÇ£‰πàÁß∞f(x)Âú®D‰∏äÁöÑÂõæÂΩ¢ÊòØÔºàÂêë‰∏äÔºâÂá∏ÁöÑÔºàÊàñÂá∏ÂºßÔºâ Ê¢ØÂ∫¶‰∏ãÈôçÁõ∏ÂÖ≥Ê¶ÇÂøµÔºö Ê≠•ÈïøÔºàLearning rateÔºâÔºöÊ≠•ÈïøÂÜ≥ÂÆö‰∫ÜÂú®Ê¢ØÂ∫¶‰∏ãÈôçËø≠‰ª£ÁöÑËøáÁ®ã‰∏≠ÔºåÊØè‰∏ÄÊ≠•Ê≤øÊ¢ØÂ∫¶Ë¥üÊñπÂêëÂâçËøõÁöÑÈïøÂ∫¶„ÄÇÁî®‰∏äÈù¢‰∏ãÂ±±ÁöÑ‰æãÂ≠êÔºåÊ≠•ÈïøÂ∞±ÊòØÂú®ÂΩìÂâçËøô‰∏ÄÊ≠•ÊâÄÂú®‰ΩçÁΩÆÊ≤øÁùÄÊúÄÈô°Â≥≠ÊúÄÊòì‰∏ãÂ±±ÁöÑ‰ΩçÁΩÆËµ∞ÁöÑÈÇ£‰∏ÄÊ≠•ÁöÑÈïøÂ∫¶„ÄÇ 2.ÁâπÂæÅÔºàfeatureÔºâÔºöÊåáÁöÑÊòØÊ†∑Êú¨‰∏≠ËæìÂÖ•ÈÉ®ÂàÜÔºåÊØîÂ¶Ç2‰∏™ÂçïÁâπÂæÅÁöÑÊ†∑Êú¨Ôºàx(0),y(0)Ôºâ,Ôºàx(1),y(1)ÔºâÔºàx(0),y(0)Ôºâ,Ôºàx(1),y(1)Ôºâ,ÂàôÁ¨¨‰∏Ä‰∏™Ê†∑Êú¨ÁâπÂæÅ‰∏∫x(0)x(0)ÔºåÁ¨¨‰∏Ä‰∏™Ê†∑Êú¨ËæìÂá∫‰∏∫y(0)y(0)„ÄÇ ÂÅáËÆæÂáΩÊï∞Ôºàhypothesis functionÔºâÔºöÂú®ÁõëÁù£Â≠¶‰π†‰∏≠Ôºå‰∏∫‰∫ÜÊãüÂêàËæìÂÖ•Ê†∑Êú¨ÔºåËÄå‰ΩøÁî®ÁöÑÂÅáËÆæÂáΩÊï∞ÔºåËÆ∞‰∏∫hŒ∏(x)hŒ∏(x)„ÄÇÊØîÂ¶ÇÂØπ‰∫éÂçï‰∏™ÁâπÂæÅÁöÑm‰∏™Ê†∑Êú¨Ôºàx(i),y(i)Ôºâ(i=1,2,‚Ä¶m)Ôºàx(i),y(i)Ôºâ(i=1,2,‚Ä¶m),ÂèØ‰ª•ÈááÁî®ÊãüÂêàÂáΩÊï∞Â¶Ç‰∏ãÔºö hŒ∏(x)=Œ∏0+Œ∏1xhŒ∏(x)=Œ∏0+Œ∏1x„ÄÇ ÊçüÂ§±ÂáΩÊï∞Ôºàloss functionÔºâÔºö‰∏∫‰∫ÜËØÑ‰º∞Ê®°ÂûãÊãüÂêàÁöÑÂ•ΩÂùèÔºåÈÄöÂ∏∏Áî®ÊçüÂ§±ÂáΩÊï∞Êù•Â∫¶ÈáèÊãüÂêàÁöÑÁ®ãÂ∫¶„ÄÇÊçüÂ§±ÂáΩÊï∞ÊûÅÂ∞èÂåñÔºåÊÑèÂë≥ÁùÄÊãüÂêàÁ®ãÂ∫¶ÊúÄÂ•ΩÔºåÂØπÂ∫îÁöÑÊ®°ÂûãÂèÇÊï∞Âç≥‰∏∫ÊúÄ‰ºòÂèÇÊï∞„ÄÇÂú®Á∫øÊÄßÂõûÂΩí‰∏≠ÔºåÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏‰∏∫Ê†∑Êú¨ËæìÂá∫ÂíåÂÅáËÆæÂáΩÊï∞ÁöÑÂ∑ÆÂèñÂπ≥Êñπ„ÄÇÊØîÂ¶ÇÂØπ‰∫ém‰∏™Ê†∑Êú¨Ôºàxi,yiÔºâ(i=1,2,‚Ä¶m)Ôºàxi,yiÔºâ(i=1,2,‚Ä¶m),ÈááÁî®Á∫øÊÄßÂõûÂΩíÔºåÊçüÂ§±ÂáΩÊï∞‰∏∫Ôºö J(Œ∏0,Œ∏1)=‚àëi=1m(hŒ∏(xi)‚àíyi)2J(Œ∏0,Œ∏1)=‚àëi=1m(hŒ∏(xi)‚àíyi)2 ÂÖ∂‰∏≠xixiË°®Á§∫Á¨¨i‰∏™Ê†∑Êú¨ÁâπÂæÅÔºåyiyiË°®Á§∫Á¨¨i‰∏™Ê†∑Êú¨ÂØπÂ∫îÁöÑËæìÂá∫ÔºåhŒ∏(xi)hŒ∏(xi)‰∏∫ÂÅáËÆæÂáΩÊï∞„ÄÇ Â±ÄÈÉ®Âä†ÊùÉÂõûÂΩíÁÆÄÂçïÊù•ËØ¥ÔºåËøô‰∏™ËøáÁ®ãÂÖ∂ÂÆûÊòØÂú®ÂÖàÊãüÂêàÂá∫‰∏ÄÊù°Êõ≤Á∫øÔºåÁÑ∂ÂêéÂÜçÁî®Ëøô‰∏™Êõ≤Á∫øÂéªÈ¢ÑÊµãÈúÄË¶ÅÈ¢ÑÊµãÁöÑÁÇπ„ÄÇ(Ê∫êËá™ÁôæÂ∫¶)‰∏∫‰ªÄ‰πàÊîπËøõË¶ÅÁî®Âä†ÊùÉÂõûÂΩíÂë¢Ôºü ÂæàÁÆÄÂçïÔºåÂõ†‰∏∫ÈùûÁ∫øÊÄßÊãüÂêàÂá∫Áõ¥Á∫øËØØÂ∑Æ‰ºöÂæàÂ§ßÔºåËøôÈáåÁöÑÂ±ÄÈÉ®Âä†ÊùÉÁ±ª‰ºº‰∫éknnÁÆóÊ≥ïÁöÑÊùÉÈáçÔºåÂç≥Ë∑ùÁ¶ª‰∏≠ÂøÉÁÇπË∂äËøëÁöÑÊùÉÈáçË∂äÂ§ßÔºåÂØπÊãüÂêàÊõ≤Á∫øÁöÑÂΩ±Âìç‰πüÂ∞±Ë∂äÂ§ßÔºåÊâÄ‰ª•‰πüÊúâ‰∫ÜÂ±ÄÈÉ®Âä†ÊùÉËøô‰∏ÄÂêçËØç ÂèÇËÄÉÊñáÁåÆÔºöhttps://blog.csdn.net/Gentle_Guan/article/details/76586689?locationNum=8&amp;fps=1]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-ÈÄªËæëÂõûÂΩíÊ®°ÂûãÁâπÂæÅÂ§ÑÁêÜ]]></title>
    <url>%2F2018%2F04%2F08%2FMathematical%20Feature%20processing%20of%20logistic%20regression%20model%2F</url>
    <content type="text"><![CDATA[Â¶ÇÊûúÊúâÂºÇÂ∏∏ÂÄºÔºå‰ΩøÁî®ÊûÅÂ§ß-ÊûÅÂ∞èÂΩí‰∏ÄÂåñÊàñÂùáÂÄº-Ê†áÂáÜÂ∑ÆÂΩí‰∏ÄÂåñÔºåËÆ°ÁÆó‰πãÂâçÈúÄË¶ÅÂ∞ÜÊûÅÁ´ØÂÄºÊéíÈô§Âú®Â§ñ„ÄÇ‰æãÂ¶ÇÔºöx‚Äô=x‚àímin/ max‚àíminËÆ°ÁÆómax‰∏éminÊó∂ÈúÄË¶ÅÁî®P1‰∏éP99Êù•‰ª£Êõø„ÄÇÊñ∞ÁîüÊàêÁöÑÂÄºÂ¶ÇÊûúË∂ÖËøá1Áî®1Ë°®Á§∫ÔºåÂ¶ÇÊûúÂ∞è‰∫é0 Áî®0Ë°®Á§∫]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-ÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ïÂÆû‰æã]]></title>
    <url>%2F2018%2F02%2F18%2FMathematical%20decision%20tree%2F</url>
    <content type="text"><![CDATA[ÂÜ≥Á≠ñÊ†ëÔºöÊúâÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÊòØ‰∏ÄÁßçÈ¢ÑÊµãÊ®°ÂûãÊòØÂú®Â∑≤Áü•ÂêÑÁßçÊÉÖÂÜµÂèëÁîüÊ¶ÇÁéáÂü∫Á°Ä‰∏äÔºåÈÄöËøáÊûÑÂª∫ÂÜ≥Á≠ñÊ†ëÊù•ËøõË°åÂàÜÊûêÁöÑ‰∏ÄÁßçÊñπÊ≥ï Ê†ëÂΩ¢ÁªìÊûÑ‰ªéË∑üËäÇÁÇπÂºÄÂßãÔºåÈ¢ÑÊµãÂæÖÂàÜÁ±ªÈ°πÂØπÂ∫îÁöÑÁâπÂæÅÂ±ûÊÄßÔºåÊåâÁÖßÂÄºÈÄâÊã©ËæìÂá∫ÂàÜÊîØÔºåÁõ¥Âà∞Âè∂Â≠êËäÇÁÇπÔºåÂ∞ÜÂè∂Â≠êËäÇÁÇπÁöÑÂ≠òÊîæÁ±ªÂà´‰Ωú‰∏∫Ê†ëÁöÑÁªìÊûú ÂÜ≥Á≠ñÊ†ëÂàÜ‰∏∫‰∏§Á±ªÔºöÂàÜÁ±ªÔºåÂõûÂΩíÂâçËÄÖÁî®‰∫éÂàÜÁ±ªÊ†áÁ≠æÂÄºÔºåÂêéËÄÖÁî®‰∫éÈ¢ÑÊµãËøûÁª≠ÂÄºÂ∏∏Áî®ÁÆóÊ≥ïID3ÔºåC4,5ÔºåCART Êï∞ÊçÆÊ†áÂáÜÂåñÔºöStandardScaler (Âü∫‰∫éÁâπÂæÅÁü©ÈòµÁöÑÂàóÔºåÂ∞ÜÂ±ûÊÄßÂÄºËΩ¨Êç¢Ëá≥Êúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏É)Ê†áÂáÜÂåñÊòØ‰æùÁÖßÁâπÂæÅÁü©ÈòµÁöÑÂàóÂ§ÑÁêÜÊï∞ÊçÆÔºåÂÖ∂ÈÄöËøáÊ±Çz-scoreÁöÑÊñπÊ≥ïÔºåÂ∞ÜÊ†∑Êú¨ÁöÑÁâπÂæÅÂÄºËΩ¨Êç¢Âà∞Âêå‰∏ÄÈáèÁ∫≤‰∏ãÂ∏∏Áî®‰∏éÂü∫‰∫éÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÁÆóÊ≥ïÔºåÊØîÂ¶ÇÂõûÂΩíÊï∞ÊçÆÂΩí‰∏ÄÂåñMinMaxScaler ÔºàÂå∫Èó¥Áº©ÊîæÔºåÂü∫‰∫éÊúÄÂ§ßÊúÄÂ∞èÂÄºÔºåÂ∞ÜÊï∞ÊçÆËΩ¨Êç¢Âà∞0,1Âå∫Èó¥‰∏äÁöÑÔºâÊèêÂçáÊ®°ÂûãÊî∂ÊïõÈÄüÂ∫¶ÔºåÊèêÂçáÊ®°ÂûãÁ≤æÂ∫¶Â∏∏ËßÅÁî®‰∫éÁ•ûÁªèÁΩëÁªúNormalizer ÔºàÂü∫‰∫éÁü©ÈòµÁöÑË°åÔºåÂ∞ÜÊ†∑Êú¨ÂêëÈáèËΩ¨Êç¢‰∏∫Âçï‰ΩçÂêëÈáèÔºâÂÖ∂ÁõÆÁöÑÂú®‰∫éÊ†∑Êú¨ÂêëÈáèÂú®ÁÇπ‰πòËøêÁÆóÊàñÂÖ∂‰ªñÊ†∏ÂáΩÊï∞ËÆ°ÁÆóÁõ∏‰ººÊÄßÊó∂ÔºåÊã•ÊúâÁªü‰∏ÄÁöÑÊ†áÂáÜÂ∏∏ËßÅÁî®‰∫éÊñáÊú¨ÂàÜÁ±ªÂíåËÅöÁ±ª„ÄÅlogisticÂõûÂΩí‰∏≠‰πü‰ºö‰ΩøÁî®ÔºåÊúâÊïàÈò≤Ê≠¢ËøáÊãüÂêà ÁâπÂæÅÈÄâÊã©Ôºö‰ªéÂ∑≤ÊúâÁöÑÁâπÂæÅ‰∏≠ÈÄâÊã©Âá∫ÂΩ±ÂìçÁõÆÊ†áÂÄºÊúÄÂ§ßÁöÑÁâπÂæÅÂ±ûÊÄß Â∏∏Áî®ÊñπÊ≥ïÔºö{ ÂàÜÁ±ªÔºöFÁªüËÆ°Èáè„ÄÅÂç°ÊñπÁ≥ªÊï∞Ôºå‰∫í‰ø°ÊÅØmutual_info_classif{ ËøûÁª≠ÔºöÁöÆÂ∞îÈÄäÁõ∏ÂÖ≥Á≥ªÊï∞ FÁªüËÆ°Èáè ‰∫í‰ø°ÊÅØmutual_info_classif SelectKBestÔºàÂç°ÊñπÁ≥ªÊï∞Ôºâ]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËÆæËÆ°Ê®°Âºè-ÁªìÊûÑÊ®°Âºè]]></title>
    <url>%2F2018%2F02%2F11%2FDesign%20pattern%20structural%2F</url>
    <content type="text"><![CDATA[1.ÈÄÇÈÖçÂô®ÊïàÊûúÂèä‰ºòÁº∫ÁÇπÔºöÂØπ‰∫éÁ±ªÈÄÇÈÖçÂô®Ôºö Áî®‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑAdapterÁ±ªÂØπAdapteeÂíåTagetËøõË°åÂåπÈÖç„ÄÇÁªìÊûúÊòØÂΩìÊàë‰ª¨ÊÉ≥Ë¶ÅÂåπÈÖç‰∏Ä‰∏™Á±ª‰ª•ÂèäÊâÄÊúâÂÆÉÁöÑÂ≠êÁ±ªÊó∂ÔºåÁ±ªAdapterÂ∞Ü‰∏çËÉΩËÉú‰ªªÂ∑•‰Ωú„ÄÇ ‰ΩøÂæóAdapterÂèØ‰ª•overrideÔºàÈáçÂÆö‰πâÔºâ AdapteeÁöÑÈÉ®ÂàÜË°å‰∏∫ÔºåÂõ†‰∏∫AdapterÊòØAdapteeÁöÑ‰∏Ä‰∏™Â≠êÁ±ª„ÄÇÂØπ‰∫éÂØπË±°ÈÄÇÈÖçÂô®Ôºö ÂÖÅËÆ∏‰∏Ä‰∏™Adapter‰∏éÂ§ö‰∏™AdapteeÔºåÂç≥AdapteeÊú¨Ë∫´‰ª•ÂèäÂÆÉÁöÑÊâÄÊúâÂ≠êÁ±ªÔºàÂ¶ÇÊûúÊúâÂ≠êÁ±ªÁöÑËØùÔºâÂêåÊó∂Â∑•‰Ωú„ÄÇAdapter‰πüÂèØ‰ª•‰∏ÄÊ¨°ÁªôÊâÄÊúâÁöÑAdapteeÊ∑ªÂä†ÂäüËÉΩ„ÄÇ ‰ΩøÂæóoverrideÔºàÈáçÂÆö‰πâÔºâAdapteeÁöÑË°å‰∏∫ÊØîËæÉÂõ∞Èöæ„ÄÇÂ¶ÇÊûú‰∏ÄÂÆöË¶Åoverride AdapteeÁöÑÊñπÊ≥ïÔºåÂ∞±Âè™Â•ΩÂÖàÂÅö‰∏Ä‰∏™AdapteeÁöÑÂ≠êÁ±ª‰ª•override AdapteeÁöÑÊñπÊ≥ïÔºåÁÑ∂ÂêéÂÜçÊääËøô‰∏™Â≠êÁ±ªÂΩì‰ΩúÁúüÊ≠£ÁöÑAdapteeÊ∫êËøõË°åÈÄÇÈÖç„ÄÇ 2.Ê°•Êé•ÁªßÊâøÊòØ‰∏ÄÁßçÂº∫ËÄ¶ÂêàÁöÑÁªìÊûúÔºåÁà∂Á±ªÂèòÔºåÂ≠êÁ±ªÂ∞±ÂøÖÈ°ªË¶ÅÂèò„ÄÇÂèØ‰ª•‰ΩøÁî®ÁªÑÂêà/ÁªßÊâøÊù•Ëß£ËÄ¶Âêà„ÄÇÂ∞ÜÊäΩË±°Âíå‰ªñÁöÑÂÆûÁé∞ÂàÜÁ¶ª 3.ÁªÑÂêàÂ∞ÜÂØπË±°ÁªÑÂêàÊàêÂ±ûÊÄßÁªìÊûÑ‰ª•Ë°®Á§∫‚ÄòÈÉ®ÂàÜ-Êï¥‰Ωì‚ÄôÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇÁªÑÂêàÊ®°Âºè‰ΩøÂæóÁî®Êà∑ÂØπÂçï‰∏™ÂØπË±°ÂíåÁªÑÂêàÂØπË±°ÁöÑ‰ΩøÁî®ÂÖ∑Êúâ‰∏ÄËá¥ÊÄß„ÄÇ ÁªÑÂêàÊ®°ÂºèÊèèËø∞‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®ÈÄíÂΩíÁöÑÁªÑÂêàÔºå‰ΩøÂÆ¢Êà∑‰∏çÁî®Âå∫ÂàÜËøô‰∫õÁ±ª 4.Ë£ÖÈÖçÂô®5.Â§ñËßÇ6.‰∫´ÂÖÉÊ®°Âºè7.‰ª£ÁêÜÊ®°ÂºèCopy-on-writedai‰ª£ÁêÜÔºöÂç≥ÂÜôÂç≥Â§çÂà∂‚ÄúÂø´ÁÖß‚ÄùËôöÊãü‰ª£ÁêÜÁöÑ‰∏ÄÁßçÔºåÊääÂ§çÂà∂ÊãñÂª∂Âà∞Âè™ÊúâÂÆ¢Êà∑Á´ØÈúÄË¶ÅÊó∂ÔºåÊâçÁúüÊ≠£ÊâßË°å‰øùÊä§‰ª£ÁêÜÔºöÂÖÅËÆ∏Âú®ËÆøÈóÆÂØπË±°Êó∂ÈôÑÂä†ÁÆ°ÁêÜ‰ªªÂä° 1.‰ªÄ‰πàÊòØ‰ª£ÁêÜÊ®°ÂºèÔºö‰æãÂ¶ÇÊàë‰ª¨ÊâæÊàøÂ≠êÊâæ‰∏≠‰ªã2.‰∏∫‰ªÄ‰πàË¶Å‰ΩøÁî®‰ª£ÁêÜÔºöÊàë‰ª¨‰∏çÈúÄË¶ÅËá™Â∑±ÊâæÊàøÂ≠ê]]></content>
      <categories>
        <category>ËÆæËÆ°Ê®°Âºè</category>
      </categories>
      <tags>
        <tag>ËÆæËÆ°Ê®°Âºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËÆæËÆ°Ê®°Âºè-Ë°å‰∏∫Ê®°Âºè]]></title>
    <url>%2F2018%2F02%2F10%2FDesign%20pattern%20behavior%2F</url>
    <content type="text"><![CDATA[Ë¥£‰ªªÈìæÊ®°ÂºèÂëΩ‰ª§Ê®°ÂºèÂëΩ‰ª§Command ‚Äî‚ÄîÂ£∞ÊòéÊâßË°åÊìç‰ΩúÁöÑÊé•Âè£„ÄÇÂÖ∑‰ΩìÂëΩ‰ª§ConcreteCommand ‚Äî‚ÄîÂÆö‰πâÊé•Êî∂ÂØπË±°ÂíåÂä®‰Ωú‰πãÈó¥ÁöÑÁªëÂÆöÂÖ≥Á≥ª„ÄÇ ‚Äî‚ÄîÈÄöËøáÂºïËµ∑Êé•Êî∂ËÄÖÁöÑÁõ∏Â∫îÂä®‰ΩúÊù•ÂÆûÁé∞ÊâßË°å„ÄÇÂÆ¢Êà∑Client ‚Äî‚Äî‰∫ßÁîü‰∏Ä‰∏™ConcreteCommandÂØπË±°ÔºåÂπ∂ËÆæÁΩÆÊé•Êî∂ËÄÖ„ÄÇÂºïÂèëËÄÖInvoker ‚Äî‚ÄîË¶ÅÊ±ÇÂëΩ‰ª§ÊâßË°åËØ∑Ê±Ç„ÄÇÊé•Êî∂ËÄÖReceiver ‚Äî‚ÄîÁü•ÈÅìÂ¶Ç‰ΩïÊâßË°å‰∏éËØ∑Ê±ÇÁõ∏ËÅîÁ≥ªÁöÑÊìç‰Ωú„ÄÇ Ëø≠‰ª£Âô®Ê®°ÂºèÊ®°ÊùøÊñπÊ≥ïÊ®°ÂºèÂáÜÂ§á‰∏Ä‰∏™ÊäΩË±°Á±ªÔºåÂÆö‰πâ‰∏Ä‰∏™ÁÆóÊ≥ïÁöÑÂ§ß‰ΩìÊ°ÜÊû∂Â∞ÜÈÉ®ÂàÜÈÄªËæë‰ª•ÂÖ∑‰ΩìÊñπÊ≥ï‰ª•ÂèäÂÖ∑‰ΩìÊûÑÈÄ†Â≠êÁöÑÂΩ¢ÂºèÂÆûÁé∞Ââ©‰ΩôÁöÑÈÄªËæëÈÄöËøáÂ£∞Êòé‰∏Ä‰∫õÊäΩË±°ÊñπÊ≥ïÊù•ÊèèËø∞Ëøô‰∫õÊäΩË±°ÊñπÊ≥ïË¶ÅÊ±ÇÂ≠êÁ±ªÂÆûÁé∞Ôºå‰∏çÂêåÁöÑÂ≠êÁ±ªÂèØ‰ª•‰ª•‰∏çÂêåÁöÑÊñπÂºèÂÆûÁé∞Ëøô‰∫õÊäΩË±°ÊñπÊ≥ïÔºå‰ªéËÄåÂØπÂâ©‰ΩôÁöÑÈÄªËæëÊúâ‰∏çÂêåÁöÑÂÆûÁé∞„ÄÇÂ≠êÁ±ª‰∏çÊîπÂèòÁÆóÊ≥ïÁöÑÁªìÊûÑËÄåÈáçÂÆö‰πâÁÆóÊ≥ï ËßÇÂØüËÄÖÊ®°ÂºèÂêå‰∏ÄÂ∫îÁî®ÂØπË±°‰∏çÂêåÂ±ïÁ§∫ÂΩ¢ÂºèÔºåÂ¶Ç‰∏ÄÁªÑÊï∞ÊçÆÊò†Â∞Ñ‰∏∫Ë°®Ê†ºÂíåÊü±Áä∂Âõæ„ÄÇÁî®Êà∑Êõ¥ÊîπË°®Ê†ºÊï∞ÊçÆÔºåÊü±Áä∂ÂõæË¶ÅÂêåÊ≠•‰øÆÊîπ ÂÖ≥ÈîÆÂØπË±°ÔºöÊäΩË±°‰∏ªÈ¢òSubjectÊèê‰æõ‰∏Ä‰∏™ËøûÊé•ËßÇÂØüËÄÖÂØπË±°ÂíåËß£Èô§ËøûÊé•ÁöÑÊé•Âè£„ÄÇÁü•ÈÅìÂÆÉÁöÑËßÇÂØüËÄÖ„ÄÇÂèØÊúâ‰ªªÊÑèÊï∞ÁõÆÁöÑËßÇÂØüËÄÖÂØπË±°ËßÇÂØü‰∏Ä‰∏™‰∏ªÈ¢ò„ÄÇÂèØ‰ª•Â¢ûÂä†ÂíåÂà†Èô§ËßÇÂØüËÄÖÂØπË±°ÔºåÂÖ∑‰Ωì‰∏ªÈ¢òConcreteSubjectÔºöÈÄöÂ∏∏Áî®‰∏Ä‰∏™ÂÖ∑‰ΩìÂ≠êÁ±ªÂÆûÁé∞„ÄÇË¥üË¥£ÂÆûÁé∞ÂØπËßÇÂØüËÄÖÂºïÁî®ÁöÑËÅöÈõÜÁöÑÁÆ°ÁêÜÂäõÊ≥®„ÄÇÂ∞ÜÊúâÂÖ≥Áä∂ÊÄÅÂ≠òÂÖ•ConcreteObserverÂØπË±°„ÄÇÂú®ÂÖ∑‰Ωì‰∏ªÈ¢òÂÜÖÈÉ®Áä∂ÊÄÅÊîπÂèòÊó∂ÂêëÂÆÉÁöÑËßÇÂØüËÄÖÂèëÈÄÅÈÄöÁü•„ÄÇ ÊäΩË±°ËßÇÂØüËÄÖObserver Ôºö‰∏ÄËà¨Áî®‰∏Ä‰∏™ÊäΩË±°Á±ªÊàñËÄÖ‰∏Ä‰∏™Êé•Âè£ÂÆûÁé∞Ôºå‰∏∫ÊâÄÊúâÁöÑÂÖ∑‰ΩìËßÇÂØüËÄÖÂÆö‰πâ‰∏Ä‰∏™Êõ¥Êñ∞Êé•Âè£Êõ¥Êñ∞Êé•Âè£ÂåÖÂê´ÁöÑÊñπÊ≥ïÂè´Êõ¥Êñ∞ÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìËßÇÂØüËÄÖConcreteObserverÈÄöÂ∏∏Áî®‰∏Ä‰∏™ÂÖ∑‰ΩìÂ≠êÁ±ªÂÆûÁé∞Ôºå‰øùÂ≠ò‰∏Ä‰∏™ÊåáÂêëConcreteSubjectÂØπË±°ÁöÑÂºïÁî®„ÄÇÂ≠òÂÇ®Ë¶Å‰∏é‰∏ªÈ¢ò‰∏ÄËá¥ÁöÑÁä∂ÊÄÅ„ÄÇÂÆûÁé∞ÊäΩË±°ËßÇÂØüËÄÖËßíËâ≤ÊâÄË¶ÅÊ±ÇÁöÑÊõ¥Êñ∞Êé•Âè£Ôºå‰ª•‰æø‰ΩøÊú¨Ë∫´ÁöÑÁä∂ÊÄÅ‰∏é‰∏ªÈ¢òÁöÑÁä∂ÊÄÅÁõ∏ÂçèË∞É„ÄÇ Áä∂ÊÄÅÊ®°ÂºèÁ≠ñÁï•Ê®°ÂºèËÆøÈóÆËÄÖÊ®°ÂºèËß£ÈáäÂô®Ê®°Âºè]]></content>
      <categories>
        <category>ËÆæËÆ°Ê®°Âºè</category>
      </categories>
      <tags>
        <tag>ËÆæËÆ°Ê®°Âºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËÆæËÆ°Ê®°ÂºèÁõÆÂΩï]]></title>
    <url>%2F2018%2F01%2F27%2FDesign%20pattern%20catalog%2F</url>
    <content type="text"><![CDATA[ÂàõÂª∫ÊÄßÊ®°ÂºèÔºö1.Á±ªÁöÑÂàõÂª∫Ê®°Âºè‚Äî‚Äî‰ΩøÁî®ÁªßÊâøÂÖ≥Á≥ªÔºåÊääÁ±ªÁöÑÂàõÂª∫Âª∂ËøüÂà∞Â≠êÁ±ª2.ÂØπË±°ÁöÑÂàõÂª∫Ê®°Âºè‚Äî‚ÄîÊääÂØπË±°ÁöÑÂàõÂª∫ËøáÁ®ãÂä®ÊÄÅÂú∞ÂßîÊ¥æÁªôÂè¶‰∏Ä‰∏™ÂØπË±° Â∞ÅË£ÖË¶ÅÂàõÂª∫ÁöÑÂÖ∑‰ΩìÁ±ªÔºàÁ±ªÁöÑÂÆû‰æãÔºâÁöÑ‰ø°ÊÅØ ÈöêËóèËøô‰∫õÁ±ªÔºàÁ±ªÁöÑÂÆû‰æãÔºâË¢´ÂàõÂª∫ÂíåÁªÑÂêàÁöÑËøáÁ®ã ÂåÖÂê´ÊäΩË±°Â∑•ÂéÇ„ÄÅÂª∫ÈÄ†ËÄÖ„ÄÅÂ∑•ÂéÇÊñπÂºè„ÄÅÂéüÂûã„ÄÅÂçï‰æã ÁªìÊûÑÊÄßÊ®°ÂºèÔºöËÄÉËôëÂ¶Ç‰ΩïÁªÑÂêàÁ±ªÂíåÂØπË±°ÊûÑÊàêËæÉÂ§ßÁöÑÁªìÊûÑ„ÄÇ1.ÁªìÊûÑÊÄßÁ±ªÊ®°ÂºèÔºö‰ΩøÁî®ÁªßÊâøÊù•ÁªÑÂêàÊé•Âè£ÊàñÂÆûÁé∞2.ÁªìÊûÑÊÄßÂØπË±°Ê®°ÂºèÔºöÂØπË±°ÂêàÊàêÂÆûÁé∞Êñ∞ÂäüËÉΩ„ÄÇÂåÖÂê´ÔºöÈÄÇÈÖçÂô®„ÄÅÊ°•Êé•„ÄÅÁªÑÂêà„ÄÅË£ÖÈ•∞ÁùÄ„ÄÅÂ§ñËßÇ„ÄÅËΩªÈáè„ÄÅ‰ª£ÁêÜ Ë°å‰∏∫Ê®°ÂºèÔºö‰∏ªË¶ÅËß£ÂÜ≥ÁÆóÊ≥ïÂíåÂØπË±°‰πãÈó¥ÁöÑË¥£‰ªªÂàÜÈÖçÈóÆÈ¢ò„ÄÇÂØπË±°ÊàñÁ±ªÁöÑÊ®°ÂºèÂÆÉ‰ª¨‰πãÈó¥ÁöÑÈÄö‰ø°Ê®°Âºè„ÄÇÂåÖÂê´ÔºöË¥£‰ªªÈìæ„ÄÅÂëΩ‰ª§„ÄÅËß£ÈáäÂô®„ÄÅËø≠‰ª£„ÄÅ‰∏≠‰ªãËÄÖ„ÄÅÂ§áÂøòÂΩï„ÄÅËßÇÂØüËÄÖ„ÄÅÁä∂ÊÄÅ„ÄÅÁ≠ñÁï•„ÄÅÊ®°ÊùøÊñπÊ≥ï„ÄÅËßÇÂØüËÄÖ Â∑•ÂéÇÊñπÊ≥ï‰∏ªË¶ÅÈíàÂØπ‰∏Ä‰∏™‰∫ßÂìÅÁ≠âÁ∫ßÁªìÊûÑÊäΩË±°Â∑•ÂéÇÊ®°ÂºèÈúÄË¶ÅÈù¢ÂØπÂ§ö‰∏™‰∫ßÂìÅÁ≠âÁ∫ßÁªìÊûÑ]]></content>
      <categories>
        <category>ËÆæËÆ°Ê®°Âºè</category>
      </categories>
      <tags>
        <tag>ËÆæËÆ°Ê®°Âºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËÆæËÆ°Ê®°Âºè-Â∑•ÂéÇÊ®°Âºè]]></title>
    <url>%2F2018%2F01%2F27%2FDesign%20pattern%20factory%2F</url>
    <content type="text"><![CDATA[ÂàõÂª∫Âá†‰∏™Â•óÁöÆËÇ§ÔºåÊâÄÊúâÁöÑUIÊéß‰ª∂ Â¶ÇÊåâÈíÆÔºåÊªöÂä®Êù°ÔºåÁ™óÂè£ ÈÉΩË¶ÅÂàõÂª∫Âá∫Êù•„ÄÇÁé∞Âú®ÈúÄË¶ÅÁ∫¢Ëâ≤‰∏ªÈ¢òÔºåÈªëËâ≤‰∏ªÈ¢òÔºåÂíåËìùËâ≤‰∏ªÈ¢ò3Â•óÁöÆËÇ§„ÄÇ Êé•Âè£Á±ªÔºö1234567891011121314public interface Button &#123; public void display();&#125;public interface ScrollBar &#123; public void display();&#125;public interface Window &#123; public void display();&#125;public interface SkinFactory &#123; public ScrollBar createScrollBar(); public Button createButton(); public Window createWindow();&#125; Á∫¢Ëâ≤ÁöÆËÇ§Â∑•ÂéÇ123456789101112131415161718192021222324252627282930public class RedSkinFactory implements SkinFactory &#123; public ScrollBar createScrollBar() &#123; return new RedScrollBar(); &#125; public Button createButton() &#123; return new RedButton(); &#125; public Window createWindow() &#123; return new RedWindow(); &#125;&#125;public class RedScrollBar implements ScrollBar &#123; public void display() &#123; System.out.println("ÂàõÂª∫Á∫¢Ëâ≤ÊªöÂä®Êù°„ÄÇ"); &#125;&#125;public class RedButton implements Button &#123; public void display() &#123; System.out.println("ÂàõÂª∫Á∫¢Ëâ≤ÊåâÈíÆ"); &#125;&#125;public class RedWindow implements Window &#123; public void display() &#123; System.out.println("ÂàõÂª∫Á∫¢Ëâ≤Á™óÂè£„ÄÇ"); &#125;&#125; ÂÆûÁé∞Á±ª123456789101112131415161718public class SkinClient &#123; public static void main(String[] args) &#123; SkinFactory BlackSkinFactory = new BlackSkinFactory(); BlackSkinFactory.createButton().display(); BlackSkinFactory.createScrollBar().display(); BlackSkinFactory.createWindow().display(); SkinFactory RedSkinFactory = new RedSkinFactory(); RedSkinFactory.createButton().display(); RedSkinFactory.createScrollBar().display(); RedSkinFactory.createWindow().display(); SkinFactory BlueSkinFactory = new BlueSkinFactory(); BlueSkinFactory.createButton().display(); BlueSkinFactory.createScrollBar().display(); BlueSkinFactory.createWindow().display(); &#125;&#125; shËæìÂá∫ÁªìÊûú123456789ÂàõÂª∫ÈªëËâ≤ÊåâÈíÆÂàõÂª∫ÈªëËâ≤ÊªöÂä®Êù°„ÄÇÂàõÂª∫ÈªëËâ≤Á™óÂè£„ÄÇÂàõÂª∫Á∫¢Ëâ≤ÊåâÈíÆÂàõÂª∫Á∫¢Ëâ≤ÊªöÂä®Êù°„ÄÇÂàõÂª∫Á∫¢Ëâ≤Á™óÂè£„ÄÇÂàõÂª∫ËìùËâ≤ÊåâÈíÆÂàõÂª∫ËìùËâ≤ÊªöÂä®Êù°„ÄÇÂàõÂª∫ËìùËâ≤Á™óÂè£„ÄÇ ÂÖ∂‰ªñÈ¢úËâ≤Âêå‰∏ä‰ª£Á†ÅÁªìÊûÑÊà™ÂõæÔºöÁªìÊûúÊà™ÂõæÔºö]]></content>
      <categories>
        <category>ËÆæËÆ°Ê®°Âºè</category>
      </categories>
      <tags>
        <tag>ËÆæËÆ°Ê®°Âºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python numpy]]></title>
    <url>%2F2018%2F01%2F10%2Fpython%20numpy%2F</url>
    <content type="text"><![CDATA[jupyter notebook:1.arangeÁîüÊàêÊï∞ÁªÑnp.arange(10)array([0,1,2,3,4,5,6,7,8,9]) 2.Á≠âÂ∑ÆÊï∞ÂàóÊòØÊåá‰ªéÁ¨¨‰∫åÈ°πËµ∑ÔºåÊØè‰∏ÄÈ°π‰∏éÂÆÉÁöÑÂâç‰∏ÄÈ°πÁöÑÂ∑ÆÁ≠â‰∫éÂêå‰∏Ä‰∏™Â∏∏Êï∞ÁöÑ‰∏ÄÁßçÊï∞ÂàóÔºåÂ∏∏Áî®A„ÄÅPË°®Á§∫Á≠âÂ∑ÆÊï∞Âàó linspacenp.linspace(1,10,5)array([1. , 3.25 ,5.5,5.75,10]) np.linspace(1,10,5,endpoint=False) Áõ∏ÂΩì‰∫éÁîüÊàê6‰∏™Êï∞Âè™ÊòæÁ§∫Ââç‰∫î‰∏™array([1. , 2.8 ,4.6,6.5,8.2]) 3.Á≠âÊØîÊï∞ÂàóÊòØÊåá‰ªéÁ¨¨‰∫åÈ°πËµ∑ÔºåÊØè‰∏ÄÈ°π‰∏éÂÆÉÁöÑÂâç‰∏ÄÈ°πÁöÑÊØîÂÄºÁ≠â‰∫éÂêå‰∏Ä‰∏™Â∏∏Êï∞ÁöÑ‰∏ÄÁßçÊï∞ÂàóÁ≠âÊØîÊï∞Âàólogspacenp.logspace(1,10,5) 4.shapearr = np.array([ [1,2,3], [2,3,4] ]) arr.shape(2, 3) 5.zeroshelp(np.zeros)zeros(shape, dtype=float, order=‚ÄôC‚Äô) Examples12345678910111213141516171819&gt;&gt;&gt; np.zeros(5)array([ 0., 0., 0., 0., 0.])&gt;&gt;&gt; np.zeros((5,), dtype=np.int)array([0, 0, 0, 0, 0])&gt;&gt;&gt; np.zeros((2, 1))array([[ 0.], [ 0.]])&gt;&gt;&gt; s = (2,2)&gt;&gt;&gt; np.zeros(s)array([[ 0., 0.], [ 0., 0.]])&gt;&gt;&gt; np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtypearray([(0, 0), (0, 0)], dtype=[('x', '&lt;i4'), ('y', '&lt;i4')]) 6.onesÁ±ª‰ººzeros,Âè™‰∏çËøáÂ°´ÂÖÖÁöÑÊòØ1 7.emptyÁ±ª‰ººzeros,Âè™‰∏çËøáÂ°´ÂÖÖÁöÑÊòØÈöèÊú∫ÂÄº 8.reshapeÊää‰∏ÄÁª¥Êï∞ÁªÑËΩ¨ÁΩÆ‰∏∫Â§öÁª¥Êï∞ÁªÑreshape ‰∏ç‰ºöÊîπÂèòÂéüÊù•ÁöÑndarrayÔºå‰ΩÜÊòØÂæóÂà∞Êñ∞ÁöÑndarrayÊòØÂéüÊï∞ÁªÑÁöÑËßÜÂõæÂØπ‰∫éndarrayÁöÑ‰∏Ä‰∫õÊñπÊ≥ïÊìç‰ΩúÔºåÈ¶ñÂÖàË¶ÅÂå∫ÂàÜÊòØÂê¶‰ºöÊîπÂèòÂéüÊù•ÂèòÈáèÔºå‰ª•Ê≠§Êù•Âà§Êñ≠ÊòØËßÜÂõæËøòÊòØÂâØÊú¨]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python‚Äîpandas]]></title>
    <url>%2F2018%2F01%2F08%2Fpython%E2%80%94pandas%2F</url>
    <content type="text"><![CDATA[from pandas import Series,DataFrameimport pandas as pd SeriesÔºö‰∏ÄÁßçÁ±ª‰ºº‰∫é‰∏ÄÁª¥Êï∞ÁªÑÁöÑÂØπË±°ÔºåÊòØÁî±‰∏ÄÁªÑÊï∞ÊçÆÔºàÂêÑÁßçNumPyÊï∞ÊçÆÁ±ªÂûãÔºâ‰ª•Âèä‰∏ÄÁªÑ‰∏é‰πãÁõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÊ†áÁ≠æÔºàÂç≥Á¥¢ÂºïÔºâÁªÑÊàê„ÄÇ Á¥¢ÂºïÂèØÈáçÂ§ç1234567from pandas import Series,DataFrameimport pandas as pdimport numpy as nparr = np.array([1,3,5,np.NaN,10])serises0=Series(arr)serises0 0 1.01 3.02 5.03 NaN4 10.0dtype: float641serises0.dtype dtype(‚Äòfloat64‚Äô) 1serises0.index RangeIndex(start=0, stop=5, step=1) 1serises0.values array([ 1., 3., 5., nan, 10.]) 12serises1=Series(data=[91,92,93],dtype=np.float64,index=[u'Êï∞Â≠¶',u'ËØ≠Êñá',u'Â§ñËØ≠'])serises1 Êï∞Â≠¶ 91.0ËØ≠Êñá 92.0Â§ñËØ≠ 93.0dtype: float64 12dict0=&#123;u'Êï∞Â≠¶':91.0,u'ËØ≠Êñá':92,u'Â§ñËØ≠':93&#125;dict0 {u‚Äô\u5916\u8bed‚Äô: 93, u‚Äô\u6570\u5b66‚Äô: 91.0, u‚Äô\u8bed\u6587‚Äô: 92} 12serises2=Series(dict0)serises2 Â§ñËØ≠ 93.0Êï∞Â≠¶ 91.0ËØ≠Êñá 92.0dtype: float64 1serises2[0] 93.0 1serises2[u'Â§ñËØ≠'] 93.0 1serises2['Â§ñËØ≠':'ËØ≠Êñá'] Â§ñËØ≠ 93.0Êï∞Â≠¶ 91.0ËØ≠Êñá 92.0dtype: float64 SeriesËøêÁÆóÔºåËá™Âä®ÂØπÈΩêÁ¥¢Âºï123serises2=Series(data=[11,12,13],dtype=np.float64,index=['p1','p2','p3'])serises3=Series(data=[22,23,24,25],dtype=np.float64,index=['p2','p3','p4','p5'])serises2 +serises3 p1 NaNp2 34.0p3 36.0p4 NaNp5 NaNdtype: float64 123serises1.name='name'serises1.index.name='ËÄÉËØïÁßëÁõÆ'serises1 ËÄÉËØïÁßëÁõÆÊï∞Â≠¶ 91.0ËØ≠Êñá 92.0Â§ñËØ≠ 93.0Name: name, dtype: float64 DataFrame Ôºö Ë°®Ê†ºÂΩ¢ÂºèÁöÑÊï∞ÊçÆÁªìÊûÑÔºåÂåÖÂê´‰∏ÄÁªÑÊúâÂ∫èÁöÑÂàóÔºåÊØèÂàóÂèØ‰ª•ÊòØ‰∏çÂêåÁöÑÂÄºÁ±ªÂûãÔºåDataFrameÊó¢ÊúâË°åÁ¥¢ÂºïÂèàÊúâÂàóÁ¥¢ÂºïÔºåÂèØ‰ª•ÁúãÂÅöÊòØÁî±SeriesÁªÑÊàêÁöÑÂ≠óÂÖ∏ÈÄöËøá‰∫åÁª¥Êï∞ÁªÑÂàõÂª∫12df01=DataFrame([['Tom','John'],[88,90]])df01 0 1 0 Tom John1 88 90 ÈÄöËøáÂ≠óÂÖ∏ÂàõÂª∫123data=&#123;'Tom':[88,55],'John':[90,22]&#125;df02=DataFrame(data)df02 John Tom0 90 881 22 55 DataFrame ÂèØ‰ª•Â¢ûÂä†Êï∞ÊçÆ1df02.ix['2']=np.NaN John Tom 0 90.0 88.01 22.0 55.02 NaN NaN Êï∞ÊçÆÂà†Èô§12df02=df02.dropna()df02 John Tom 0 90.0 88.01 22.0 55.0 123456arr1=np.random.randint(5,10,(4,4))df1=pd.DataFrame(arr1)df1df1.ix[:2,1]=np.NANdf1.ix[:1,2]=np.NANdf1 0 1 2 3 0 8 NaN NaN 71 5 NaN NaN 92 5 NaN 5.0 53 9 8.0 7.0 5 loc ilociloc ÂØπ‰∫é‰∏ãÊ†áËøõË°åÊìç‰Ωúloc ÂØπ‰∫éÁ¥¢ÂºïÂÄºËøõË°åÊìç‰Ωú]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark Êú∫Âô®Â≠¶‰π†ÂÖ•Èó®(‰∫å)]]></title>
    <url>%2F2017%2F10%2F19%2Fspark%20ml2%2F</url>
    <content type="text"><![CDATA[Spark MLlib Ê†∏ÂøÉÁªÑ‰ª∂√ò DataFrame ÂåÖÂê´‰∏çÂêåÁöÑÂàóÔºåÂèØÂ≠òÂÇ®ÊñáÊú¨„ÄÅ ÂõæÁâá„ÄÅ Ê†áÁ≠æ„ÄÅ ÁâπÂæÅÂêëÈáè„ÄÅ È¢ÑÊµãÂÄºÁ≠â√ò Estimator Áî®‰∫é‰ªéËÆ≠ÁªÉÊï∞ÊçÆÁÆóÂá∫Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºåËæìÂÖ•DataFrameËæìÂá∫Model√ò Transformer ËæìÂÖ•DataFrameËæìÂá∫DataFrame‚Ä¢ ModelÂç≥‰∏∫‰∏ÄÁßçTransformerÔºåÁî®‰∫é‰ªéÊµãËØïÊï∞ÊçÆÁîüÊàêÈ¢ÑÊµãÁªìÊûú‚Ä¢ ÁâπÂæÅËΩ¨Êç¢TransformerËΩ¨Êç¢‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÁâπÂæÅÔºåÂπ∂Â∞ÜËΩ¨Êç¢ÂêéÁöÑÁâπÂæÅÂàóËøΩÂä†Âà∞ËæìÂá∫DataFrame‰∏≠√ò Pipeline‚Ä¢ Â∞ÜÂ§ö‰∏™TransformerÂíåEstimatorÈìæÊé•Ëµ∑Êù•ÂΩ¢Êàê‰∏Ä‰∏™Êú∫Âô®Â≠¶‰π†workflow‚Ä¢ Pipeline‰πüÊòØ‰∏ÄÁßçEstimatorÔºåÁîüÊàê‰∏Ä‰∏™PipelineModel√ò Parameter ÊâÄÊúâÁöÑTransformerÂíåEstimatorÂÖ±‰∫´ËØ•ÈÄöÁî®ÁöÑAPIÊù•ÊåáÂÆöÂèÇÊï∞√ò PipelineModel ‰øùËØÅÂØπÊµãËØïÊï∞ÊçÆ‰ΩøÁî®‰∏éËÆ≠ÁªÉÊï∞ÊçÆÂÆåÂÖ®Áõ∏ÂêåÁöÑÊï∞ÊçÆËΩ¨Êç¢ ËÆ≠ÁªÉÊ®°ÂûãÔºö Tokenizer and HashingTF ÊòØTransformersLogisticRegression ÊòØEstimator È¢ÑÊµãÁªìÊûúPipeline ÊòØ Estimatorfit()ÊñπÊ≥ï‰ºö‰∫ßÁîü‰∏Ä‰∏™PipelineModeltransform()ÊñπÊ≥ïPipelines and PipelineModelsÂ∏ÆÂä©Á°Æ‰øùËÆ≠ÁªÉÊï∞ÊçÆÂíåÊµãËØïÊï∞ÊçÆÁªèËøáÁõ∏ÂêåÁâπÂæÅÁöÑÂ§ÑÁêÜÊ≠•È™§„ÄÇ SparseMatrix‰ªãÁªçÊé®ËçêÂçö‰∏ªÔºöhttp://blog.csdn.net/sinat_29508201/article/details/54089771]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark Êú∫Âô®Â≠¶‰π†ÂÖ•Èó®(‰∏Ä)]]></title>
    <url>%2F2017%2F10%2F18%2Fspark%20ml1%2F</url>
    <content type="text"><![CDATA[ÁõÆÂâçÂü∫‰∫éRDDÁöÑMLlibÂ∑≤ÁªèËøõÂÖ•Áª¥Êä§Ëé´ÊñØ„ÄÇÂ§ßÊ¶ÇÂú®spark2.3Âü∫‰∫éRDDÁöÑMLlib APIÂ∞ÜË¶ÅË¢´Â∫üÂºÉ„ÄÇÊú™Êù•ÊòØÂü∫‰∫éDataFrameÁöÑAPI 1.Âü∫Êú¨ÁªüËÆ°ËÆ°ÁÆó‰∏§ÁªÑÊï∞ÊçÆ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß ÁöÆÂ∞îÊ£ÆÁõ∏ÂÖ≥Á≥ªÊï∞ÔºàPearson correlation coefficientÔºâ‰πüÁß∞ÁöÆÂ∞îÊ£ÆÁßØÁü©Áõ∏ÂÖ≥Á≥ªÊï∞(Pearson product-moment correlation coefficient) ÔºåÊòØ‰∏ÄÁßçÁ∫øÊÄßÁõ∏ÂÖ≥Á≥ªÊï∞„ÄÇÁöÆÂ∞îÊ£ÆÁõ∏ÂÖ≥Á≥ªÊï∞ÊòØÁî®Êù•ÂèçÊò†‰∏§‰∏™ÂèòÈáèÁ∫øÊÄßÁõ∏ÂÖ≥Á®ãÂ∫¶ÁöÑÁªüËÆ°Èáè„ÄÇÁõ∏ÂÖ≥Á≥ªÊï∞Áî®rË°®Á§∫ÔºåÂÖ∂‰∏≠n‰∏∫Ê†∑Êú¨ÈáèÔºåÂàÜÂà´‰∏∫‰∏§‰∏™ÂèòÈáèÁöÑËßÇÊµãÂÄºÂíåÂùáÂÄº„ÄÇrÊèèËø∞ÁöÑÊòØ‰∏§‰∏™ÂèòÈáèÈó¥Á∫øÊÄßÁõ∏ÂÖ≥Âº∫Âº±ÁöÑÁ®ãÂ∫¶„ÄÇrÁöÑÁªùÂØπÂÄºË∂äÂ§ßË°®ÊòéÁõ∏ÂÖ≥ÊÄßË∂äÂº∫ ÊåâÁÖßÈ´ò‰∏≠Êï∞Â≠¶Ê∞¥Âπ≥Êù•ÁêÜËß£, ÂÆÉÂæàÁÆÄÂçï, ÂèØ‰ª•ÁúãÂÅöÂ∞Ü‰∏§ÁªÑÊï∞ÊçÆÈ¶ñÂÖàÂÅöZÂàÜÊï∞Â§ÑÁêÜ‰πãÂêé, ÁÑ∂Âêé‰∏§ÁªÑÊï∞ÊçÆÁöÑ‰πòÁßØÂíåÈô§‰ª•Ê†∑Êú¨Êï∞ ZÂàÜÊï∞‰∏ÄËà¨‰ª£Ë°®Ê≠£ÊÄÅÂàÜÂ∏É‰∏≠, Êï∞ÊçÆÂÅèÁ¶ª‰∏≠ÂøÉÁÇπÁöÑË∑ùÁ¶ª.Á≠â‰∫éÂèòÈáèÂáèÊéâÂπ≥ÂùáÊï∞ÂÜçÈô§‰ª•Ê†áÂáÜÂ∑Æ.(Â∞±ÊòØÈ´òËÄÉÁöÑÊ†áÂáÜÂàÜÁ±ª‰ººÁöÑÂ§ÑÁêÜ) Ê†áÂáÜÂ∑ÆÂàôÁ≠â‰∫éÂèòÈáèÂáèÊéâÂπ≥ÂùáÊï∞ÁöÑÂπ≥ÊñπÂíå,ÂÜçÈô§‰ª•Ê†∑Êú¨Êï∞,ÊúÄÂêéÂÜçÂºÄÊñπ. ÊâÄ‰ª•, Ê†πÊçÆËøô‰∏™ÊúÄÊú¥Á¥†ÁöÑÁêÜËß£,Êàë‰ª¨ÂèØ‰ª•Â∞ÜÂÖ¨Âºè‰æùÊ¨°Á≤æÁÆÄ‰∏∫: spearmanÁõ∏ÂÖ≥Á≥ªÊï∞ÔºöÊòØË°°ÈáèÂàÜÁ∫ßÂÆöÂ∫èÂèòÈáè‰πãÈó¥ÁöÑÁõ∏ÂÖ≥Á®ãÂ∫¶ÁöÑÁªüËÆ°ÈáèÔºåÂØπ‰∏çÊúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑËµÑÊñô„ÄÅÂéüÂßãËµÑÊñôÁ≠âÁ∫ßËµÑÊñô„ÄÅ‰∏Ä‰æßÂºÄÂè£ËµÑÊñô„ÄÅÊÄª‰ΩìÂàÜÂ∏ÉÁ±ªÂûãÊú™Áü•ÁöÑËµÑÊñô‰∏çÁ¨¶Âêà‰ΩøÁî®ÁßØÁü©Áõ∏ÂÖ≥Á≥ªÊï∞Êù•ÊèèËø∞ÂÖ≥ËÅîÊÄß„ÄÇÊ≠§Êó∂ÂèØÈááÁî®Áß©Áõ∏ÂÖ≥Ôºàrank correlationÔºâÔºå‰πüÁß∞Á≠âÁ∫ßÁõ∏ÂÖ≥ÔºåÊù•ÊèèËø∞‰∏§‰∏™ÂèòÈáè‰πãÈó¥ÁöÑÂÖ≥ËÅîÁ®ãÂ∫¶‰∏éÊñπÂêë„ÄÇ 1234567891011121314151617181920212223242526272829303132import org.apache.spark.ml.linalg.&#123;Matrix, Vectors&#125;import org.apache.spark.ml.stat.Correlationimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.Rowobject ml_1 &#123;def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder .master("local") .getOrCreate() import spark.implicits._ // $example on$ val data = Seq( Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))), Vectors.dense(4.0, 5.0, 0.0, 3.0), Vectors.dense(6.0, 7.0, 0.0, 8.0), Vectors.sparse(4, Seq((0, 9.0), (3, 1.0))) ) val df = data.map(Tuple1.apply).toDF("features") val Row(coeff1: Matrix) = Correlation.corr(df, "features").head println("Pearson correlation matrix:\n" + coeff1.toString) val Row(coeff2: Matrix) = Correlation.corr(df, "features", "spearman").head println("Spearman correlation matrix:\n" + coeff2.toString) // $example off$ spark.stop() &#125;&#125; 1.0‰∏∫Á¨¨‰∏ÄÂàóÂíåÁ¨¨‰∏ÄÂàóËÆ°ÁÆó0.055641488407465814‰∏∫Á¨¨‰∏ÄÂàóÂíåÁ¨¨‰∫åÂàóËÆ°ÁÆó0.4004714203168137 ‰∏∫Á¨¨‰∏âÂàóÂíåÁ¨¨ÂõõÂàóËÆ°ÁÆó ËÆ°ÁÆóÁªìÊûú:12345678910Pearson correlation matrix:1.0 0.055641488407465814 NaN 0.4004714203168137 0.055641488407465814 1.0 NaN 0.9135958615342522 NaN NaN 1.0 NaN 0.4004714203168137 0.9135958615342522 NaN 1.0 Spearman correlation matrix:1.0 0.10540925533894532 NaN 0.40000000000000174 0.10540925533894532 1.0 NaN 0.9486832980505141 NaN NaN 1.0 NaN 0.40000000000000174 0.9486832980505141 NaN 1.0 Âç°ÊñπÊ£ÄÈ™åÂç°ÊñπÊ£ÄÈ™åÊòØÁî®ÈÄîÈùûÂ∏∏ÂπøÁöÑ‰∏ÄÁßçÂÅáËÆæÊ£ÄÈ™åÊñπÊ≥ïÔºåÂÆÉÂú®ÂàÜÁ±ªËµÑÊñôÁªüËÆ°Êé®Êñ≠‰∏≠ÁöÑÂ∫îÁî®ÔºåÂåÖÊã¨Ôºö‰∏§‰∏™ÁéáÊàñ‰∏§‰∏™ÊûÑÊàêÊØîÊØîËæÉÁöÑÂç°ÊñπÊ£ÄÈ™åÔºõÂ§ö‰∏™ÁéáÊàñÂ§ö‰∏™ÊûÑÊàêÊØîÊØîËæÉÁöÑÂç°ÊñπÊ£ÄÈ™å‰ª•ÂèäÂàÜÁ±ªËµÑÊñôÁöÑÁõ∏ÂÖ≥ÂàÜÊûêÁ≠â„ÄÇÂç°ÊñπÊ£ÄÈ™åÂ∞±ÊòØÁªüËÆ°Ê†∑Êú¨ÁöÑÂÆûÈôÖËßÇÊµãÂÄº‰∏éÁêÜËÆ∫Êé®Êñ≠ÂÄº‰πãÈó¥ÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶ÔºåÂÆûÈôÖËßÇÊµãÂÄº‰∏éÁêÜËÆ∫Êé®Êñ≠ÂÄº‰πãÈó¥ÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶Â∞±ÂÜ≥ÂÆöÂç°ÊñπÂÄºÁöÑÂ§ßÂ∞èÔºåÂç°ÊñπÂÄºË∂äÂ§ßÔºåË∂ä‰∏çÁ¨¶ÂêàÔºõÂç°ÊñπÂÄºË∂äÂ∞èÔºåÂÅèÂ∑ÆË∂äÂ∞èÔºåË∂äË∂ã‰∫éÁ¨¶ÂêàÔºåËã•‰∏§‰∏™ÂÄºÂÆåÂÖ®Áõ∏Á≠âÊó∂ÔºåÂç°ÊñπÂÄºÂ∞±‰∏∫0ÔºåË°®ÊòéÁêÜËÆ∫ÂÄºÂÆåÂÖ®Á¨¶Âêà„ÄÇ 1234567891011121314151617181920212223242526272829303132333435import org.apache.spark.ml.linalg.Vectorimport org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.ml.stat.ChiSquareTestimport org.apache.spark.sql.SparkSession/** * Created by Administrator on 2017/10/18. */object ChiSquareTest &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder .master("local") .getOrCreate() import spark.implicits._ val data = Seq( (0.0, Vectors.dense(0.5, 10.0)), (0.0, Vectors.dense(1.5, 20.0)), (1.0, Vectors.dense(1.5, 30.0)), (0.0, Vectors.dense(3.5, 30.0)), (0.0, Vectors.dense(3.5, 40.0)), (1.0, Vectors.dense(3.5, 40.0)) ) val df = data.toDF("label", "features") val chi = ChiSquareTest.test(df, "features", "label").head println("pValues = " + chi.getAs[Vector](0)) println("degreesOfFreedom = " + chi.getSeq[Int](1).mkString("[", ",", "]")) println("statistics = " + chi.getAs[Vector](2)) spark.stop() &#125;&#125; ÁªìÊûúÔºö123pValues = [0.6872892787909721,0.6822703303362126]degreesOfFreedom = [2,3]statistics = [0.75,1.5]]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark shuffle Ë∞É‰ºò]]></title>
    <url>%2F2017%2F08%2F30%2Fspark%20shuffle%20%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[shuffleÂÆûÁé∞ÁöÑÂÖ∑‰ΩìËøáÁ®ã1.SparkÁ≥ªÁªüÂú®ËøêË°åÂê´shuffleËøáÁ®ãÁöÑÂ∫îÁî®Êó∂ÔºåExecutorËøõÁ®ãÈô§‰∫ÜËøêË°åtaskÔºåËøòË¶ÅË¥üË¥£ÂÜôshuffle Êï∞ÊçÆÔºåÁªôÂÖ∂‰ªñExecutorÊèê‰æõshuffleÊï∞ÊçÆ„ÄÇ ÂΩìExecutorËøõÁ®ã‰ªªÂä°ËøáÈáçÔºåÂØºËá¥GCËÄå‰∏çËÉΩ‰∏∫ÂÖ∂ ‰ªñExecutorÊèê‰æõshuffleÊï∞ÊçÆÊó∂Ôºå‰ºöÂΩ±Âìç‰ªªÂä°ËøêË°å„ÄÇ ËøôÈáåÂÆûÈôÖ‰∏äÊòØÂà©Áî®External Shuffle Service Êù•ÊèêÂçáÊÄßËÉΩÔºåExternal shuffle ServiceÊòØÈïøÊúüÂ≠òÂú®‰∫éNodeManagerËøõÁ®ã‰∏≠ÁöÑ‰∏Ä‰∏™ËæÖÂä©ÊúçÂä°„ÄÇ ÈÄöËøáËØ•ÊúçÂä° Êù•ÊäìÂèñshuffleÊï∞ÊçÆÔºåÂáèÂ∞ë‰∫ÜExecutorÁöÑÂéãÂäõÔºåÂú®Executor GCÁöÑÊó∂ÂÄô‰πü‰∏ç‰ºöÂΩ±ÂìçÂÖ∂‰ªñ ExecutorÁöÑ‰ªªÂä°ËøêË°å„ÄÇ ÂêØÁî®ÊñπÊ≥ïÔºö ‰∏Ä. Âú®NodeManager‰∏≠ÂêØÂä®External shuffle Service„ÄÇa. Âú®‚Äúyarn-site.xml‚Äù‰∏≠Ê∑ªÂä†Â¶Ç‰∏ãÈÖçÁΩÆÈ°πÔºö &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;spark_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt; &lt;/property&gt; ÈÖçÁΩÆÂèÇÊï∞ÊèèËø∞ yarn.nodemanager.aux-services ÔºöNodeManager‰∏≠‰∏Ä‰∏™ÈïøÊúüËøêË°åÁöÑËæÖÂä©ÊúçÂä°ÔºåÁî®‰∫éÊèêÂçáShuffleËÆ°ÁÆóÊÄßËÉΩ„ÄÇ yarn.nodemanager.auxservices.spark_shuffle.class ÔºöNodeManager‰∏≠ËæÖÂä©ÊúçÂä°ÂØπÂ∫îÁöÑÁ±ª„ÄÇ spark.shuffle.service.port ÔºöShuffleÊúçÂä°ÁõëÂê¨Êï∞ÊçÆËé∑ÂèñËØ∑Ê±ÇÁöÑÁ´ØÂè£„ÄÇÂèØÈÄâÈÖçÁΩÆÔºåÈªòËÆ§ÂÄº‰∏∫‚Äú7337‚Äù„ÄÇ b. Ê∑ªÂä†‰æùËµñÁöÑjarÂåÖ Êã∑Ë¥ù‚Äú${SPARK_HOME}/lib/spark-1.3.0-yarn-shuffle.jar‚ÄùÂà∞‚Äú${HADOOP_HOME}/share/hadoop/yarn/lib/‚ÄùÁõÆÂΩï‰∏ã„ÄÇ c. ÈáçÂêØNodeManagerËøõÁ®ãÔºå‰πüÂ∞±ÂêØÂä®‰∫ÜExternal shuffle Service„ÄÇ ‰∫å. SparkÂ∫îÁî®‰ΩøÁî®External shuffle Service„ÄÇÂú®‚Äúspark-defaults.conf‚Äù‰∏≠ÂøÖÈ°ªÊ∑ªÂä†Â¶Ç‰∏ãÈÖçÁΩÆÈ°πÔºö spark.shuffle.service.enabled true spark.shuffle.service.port 7337 ËØ¥Êòé 1.Â¶ÇÊûú1.Â¶ÇÊûú‚Äúyarn.nodemanager.aux-services‚ÄùÈÖçÁΩÆÈ°πÂ∑≤Â≠òÂú®ÔºåÂàôÂú®value‰∏≠Ê∑ªÂä† ‚Äúspark_shuffle‚ÄùÔºå‰∏îÁî®ÈÄóÂè∑ÂíåÂÖ∂‰ªñÂÄºÂàÜÂºÄ„ÄÇ 2.‚Äúspark.shuffle.service.port‚ÄùÁöÑÂÄºÈúÄË¶ÅÂíå‰∏äÈù¢‚Äúyarn-site.xml‚Äù‰∏≠ÁöÑÂÄº‰∏ÄÊ†∑„ÄÇ ÈÖçÁΩÆÂèÇÊï∞ÊèèËø∞ spark.shuffle.service.enabled ÔºöNodeManager‰∏≠‰∏Ä‰∏™ÈïøÊúüËøêË°åÁöÑËæÖÂä©ÊúçÂä°ÔºåÁî®‰∫éÊèêÂçáShuffle ËÆ°ÁÆóÊÄßËÉΩ„ÄÇÈªòËÆ§‰∏∫falseÔºåË°®Á§∫‰∏çÂêØÁî®ËØ•ÂäüËÉΩ„ÄÇ spark.shuffle.service.port ÔºöShuffleÊúçÂä°ÁõëÂê¨Êï∞ÊçÆËé∑ÂèñËØ∑Ê±ÇÁöÑÁ´ØÂè£„ÄÇÂèØÈÄâÈÖçÁΩÆÔºåÈªòËÆ§ÂÄº ‰∏∫‚Äú7337‚Äù„ÄÇ Hash Shuffle‰∏çË∂≥map task‰ºöÊ†πÊçÆreduceÁöÑÊï∞ÈáèÔºàpartitionÔºâ ÁîüÊàêÁõ∏Â∫îÁöÑbucket ÂÜôshuffle blockFile Â¶ÇÊûúmap ÂíåreduceÊï∞ÈáèËøáÂ§öÔºå‰ºöÂÜôÂæàÂ§öblockFileÔºåÈÄ†ÊàêÈóÆÈ¢ò1ÔºöË∂ÖËøáÊìç‰ΩúÁ≥ªÁªüÊâÄËÉΩÊâìÂºÄÊúÄÂ§ßÊñá‰ª∂Êï∞ÔºåÈóÆÈ¢ò2ÔºöÂ§ßÈáèÈöèÊú∫ÂÜôÈöèÊú∫ËØª Ëß£ÂÜ≥ÊñπÊ°àÔºö 1.shuffle ÂèÇÊï∞Ôºöspark.shuffle.consolidateFiles ÈªòËÆ§‰∏∫falseÂ¶ÇÊûúËÆæÁΩÆ‰∏∫‚Äùtrue‚ÄùÔºåÂú®shuffleÊúüÈó¥ÔºåÂêàÂπ∂ÁöÑ‰∏≠Èó¥Êñá‰ª∂Â∞Ü‰ºöË¢´ÂàõÂª∫„ÄÇÂàõÂª∫Êõ¥Â∞ëÁöÑÊñá‰ª∂ÂèØ‰ª•Êèê‰æõÊñá‰ª∂Á≥ªÁªüÁöÑshuffleÁöÑÊïà Áéá„ÄÇËøô‰∫õshuffleÈÉΩ‰º¥ÈöèÁùÄÂ§ßÈáèÈÄíÂΩí‰ªªÂä°„ÄÇÂΩìÁî®ext4ÂíådfsÊñá‰ª∂Á≥ªÁªüÊó∂ÔºåÊé®ËçêËÆæÁΩÆ‰∏∫‚Äùtrue‚Äù„ÄÇÂú®ext3‰∏≠ÔºåÂõ†‰∏∫Êñá‰ª∂Á≥ªÁªüÁöÑÈôêÂà∂ÔºåËøô‰∏™ÈÄâÈ°πÂèØ ËÉΩÊú∫Âô®ÔºàÂ§ß‰∫é8Ê†∏ÔºâÈôç‰ΩéÊïàÁéá 2.sort shuffleÊØè‰∏™mapÂè™ÂÜôÂà∞‰∏Ä‰∏™Êñá‰ª∂ÔºåÂíå‰∏äÈù¢ÁöÑÂÜôÂà∞reduce‰∏™Êï∞‰∏™Êñá‰ª∂‰∏çÂêå ‰∏çÂêåÁÆóÂ≠êÂΩ±ÂìçshuffleË°®Áé∞ÂΩ¢Âºè‰∏çÂêåreduceByKey(func, numPartitions=None)‰πüÂ∞±ÊòØÔºåreduceByKeyÁî®‰∫éÂØπÊØè‰∏™keyÂØπÂ∫îÁöÑÂ§ö‰∏™valueËøõË°åmergeÊìç‰ΩúÔºåÊúÄÈáçË¶ÅÁöÑÊòØÂÆÉËÉΩÂ§üÂú®Êú¨Âú∞ÂÖàËøõË°åmergeÊìç‰ΩúÔºåÂπ∂‰∏îmergeÊìç‰ΩúÂèØ‰ª•ÈÄöËøáÂáΩÊï∞Ëá™ÂÆö‰πâ„ÄÇ groupByKey(numPartitions=None)‰πüÂ∞±ÊòØÔºågroupByKey‰πüÊòØÂØπÊØè‰∏™keyËøõË°åÊìç‰ΩúÔºå‰ΩÜÂè™ÁîüÊàê‰∏Ä‰∏™sequence„ÄÇÈúÄË¶ÅÁâπÂà´Ê≥®ÊÑè‚ÄúNote‚Äù‰∏≠ÁöÑËØùÔºåÂÆÉÂëäËØâÊàë‰ª¨ÔºöÂ¶ÇÊûúÈúÄË¶ÅÂØπsequenceËøõË°åaggregationÊìç‰ΩúÔºàÊ≥®ÊÑèÔºågroupByKeyÊú¨Ë∫´‰∏çËÉΩËá™ÂÆö‰πâÊìç‰ΩúÂáΩÊï∞ÔºâÔºåÈÇ£‰πàÔºåÈÄâÊã©reduceByKey/aggregateByKeyÊõ¥Â•Ω„ÄÇËøôÊòØÂõ†‰∏∫groupByKey‰∏çËÉΩËá™ÂÆö‰πâÂáΩÊï∞ÔºåÊàë‰ª¨ÈúÄË¶ÅÂÖàÁî®groupByKeyÁîüÊàêRDDÔºåÁÑ∂ÂêéÊâçËÉΩÂØπÊ≠§RDDÈÄöËøámapËøõË°åËá™ÂÆö‰πâÂáΩÊï∞Êìç‰Ωú„ÄÇ]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark Êú∫Âô®Â≠¶‰π†ÂÖ•Èó®(‰∫å)]]></title>
    <url>%2F2017%2F08%2F29%2Fspark%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[ÈÉ®ÁΩ≤‰ºòÂåñÔºöÁ£ÅÁõòÔºöÊåÇËΩΩÁ£ÅÁõòÊó∂‰ΩøÁî®noatimeÂíånodiratimeÈÄâÈ°πÂáèÂ∞ëÂÜôÁöÑÂºÄÈîÄ linuxÊØè‰∏™Êñá‰ª∂ÈÉΩ‰ºö‰øùÁïô3‰∏™Êó∂Èó¥Êà≥Áî®stat Êñá‰ª∂Âêç Êù•Êü•ÁúãAcessÔºöÊñá‰ª∂ËÆøÈóÆÊó∂Èó¥ModfiyÔºöÂÜÖÂÆπ‰øÆÊîπÊó∂Èó¥ChangeÔºöÊñá‰ª∂Âêç‰øÆÊîπÊó∂Èó¥ ÂèÇÊï∞Âê´‰πâÔºöÁ£ÅÁõò‰∏ãÁöÑÊâÄÊúâÊñá‰ª∂‰∏çÊõ¥Êñ∞ËÆøÈóÆÊó∂Èó¥ÂÜÖÂ≠òÔºöJVM ÂÜÖÂ≠ò‰∏çÂª∫ËÆÆÊØè‰∏™executor Ë∂ÖËøá200G CPUÊØèÂè∞Êú∫Âô®ÁöÑVcoreÊï∞‰∏çÂª∫ËÆÆÂ∞è‰∫é8 JOBË∞ÉÂ∫¶ÔºöFail Schedule ÂèØÊúÄÂ§ßÁ®ãÂ∫¶‰øùËØÅÂêÑ‰∏™JobÈÉΩÊúâÊú∫‰ºöËé∑ÂèñËµÑÊ∫ê Êï∞ÊçÆÂ∫èÂàóÂåñÔºöKyro serializationÂ∫èÂàóÂåñÈÄüÂ∫¶Êõ¥Âø´ÔºåÁªìÊûúÊõ¥Á¥ßÂáë‰∏∫‰∫ÜÊõ¥Â•ΩÁöÑÊÄßËÉΩÔºåÈúÄÊèêÂâçÊ≥®ÂÜåË¢´Â∫èÂàóÂåñÁöÑÁ±ªÔºåÂê¶Âàô‰ºöÂ≠òÂú®Â§ßÈáèÁöÑÁ©∫Èó¥Êµ™Ë¥πÈÄöËøáspark.serializerÊåáÂÆö ÂáèÂ∞ëÂÜÖÂ≠òÊ∂àËÄóÔºöÂ∞ΩÈáè‰ΩøÁî®Âü∫Êú¨Êï∞ÊçÆÁ±ªÂûãÂíåÊï∞ÁªÑÔºåÈÅøÂÖç‰ΩøÁî®javaÈõÜÂêàÁ±ªÂ∞ΩÈáèÂáèÂ∞ëÂåÖÂê´Â§ßÈáèÂ∞èÂØπË±°ÁöÑÂµåÂ•óÁªìÊûÑKeyÂ∞ΩÈáè‰ΩøÁî®Êï∞ÂÄºÊàñÊûö‰∏æÁ±ªÂûãËÄå‰∏çÊòØÂ≠óÁ¨¶‰∏≤RAMÂ∞è‰∫é32GBÊó∂Ôºå‰ΩøÁî®-XX:+UseCompressedOops‰ΩøÁî®4Â≠óËäÇÔºàËÄåÈùû8Â≠óËäÇÔºâÁöÑÊåáÈíà Ë∞ÉÊï¥Âπ∂Ë°åÂ∫¶ÔºöË∞ÉÊï¥Map‰æßÂπ∂Ë°åÂ∫¶ÂØπ‰∫ékafka direct stream ÂèØÈÄöËøáË∞ÉÊï¥TopicÁöÑPatition‰∏™Êï∞Ë∞ÉÊï¥Spark Map‰æßÂπ∂Ë°åÂ∫¶ÂØπ‰∫éspark.textFileÔºåÈÄöËøáÂèÇÊï∞Ë∞ÉÊï¥ Ë∞ÉÊï¥Reduce‰æßÂπ∂Ë°åÂ∫¶ÈÄöËøáspark.default.parallelismËÆæÁΩÆshuffleÊó∂ÈªòËÆ§Âπ∂Ë°åÂ∫¶]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂºÄÊ∫êÁà¨Ëô´‰ªãÁªç]]></title>
    <url>%2F2017%2F08%2F05%2F%E5%BC%80%E6%BA%90%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[https://github.com/Chyroc/WechatSogou ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑Áà¨Ëô´„ÄÇÂü∫‰∫éÊêúÁãóÂæÆ‰ø°ÊêúÁ¥¢ÁöÑÂæÆ‰ø°ÂÖ¨‰ºóÂè∑Áà¨Ëô´Êé•Âè£ÔºåÂèØ‰ª•Êâ©Â±ïÊàêÂü∫‰∫éÊêúÁãóÊêúÁ¥¢ÁöÑÁà¨Ëô´ÔºåËøîÂõûÁªìÊûúÊòØÂàóË°®ÔºåÊØè‰∏ÄÈ°πÂùáÊòØÂÖ¨‰ºóÂè∑ÂÖ∑‰Ωì‰ø°ÊÅØÂ≠óÂÖ∏„ÄÇ https://github.com/lanbing510/DouBanSpider Ë±ÜÁì£ËØª‰π¶Áà¨Ëô´„ÄÇÂèØ‰ª•Áà¨‰∏ãË±ÜÁì£ËØª‰π¶Ê†áÁ≠æ‰∏ãÁöÑÊâÄÊúâÂõæ‰π¶ÔºåÊåâËØÑÂàÜÊéíÂêç‰æùÊ¨°Â≠òÂÇ®ÔºåÂ≠òÂÇ®Âà∞Excel‰∏≠ÔºåÂèØÊñπ‰æøÂ§ßÂÆ∂Á≠õÈÄâÊêúÁΩóÔºåÊØîÂ¶ÇÁ≠õÈÄâËØÑ‰ª∑‰∫∫Êï∞&gt;1000ÁöÑÈ´òÂàÜ‰π¶Á±çÔºõÂèØ‰æùÊçÆ‰∏çÂêåÁöÑ‰∏ªÈ¢òÂ≠òÂÇ®Âà∞Excel‰∏çÂêåÁöÑSheet ÔºåÈááÁî®User Agent‰º™Ë£Ö‰∏∫ÊµèËßàÂô®ËøõË°åÁà¨ÂèñÔºåÂπ∂Âä†ÂÖ•ÈöèÊú∫Âª∂Êó∂Êù•Êõ¥Â•ΩÁöÑÊ®°‰ªøÊµèËßàÂô®Ë°å‰∏∫ÔºåÈÅøÂÖçÁà¨Ëô´Ë¢´Â∞Å„ÄÇ https://github.com/LiuRoy/zhihu_spiderÁü•‰πéÁà¨Ëô´„ÄÇÊ≠§È°πÁõÆÁöÑÂäüËÉΩÊòØÁà¨ÂèñÁü•‰πéÁî®Êà∑‰ø°ÊÅØ‰ª•Âèä‰∫∫ÈôÖÊãìÊâëÂÖ≥Á≥ªÔºåÁà¨Ëô´Ê°ÜÊû∂‰ΩøÁî®scrapyÔºåÊï∞ÊçÆÂ≠òÂÇ®‰ΩøÁî®mongo https://github.com/airingursb/bilibili-userBilibiliÁî®Êà∑Áà¨Ëô´„ÄÇÊÄªÊï∞ÊçÆÊï∞Ôºö20119918ÔºåÊäìÂèñÂ≠óÊÆµÔºöÁî®Êà∑idÔºåÊòµÁß∞ÔºåÊÄßÂà´ÔºåÂ§¥ÂÉèÔºåÁ≠âÁ∫ßÔºåÁªèÈ™åÂÄºÔºåÁ≤â‰∏ùÊï∞ÔºåÁîüÊó•ÔºåÂú∞ÂùÄÔºåÊ≥®ÂÜåÊó∂Èó¥ÔºåÁ≠æÂêçÔºåÁ≠âÁ∫ß‰∏éÁªèÈ™åÂÄºÁ≠â„ÄÇÊäìÂèñ‰πãÂêéÁîüÊàêBÁ´ôÁî®Êà∑Êï∞ÊçÆÊä•Âëä„ÄÇ https://github.com/LiuXingMing/SinaSpiderÊñ∞Êµ™ÂæÆÂçöÁà¨Ëô´„ÄÇ‰∏ªË¶ÅÁà¨ÂèñÊñ∞Êµ™ÂæÆÂçöÁî®Êà∑ÁöÑ‰∏™‰∫∫‰ø°ÊÅØ„ÄÅÂæÆÂçö‰ø°ÊÅØ„ÄÅÁ≤â‰∏ùÂíåÂÖ≥Ê≥®„ÄÇ‰ª£Á†ÅËé∑ÂèñÊñ∞Êµ™ÂæÆÂçöCookieËøõË°åÁôªÂΩïÔºåÂèØÈÄöËøáÂ§öË¥¶Âè∑ÁôªÂΩïÊù•Èò≤Ê≠¢Êñ∞Êµ™ÁöÑÂèçÊâí„ÄÇ‰∏ªË¶Å‰ΩøÁî® scrapy Áà¨Ëô´Ê°ÜÊû∂ https://github.com/gnemoug/distribute_crawlerÂ∞èËØ¥‰∏ãËΩΩÂàÜÂ∏ÉÂºèÁà¨Ëô´„ÄÇ‰ΩøÁî®scrapy,Redis, MongoDB,graphiteÂÆûÁé∞ÁöÑ‰∏Ä‰∏™ÂàÜÂ∏ÉÂºèÁΩëÁªúÁà¨Ëô´,Â∫ïÂ±ÇÂ≠òÂÇ®mongodbÈõÜÁæ§,ÂàÜÂ∏ÉÂºè‰ΩøÁî®redisÂÆûÁé∞,Áà¨Ëô´Áä∂ÊÄÅÊòæÁ§∫‰ΩøÁî®graphiteÂÆûÁé∞Ôºå‰∏ªË¶ÅÈíàÂØπ‰∏Ä‰∏™Â∞èËØ¥Á´ôÁÇπ„ÄÇ https://github.com/yanzhou/CnkiSpider‰∏≠ÂõΩÁü•ÁΩëÁà¨Ëô´„ÄÇËÆæÁΩÆÊ£ÄÁ¥¢Êù°‰ª∂ÂêéÔºåÊâßË°åsrc/CnkiSpider.pyÊäìÂèñÊï∞ÊçÆÔºåÊäìÂèñÊï∞ÊçÆÂ≠òÂÇ®Âú®/dataÁõÆÂΩï‰∏ãÔºåÊØè‰∏™Êï∞ÊçÆÊñá‰ª∂ÁöÑÁ¨¨‰∏ÄË°å‰∏∫Â≠óÊÆµÂêçÁß∞„ÄÇ https://github.com/lanbing510/LianJiaSpiderÈìæÂÆ∂ÁΩëÁà¨Ëô´„ÄÇÁà¨ÂèñÂåó‰∫¨Âú∞Âå∫ÈìæÂÆ∂ÂéÜÂπ¥‰∫åÊâãÊàøÊàê‰∫§ËÆ∞ÂΩï„ÄÇÊ∂µÁõñÈìæÂÆ∂Áà¨Ëô´‰∏ÄÊñáÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºåÂåÖÊã¨ÈìæÂÆ∂Ê®°ÊãüÁôªÂΩï‰ª£Á†Å„ÄÇ https://github.com/taizilongxu/scrapy_jingdong‰∫¨‰∏úÁà¨Ëô´„ÄÇÂü∫‰∫éscrapyÁöÑ‰∫¨‰∏úÁΩëÁ´ôÁà¨Ëô´Ôºå‰øùÂ≠òÊ†ºÂºè‰∏∫csv„ÄÇ https://github.com/caspartse/QQ-Groups-SpiderQQ Áæ§Áà¨Ëô´„ÄÇÊâπÈáèÊäìÂèñ QQ Áæ§‰ø°ÊÅØÔºåÂåÖÊã¨Áæ§ÂêçÁß∞„ÄÅÁæ§Âè∑„ÄÅÁæ§‰∫∫Êï∞„ÄÅÁæ§‰∏ª„ÄÅÁæ§ÁÆÄ‰ªãÁ≠âÂÜÖÂÆπÔºåÊúÄÁªàÁîüÊàê XLS(X) / CSV ÁªìÊûúÊñá‰ª∂„ÄÇ https://github.com/hanc00l/wooyun_public‰πå‰∫ëÁà¨Ëô´„ÄÇ ‰πå‰∫ëÂÖ¨ÂºÄÊºèÊ¥û„ÄÅÁü•ËØÜÂ∫ìÁà¨Ëô´ÂíåÊêúÁ¥¢„ÄÇÂÖ®ÈÉ®ÂÖ¨ÂºÄÊºèÊ¥ûÁöÑÂàóË°®ÂíåÊØè‰∏™ÊºèÊ¥ûÁöÑÊñáÊú¨ÂÜÖÂÆπÂ≠òÂú®mongodb‰∏≠ÔºåÂ§ßÊ¶ÇÁ∫¶2GÂÜÖÂÆπÔºõÂ¶ÇÊûúÊï¥Á´ôÁà¨ÂÖ®ÈÉ®ÊñáÊú¨ÂíåÂõæÁâá‰Ωú‰∏∫Á¶ªÁ∫øÊü•ËØ¢ÔºåÂ§ßÊ¶ÇÈúÄË¶Å10GÁ©∫Èó¥„ÄÅ2Â∞èÊó∂Ôºà10MÁîµ‰ø°Â∏¶ÂÆΩÔºâÔºõÁà¨ÂèñÂÖ®ÈÉ®Áü•ËØÜÂ∫ìÔºåÊÄªÂÖ±Á∫¶500MÁ©∫Èó¥„ÄÇÊºèÊ¥ûÊêúÁ¥¢‰ΩøÁî®‰∫ÜFlask‰Ωú‰∏∫web serverÔºåbootstrap‰Ωú‰∏∫ÂâçÁ´Ø„ÄÇ https://github.com/fankcoder/findtripÊú∫Á•®Áà¨Ëô´ÔºàÂéªÂì™ÂÑøÂíåÊê∫Á®ãÁΩëÔºâ„ÄÇFindtripÊòØ‰∏Ä‰∏™Âü∫‰∫éScrapyÁöÑÊú∫Á•®Áà¨Ëô´ÔºåÁõÆÂâçÊï¥Âêà‰∫ÜÂõΩÂÜÖ‰∏§Â§ßÊú∫Á•®ÁΩëÁ´ôÔºàÂéªÂì™ÂÑø + Êê∫Á®ãÔºâ„ÄÇ https://github.com/leyle/163spiderÂü∫‰∫érequests„ÄÅMySQLdb„ÄÅtorndbÁöÑÁΩëÊòìÂÆ¢Êà∑Á´ØÂÜÖÂÆπÁà¨Ëô´ https://github.com/fanpei91/doubanspidersË±ÜÁì£ÁîµÂΩ±„ÄÅ‰π¶Á±ç„ÄÅÂ∞èÁªÑ„ÄÅÁõ∏ÂÜå„ÄÅ‰∏úË•øÁ≠âÁà¨Ëô´ÈõÜ https://github.com/LiuXingMing/QQSpiderQQÁ©∫Èó¥Áà¨Ëô´ÔºåÂåÖÊã¨Êó•Âøó„ÄÅËØ¥ËØ¥„ÄÅ‰∏™‰∫∫‰ø°ÊÅØÁ≠âÔºå‰∏ÄÂ§©ÂèØÊäìÂèñ 400 ‰∏áÊù°Êï∞ÊçÆ„ÄÇ https://github.com/Shu-Ji/baidu-music-spiderÁôæÂ∫¶mp3ÂÖ®Á´ôÁà¨Ëô´Ôºå‰ΩøÁî®redisÊîØÊåÅÊñ≠ÁÇπÁª≠‰º†„ÄÇ https://github.com/pakoo/tbcrawlerÊ∑òÂÆùÂíåÂ§©Áå´ÁöÑÁà¨Ëô´,ÂèØ‰ª•Ê†πÊçÆÊêúÁ¥¢ÂÖ≥ÈîÆËØç,Áâ©ÂìÅidÊù•ÊäìÂéªÈ°µÈù¢ÁöÑ‰ø°ÊÅØÔºåÊï∞ÊçÆÂ≠òÂÇ®Âú®mongodb„ÄÇ https://github.com/benitoro/stockholm‰∏Ä‰∏™ËÇ°Á•®Êï∞ÊçÆÔºàÊ≤™Ê∑±ÔºâÁà¨Ëô´ÂíåÈÄâËÇ°Á≠ñÁï•ÊµãËØïÊ°ÜÊû∂„ÄÇÊ†πÊçÆÈÄâÂÆöÁöÑÊó•ÊúüËåÉÂõ¥ÊäìÂèñÊâÄÊúâÊ≤™Ê∑±‰∏§Â∏ÇËÇ°Á•®ÁöÑË°åÊÉÖÊï∞ÊçÆ„ÄÇÊîØÊåÅ‰ΩøÁî®Ë°®ËææÂºèÂÆö‰πâÈÄâËÇ°Á≠ñÁï•„ÄÇÊîØÊåÅÂ§öÁ∫øÁ®ãÂ§ÑÁêÜ„ÄÇ‰øùÂ≠òÊï∞ÊçÆÂà∞JSONÊñá‰ª∂„ÄÅCSVÊñá‰ª∂„ÄÇ https://github.com/k1995/BaiduyunSpiderÁôæÂ∫¶‰∫ëÁõòÁà¨Ëô´]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH ÂÆâË£Öspark2.2]]></title>
    <url>%2F2017%2F08%2F05%2FCDH%20%E5%AE%89%E8%A3%85spark2.2%2F</url>
    <content type="text"><![CDATA[ÂÆòÊñπÈìæÊé•https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html 1.‰∏ãËΩΩSpark2 CSDhttps://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging 1.1.1 CSDÁ¨îËÄÖ‰∏ãËΩΩ2.2ÁâàÊú¨http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar1.1.2 Parcelhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/ 1.1.2.1 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcelhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel 1.1.2.2 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel.sha1ÁÑ∂ÂêéÂ∞ÜSPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1ÊîπÂêç‰∏∫SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha 1.1.2.3 manifest.jsonhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/manifest.json ÂÅúÊ≠¢ÊúçÂä°/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server stop/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent stop Â∞ÜcsdÊñá‰ª∂ÊîæÂà∞/soft/bigdata/clouderamanager/cloudera/csd Â∞ÜparcelÊñá‰ª∂ÊîæÂà∞/soft/bigdata/clouderamanager/cloudera/parcel-repo ‰øÆÊîπÊùÉÈôêchgrp cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jarchown cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jar ÂºÄÂêØÊúçÂä°/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start ‰∏ªÊú∫-&gt; ParcelÂ∞±ËÉΩÁúãÂà∞spark2‰∫Ü ÂàÜÈÖçÔºåÊøÄÊ¥ª„ÄÇÁÑ∂ÂêéÂ∞±ÂèØ‰ª•Ê∑ªÂä†ÊúçÂä°‰∫Ü Â¶ÇÊûúÊ∑ªÂä†ÊúçÂä°‰∏çÊàêÂäüÈúÄË¶ÅÊääjarÊñá‰ª∂ÊîæÂà∞/opt/cloudera/csd]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 2.2.0Ê∫êÁ†ÅÁºñËØë]]></title>
    <url>%2F2017%2F08%2F02%2Fspark%202.2.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[ÂÆòÁΩë‰∏ãËΩΩsparkÊ∫êÁ†Åhttps://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz ÁÑ∂ÂêéÂú®idea‰∏≠ÂØºÂÖ•sparkÊ∫êÁ†ÅÈ°πÁõÆ(idea mavenÈÖçÁΩÆÊ≠£Á°Æ)ÔºåÁÑ∂ÂêéÂØπsparkÈ°πÁõÆbuild„ÄÇBuildÊàêÂäüÂêéÂú®ËøõË°åÁºñËØë„ÄÇBuildËøáÁ®ã‰∏≠ÈÅáÂà∞ÈóÆÈ¢òÔºö Êâæ‰∏çÂà∞org.apache.spark.streaming.flume.sink.SparkFlumeProtocolÊâæ‰∏çÂà∞org.apache.spark.sql.catalyst.parser.SqlBaseParser ËÆæÁΩÆmavenÁöÑÂèÇÊï∞ÔºåÂê¶Âàô‰∏ÄÁõ¥Âá∫Áé∞outofMemoryÂú®apache-maven-3.3.9-bin\bin‰∏ãÈù¢ÁöÑmvn.cmdÊñá‰ª∂ÈáåÁöÑÔºö@REM set MAVEN_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000‰∏ãÈù¢Ê∑ªÂä†set MAVEN_OPTS= -Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m Âú®git bash ÈáåÁºñËØëËøõÂÖ•sparkÊ∫êÁ†ÅÁõÆÂΩïmvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ΩøÁî®AmbariÂÆâË£ÖÈÉ®ÁΩ≤SparkÈõÜÁæ§]]></title>
    <url>%2F2017%2F06%2F30%2F%E4%BD%BF%E7%94%A8Ambari%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Spark%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[‰∏ãËΩΩËøõÂÖ•ÂÆòÁΩëÔºåÈÄâÊã©‰∫ßÂìÅÈáåÈù¢ÁöÑ‰∏ãËΩΩÈ°µÈù¢ÊàñËÄÖÁõ¥Êé•ÁôªÂΩïhttps://hortonworks.com/downloads/ÈÄâÊã©HDP¬Æ 2.6: Ready for the enterprise‰∏ãÈù¢ÁöÑAutomated Install Guide Âõ†‰∏∫Âçö‰∏ªÊòØCENTOS 7Âú®Ëøô‰∏™È°µÈù¢Áõ¥Êé•ÈÄâÊã©ÔºöRHEL/CentOS/Oracle Linux 7 wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo -O /etc/yum.repos.d/ambari.repoyum repolist yum install ambari-server ËøõÂÖ•‚ÄãSetup Optionsambari-server setup ‚Äìj /usr/java/defaultÊèêÁ§∫ÂèÇÊï∞Âè™ËÉΩÊòØ‰∏Ä‰∏™ÔºåÁúãÊù•jdkË¶ÅÂêéËÆæÁΩÆ ambari-server setupÂêÑÁßçÂõûËΩ¶ÔºåjdkÁöÑÊó∂ÂÄôÈÄâÊã©ÂÆâË£Ö1.8ÁöÑÂ¶ÇÊûúËæìÂÖ•Ëá™ÂÆö‰πâÁöÑjdkË¶ÅÊ≥®ÊÑèÊùÉÈôêÈóÆÈ¢ò ÂêØÂä®Ôºöambari-server start Êü•ÁúãÔºö[root@storm01 storm]# lsof -i:8080COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 7648 storm 1438u IPv6 124470 0t0 TCP *:webcache (LISTEN) ÂÖ≥Èó≠Èò≤ÁÅ´Â¢ôsystemctl disable firewalld.servicesystemctl stop firewalld.service Âú®ÊµèËßàÂô®ËæìÂÖ•http://storm01:8080Âç≥ÂèØËøõÂÖ•UIÁî®Êà∑ÂØÜÁ†ÅÈÉΩ‰∏∫admin Âú®ÊØè‰∏™Êú∫Âô®‰∏äÂÆâË£Öagentyum install -y ambari-agent ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ vi /etc/ambari-agent/conf/ambari-agent.ini hostname=localhostÊõ¥Êîπ‰∏∫ hostname=storm01 ÂêØÂä®agent service ambari-agent start ËÆæÁΩÆhiveÁî®Êà∑ÂØÜÁ†ÅÈÉΩ‰∏∫hive ÈúÄË¶ÅPostgreSQLÊîØÊåÅËøúÁ®ãËøûÊé•find / -name pg_hba.conf vi /var/lib/pgsql/data/pg_hba.confÂ∞Ülocal all postgres peerÊîπÊàêlocal all postgres trusthost all postgres 127.0.0.1/32 identÊîπÊàêhost all postgres 127.0.0.1/32 trustlocal all ambari,mapred md5ÊîπÊàêlocal all ambari,mapred,hive trusthost all ambari,mapred 0.0.0.0/0 md5ÊîπÊàêhost all ambari,mapred,hive 0.0.0.0/0 md5host all ambari,mapred ::/0 md5ÊîπÊàêhost all ambari,mapred,hive ::/0 md5 ÈáçÂêØÊï∞ÊçÆÂ∫ìservice postgresql restart Ê£ÄÊü•lsof -i:5432 Âéª‰ªìÂ∫ì‰∏ãËΩΩpostgresql http://mvnrepository.com/ ‰∏ãËΩΩ PostgreSQL JDBC Driver JDBC 4.2ÁöÑ9.2-1002-jdbc4 ÊàñËÄÖÁõ¥Êé•ËæìÂÖ•Ôºöwget http://central.maven.org/maven2/org/postgresql/postgresql/9.2-1002-jdbc4/postgresql-9.2-1002-jdbc4.jar ambari-server setup ‚Äìjdbc-db=postgres ‚Äìjdbc-driver=/root/postgresql-9.2-1002-jdbc4.jar ÂàõÂª∫Áî®Êà∑psql -U postgres -d postgrescreate user hive;alter user hive password ‚Äòhive‚Äô;create database hive;grant all on database hive to hive;alter database hive owner to hive; Ê≠§Êó∂test ConnectionConnection Â∞±ÂèØ‰ª•ÊµãËØïÊàêÂäü ËÆæÁΩÆAmbari MetricsÂíåSmart SenseÁî®Êà∑ÂØÜÁ†ÅÈÉΩ‰∏∫admin Â¶ÇÊûúÂèëÁé∞Â§±Ë¥•ÔºåÈúÄË¶ÅËÆæÁΩÆ/etc/hostsÊñá‰ª∂192.168.247.180 spark01192.168.247.181 spark02192.168.247.182 spark03 ÂÆâË£ÖÂÆåÊàê]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phoenixÂàõÂª∫‰∫åÁ∫ßÁ¥¢Âºï]]></title>
    <url>%2F2017%2F05%2F25%2Fphoenix%E5%88%9B%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[Á¥¢ÂºïÂàÜ‰∏∫‰∏§ÁßçÔºöGlobal IndexingLocal Indexing Global Indexing Ôºö‰ΩøÁî®‰∫éËØªÂ§öÔºåÂÜôÂ∞ëÁöÑÂú∫ÊôØ.select Êü•Âá∫Êù•ÁöÑÊï∞ÊçÆÂøÖÈ°ªÊòØÁ¥¢ÂºïÂ≠óÊÆµÊâçËÉΩ‰ΩøÁî®Âà∞Á¥¢Âºï Local Indexing Ôºöhbase bulk loadÊï∞ÊçÆ‰∏ä‰º†Âà∞hdfsÔºö/phoenix_data/fanpu.csv HADOOP_CLASSPATH=$(hbase classpath) hadoop jar phoenix-4.8.0-cdh5.8.0-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool-libjars /soft/phoenix/phoenix-4.8.0-cdh5.8.0/lib/commons-csv-1.0.jar,/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib/joda-time-2.7.jar ‚Äìtable fanpu ‚Äìinput /phoenix_data/fanpu.csv ÊàëÂ¶ÇÊûú‰ΩøÁî®Global IndexingÔºöÂàõÂª∫Á¥¢ÂºïÔºöcreate index fanpu_index on fanpu (‚Äúfamily‚Äù.‚Äùid_no‚Äù,‚Äùfamily‚Äù.‚Äùname‚Äù,‚Äùfamily‚Äù.‚Äùmobile‚Äù); select from fanpu where ‚Äúfamily‚Äù.‚Äùid_no‚Äù=‚Äôxxxx‚Äô and ‚Äúfamily‚Äù.‚Äùname‚Äù=‚ÄôÈ©¨ÊñáÂ≠¶‚Äô and ‚Äúfamily‚Äù.‚Äùmobile‚Äù=‚Äôxxxx‚Äô; ÊòØ‰∏çËÉΩ‰ΩøÁî®Á¥¢ÂºïÁöÑ*√ó select ‚Äúfamily‚Äù.‚Äùid_no‚Äù,‚Äùfamily‚Äù.‚Äùname‚Äù from fanpu where ‚Äúfamily‚Äù.‚Äùid_no‚Äù=‚Äôxxxx‚Äô and ‚Äúfamily‚Äù.‚Äùname‚Äù=‚ÄôÈ©¨ÊñáÂ≠¶‚Äô and ‚Äúfamily‚Äù.‚Äùmobile‚Äù=‚Äôxxxx‚Äô; ÊâçÂèØ‰ª•‚àö 12345678910111213141516171819202122230: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_RECEIPT_NO&quot;);76,392 rows affected (6.256 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER on &quot;LENDER-LEN_TRANSFER&quot; (&quot;family&quot;.&quot;LEN_LOAN_SELL_CONFIRM_ID&quot;);99,937 rows affected (6.256 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER_DTL on &quot;LENDER-LEN_TRANSFER_DTL&quot; (&quot;family&quot;.&quot;LENDERINFOID&quot;,&quot;family&quot;.&quot;LOAN_TRANSFER_MODEL_ID&quot;);153,029 rows affected (6.239 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL2 on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_RECEIPT_NO&quot;);76,392 rows affected (6.231 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL3 on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_ID&quot;);76,392 rows affected (6.234 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LOAN_MAIN on &quot;LENDER-LEN_LOAN_MAIN&quot; (&quot;family&quot;.&quot;ASSUME_DEBTOR_ID&quot;);20,865 rows affected (6.233 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LOAN_MAIN2 on &quot;LENDER-LEN_LOAN_MAIN&quot; (&quot;family&quot;.&quot;LOAN_CONTRACT_ID&quot;);20,865 rows affected (6.231 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_INFO on &quot;LENDER-LEN_LENDER_INFO&quot; (&quot;family&quot;.&quot;CUSTOMER_ID&quot;);17,053 rows affected (6.235 seconds) 0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_SELL_CONFIRM on ‚ÄúLENDER-LEN_SELL_CONFIRM‚Äù (‚ÄúID‚Äù);9,417 rows affected (6.245 seconds) 0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER3 on ‚ÄúLENDER-LEN_TRANSFER‚Äù (‚Äúfamily‚Äù.‚ÄùSELLER_LOAN_DTL_ID‚Äù);99,937 rows affected (6.251 seconds)]]></content>
      <categories>
        <category>phoenix</category>
      </categories>
      <tags>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH oozie]]></title>
    <url>%2F2017%2F05%2F18%2FCDH%20oozie%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[ÂÆâË£ÖÂÆåoozieÔºåÊâìÂºÄUI:http://master1:11000/oozie/ ÊòæÁ§∫Oozie web console is disabled.Ëß£ÂÜ≥ÊñπÊ°à;ÂéüÂõ†ÊòØoozieÁöÑ/var/lib/oozieÁõÆÂΩïÈáåÁº∫Â∞ëEXTÁöÑÂåÖ ÁÇπÂáªDocumentationÈìæÊé•ÈáåquickstartÁªôÂá∫‰∫ÜËß£ÂÜ≥ÊñπÊ°àDownload ExtJS library (it must be version 2.2) ‰∏ãËΩΩÂú∞ÂùÄhttp://dev.sencha.com/deploy/ext-2.2.zip Â¶ÇÊûú‰∏ãËΩΩ‰∏ç‰∏ãÊù•ÂèØ‰ª•ËØïËØïCSDNhttp://download.csdn.net/download/start_baby/6280675ÊàñËÄÖÔºöhttp://archive.cloudera.com/gplextras/misc/ext-2.2.zip unzipËß£ÂéãÁÑ∂ÂêéÂà∑Êñ∞È°µÈù¢ÊàêÂäüËøõÂÖ•oozieÁöÑwebÁïåÈù¢ ÁºñÂÜôjob.propertiesÔºànameNodeË¶ÅÂΩìÊó∂activeÁöÑÔºâ123456nameNode=hdfs://master2:8020jobTracker=master2:8032queueName=defaultexamplesRoot=user/oozie/my-apps/shelloozie.wf.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/workflow.xmlEXEC=emp-join-demp.sh ÁºñÂÜôworkflow.xml1234567891011121314151617181920212223&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt; &lt;start to=&quot;shell-node&quot;/&gt; &lt;action name=&quot;shell-node&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt; &lt;file&gt;$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;!--Copy the executable to compute node&apos;s current working directory --&gt; &lt;/shell&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; ÁºñÂÜôemp-join-demp.sh12#!/bin/shjava -cp PhoenixAPI-1.0-SNAPSHOT.jar com.zwjf.Month [hdfs@master1 data]$ hadoop fs -mkdir -p /user/oozie/my-apps/shell[hdfs@master1 data]$ hadoop fs -put workflow.xml /user/oozie/my-apps/shell[hdfs@master1 data]$ hadoop fs -put emp-join-demp.sh /user/oozie/my-apps/shell ‰∏ä‰º†ÊàëshellËÑöÊú¨ÈáåÊâßË°åÁöÑjarÂåÖÔºå‰Ω†‰ª¨Ê†πÊçÆËá™Â∑±ÁöÑshellÂÜ≥ÂÆöÂ¶Ç‰ΩïÊìç‰Ωúhadoop fs -put PhoenixAPI-1.0-SNAPSHOT.jar /user/oozie/my-apps/shell [hdfs@master1 data]$ /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie job -oozie http://master1:11000/oozie -config /soft/data/job.properties -run Êèê‰∫§Âá∫ÈîôÔºöÂéªÂéÜÂè≤ÊúçÂä°Âô®Êü•ÁúãÈóÆÈ¢òhttp://master2:19888/jobhistory Êä•ÈîôÔºöLauncher ERROR, reason: Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1] ÊòØÂõ†‰∏∫ÂàÜÂèëÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞Áî®Êà∑ÁöÑjarÂåÖÔºåÂú®workflow.xmlÁöÑ ${nameNode}/${examplesRoot}/${EXEC}#${EXEC}Ê∑ªÂä†‰Ω†ÁöÑjarÂåÖÂπ∂‰∏ä‰º†hdfsÂ∞±ÂèØ‰ª•## ‰æãÂ¶ÇÔºö${nameNode}/${examplesRoot}/PhoenixAPI-1.0-SNAPSHOT.jar#PhoenixAPI-1.0-SNAPSHOT.jar ÂèØÂèÇËÄÉÂ¶Ç‰∏ãÊñáÁ´†http://blog.sina.com.cn/s/blog_e699b42b0102xh3o.html Â¢ûÂä†ÂÆöÊó∂‰ªªÂä°job.propertiesÂ¢ûÂä†123oozie.coord.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/coordinator.xmlstart=2017-05-18T16:30Zend=2019-07-30T16:00Z oozie.wf.application.path=${nameNode}/${examplesRoot}/workflow.xmlÊîπÊàêworkflowAppUri=${nameNode}/${examplesRoot}/workflow.xml Êñ∞Âª∫coordinator.xml12345678910111213141516171819202122232425&lt;coordinator-app name=&quot;coordinator&quot; frequency=&quot;$&#123;coord:minutes(10)&#125;&quot; start=&quot;$&#123;start&#125;&quot; end=&quot;$&#123;end&#125;&quot; timezone=&quot;Asia/Shanghai&quot; xmlns=&quot;uri:oozie:coordinator:0.2&quot;&gt; &lt;action&gt; &lt;workflow&gt; &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;jobTracker&lt;/name&gt; &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;EXEC&lt;/name&gt; &lt;value&gt;$&#123;EXEC&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nameNode&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;queueName&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/workflow&gt; &lt;/action&gt;&lt;/coordinator-app&gt; ‰∏ä‰º†hadoop fs -put coordinator.xml /user/oozie/my-apps/shell ÂÅúÊ≠¢‰ªªÂä°Ôºö/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie job -oozie http://master1:11000/oozie -kill 0000005-170518154227460-oozie-oozi-C Ê≥®ÔºöËÆæÁΩÆÁöÑÊó∂Èó¥‰∏çËÉΩÂ∞è‰∫éÂΩìÂâçÊó∂Èó¥ÔºåÂê¶Âàô‰ºöÊää‰πãÂâçÊ≤°ÊâßË°åÁöÑÈÉΩÊâßË°å]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafkaÊ∫êÁ†ÅÈòÖËØª2]]></title>
    <url>%2F2017%2F05%2F08%2Fkafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB2%2F</url>
    <content type="text"><![CDATA[ÂØºÂÖ•IDEAÂç≥ÂèØÁúãkafkaÊ∫êÁ†ÅÔºöÂêØÂä®‰πãÂâçÈúÄË¶ÅÂÆâË£ÖzookeeperÂú∞ÂùÄÔºöhttp://apache.fayea.com/zookeeper/http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz Ëß£ÂéãÂêéÂÜçÂΩìÂâçÁõÆÂΩïÂ¢ûÂä†dataLogDirÂíådataÁõÆÂΩïÂ§çÂà∂‰∏Ä‰ªΩÈÖçÁΩÆÊñá‰ª∂ÊîπÂêç‰∏∫zoo.cfg‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂Ôºözoo.cfg‰øÆÊîπÂπ∂Â¢ûÂä†dataDir=D:\tool\zookeeper-3.4.6\datadataLogDir=D:\tool\zookeeper-3.4.6\dataLogDir ÂêØÂä®zkServer.cmd ÂêØÂä®ZkCli.cmd kafkaÂêØÂä®„ÄÇÂú®ÈÖçÁΩÆÊñá‰ª∂‰øÆÊîπProgram argumentsÔºöconfig/server.properties ÁÑ∂Âêé‰øÆÊîπserver.propertiesÈáåÈù¢ÁöÑÂèÇÊï∞Âç≥ÂèØ„ÄÇ ÂêØÂä®ÂâçÔºö ÂêØÂä®ÂêéÔºö Ê∫êÁ†ÅÈòÖËØªÔºà‰∏ÄÔºâ‰ªéÂêØÂä®ÂÖ•Âè£ÂàÜÊûêÔºöKafka.scala 12345678910111213141516171819202122232425262728293031323334def main(args: Array[String]): Unit = &#123; try &#123; /*‰ªéÈÖçÁΩÆÊñá‰ª∂ËØªÂèñkafkaÊúçÂä°Âô®ÂêØÂä®ÂèÇÊï∞--Â∞Ü‰º†ÂÖ•ÁöÑÂèÇÊï∞ËΩ¨Êç¢ÊàêProperties Êñá‰ª∂ÔºåÂ¶ÇÊûúÂèÇÊï∞‰∏∫Á©∫Â∞ÜÊèêÁ§∫Ôºö * USAGE: java [options] KafkaServer server.properties [--override property=value]* * Option Description * ------ ----------- * --override &lt;String&gt; Optional property that should override values set in * server.properties file * * Âà§Êñ≠ÂèÇÊï∞ÊòØÂê¶Â§ß‰∫é1ÔºåÂ∞ÜÂêéÈù¢ÁöÑÂèÇÊï∞ÊîæÂà∞PropertiesÈáå * */ val serverProps = getPropsFromArgs(args) //ÂàõÂª∫KafkaServerStartableÂØπË±° val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps) // attach shutdown handler to catch control-c Runtime.getRuntime().addShutdownHook(new Thread() &#123; override def run() = &#123; kafkaServerStartable.shutdown &#125; &#125;) kafkaServerStartable.startup kafkaServerStartable.awaitShutdown &#125; catch &#123; case e: Throwable =&gt; fatal(e) System.exit(1) &#125; System.exit(0) &#125; Ëøô‰∏äÈù¢Êúâ‰∏™Â∞èÁü•ËØÜÁÇπÔºö_* ÂëäËØâÁºñËØëÂô®‰Ω†Â∏åÊúõÂ∞ÜÊüê‰∏™ÂèÇÊï∞ÂΩì‰ΩúÂèÇÊï∞Â∫èÂàóÂ§ÑÁêÜ123456def echo(args: String*) = for (arg &lt;- args) println(arg)def main(args: Array[String]) = &#123; var args = Array("config/server.properties","canshu1","canshu2") echo(args.slice(1, args.length): _*)&#125; ËæìÂá∫ÊòØcanshu1canshu2 kafkaServerStartableÂ∞ÅË£Ö‰∫ÜKafkaServer 1.ÂÖ∑‰ΩìÁöÑÂêØÂä®Á±ªÂú®ÔºöKafkaServerStartableÁöÑstartupÊñπÊ≥ï123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124def startup() &#123; try &#123; info("starting") if(isShuttingDown.get) throw new IllegalStateException("Kafka server is still shutting down, cannot re-start!") if(startupComplete.get) return val canStartup = isStartingUp.compareAndSet(false, true) if (canStartup) &#123; brokerState.newState(Starting) /* start scheduler */ kafkaScheduler.startup() /* setup zookeeper */ zkUtils = initZk() /* Get or create cluster_id */ _clusterId = getOrGenerateClusterId(zkUtils) info(s"Cluster ID = $clusterId") /* generate brokerId */ config.brokerId = getBrokerId this.logIdent = "[Kafka Server " + config.brokerId + "], " /* create and configure metrics */ val reporters = config.getConfiguredInstances(KafkaConfig.MetricReporterClassesProp, classOf[MetricsReporter], Map[String, AnyRef](KafkaConfig.BrokerIdProp -&gt; (config.brokerId.toString)).asJava) reporters.add(new JmxReporter(jmxPrefix)) val metricConfig = KafkaServer.metricConfig(config) metrics = new Metrics(metricConfig, reporters, time, true) quotaManagers = QuotaFactory.instantiate(config, metrics, time) notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala) /* start log manager */ logManager = createLogManager(zkUtils.zkClient, brokerState) logManager.startup() metadataCache = new MetadataCache(config.brokerId) credentialProvider = new CredentialProvider(config.saslEnabledMechanisms) socketServer = new SocketServer(config, metrics, time, credentialProvider) socketServer.startup() /* start replica manager */ replicaManager = new ReplicaManager(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower) replicaManager.startup() /* start kafka controller */ kafkaController = new KafkaController(config, zkUtils, brokerState, time, metrics, threadNamePrefix) kafkaController.startup() adminManager = new AdminManager(config, metrics, metadataCache, zkUtils) /* start group coordinator */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM) groupCoordinator.startup() /* Get the authorizer and initialize it if one is specified.*/ authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt; val authZ = CoreUtils.createObject[Authorizer](authorizerClassName) authZ.configure(config.originals()) authZ &#125; /* start processing requests */ apis = new KafkaApis(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator, kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers, clusterId, time) requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, time, config.numIoThreads) Mx4jLoader.maybeLoad() /* start dynamic config manager */ dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers), ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers), ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider), ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers)) // Create the config manager. start listening to notifications dynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers) dynamicConfigManager.startup() /* tell everyone we are alive */ val listeners = config.advertisedListeners.map &#123; endpoint =&gt; if (endpoint.port == 0) endpoint.copy(port = socketServer.boundPort(endpoint.listenerName)) else endpoint &#125; kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack, config.interBrokerProtocolVersion) kafkaHealthcheck.startup() // Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it checkpointBrokerId(config.brokerId) /* register broker metrics */ registerStats() brokerState.newState(RunningAsBroker) shutdownLatch = new CountDownLatch(1) startupComplete.set(true) isStartingUp.set(false) AppInfoParser.registerAppInfo(jmxPrefix, config.brokerId.toString) info("started") &#125; &#125; catch &#123; case e: Throwable =&gt; fatal("Fatal error during KafkaServer startup. Prepare to shutdown", e) isStartingUp.set(false) shutdown() throw e &#125; &#125; Starting ÁªßÊâøBrokerStatesÔºåBrokerStatesÊòØ‰∏Ä‰∏™sealed trait1234567sealed trait BrokerStates &#123; def state: Byte &#125;case object NotRunning extends BrokerStates &#123; val state: Byte = 0 &#125;case object Starting extends BrokerStates &#123; val state: Byte = 1 &#125;case object RecoveringFromUncleanShutdown extends BrokerStates &#123; val state: Byte = 2 &#125;case object RunningAsBroker extends BrokerStates &#123; val state: Byte = 3 &#125;case object PendingControlledShutdown extends BrokerStates &#123; val state: Byte = 6 &#125;case object BrokerShuttingDown extends BrokerStates &#123; val state: Byte = 7 &#125; traitÂÆö‰πâ‰∏∫sealed Êúâ‰∏§Â±ÇÂê´‰πâ1.ÂÖ∂‰øÆÈ•∞ÁöÑtrait classÂè™ËÉΩÂú®ÂΩìÂâçÊñá‰ª∂ÈáåÈù¢Ë¢´ÁªßÊâø2.Áî®sealed‰øÆÈ•∞ËøôÊ†∑ÂÅöÁöÑÁõÆÁöÑÊòØÂëäËØâscalaÁºñËØëÂô®Âú®Ê£ÄÊü•Ê®°ÂºèÂåπÈÖçÁöÑÊó∂ÂÄôÔºåËÆ©scalaÁü•ÈÅìËøô‰∫õcaseÁöÑÊâÄÊúâÊÉÖÂÜµÔºåscalaÂ∞±ËÉΩÂ§üÂú®ÁºñËØëÁöÑÊó∂ÂÄôËøõË°åÊ£ÄÊü•ÔºåÁúã‰Ω†ÂÜôÁöÑ‰ª£Á†ÅÊòØÂê¶ÊúâÊ≤°ÊúâÊºèÊéâ‰ªÄ‰πàÊ≤°caseÂà∞ÔºåÂáèÂ∞ëÁºñÁ®ãÁöÑÈîôËØØ„ÄÇ 2.start scheduler:1kafkaScheduler.startup() 12345678910111213141516171819202122class KafkaScheduler(val threads: Int, val threadNamePrefix: String = "kafka-scheduler-", daemon: Boolean = true) extends Scheduler with Logging &#123; private var executor: ScheduledThreadPoolExecutor = null private val schedulerThreadId = new AtomicInteger(0) override def startup() &#123; debug("Initializing task scheduler.") this synchronized &#123; if(isStarted) throw new IllegalStateException("This scheduler has already been started!") executor = new ScheduledThreadPoolExecutor(threads) executor.setContinueExistingPeriodicTasksAfterShutdownPolicy(false) executor.setExecuteExistingDelayedTasksAfterShutdownPolicy(false) executor.setThreadFactory(new ThreadFactory() &#123; def newThread(runnable: Runnable): Thread = Utils.newThread(threadNamePrefix + schedulerThreadId.getAndIncrement(), runnable, daemon) &#125;) &#125; &#125; ...&#125; 1.Ê≠§Â§Ñ‰ΩøÁî®‰∫ÜÂêåÊ≠•ÈîÅÔºåÂ¶ÇÊûúÂ∑≤ÁªèÂêØÂä®ÔºåÁõ¥Êé•Êäõ‰∏™IllegalStateExceptionÂºÇÂ∏∏ÔºåÁî±Â§ñÈù¢ÈÄöÁî®ÂºÇÂ∏∏ThrowableÊçïËé∑„ÄÇ2.Ê†πÊçÆÈÖçÁΩÆÊñá‰ª∂ÁöÑbackground.threads ÂàõÂª∫‰∏Ä‰∏™ScheduledThreadPoolExecutor(threads)„Äêjava.util.concurrentÂåÖ‰∏ãÁöÑ„Äë 3.setup zookeeper1zkUtils = initZk() 123456789101112131415161718192021222324252627282930313233private def initZk(): ZkUtils = &#123; info(s"Connecting to zookeeper on $&#123;config.zkConnect&#125;") val chrootIndex = config.zkConnect.indexOf("/") val chrootOption = &#123; if (chrootIndex &gt; 0) Some(config.zkConnect.substring(chrootIndex)) else None &#125; val secureAclsEnabled = config.zkEnableSecureAcls val isZkSecurityEnabled = JaasUtils.isZkSecurityEnabled() if (secureAclsEnabled &amp;&amp; !isZkSecurityEnabled) throw new java.lang.SecurityException(s"$&#123;KafkaConfig.ZkEnableSecureAclsProp&#125; is true, but the verification of the JAAS login file failed.") chrootOption.foreach &#123; chroot =&gt; val zkConnForChrootCreation = config.zkConnect.substring(0, chrootIndex) val zkClientForChrootCreation = ZkUtils(zkConnForChrootCreation, sessionTimeout = config.zkSessionTimeoutMs, connectionTimeout = config.zkConnectionTimeoutMs, secureAclsEnabled) zkClientForChrootCreation.makeSurePersistentPathExists(chroot) info(s"Created zookeeper path $chroot") zkClientForChrootCreation.zkClient.close() &#125; val zkUtils = ZkUtils(config.zkConnect, sessionTimeout = config.zkSessionTimeoutMs, connectionTimeout = config.zkConnectionTimeoutMs, secureAclsEnabled) zkUtils.setupCommonPaths() zkUtils &#125; 1.ÂàõÂª∫ËøûÊé•123456789val persistentZkPaths = Seq(ConsumersPath, BrokerIdsPath, BrokerTopicsPath, ConfigChangesPath, getEntityConfigRootPath(ConfigType.Topic), getEntityConfigRootPath(ConfigType.Client), DeleteTopicsPath, BrokerSequenceIdPath, IsrChangeNotificationPath) 2.ËÆæÁΩÆÈÄöÁî®Ë∑ØÂæÑ/consumers/brokers/ids/brokers/topics/config/changes/config/topics/config/clients/admin/delete_topics/brokers/seqid/isr_change_notification ISRÔºöKafkaÂú®Zookeeper‰∏≠Âä®ÊÄÅÁª¥Êä§‰∫Ü‰∏Ä‰∏™ISRÔºàin-sync replicasÔºâ setÔºåËøô‰∏™setÈáåÁöÑÊâÄÊúâreplicaÈÉΩË∑ü‰∏ä‰∫ÜleaderÔºåÂè™ÊúâISRÈáåÁöÑÊàêÂëòÊâçÊúâË¢´ÈÄâ‰∏∫leaderÁöÑÂèØËÉΩ 4.Get or create cluster_id12_clusterId = getOrGenerateClusterId(zkUtils)info(s"Cluster ID = $clusterId") 1234567891011121314151617181920212223242526 def getOrGenerateClusterId(zkUtils: ZkUtils): String = &#123; zkUtils.getClusterId.getOrElse(zkUtils.createOrGetClusterId(CoreUtils.generateUuidAsBase64)) &#125; def createOrGetClusterId(proposedClusterId: String): String = &#123; try &#123; createPersistentPath(ClusterIdPath, ClusterId.toJson(proposedClusterId)) proposedClusterId &#125; catch &#123; case _: ZkNodeExistsException =&gt; getClusterId.getOrElse(throw new KafkaException("Failed to get cluster id from Zookeeper. This can only happen if /cluster/id is deleted from Zookeeper.")) &#125; &#125;/** * Create an persistent node with the given path and data. Create parents if necessary. */ def createPersistentPath(path: String, data: String = "", acls: java.util.List[ACL] = UseDefaultAcls): Unit = &#123; val acl = if (acls eq UseDefaultAcls) ZkUtils.defaultAcls(isSecure, path) else acls try &#123; ZkPath.createPersistent(zkClient, path, data, acl) &#125; catch &#123; case _: ZkNoNodeException =&gt; createParentPath(path) ZkPath.createPersistent(zkClient, path, data, acl) &#125; &#125; Ê≠§Â§Ñ‰ºöÂàõÂª∫‰∏Ä‰∏™persistentËäÇÁÇπ/cluster/id Â¶ÇÊûúËäÇÁÇπÂ∑≤ÁªèÂ≠òÂú®ÔºåÂàôÂà®Èô§ÂºÇÂ∏∏Ôºå‰∏äÊ¨°Ëé∑ÂèñÂºÇÂ∏∏ÔºåÁÑ∂ÂêéÂéªËäÇÁÇπ‰∏ãËé∑Âèñ_clusterIdÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÔºåÂ∞ÜÂàõÂª∫ÁöÑproposedClusterIdËøîÂõû Ê≠§Â§ÑÈúÄË¶ÅÊ≥®ÊÑèzookeeperÁöÑËäÇÁÇπÁ±ªÂûãÂàÜ‰∏∫ÔºöÊåÅ‰πÖËäÇÁÇπÔºàPERSISTENTÔºâÊåÅ‰πÖÈ°∫Â∫èËäÇÁÇπÔºàPERSISTENT_SEQUENTIALÔºâ‰∏¥Êó∂ËäÇÁÇπÔºàEPHEMERALÔºâ‰∏¥Êó∂È°∫Â∫èËäÇÁÇπÔºàEPHEMERAL_SEQUENTIALÔºâÈ°∫Â∫èËäÇÁÇπÂç≥ÂàõÂª∫ÊúâÂ∫èÁöÑËäÇÁÇπÔºåËäÇÁÇπÂêçÂä†‰∏ä‰∏Ä‰∏™Êï∞Â≠óÂêéÁºÄ„ÄÇ‰∏¥Êó∂ËäÇÁÇπÂíåÂÆ¢Êà∑Á´ØÁªëÂÆöÔºå‰ºöËØùÂ§±ÊïàÔºàÈùûËøûÊé•Êñ≠ÂºÄÔºâÂàôËá™Âä®Ê∏ÖÊ•ö ‰∏¥Êó∂È°∫Â∫èËäÇÁÇπÂèØÁî®Êù•ÂÆûÁé∞ÂàÜÂ∏ÉÂºèÈîÅ1.ÂÆ¢Êà∑Á´ØË∞ÉÁî®create()ÊñπÊ≥ïÂàõÂª∫Âêç‰∏∫‚Äúlocknode/guid-lock-‚ÄùÁöÑËäÇÁÇπÔºåÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËøôÈáåËäÇÁÇπÁöÑÂàõÂª∫Á±ªÂûãÈúÄË¶ÅËÆæÁΩÆ‰∏∫EPHEMERAL_SEQUENTIAL„ÄÇ2.ÂÆ¢Êà∑Á´ØË∞ÉÁî®getChildren(‚Äúlocknode‚Äù)ÊñπÊ≥ïÊù•Ëé∑ÂèñÊâÄÊúâÂ∑≤ÁªèÂàõÂª∫ÁöÑÂ≠êËäÇÁÇπÔºåÊ≥®ÊÑèÔºåËøôÈáå‰∏çÊ≥®ÂÜå‰ªª‰ΩïWatcher„ÄÇ3.ÂÆ¢Êà∑Á´ØËé∑ÂèñÂà∞ÊâÄÊúâÂ≠êËäÇÁÇπpath‰πãÂêéÔºåÂ¶ÇÊûúÂèëÁé∞Ëá™Â∑±Âú®Ê≠•È™§1‰∏≠ÂàõÂª∫ÁöÑËäÇÁÇπÂ∫èÂè∑ÊúÄÂ∞èÔºåÈÇ£‰πàÂ∞±ËÆ§‰∏∫Ëøô‰∏™ÂÆ¢Êà∑Á´ØËé∑Âæó‰∫ÜÈîÅ„ÄÇ4.Â¶ÇÊûúÂú®Ê≠•È™§3‰∏≠ÂèëÁé∞Ëá™Â∑±Âπ∂ÈùûÊâÄÊúâÂ≠êËäÇÁÇπ‰∏≠ÊúÄÂ∞èÁöÑÔºåËØ¥ÊòéËá™Â∑±ËøòÊ≤°ÊúâËé∑ÂèñÂà∞ÈîÅ„ÄÇÊ≠§Êó∂ÂÆ¢Êà∑Á´ØÈúÄË¶ÅÊâæÂà∞ÊØîËá™Â∑±Â∞èÁöÑÈÇ£‰∏™ËäÇÁÇπÔºåÁÑ∂ÂêéÂØπÂÖ∂Ë∞ÉÁî®exist()ÊñπÊ≥ïÔºåÂêåÊó∂Ê≥®ÂÜå‰∫ã‰ª∂ÁõëÂê¨„ÄÇ5.‰πãÂêéÂΩìËøô‰∏™Ë¢´ÂÖ≥Ê≥®ÁöÑËäÇÁÇπË¢´ÁßªÈô§‰∫ÜÔºåÂÆ¢Êà∑Á´Ø‰ºöÊî∂Âà∞Áõ∏Â∫îÁöÑÈÄöÁü•„ÄÇËøô‰∏™Êó∂ÂÄôÂÆ¢Êà∑Á´ØÈúÄË¶ÅÂÜçÊ¨°Ë∞ÉÁî®getChildren(‚Äúlocknode‚Äù)ÊñπÊ≥ïÊù•Ëé∑ÂèñÊâÄÊúâÂ∑≤ÁªèÂàõÂª∫ÁöÑÂ≠êËäÇÁÇπÔºåÁ°Æ‰øùËá™Â∑±Á°ÆÂÆûÊòØÊúÄÂ∞èÁöÑËäÇÁÇπ‰∫ÜÔºåÁÑ∂ÂêéËøõÂÖ•Ê≠•È™§3„ÄÇ 5.generate brokerId12config.brokerId = getBrokerIdthis.logIdent = "[Kafka Server " + config.brokerId + "], " Áï• 6.create and configure metricsÂÜÖÈÉ®Áä∂ÊÄÅÁöÑÁõëÊéßÊ®°Âùó12345678val reporters = config.getConfiguredInstances(KafkaConfig.MetricReporterClassesProp, classOf[MetricsReporter], Map[String, AnyRef](KafkaConfig.BrokerIdProp -&gt; (config.brokerId.toString)).asJava)reporters.add(new JmxReporter(jmxPrefix))val metricConfig = KafkaServer.metricConfig(config)metrics = new Metrics(metricConfig, reporters, time, true)quotaManagers = QuotaFactory.instantiate(config, metrics, time)notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala) ‰ªéÈÖçÁΩÆÂèÇÊï∞metric.reporters Ëé∑ÂèñÁõëÊéßÁ±ª Ê≠§Â§ÑÊúâ‰∏™Â∞èÊ¶ÇÂøµ;ÈõÜÂêàÂÖÅËÆ∏‰ΩøÁî®asScalaÂíåasJavaÊñπÊ≥ïÊù•ÂÅöscalaÂíåjava‰πãÈó¥ÁöÑËΩ¨Êç¢ metricConfigÈáåÂ∞ÅË£Ö‰∫Ümetrics.num.samplesÔºà Áî®‰∫éÁª¥Êä§metricsÁöÑÊ†∑Êú¨Êï∞Ôºâmetrics.recording.levelmetrics.sample.window.msÔºàmetricsÁ≥ªÁªüÁª¥Êä§ÂèØÈÖçÁΩÆÁöÑÊ†∑Êú¨Êï∞ÈáèÔºåÂú®‰∏Ä‰∏™ÂèØ‰øÆÊ≠£ÁöÑwindow size„ÄÇËøôÈ°πÈÖçÁΩÆÈÖçÁΩÆ‰∫ÜÁ™óÂè£Â§ßÂ∞èÔºå‰æãÂ¶Ç„ÄÇÊàë‰ª¨ÂèØËÉΩÂú®30sÁöÑÊúüÈó¥Áª¥Êä§‰∏§‰∏™Ê†∑Êú¨„ÄÇÂΩì‰∏Ä‰∏™Á™óÂè£Êé®Âá∫ÂêéÔºåÊàë‰ª¨‰ºöÊì¶Èô§Âπ∂ÈáçÂÜôÊúÄËÄÅÁöÑÁ™óÂè£Ôºâ Â∞èÊ¶ÇÂøµÔºö4ÁßçÊìç‰ΩúÁ¨¶ÁöÑÂå∫Âà´ÂíåËÅîÁ≥ª:: ËØ•ÊñπÊ≥ïË¢´Áß∞‰∏∫consÔºåÊÑè‰∏∫ÊûÑÈÄ†ÔºåÂêëÈòüÂàóÁöÑÂ§¥ÈÉ®ËøΩÂä†Êï∞ÊçÆÔºåÂàõÈÄ†Êñ∞ÁöÑÂàóË°®„ÄÇÁî®Ê≥ï‰∏∫ x::list,ÂÖ∂‰∏≠x‰∏∫Âä†ÂÖ•Âà∞Â§¥ÈÉ®ÁöÑÂÖÉÁ¥†ÔºåÊó†ËÆ∫xÊòØÂàóË°®‰∏éÂê¶ÔºåÂÆÉÈÉΩÂè™Â∞ÜÊàê‰∏∫Êñ∞ÁîüÊàêÂàóË°®ÁöÑÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†Ôºå‰πüÂ∞±ÊòØËØ¥Êñ∞ÁîüÊàêÁöÑÂàóË°®ÈïøÂ∫¶‰∏∫listÁöÑÈïøÂ∫¶Ôºã1(BTW Ôºå x::listÁ≠â‰ª∑‰∫élist.::(x)):+Âíå+: ‰∏§ËÄÖÁöÑÂå∫Âà´Âú®‰∫é:+ÊñπÊ≥ïÁî®‰∫éÂú®Â∞æÈÉ®ËøΩÂä†ÂÖÉÁ¥†Ôºå+:ÊñπÊ≥ïÁî®‰∫éÂú®Â§¥ÈÉ®ËøΩÂä†ÂÖÉÁ¥†ÔºåÂíå::ÂæàÁ±ª‰ººÔºå‰ΩÜÊòØ::ÂèØ‰ª•Áî®‰∫épattern match ÔºåËÄå+:Âàô‰∏çË°å. ÂÖ≥‰∫é+:Âíå:+,Âè™Ë¶ÅËÆ∞‰ΩèÂÜíÂè∑Ê∞∏ËøúÈù†ËøëÈõÜÂêàÁ±ªÂûãÂ∞±OK‰∫Ü„ÄÇ++ ËØ•ÊñπÊ≥ïÁî®‰∫éËøûÊé•‰∏§‰∏™ÈõÜÂêàÔºålist1++list2::: ËØ•ÊñπÊ≥ïÂè™ËÉΩÁî®‰∫éËøûÊé•‰∏§‰∏™ListÁ±ªÂûãÁöÑÈõÜÂêà 7.start log manager12345678logManager = createLogManager(zkUtils.zkClient, brokerState)logManager.startup()metadataCache = new MetadataCache(config.brokerId)credentialProvider = new CredentialProvider(config.saslEnabledMechanisms)socketServer = new SocketServer(config, metrics, time, credentialProvider)socketServer.startup() Ê†πÊçÆ‰∏ÄÁ≥ªÂàóÈÖçÁΩÆÂèÇÊï∞ÔºåÂêØÂä®LogManager„ÄÇËØ¶ÊÉÖËßÅkafka.log.CleanerConfigÂíåkafka.log.LogManager startupÂàõÂª∫‰∫Ü4‰∏™Á∫øÁ®ãÔºåÂàÜÂà´Ë¥üË¥£ÂàõÂª∫Êó•ÂøóÔºåÂÜôÊó•ÂøóÔºåÊ£ÄÁ¥¢Êó•ÂøóÔºåÊ∏ÖÁêÜÊó•Âøó1234567891011121314151617181920212223242526272829def startup() &#123; /* Schedule the cleanup task to delete old logs */ if(scheduler != null) &#123; info("Starting log cleanup with a period of %d ms.".format(retentionCheckMs)) scheduler.schedule("kafka-log-retention", cleanupLogs, delay = InitialTaskDelayMs, period = retentionCheckMs, TimeUnit.MILLISECONDS) info("Starting log flusher with a default period of %d ms.".format(flushCheckMs)) scheduler.schedule("kafka-log-flusher", flushDirtyLogs, delay = InitialTaskDelayMs, period = flushCheckMs, TimeUnit.MILLISECONDS) scheduler.schedule("kafka-recovery-point-checkpoint", checkpointRecoveryPointOffsets, delay = InitialTaskDelayMs, period = flushCheckpointMs, TimeUnit.MILLISECONDS) scheduler.schedule("kafka-delete-logs", deleteLogs, delay = InitialTaskDelayMs, period = defaultConfig.fileDeleteDelayMs, TimeUnit.MILLISECONDS) &#125; if(cleanerConfig.enableCleaner) cleaner.startup() &#125; 8.start replica manager123replicaManager = new ReplicaManager(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower)replicaManager.startup() ÂêØÂä®isr-expirationÁ∫øÁ®ãÂêØÂä®isr-change-propagationÁ∫øÁ®ãÂú®/controller‰∏ãÂª∫‰∫Ü‰∏Ä‰∏™ÁõëÂê¨ Ê≠§Â§ÑÊúâ‰∏™Â∞èÊäÄÂ∑ß12345678def inLock[T](lock: Lock)(fun: =&gt; T): T = &#123; lock.lock() try &#123; fun &#125; finally &#123; lock.unlock() &#125; &#125; : =&gt;Ê≥®ÊÑè:ÂêéÈù¢Ë¶ÅÊúâÁ©∫Ê†ºÔºåÊ≠§Â§ÑÊ†áÊòéË∞ÉÁî®ÁöÑÊó∂ÂÄôÊâçÊâßË°åÔºåÂê¶ÂàôÂú®Áî®inlockÁöÑÂáΩÊï∞Êó∂ÂÄôfunÂ∑≤ÁªèÂú®ÈîÅÂ§ñÈù¢ÊâßË°å‰∫Ü. ÂèÇËÄÉÔºöhttp://www.jianshu.com/p/f53e0b54a44a 9.start kafka controller1234kafkaController = new KafkaController(config, zkUtils, brokerState, time, metrics, threadNamePrefix)kafkaController.startup()adminManager = new AdminManager(config, metrics, metadataCache, zkUtils) 123456789def startup() = &#123; inLock(controllerContext.controllerLock) &#123; info("Controller starting up") registerSessionExpirationListener() isRunning = true controllerElector.startup info("Controller startup complete") &#125;&#125; ÂàÜÊîØ1.registerSessionExpirationListener-&gt;SessionExpirationListener-&gt;handleNewSession ÂΩì‰ºöËØùË∂ÖÊó∂ÔºåÈáçÊñ∞ËøûÊé•‰∏äÁöÑÊó∂ÂÄôÔºåË∞ÉÁî®‰πãÂâçÊ≥®ÂÜåÂú®ZookeeperLeaderElectorÁöÑonControllerResignationÂáΩÊï∞controllerElector.elect ÈáçÊñ∞ÈÄâ‰∏æ ÂàÜÊîØ2.ZookeeperLeaderElector-&gt;ÔºàonControllerFailoverÔºåonControllerResignationÔºâ-&gt;LeaderChangeListenercontrollerElectorÂ∞±ÊòØZookeeperLeaderElector ÊòØkafkaÁöÑÈÄâ‰∏æÊú∫Âà∂ZookeeperLeaderElectorÔºöÈÄöËøázkÂàõÂª∫Ephemeral NodeÔºà‰∏¥Êó∂ËäÇÁÇπÔºâÁöÑÊñπÂºèÊù•ËøõË°åÈÄâ‰∏æÔºåÂç≥Â¶ÇÊûúÂ≠òÂú®Âπ∂ÂèëÊÉÖÂÜµ‰∏ãÂêëzkÁöÑÂêå‰∏Ä‰∏™Ë∑ØÂæÑÂàõÂª∫nodeÁöÑËØùÔºåÊúâ‰∏îÂè™Êúâ1‰∏™ÂÆ¢Êà∑Á´Ø‰ºöÂàõÂª∫ÊàêÂäüÔºåÂÖ∂ÂÆÉÂÆ¢Êà∑Á´ØÂàõÂª∫Â§±Ë¥•Ôºå‰ΩÜÊòØÂΩìÂàõÂª∫ÊàêÂäüÁöÑÂÆ¢Êà∑Á´ØÂíåzkÁöÑÈìæÊé•Êñ≠ÂºÄ‰πãÂêéÔºåËøô‰∏™node‰πü‰ºöÊ∂àÂ§±ÔºåÂÖ∂ÂÆÉÁöÑÂÆ¢Êà∑Á´Ø‰ªéËÄåÁªßÁª≠Á´û‰∫â 123456def startup &#123; inLock(controllerContext.controllerLock) &#123; controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener) elect &#125; &#125; 1.ÁõëÂê¨electionPathÔºà/controllerÔºâ2.electÈÄâ‰∏æ 1234val ControllerPath = "/controller"val electString = Json.encode(Map("version" -&gt; 1, "brokerid" -&gt; brokerId, "timestamp" -&gt; timestamp))val zkCheckedEphemeral = new ZKCheckedEphemeral(electionPath,lectString, controllerContext.zkUtils.zkConnection.getZookeeper, JaasUtils.isZkSecurityEnabled())leaderId = getControllerID Ê≠§Â§Ñ‰ºöÂú®/controller ‰∏ãÈù¢ÂÜô‰∏Ä‰∏™Á±ª‰ººÂ¶Ç‰∏ãÁöÑÂÜÖÂÆπÔºö{‚Äúversion‚Äù:1,‚Äùbrokerid‚Äù:102,‚Äùtimestamp‚Äù:‚Äù1495880001272‚Äù}ÈÄöËøágetControllerIDËé∑ÂèñÂΩìÂâçÁöÑleaderIdÁÑ∂ÂêéÈÄöËøáamILeaderÁúãËá™Â∑±ÊòØÂê¶ÊòØleader ZookeeperLeaderElecto12345678910111213141516171819202122232425262728293031323334353637383940class LeaderChangeListener extends IZkDataListener with Logging &#123; /** * Called when the leader information stored in zookeeper has changed. Record the new leader in memory * @throws Exception On any error. */ @throws[Exception] def handleDataChange(dataPath: String, data: Object) &#123; val shouldResign = inLock(controllerContext.controllerLock) &#123; val amILeaderBeforeDataChange = amILeader leaderId = KafkaController.parseControllerId(data.toString) info("New leader is %d".format(leaderId)) // The old leader needs to resign leadership if it is no longer the leader amILeaderBeforeDataChange &amp;&amp; !amILeader &#125; if (shouldResign) onResigningAsLeader() &#125; /** * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader * @throws Exception * On any error. */ @throws[Exception] def handleDataDeleted(dataPath: String) &#123; val shouldResign = inLock(controllerContext.controllerLock) &#123; debug("%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader" .format(brokerId, dataPath)) amILeader &#125; if (shouldResign) onResigningAsLeader() inLock(controllerContext.controllerLock) &#123; elect &#125; &#125; &#125; Â¶ÇÊûúËäÇÁÇπ‰∏ãÁ∫ø‰ºöË∞ÉÁî®handleDataDeleted„ÄÇÁúãËá™Â∑±ÊòØÂê¶ÊòØleaderÔºåÂ¶ÇÊûúÊòØÈúÄË¶ÅÂÖàÈÄÄ‰ºëonResigningAsLeader„ÄÇÁÑ∂ÂêéÈÄâ‰∏æ123456789101112131415161718192021222324try &#123; val zkCheckedEphemeral = new ZKCheckedEphemeral(electionPath, electString, controllerContext.zkUtils.zkConnection.getZookeeper, JaasUtils.isZkSecurityEnabled()) zkCheckedEphemeral.create() info(brokerId + " successfully elected as leader") leaderId = brokerId onBecomingLeader() &#125; catch &#123; case _: ZkNodeExistsException =&gt; // If someone else has written the path, then leaderId = getControllerID if (leaderId != -1) debug("Broker %d was elected as leader instead of broker %d".format(leaderId, brokerId)) else warn("A leader has been elected but just resigned, this will result in another round of election") case e2: Throwable =&gt; error("Error while electing or becoming leader on broker %d".format(brokerId), e2) resign() &#125; amILeader ÂàõÂª∫‰∏¥Êó∂ËäÇÁÇπ onControllerFailover:12345678910111213141516171819202122232425262728293031323334353637383940def onControllerFailover() &#123; if(isRunning) &#123; info("Broker %d starting become controller state transition".format(config.brokerId)) readControllerEpochFromZookeeper() incrementControllerEpoch(zkUtils.zkClient) // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks registerReassignedPartitionsListener() registerIsrChangeNotificationListener() registerPreferredReplicaElectionListener() partitionStateMachine.registerListeners() replicaStateMachine.registerListeners() initializeControllerContext() // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and // partitionStateMachine.startup(). sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq) replicaStateMachine.startup() partitionStateMachine.startup() // register the partition change listeners for all existing topics on failover controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic)) info("Broker %d is ready to serve as the new controller with epoch %d".format(config.brokerId, epoch)) maybeTriggerPartitionReassignment() maybeTriggerPreferredReplicaElection() if (config.autoLeaderRebalanceEnable) &#123; info("starting the partition rebalance scheduler") autoRebalanceScheduler.startup() autoRebalanceScheduler.schedule("partition-rebalance-thread", checkAndTriggerPartitionRebalance, 5, config.leaderImbalanceCheckIntervalSeconds.toLong, TimeUnit.SECONDS) &#125; deleteTopicManager.start() &#125; else info("Controller has been shut down, aborting startup/failover") &#125; Âú®/admin/reassign_partitionsÁõÆÂΩïÊ≥®ÂÜåPartitionsReassignedListenerÁõëÂê¨ÂáΩÊï∞Âú®/isr_change_notificationÁõÆÂΩïÊ≥®ÂÜåIsrChangeNotificationListenerÁõëÂê¨ÂáΩÊï∞Âú®/admin/preferred_replica_electionÁõÆÂΩïÊ≥®ÂÜåPreferredReplicaElectionListenerÁõëÂê¨ÂáΩÊï∞Âú®/brokers/topicsÁõÆÂΩïÊ≥®ÂÜåTopicChangeListenerÁõëÂê¨ÂáΩÊï∞Âú®/admin/delete_topicsÁõÆÂΩïÊ≥®ÂÜåDeleteTopicsListenerÁõëÂê¨ÂáΩÊï∞Âú®/brokers/idsÁõÆÂΩïÊ≥®ÂÜåBrokerChangeListenerÁõëÂê¨ÂáΩÊï∞ ÁõëÂê¨ÊòØË∞ÉÁî®zkÁöÑzkUtils.zkClient.subscribeChildChangesÂáΩÊï∞ÔºåÂèÇÊï∞ÊòØË∑ØÂæÑÂíåÁõëÂê¨ÂáΩÊï∞ÁõëÂê¨ÂáΩÊï∞ÂÆûÁé∞IZkChildListenerÊé•Âè£ÂÆûÁé∞handleChildChangeÊñπÊ≥ï ÂàùÂßãÂåñControllerContext‰∏ä‰∏ãÊñá,ÈáåÈù¢ÂåÖÂê´Â≠òÊ¥ªÁöÑbrokerÔºåÊâÄÊúâ‰∏ªÈ¢òÔºåÂàÜÂå∫ÂâØÊú¨ÔºåÂàÜÂå∫ÁöÑleaderÂíåÂ∑≤Áªè‰∏ãÁ∫øÁöÑbroker„ÄÇÊõ¥Êñ∞leaderÂíåisrÁºìÂ≠ò„ÄÇÂêØÂä®ControllerChannelManagerÂàùÂßãÂåñÊâÄÊúâÁöÑreplicaÁä∂ÊÄÅÂàùÂßãÂåñÊâÄÊúâÁöÑpartitionÁä∂ÊÄÅÂ¶ÇÊûúauto.leader.rebalance.enable ‰∏∫true‰ºöÂêØÂä®RebalanceË∞ÉÂ∫¶ÊúÄÂêéÂà†Èô§‰∏ªÈ¢ò ÈÄöËøáreplicaStateMachineÂàùÂßãÂåñÊâÄÊúâÁöÑreplicaÁä∂ÊÄÅreplicaStateMachineÁöÑhandleStateChanges12345678910111213def handleStateChanges(replicas: Set[PartitionAndReplica], targetState: ReplicaState, callbacks: Callbacks = (new CallbackBuilder).build) &#123; if(replicas.nonEmpty) &#123; info("Invoking state change to %s for replicas %s".format(targetState, replicas.mkString(","))) try &#123; brokerRequestBatch.newBatch() replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks)) brokerRequestBatch.sendRequestsToBrokers(controller.epoch) &#125;catch &#123; case e: Throwable =&gt; error("Error while moving some replicas to %s state".format(targetState), e) &#125; &#125; &#125; ÈÄöËøápartitionStateMachineÂàùÂßãÂåñÊâÄÊúâÁöÑpartitionÁä∂ÊÄÅ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private def handleStateChange(topic: String, partition: Int, targetState: PartitionState, leaderSelector: PartitionLeaderSelector, callbacks: Callbacks) &#123; val topicAndPartition = TopicAndPartition(topic, partition) if (!hasStarted.get) throw new StateChangeFailedException(("Controller %d epoch %d initiated state change for partition %s to %s failed because " + "the partition state machine has not started") .format(controllerId, controller.epoch, topicAndPartition, targetState)) val currState = partitionState.getOrElseUpdate(topicAndPartition, NonExistentPartition) try &#123; targetState match &#123; case NewPartition =&gt; // pre: partition did not exist before this assertValidPreviousStates(topicAndPartition, List(NonExistentPartition), NewPartition) partitionState.put(topicAndPartition, NewPartition) val assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(",") stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, assignedReplicas)) // post: partition has been assigned replicas case OnlinePartition =&gt; assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OnlinePartition) partitionState(topicAndPartition) match &#123; case NewPartition =&gt; // initialize leader and isr path for new partition initializeLeaderAndIsrForPartition(topicAndPartition) case OfflinePartition =&gt; electLeaderForPartition(topic, partition, leaderSelector) case OnlinePartition =&gt; // invoked when the leader needs to be re-elected electLeaderForPartition(topic, partition, leaderSelector) case _ =&gt; // should never come here since illegal previous states are checked above &#125; partitionState.put(topicAndPartition, OnlinePartition) val leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader stateChangeLogger.trace("Controller %d epoch %d changed partition %s from %s to %s with leader %d" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader)) // post: partition has a leader case OfflinePartition =&gt; // pre: partition should be in New or Online state assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OfflinePartition) // should be called when the leader for a partition is no longer alive stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState)) partitionState.put(topicAndPartition, OfflinePartition) // post: partition has no alive leader case NonExistentPartition =&gt; // pre: partition should be in Offline state assertValidPreviousStates(topicAndPartition, List(OfflinePartition), NonExistentPartition) stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState)) partitionState.put(topicAndPartition, NonExistentPartition) // post: partition state is deleted from all brokers and zookeeper &#125; &#125; catch &#123; case t: Throwable =&gt; stateChangeLogger.error("Controller %d epoch %d initiated state change for partition %s from %s to %s failed" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState), t) &#125; &#125; PartitionStateMachineÂÆûÁé∞‰∫ÜtopicÁöÑÂàÜÂå∫Áä∂ÊÄÅÂàáÊç¢ÂäüËÉΩÔºåPartitionÂ≠òÂú®ÁöÑÁä∂ÊÄÅÂ¶Ç‰∏ãÔºöNewPartition ÂàÜÂå∫‰πãÂâç‰∏çÂ≠òÂú®ÔºåÂàõÂª∫ÂêéË¢´ÂàÜÈÖç‰∫ÜreplicasÔºå‰ΩÜÊòØËøòÊ≤°Êúâleader/isrOnlinePartition partitionÂú®replicas‰∏≠ÈÄâ‰∏æÊüê‰∏™Êàê‰∏∫leader‰πãÂêéOfflinePartition partitionÁöÑreplicas‰∏≠ÁöÑleader‰∏ãÁ∫ø‰πãÂêéÔºåÊ≤°ÊúâÈáçÊñ∞ÈÄâ‰∏æÊñ∞ÁöÑleader‰πãÂâç Êàñ partitionÂàõÂª∫‰πãÂêéÁõ¥Êé•Ë¢´‰∏ãÁ∫øNonExistentPartition partitionÈáçÊù•Ê≤°ÊúâË¢´ÂàõÂª∫ Êàñ partitionÂàõÂª∫‰πãÂêéË¢´Âà†Èô§ scalaÂ∞èÁü•ËØÜÔºömkString1234567891011121314151617scala &gt; val a = Array("apple", "banana", "cherry")a: Array[String] = Array(apple, banana, cherry)scala &gt; a.mkString(",")res2: String = apple,banana,cherryscala&gt; a.mkString("[", ", ", "]")res3: String = [apple, banana, cherry]Â¶ÇÊûúÊòØÊï∞ÁªÑÈúÄË¶ÅÂÖàÂ±ïÂºÄÊï∞ÁªÑscala&gt; val b = Array(Array("a", "b"), Array("c", "d"))b: Array[Array[String]] = Array(Array(a, b), Array(c, d))ÈîôËØØÁöÑscala&gt; b.mkString(",")res4: String = [Ljava.lang.String;@64a9fca7,[Ljava.lang.String;@22f756c5Ê≠£Á°ÆÁöÑscala&gt; b.flatten.mkString(",")res5: String = a,b,c,d OnlinePartition ÔºöÊ£ÄÊü•ÂâçÁΩÆÁä∂ÊÄÅÊòØÂê¶‰∏∫NewPartition, OnlinePartition, OfflinePartition‰∏≠ÁöÑ‰∏ÄÁßçÔºå1.Â¶ÇÊûúÊòØNewPartitionÔºö1234567891011121314151617181920212223242526272829303132333435363738394041424344private def initializeLeaderAndIsrForPartition(topicAndPartition: TopicAndPartition) &#123; val replicaAssignment = controllerContext.partitionReplicaAssignment(topicAndPartition) val liveAssignedReplicas = replicaAssignment.filter(r =&gt; controllerContext.liveBrokerIds.contains(r)) liveAssignedReplicas.size match &#123; case 0 =&gt; val failMsg = ("encountered error during state change of partition %s from New to Online, assigned replicas are [%s], " + "live brokers are [%s]. No assigned replica is alive.") .format(topicAndPartition, replicaAssignment.mkString(","), controllerContext.liveBrokerIds) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) case _ =&gt; debug("Live assigned replicas for partition %s are: [%s]".format(topicAndPartition, liveAssignedReplicas)) // make the first replica in the list of assigned replicas, the leader //Ê†πÊçÆpartitionReplicaAssignment‰∏≠‰ø°ÊÅØÈÄâÊã©Á¨¨‰∏Ä‰∏™liveÁöÑreplica‰∏∫leader,ÂÖ∂‰Ωô‰∏∫isr val leader = liveAssignedReplicas.head val leaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(new LeaderAndIsr(leader, liveAssignedReplicas.toList), controller.epoch) debug("Initializing leader and isr for partition %s to %s".format(topicAndPartition, leaderIsrAndControllerEpoch)) try &#123; //Â∞ÜleaderÂíåisrÊåÅ‰πÖÂåñÂà∞zookeeper zkUtils.createPersistentPath( getTopicPartitionLeaderAndIsrPath(topicAndPartition.topic, topicAndPartition.partition), zkUtils.leaderAndIsrZkData(leaderIsrAndControllerEpoch.leaderAndIsr, controller.epoch)) // NOTE: the above write can fail only if the current controller lost its zk session and the new controller // took over and initialized this partition. This can happen if the current controller went into a long // GC pause //Êõ¥Êñ∞controllerContext‰∏≠ÁöÑpartitionLeadershipInfo controllerContext.partitionLeadershipInfo.put(topicAndPartition, leaderIsrAndControllerEpoch) //Â∞ÅË£ÖÂèëÈÄÅÁªôËøô‰∫õreplicaÊâÄÂú®ÁöÑbrokerÁöÑLeaderAndIsrRequestËØ∑Ê±ÇÔºå‰∫§Áî±ControllerBrokerRequestBatch(brokerRequestBatch)Â§ÑÁêÜ brokerRequestBatch.addLeaderAndIsrRequestForBrokers(liveAssignedReplicas, topicAndPartition.topic, topicAndPartition.partition, leaderIsrAndControllerEpoch, replicaAssignment) &#125; catch &#123; case _: ZkNodeExistsException =&gt; // read the controller epoch val leaderIsrAndEpoch = ReplicationUtils.getLeaderIsrAndEpochForPartition(zkUtils, topicAndPartition.topic, topicAndPartition.partition).get val failMsg = ("encountered error while changing partition %s's state from New to Online since LeaderAndIsr path already " + "exists with value %s and controller epoch %d") .format(topicAndPartition, leaderIsrAndEpoch.leaderAndIsr.toString(), leaderIsrAndEpoch.controllerEpoch) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) &#125; &#125; &#125; 2.Â¶ÇÊûúÊòØOfflinePartitionÔºåOnlinePartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071def electLeaderForPartition(topic: String, partition: Int, leaderSelector: PartitionLeaderSelector) &#123; val topicAndPartition = TopicAndPartition(topic, partition) // handle leader election for the partitions whose leader is no longer alive stateChangeLogger.trace("Controller %d epoch %d started leader election for partition %s" .format(controllerId, controller.epoch, topicAndPartition)) try &#123; var zookeeperPathUpdateSucceeded: Boolean = false var newLeaderAndIsr: LeaderAndIsr = null var replicasForThisPartition: Seq[Int] = Seq.empty[Int] while(!zookeeperPathUpdateSucceeded) &#123; val currentLeaderIsrAndEpoch = getLeaderIsrAndEpochOrThrowException(topic, partition) val currentLeaderAndIsr = currentLeaderIsrAndEpoch.leaderAndIsr val controllerEpoch = currentLeaderIsrAndEpoch.controllerEpoch if (controllerEpoch &gt; controller.epoch) &#123; val failMsg = ("aborted leader election for partition [%s,%d] since the LeaderAndIsr path was " + "already written by another controller. This probably means that the current controller %d went through " + "a soft failure and another controller was elected with epoch %d.") .format(topic, partition, controllerId, controllerEpoch) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) &#125; // elect new leader or throw exception val (leaderAndIsr, replicas) = leaderSelector.selectLeader(topicAndPartition, currentLeaderAndIsr) val (updateSucceeded, newVersion) = ReplicationUtils.updateLeaderAndIsr(zkUtils, topic, partition, leaderAndIsr, controller.epoch, currentLeaderAndIsr.zkVersion) //Ê†πÊçÆ‰∏çÂêåÁöÑleaderSelectorÈÄâ‰∏æÊñ∞ÁöÑleaderÔºåËøôÈáå‰∏ÄËà¨Ë∞ÉÁî®ÁöÑÊòØOfflinePartitionLeaderSelector newLeaderAndIsr = leaderAndIsr newLeaderAndIsr.zkVersion = newVersion //Â∞ÜleaderÂíåisrÊåÅ‰πÖÂåñÂà∞zookeeper zookeeperPathUpdateSucceeded = updateSucceeded replicasForThisPartition = replicas &#125; val newLeaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(newLeaderAndIsr, controller.epoch) // update the leader cache //Êõ¥Êñ∞controllerContext‰∏≠ÁöÑpartitionLeadershipInfo controllerContext.partitionLeadershipInfo.put(TopicAndPartition(topic, partition), newLeaderIsrAndControllerEpoch) stateChangeLogger.trace("Controller %d epoch %d elected leader %d for Offline partition %s" .format(controllerId, controller.epoch, newLeaderAndIsr.leader, topicAndPartition)) val replicas = controllerContext.partitionReplicaAssignment(TopicAndPartition(topic, partition)) // store new leader and isr info in cache //Â∞ÅË£ÖÂèëÈÄÅÁªôËøô‰∫õreplicaÊâÄÂú®ÁöÑbrokerÁöÑLeaderAndIsrRequestËØ∑Ê±ÇÔºå‰∫§Áî±ControllerBrokerRequestBatch(brokerRequestBatch)Â§ÑÁêÜ brokerRequestBatch.addLeaderAndIsrRequestForBrokers(replicasForThisPartition, topic, partition, newLeaderIsrAndControllerEpoch, replicas) &#125; catch &#123; case _: LeaderElectionNotNeededException =&gt; // swallow case nroe: NoReplicaOnlineException =&gt; throw nroe case sce: Throwable =&gt; val failMsg = "encountered error while electing leader for partition %s due to: %s.".format(topicAndPartition, sce.getMessage) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg, sce) &#125; debug("After leader election, leader cache is updated to %s".format(controllerContext.partitionLeadershipInfo.map(l =&gt; (l._1, l._2)))) &#125;¬∑¬∑¬∑Âú®brokers/topics/***(ÂÖ∑‰ΩìÁöÑtopicÂêçÂ≠ó)/ÁõÆÂΩï‰∏ãÊ≥®ÂÜåPartitionModificationsListener-&gt;AddPartitionsListenerÁõëÂê¨ÈÄöËøáÂ§ÑÁêÜ‰πãÂâçÂêØÂä®Áïô‰∏ãÁöÑpartitionÈáçÂàÜÈÖçÁöÑÊÉÖÂÜµÂ§ÑÁêÜ‰πãÂâçÂêØÂä®Áïô‰∏ãÁöÑreplicaÈáçÊñ∞ÈÄâ‰∏æÁöÑÊÉÖÂÜµÂêëÂÖ∂ÂÆÉKafkaServerÂèëÈÄÅÈõÜÁæ§topicÁöÑÂÖÉÊï∞ÊçÆ‰ø°ÊÅØÂ∑≤ËøõË°åÊï∞ÊçÆÁöÑÂêåÊ≠•Êõ¥Êñ∞Ê†πÊçÆÈÖçÁΩÆÊòØÂê¶ÂºÄÂêØËá™Âä®ÂùáË°°ÂºÄÂßãÂà†Èô§topic### 10.start group coordinator```scala// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issuegroupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM)groupCoordinator.startup() 11.Get the authorizer and initialize it if one is specified.12345authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt; val authZ = CoreUtils.createObject[Authorizer](authorizerClassName) authZ.configure(config.originals()) authZ&#125; 12.start processing requests1234567apis = new KafkaApis(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator, kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers, clusterId, time)requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, time, config.numIoThreads)Mx4jLoader.maybeLoad()&#125; 13.start dynamic config manager123456 dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers),ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers),ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider),ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers))// Create the config manager. start listening to notificationsdynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers)dynamicConfigManager.startup()&#125; 14.tell everyone we are alive12345678910111213val listeners = config.advertisedListeners.map &#123; endpoint =&gt; if (endpoint.port == 0) endpoint.copy(port = socketServer.boundPort(endpoint.listenerName)) else endpoint&#125;kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack, config.interBrokerProtocolVersion)kafkaHealthcheck.startup()// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint itcheckpointBrokerId(config.brokerId)&#125; kafkaHealthcheck.startup()12345678910111213141516171819202122232425def startup() &#123; zkUtils.zkClient.subscribeStateChanges(sessionExpireListener) register() &#125; /** * Register this broker as "alive" in zookeeper */ def register() &#123; val jmxPort = System.getProperty("com.sun.management.jmxremote.port", "-1").toInt val updatedEndpoints = advertisedEndpoints.map(endpoint =&gt; if (endpoint.host == null || endpoint.host.trim.isEmpty) endpoint.copy(host = InetAddress.getLocalHost.getCanonicalHostName) else endpoint ) // the default host and port are here for compatibility with older clients that only support PLAINTEXT // we choose the first plaintext port, if there is one // or we register an empty endpoint, which means that older clients will not be able to connect val plaintextEndpoint = updatedEndpoints.find(_.securityProtocol == SecurityProtocol.PLAINTEXT).getOrElse( new EndPoint(null, -1, null, null)) zkUtils.registerBrokerInZk(brokerId, plaintextEndpoint.host, plaintextEndpoint.port, updatedEndpoints, jmxPort, rack, interBrokerProtocolVersion) &#125; Ê≥®ÂÜåÊñ∞ÁöÑbrokerid 15.register broker metrics.12 registerStats()&#125;]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafkaÊ∫êÁ†ÅÈòÖËØª]]></title>
    <url>%2F2017%2F05%2F02%2Fkafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[‰∏ãËΩΩkafkaÁöÑÊ∫êÁ†ÅÂåÖÔºöhttp://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz ÂÆâË£Ögradle 1.ÂÆòÁΩë‰∏ãËΩΩhttps://gradle.org/releasesÈÄâÊã©ÂΩìÊó∂ÊúÄÊñ∞ÁâàÊú¨Ôºöhttps://downloads.gradle.org/distributions/gradle-3.5-all.zip 2.Ëß£ÂéãËß£ÂéãÂà∞D:\tool\gradle-3.5 3.ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèGRADLE_HOMED:\tool\gradle-3.5 PathËøΩÂä†;%GRADLE_HOME%\BIN;4.ÊµãËØïÔºögradle -v ÂÆâË£ÖÂèÇËÄÉ:http://blog.csdn.net/lizhitao/article/details/26875463 ÁºñËØëÔºöÔºàÊ≠§Â§ÑÂçö‰∏ªÁî®Êüê‰∫ë‰∏ªÊú∫ÁºñËØëÁöÑÔºåÂæàÂø´Ôºâgradle idea ÔºàÁºñËØëÂÆåÊää/root/.gradle/caches/modules-2‰∏ã‰∏ãËΩΩÁöÑÊñá‰ª∂ÊîæÂà∞Êàë‰ª¨ÁöÑÁéØÂ¢É‰∏≠C:\Users\Administrator.gradle\caches\modules-2 ÂÜçwindows‰∏äÁºñËØë Ôºâ Áî®IDEAÂ∑•ÂÖ∑ÊâìÂºÄ ÁõÆÂΩï‰ªãÁªçÔºö Ê®°ÂùóÂêç Âê´‰πâ admin ÁÆ°ÁêÜÂëòÊ®°ÂùóÔºåÊìç‰ΩúÂíåÁÆ°ÁêÜtopicÔºåparitionsÁõ∏ÂÖ≥ÔºåÂåÖÂê´create/delete topic,Êâ©Â±ïpatitions api ‰∏ªË¶ÅË¥üË¥£‰∫§‰∫íÊï∞ÊçÆÁöÑÁªÑË£ÖÔºåÂÆ¢Êà∑Á´Ø‰∏éÊúçÂä°Á´Ø‰∫§‰∫íÊï∞ÊçÆÁºñËß£Á†Å client ProducerËØªÂèñkafka brokerÂÖÉÊï∞ÊçÆ‰ø°ÊÅØÔºåtopicÂíåpartitionsÔºå‰ª•Âèäleader cluster ÂåÖÂê´Âá†‰∏™ÂÆû‰ΩìÁ±ªÔºåBroker,Cluster,Partition,Replica,Ëß£Èáä‰ªñ‰ª¨‰πãÈó¥ÂÖ≥Á≥ªÔºö ClusterÁî±Â§ö‰∏™brokerÁªÑÊàêÔºå‰∏Ä‰∏™BrokerÂåÖÂê´Â§ö‰∏™partitionÔºå‰∏Ä‰∏™topicÁöÑÊâÄÊúâpartitionsÂàÜÂ∏ÉÂú®‰∏çÂêåbrokerÁöÑ‰∏≠Ôºå‰∏Ä‰∏™ReplicaÂåÖÂê´Â§ö‰∏™Partition„ÄÇ common ÂºÇÂ∏∏Á±ªÂíåÈîôËØØÈ™åËØÅ consumer Ë¥üË¥£ÊâÄÊúâÂÆ¢Êà∑Á´ØÊ∂àË¥πËÄÖÊï∞ÊçÆÂíåÈÄªËæëÂ§ÑÁêÜ controller Ë¥üË¥£‰∏≠Â§ÆÊéßÂà∂Âô®ÈÄâ‰∏æÔºåpartitionÁöÑleaderÈÄâ‰∏æÔºåÂâØÊú¨ÂàÜÈÖçÔºåÂâØÊú¨ÈáçÊñ∞ÂàÜÈÖçÔºåpartitionÂíåreplicaÊâ©ÂÆπ„ÄÇ coordinator partitionÂàÜÈÖçÊú∫Âà∂ javaapi Êèê‰æõjavaÁöÑproducerÂíåconsumerÊé•Âè£api log KafkaÊñá‰ª∂Â≠òÂÇ®Ê®°ÂùóÔºåË¥üË¥£ËØªÂÜôÊâÄÊúâkafkaÁöÑtopicÊ∂àÊÅØÊï∞ÊçÆ„ÄÇ message Â∞ÅË£ÖÂ§ö‰∏™Ê∂àÊÅØÁªÑÊàê‰∏Ä‰∏™‚ÄúÊ∂àÊÅØÈõÜ‚ÄùÊàñÂéãÁº©Ê∂àÊÅØÈõÜ„ÄÇ metrics ÂÜÖÈÉ®Áä∂ÊÄÅÁöÑÁõëÊéßÊ®°Âùó network ÁΩëÁªú‰∫ã‰ª∂Â§ÑÁêÜÊ®°ÂùóÔºåË¥üË¥£Â§ÑÁêÜÂíåÊé•Êî∂ÂÆ¢Êà∑Á´ØËøûÊé• producer producerÂÆûÁé∞Ê®°ÂùóÔºåÂåÖÊã¨ÂêåÊ≠•ÂíåÂºÇÊ≠•ÂèëÈÄÅÊ∂àÊÅØ„ÄÇ security ÂÆâÂÖ® serializer Â∫èÂàóÂåñÊàñÂèçÂ∫èÂàóÂåñÂΩìÂâçÊ∂àÊÅØ server tools Â∑•ÂÖ∑Ê®°ÂùóÔºå ÂåÖÂê´a.ÂØºÂá∫ÂØπÂ∫îconsumerÁöÑoffsetÂÄº.b.ÂØºÂá∫LogSegments‰ø°ÊÅØÔºåÂΩìÂâçtopicÁöÑlogÂÜôÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØ.c.ÂØºÂá∫zk‰∏äÊâÄÊúâconsumerÁöÑoffsetÂÄº.d.‰øÆÊîπÊ≥®ÂÜåÂú®zkÁöÑconsumerÁöÑoffsetÂÄº.f.producerÂíåconsumerÁöÑ‰ΩøÁî®‰æãÂ≠ê. utils JsonÂ∑•ÂÖ∑Á±ªÔºåZkutilsÂ∑•ÂÖ∑Á±ªÔºåUtilsÂàõÂª∫Á∫øÁ®ãÂ∑•ÂÖ∑Á±ªÔºåKafkaSchedulerÂÖ¨ÂÖ±Ë∞ÉÂ∫¶Âô®Á±ªÔºåÂÖ¨ÂÖ±Êó•ÂøóÁ±ªÁ≠âÁ≠â„ÄÇ]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH phoenixÂÆâË£Ö]]></title>
    <url>%2F2017%2F04%2F18%2FCDH%20phoenix%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[È¶ñÂÖà‰∏ãËΩΩjdk Áï•ÂÆâË£Ömaven ‰∏ãËΩΩyumÊ∫êwget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo ÂÆâË£ÖmavenÔºöyum -y install apache-maven ÁºñËØëphoenixÊâæÂà∞ÊúÄÊñ∞ÁâàÊú¨ÁöÑphoenixhttps://github.com/chiastic-security/phoenix-for-cloudera/tree/4.8-HBase-1.2-cdh5.8 ‰∏ãËΩΩÔºàÂçö‰∏ª‰∏ãËΩΩÂà∞/soft‰∏ãÔºâÂπ∂ÁºñËØëmvn clean package -DskipTests -Dcdh.flume.version=1.6.0 ÊâìÂºÄË∑ØÂæÑÔºö/soft/phoenix-for-cloudera-4.8-HBase-1.2-cdh5.8/phoenix-assembly/target ÊâæÂà∞phoenix-4.8.0-cdh5.8.0.tar.gzÂ∞Üphoenix-4.8.0-cdh5.8.0‰∏≠ÁöÑphoenix-4.8.0-cdh5.8.0-server.jarÊã∑Ë¥ùÂà∞ÊØè‰∏Ä‰∏™RegionServer‰∏ã/opt/cloudera/parcels/CDH/lib/hbase/lib ÂêØÂä®Ôºö./sqlline.py slave1:2181/hbase Â¶ÇÊûúÊä•ÈîôÔºöError: org.apache.hadoop.hbase.DoNotRetryIOException: Class org.apache.phoenix.coprocessor.MetaDataEndpointImpl cannot be loaded Set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks at org.apache.hadoop.hbase.master.HMaster.warnOrThrowExceptionForFailure(HMaster.java:1741) at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1602) at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1531) at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:469) at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55682) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:185) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:165) (state=08000,code=101) ÈúÄË¶ÅÁÇπÂáªCDH-&gt;hbase -&gt;ÈÖçÁΩÆ-&gt;È´òÁ∫ßhbase-site.xml ÁöÑ HBase ÊúçÂä°È´òÁ∫ßÈÖçÁΩÆ‰ª£Á†ÅÊÆµÔºàÂÆâÂÖ®ÈòÄÔºâÊ∑ªÂä†hbase.table.sanity.checksfalse ÈáçÂêØÂç≥ÂèØÂ¶ÇÊûúÊÉ≥Ë¶ÅÂÆâË£ÖphonenixÂÖà‰∏ãËΩΩhttp://squirrel-sql.sourceforge.net/#installation ÂÆâË£ÖÔºàÂÆâË£ÖÁöÑÊó∂ÂÄôÂãæÈÄâimort-dataÂíåmysqlÔºâÊ≥®Ôºö‰∏ãËΩΩÂêéÁõ¥Êé•ÂÆâË£ÖjarÂåÖÂç≥ÂèØÔºå‰∏çË¶ÅËß£ÂéãÁº©Áî±‰∫éÊòØCDH ÊàëÊã∑Ë¥ù‰∫ÜÂ¶Ç‰∏ãjarÈò≤Áõólib‰∏ãÈù¢phoenix-core-4.8.0-cdh5.8.0.jarphoenix-4.8.0-cdh5.8.0-client.jarphoenix-pherf-4.8.0-cdh5.8.0-minimal.jar ÈìæÊé•ÂèÇËÄÉ;http://www.cnblogs.com/raphael5200/p/5260198.htmlÊúâÂõæ phoenix API http://phoenix.apache.org/language/functions.html http://phoenix.apache.org/language/index.html]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH oozie]]></title>
    <url>%2F2017%2F04%2F18%2FCDH%20oozie%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Ê≥®ÔºöÊú¨Ê¨°oozieÊòØÂÆâË£ÖÂà∞Ëá™ÂÆö‰πâÁöÑÈõÜÁæ§‰∏äÔºåÂõ†‰∏∫ÂÖ∂‰ªñËäÇÁÇπÈÉΩÊòØÁî®CDHÊâãÂä®ÂåÖÂÆâË£ÖÁöÑÔºå‰∏çÊòØCDHËá™Âä®‰∏ÄÈîÆÂºèÂÆâË£ÖÊñπÂºè ‰∏ãËΩΩCDHÁöÑoozie ÔºåÂ∑≤ÁªèÁºñËØëÂ•ΩÔºåÊó†ÈúÄÁºñËØëÊú¨Ê¨°ÂÆâË£ÖÈÄâÊã©‰∫Ü‰∏ãÈù¢ÁΩëÂùÄÁöÑCDH 5.7.0 ÁâàÊú¨CDHhttps://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball_57.html#concept_0vc_ddn_yk LinuxÊú∫Âô®‰∏äÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§‰∏ãËΩΩÔºöwget https://archive.cloudera.com/cdh5/cdh/5/oozie-4.1.0-cdh5.7.0.tar.gz Ëß£ÂéãÂêéÂ∞ÜÂ¶Ç‰∏ãÂÜÖÂÆπÊ∑ªÂä†Âà∞core-site.xml12345678910vi $HADOOP_HOME/etc/hadoop/core-site.xml &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;hadoop000,hadoop001,hadoop002&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; Ê≥®ÊÑèproxyuserÂêéÈù¢ÈÇ£‰∏™rootÊòØ‰ΩøÁî®oozieÁöÑÁî®Êà∑Ôºåhadoop000,hadoop001,hadoop002ÊòØÈõÜÁæ§ÁöÑ‰∏ªÊú∫ Â∞ÜoozieÊ∑ªÂä†Âà∞ÁéØÂ¢ÉÂèòÈáè12export OOIZE_HOME=/home/app/oozie-4.1.0-cdh5.7.0export PATH=$PATH:$OOIZE_HOME/bin:$PATH Ê∑ªÂä†‰ª•‰∏ãÂÜÖÂÆπÂà∞oozieÈÖçÁΩÆÊñá‰ª∂Ôºå$OOIZE_HOME/conf/oozie-site.xml12345678910111213141516171819202122232425262728293031323334&lt;property&gt; &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt; &lt;value&gt;*=/root/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop&lt;/value&gt; &lt;description&gt; Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of the Hadoop service (JobTracker, HDFS). The wildcard &apos;*&apos; configuration is used when there is no exact match for an authority. The HADOOP_CONF_DIR contains the relevant Hadoop *-site.xml files. If the path is relative is looked within the Oozie configuration directory; though the path can be absolute (i.e. to point to Hadoop client conf/ directories in the local filesystem. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.driver&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.url&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/oozie?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.username&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.password&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.processing.timezone&lt;/name&gt; &lt;value&gt;GMT+0800&lt;/value&gt;&lt;/property&gt; Âú®Ëß£Âéãoozie‰∫åËøõÂà∂ÂèëË°åÂåÖÁöÑÁõÆÂΩï‰∏≠ÔºåËß£ÂéãhadooplibsÂèëË°åÂåÖÔºå‰πüÂ∞±ÊòØoozie-hadooplibs-4.1.0-cdh5.7.0.tar.gz[hadoop@h71 oozie-4.1.0-cdh5.5.2]$ tar -zxvf oozie-hadooplibs-4.1.0-cdh5.5.2.tar.gz Âú®oozieÁöÑËß£ÂéãÁõÆÂΩï‰∏ãÂàõÂª∫libextÁõÆÂΩï„ÄÇÂπ∂Â∞Ühadooplibs‰∏ãÁöÑjarÂåÖÊã∑Ë¥ùÂà∞Ëøô‰∏™ÁõÆÂΩïÈáåÔºåÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØhadooplibsÁõÆÂΩï‰∏ãÊúâ‰∏™Êñá‰ª∂Â§πhadooplib-2.6.0-cdh5.7.0.oozie-4.1.0-cdh5.7.0 hadooplib-2.6.0-mr1-cdh5.7.0.oozie-4.1.0-cdh5.7.0ÔºõÂêéËÄÖÂØπÂ∫î‰∫émapreduce1ÔºåÊâÄ‰ª•Êàë‰ª¨Êã∑Ë¥ùÁ¨¨‰∏Ä‰∏™Êñá‰ª∂Â§π‰∏ãÁöÑjarÂåÖÂç≥ÂèØ„ÄÇ‰∏ä‰º†extÊñá‰ª∂Âà∞hadooplibs‰∏ä‰º†mysqlÁöÑÈ©±Âä®Êñá‰ª∂Âà∞hadooplibsÂÖ∑‰Ωì‰∏ä‰º†‰ªÄ‰πàÁâàÊú¨ÁöÑextÂèØ‰ª•Áúãoozie-setup.shËÑöÊú¨ÈáåÊåáÂÆöÁöÑext-ÂêéÈù¢ÁöÑÁâàÊú¨Âè∑ Â∞ÜhadooplibsÈáåÈù¢ÁöÑÊñá‰ª∂Êã∑Ë¥ùÂà∞libext 1234567891011121314151617[root@hadoop000 libext]# ls /home/app/oozie-4.1.0-cdh5.7.0/libextactivation-1.1.jar commons-digester-1.8.jar hadoop-aws-2.6.0-cdh5.7.0.jar jackson-annotations-2.2.3.jar netty-all-4.0.23.Final.jarapacheds-i18n-2.0.0-M15.jar commons-httpclient-3.1.jar hadoop-client-2.6.0-cdh5.7.0.jar jackson-core-2.2.3.jar paranamer-2.3.jarapacheds-kerberos-codec-2.0.0-M15.jar commons-io-2.4.jar hadoop-common-2.6.0-cdh5.7.0.jar jackson-core-asl-1.8.8.jar protobuf-java-2.5.0.jarapi-asn1-api-1.0.0-M20.jar commons-lang-2.4.jar hadoop-hdfs-2.6.0-cdh5.7.0.jar jackson-databind-2.2.3.jar servlet-api-2.5.jarapi-util-1.0.0-M20.jar commons-logging-1.1.jar hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar jackson-jaxrs-1.8.8.jar slf4j-api-1.7.5.jaravro-1.7.6-cdh5.7.0.jar commons-math3-3.1.1.jar hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar jackson-mapper-asl-1.8.8.jar slf4j-log4j12-1.7.5.jaraws-java-sdk-core-1.10.6.jar commons-net-3.1.jar hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar jackson-xc-1.8.8.jar snappy-java-1.0.4.1.jaraws-java-sdk-kms-1.10.6.jar curator-client-2.7.1.jar hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar jaxb-api-2.2.2.jar stax-api-1.0-2.jaraws-java-sdk-s3-1.10.6.jar curator-framework-2.7.1.jar hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar jersey-client-1.9.jar xercesImpl-2.10.0.jarcommons-beanutils-1.7.0.jar curator-recipes-2.7.1.jar hadoop-yarn-api-2.6.0-cdh5.7.0.jar jersey-core-1.9.jar xml-apis-1.4.01.jarcommons-beanutils-core-1.8.0.jar ext-2.2 hadoop-yarn-client-2.6.0-cdh5.7.0.jar jetty-util-6.1.26.cloudera.2.jar xmlenc-0.52.jarcommons-cli-1.2.jar ext-2.2.zip hadoop-yarn-common-2.6.0-cdh5.7.0.jar jsr305-3.0.0.jar xz-1.0.jarcommons-codec-1.4.jar gson-2.2.4.jar hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar leveldbjni-all-1.8.jar zookeeper-3.4.5-cdh5.7.0.jarcommons-collections-3.2.2.jar guava-11.0.2.jar htrace-core4-4.0.1-incubating.jar log4j-1.2.17.jarcommons-compress-1.4.1.jar hadoop-annotations-2.6.0-cdh5.7.0.jar httpclient-4.2.5.jar mysql-connector-java-5.1.33-bin.jarcommons-configuration-1.6.jar hadoop-auth-2.6.0-cdh5.7.0.jar httpcore-4.2.5.jar netty-3.6.2.Final.jar ‰ΩøÁî®ÂëΩ‰ª§ÂàõÂª∫Êï∞ÊçÆÂ∫ìbin/oozie-setup.sh db create -run oozie.sql ÁºñËØëwarÂåÖ bin/oozie-setup.sh prepare-war Âú®$oozie_home‰∏ãÔºåÊúâ2‰∏™sharelibÂéãÁº©ÂåÖÔºåÂàÜÂà´ÊòØ oozie-sharelib-4.1.0-cdh5.7.0.tar.gz Âíåoozie-sharelib-4.1.0-cdh5.7.0-yarn.tar.gz ÂæàÊòéÊòæÔºåÊàë‰ª¨ÂøÖÈ°ªÊã∑Ë¥ùÁ¨¨‰∫å‰∏™Â∏¶yarnÁöÑÂéãÁº©ÂåÖÔºàÂâçËæπÁöÑÊòØ1.0ÁâàÊú¨ÁöÑÔºå‰∏çÂ∏¶yarnÁöÑÔºâ bin/oozie-setup.sh sharelib create -fs hdfs://hadoop000:9000 -locallib oozie-sharelib-4.1.0-cdh5.7.0-yarn.tar.gz ÈáçÂêØhadoop ÂêØÂä®oozie bin/oozied.sh startÂêØÂä®ÂêéÔºåËÆøÈóÆÁΩëÂùÄhttp://hadoop000:11000/oozie/ ÊµãËØïOOZIE Ëß£Âéãtar -zxvf oozie-examples.tar.gzËøõÂÖ•cd examples/apps ‰ª•map-reduceÁõÆÂΩï‰∏∫‰æãÂ≠ê1234567891011121314151617181920212223242526[root@hadoop000 map-reduce]# vi job.properties ## Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#nameNode=hdfs://hadoop000:9000jobTracker=hadoop000:8032queueName=defaultexamplesRoot=examplesoozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/apps/map-reduce/workflow.xmloutputDir=map-reduce ‰øÆÊîπworkflow.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@hadoop000 map-reduce]# vi workflow.xml &lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.2&quot; name=&quot;map-reduce-wf&quot;&gt; &lt;start to=&quot;mr-node&quot;/&gt; &lt;action name=&quot;mr-node&quot;&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&quot;/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/ ‰∏ä‰º†Êñá‰ª∂hadoop fs -put examples/ /user/root ÂêØÂä® [root@hadoop000 oozie-4.1.0-cdh5.7.0]# bin/oozie job -oozie http://hadoop000:11000/oozie -config examples/apps/map-reduce/job.properties -runjob: 0000000-180830181747338-oozie-root-W ÊùÄÊéâjobbin/oozie job -oozie http://hadoop000:11000/oozie -kill 0000000-180830181747338-oozie-root-W Â¶ÇÊûúÂêØÂä®ÂêéÂèëÁé∞Êó∂Èó¥‰∏çÂØπÔºåÈúÄË¶Å‰øÆÊîπÊó∂Èó¥vi /home/app/oozie-4.1.0-cdh5.7.0/src/webapp/src/main/webapp/oozie-console.js function getTimeZone() {Ext.state.Manager.setProvider(new Ext.state.CookieProvider());return Ext.state.Manager.get(‚ÄúTimezoneId‚Äù,‚ÄùGMT+0800‚Äù);} OozieÈáçÂêØÊ≠•È™§Ôºö 1.ËøõÂÖ•oozieÁõÆÂΩï 2.Â∞ÜOozieÂÅúÊ≠¢ËøêË°å./bin/oozie-stop.sh 3.ÈáçÊñ∞ËøõË°åÊâìÂåÖwarÂåÖ ./bin/oozie-setup.sh prepare-war 4.ÈáçÊñ∞ÊâìÂºÄOozieÔºö./bin/oozie-start.sh Âè¶Â§ñÂú®‰øÆÊîπwebÊòæÁ§∫ÁöÑÊó∂Èó¥ oozieÊ∑ªÂä†ÂëäË≠¶ÈÖçÁΩÆoozie-site.xmlÊñá‰ª∂ oozie.email.smtp.host smtp.163.com oozie.email.from.address test@163.com oozie.email.smtp.auth true oozie.email.smtp.username test@163.com oozie.email.smtp.password 123456 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.2&quot; name=&quot;map-reduce-wf&quot;&gt; &lt;start to=&quot;mr-node&quot;/&gt; &lt;action name=&quot;mr-node&quot;&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&quot;/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to=&quot;goemail&quot; /&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt;&lt;action name=&quot;goemail&quot;&gt; &lt;email xmlns=&quot;uri:oozie:email-action:0.1&quot;&gt; &lt;to&gt;5lovezhm@163.com&lt;/to&gt; &lt;subject&gt;Email notifications for $&#123;wf:id()&#125;&lt;/subject&gt; &lt;body&gt;The wf $&#123;wf:id()&#125; successfully completed.&lt;/body&gt; &lt;/email&gt; &lt;ok to =&quot;end&quot;/&gt; &lt;error to =&quot;fail&quot;/&gt; &lt;/action&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; Â¶ÇÊûúÊä•ÈîôÔºö JA006: Call From hadoop000/xxx.xx.xx.xx to hadoop000:10020 failed on connection exception: java.net.ConnectException: ????; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused ÂêØÂä®history$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH-spark]]></title>
    <url>%2F2017%2F03%2F29%2FCDH-spark%2F</url>
    <content type="text"><![CDATA[Ê∑ªÂä†ÊúçÂä°ÈÄâÊã©spark on yarnÂÆâË£ÖÁöÑÊó∂ÂÄô‰ºöÈÄâÊã©history serverÔºåÂçö‰∏ªÈÄâÊã©ÁöÑÊòØslave1ÔºåÂêéÁª≠ÂèØ‰ª•ÈÄöËøáËøô‰∏™Âú∞ÂùÄÁúãsparkÁöÑUI Âçö‰∏ªÂÆâË£ÖÁöÑÊòØÔºöscala version 2.10.5ÔºàÂêéÁª≠ÂÜçUI‰∏äÁúãÂà∞ÁöÑÔºâspark version 1.6.0ÔºàÊâßË°åspark-shellÊó∂ÁúãÂà∞ÁöÑÔºâ ÂÆâË£ÖÂÆåÂêéÊµãËØïÔºö su hdfsspark-submit ‚Äìclass org.apache.spark.examples.SparkPi ‚Äìexecutor-memory 1G ‚Äìtotal-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100 Êü•ÁúãUIÔºöhttp://slave1:18088/ ÁºñÂÜô‰ª£Á†ÅÊµãËØïÁ¶ªÁ∫øÂäüËÉΩÔºö 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.zwjf.spark&lt;/groupId&gt; &lt;artifactId&gt;SpakrZwujf&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.10.5&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-make:transitive&lt;/arg&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;cn.zwjf.uidPhone&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 1234567891011121314151617181920212223package cn.zwjfimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/29. */object uidPhone &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName("uid_phone_etl") val sc = new SparkContext(conf) sc.textFile(args(0)).map(line =&gt; &#123; val arr = line.split(",") if(arr.length &gt; 1 &amp;&amp; !arr(1).equals("null"))&#123; arr(0)+","+arr(1) &#125;else&#123; "null" &#125; &#125;).filter(!_.equals("null")).saveAsTextFile(args(1)) sc.stop() &#125;&#125; ÊâìÂåÖÊèê‰∫§Âà∞ÊúçÂä°Âô® spark-submit \‚Äìclass cn.zwjf.uidPhone ‚Äìexecutor-memory 2G ‚Äìtotal-executor-cores 4 \/var/lib/hadoop-hdfs/data/spark/SpakrZwujf-1.0-SNAPSHOT.jar \/bigdata/data/hdfs/Baidu/uid_phone/* \/bigdata/data/hdfs/Baidu/uid_phone3 /bigdata/data/hdfs/Baidu/uid_phone/* /bigdata/data/hdfs/Baidu/uid_phone2 Êú¨Âú∞Ë∞ÉËØïÊñπÊ≥ïÔºö„ÄÅhttp://www.jianshu.com/p/c801761ce088]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[strom ÁõëÊéß]]></title>
    <url>%2F2017%2F03%2F26%2Fstrom-%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[ÁúãÂΩìÂâçGCÁöÑÊÉÖÂÜµjstat -gcutil Á´ØÂè£Âè∑ 1000 ÁúãwaitÁöÑÊòØÂê¶ËøáÈ´òtop -p Á´ØÂè£Âè∑ ÊääÊüê‰∏™Á´ØÂè£ÂΩìÂâçÁöÑÂ†ÜÊ†à‰ø°ÊÅØdumpÂà∞Êüê‰∏™Êñá‰ª∂jstack Á´ØÂè£Âè∑ &gt; Êñá‰ª∂ ÁúãÁ£ÅÁõòiostat -x 1 Â¶ÇÊûúÂèëÁé∞ÂàÜÈÖç‰∏çÂùáÂåÄÔºå‰æãÂ¶ÇFiledGrouping userid=1ÁâπÂà´Â§öÔºåÂØºËá¥‰∏ãÊ∏∏noltÊàñexecuteÊêûÊ≠ª‰∫ÜÔºåÈáçÊñ∞ÂêØÂä®‰πüÂæàÂø´Â∞±Ê≠ªÊéâ„ÄÇÂèØ‰ª•ÂØπuserid ÂÅöhashÊàñmd5ÊàñËÄÖÁªÑÊàêÊõ¥Â§öÊù°‰ª∂ÔºåËÆ©‰∏ãÊ∏∏BoltÂàÜÂ∏ÉÊõ¥ÂùáÂåÄ complete latency(ËøôÈáå‰∏ªË¶ÅÈíàÂØπspout)ÊòØ‰∏Ä‰∏™tuple‰ªéÂèëÂá∫, Âà∞ÁªèËøáboltÂ§ÑÁêÜÂÆåÊàê, ÊúÄÁªàË∞ÉÁî®spoutÁöÑackËøô‰∏™ÂÆåÊï¥ÁöÑËøáÁ®ãÊâÄËä±ÁöÑÊó∂Èó¥.complete latencyÂíåÂêûÂêêÈáèÂèØ‰ª•ËÆ§‰∏∫ÊòØ‰∫íÊñ•ÁöÑ, ‰∫åËÄÖ‰∏çÂèØÂÖºÂæó, Ë¶ÅÊèêÈ´òÂêûÂêêÈáè, ÂøÖÁÑ∂‰ºöÂ¢ûÂä†complete latency, Âèç‰πã‰∫¶ÁÑ∂. ÊâÄ‰ª•Êàë‰ª¨ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÂú∫ÊôØ, Âú®‰∫åËÄÖ‰πãÈó¥ÊâæÂà∞‰∏Ä‰∏™Âπ≥Ë°°.complete latencyÂèóÂà∞‰∏§‰∏™Âõ†Á¥†ÁöÑÂΩ±Âìç:boltÁöÑÂ§ÑÁêÜÊó∂Èó¥spoutÁöÑparallelismÊï∞Èáè ÂÆòÁΩëÂØπÂπ∂Ë°åÂ∫¶Á≠âÁöÑËÆæÁΩÆÊñπÂºèÔºöstorm.apache.org/releases/1.0.3/Understanding-the-parallelism-of-a-Storm-topology.html Â¶ÇÊûúexecutorsÊÄªÊòØÊåÇÂ∞±ÈúÄË¶ÅÂÖ≥Ê≥®]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[strom-Metrics]]></title>
    <url>%2F2017%2F03%2F25%2Fstrom-Metrics%2F</url>
    <content type="text"><![CDATA[Storm Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØ‰ª•Ëé∑ÂèñÊï¥‰∏™ÊãìÊâë‰∏≠ÊâÄÊúâÁöÑÁªüËÆ°‰ø°ÊÅØÁöÑmetricsÊé•Âè£„ÄÇStorm ÂÜÖÈÉ®ÈÄöËøáËØ•Êé•Âè£Ë∑üË∏™ÂêÑÁ±ªÁªüËÆ°Êï∞Â≠óÔºöexecutor Âíå acker ÁöÑÊï∞Èáè„ÄÅÊØè‰∏™ bolt ÁöÑÂπ≥ÂùáÂ§ÑÁêÜÊó∂Âª∂„ÄÅworker ‰ΩøÁî®ÁöÑÊúÄÂ§ßÂ†ÜÂÆπÈáèÁ≠âÁ≠âÔºåËøô‰∫õ‰ø°ÊÅØÈÉΩÂèØ‰ª•Âú® Nimbus ÁöÑ UI ÁïåÈù¢‰∏≠ÁúãÂà∞„ÄÇ‰ΩøÁî® Metrics Âè™ÈúÄË¶ÅÂÆûÁé∞‰∏Ä‰∏™Êé•Âè£ÊñπÊ≥ïÔºögetValueAndResetÔºåÂú®ÊñπÊ≥ï‰∏≠ÂèØ‰ª•Êü•ÊâæÊ±áÊÄªÂÄº„ÄÅÂπ∂Â∞ÜËØ•ÂÄºÂ§ç‰Ωç‰∏∫ÂàùÂßãÂÄº„ÄÇ‰æãÂ¶ÇÔºåÂú® MeanReducer ‰∏≠Â∞±ÂÆûÁé∞‰∫ÜÈÄöËøáËøêË°åÊÄªÊï∞Èô§‰ª•ÂØπÂ∫îÁöÑËøêË°åËÆ°Êï∞ÁöÑÊñπÂºèÊù•Ê±ÇÂèñÂùáÂÄºÔºåÁÑ∂ÂêéÂ∞Ü‰∏§‰∏™ÂÄºÈÉΩÈáçÊñ∞ËÆæÁΩÆ‰∏∫ 0„ÄÇ Storm Êèê‰æõ‰∫Ü‰ª•‰∏ãÂá†Áßç metric Á±ªÂûãÔºö AssignableMetric ‚Äì Â∞Ü metric ËÆæÁΩÆ‰∏∫ÊåáÂÆöÂÄº„ÄÇÊ≠§Á±ªÂûãÂú®‰∏§ÁßçÊÉÖÂÜµ‰∏ãÊúâÁî®Ôºö1. metric Êú¨Ë∫´‰∏∫Â§ñÈÉ®ËÆæÁΩÆÁöÑÂÄºÔºõ2. ‰Ω†Â∑≤ÁªèÂè¶Â§ñËÆ°ÁÆóÂá∫‰∫ÜÊ±áÊÄªÁöÑÁªüËÆ°ÂÄº„ÄÇCombinedMetric ‚Äì ÂèØ‰ª•ÂØπ metric ËøõË°åÂÖ≥ËÅîÊõ¥Êñ∞ÁöÑÈÄöÁî®Êé•Âè£„ÄÇCountMetric ‚Äì ËøîÂõû metric ÁöÑÊ±áÊÄªÁªìÊûú„ÄÇÂèØ‰ª•Ë∞ÉÁî® incr() ÊñπÊ≥ïÊù•Â∞ÜÁªìÊûúÂä†‰∏ÄÔºõË∞ÉÁî® incrBy(n) ÊñπÊ≥ïÊù•Â∞ÜÁªìÊûúÂä†‰∏äÁªôÂÆöÂÄº„ÄÇMultiCountMetric ‚Äì ËøîÂõûÂåÖÂê´‰∏ÄÁªÑ CountMetric ÁöÑ HashMapReducedMetricMeanReducer ‚Äì Ë∑üË∏™Áî±ÂÆÉÁöÑ reduce() ÊñπÊ≥ïÊèê‰æõÁöÑËøêË°åÁä∂ÊÄÅÂùáÂÄºÁªìÊûúÔºàÂèØ‰ª•Êé•Âèó Double„ÄÅInteger„ÄÅLong Á≠âÁ±ªÂûãÔºåÂÜÖÁΩÆÁöÑÂùáÂÄºÁªìÊûúÊòØ Double Á±ªÂûãÔºâ„ÄÇMeanReducer Á°ÆÂÆûÊòØ‰∏Ä‰∏™Áõ∏ÂΩìÊ£íÁöÑÂÆ∂‰ºô„ÄÇMultiReducedMetric ‚Äì ËøîÂõûÂåÖÂê´‰∏ÄÁªÑ ReducedMetric ÁöÑ HashMap Ëá™ÂÆö‰πâMetricÔºö‰ª£Á†ÅÊ≥®ÂÜåÔºöconf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);ÊàñËÄÖ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂topology.metrics.consumer.register: class: ‚Äúorg.apache.storm.metric.LoggingMetricsConsumer‚Äùparallelism.hint: 1 class: ‚Äúorg.apache.storm.metric.HttpForwardingMetricsConsumer‚Äùparallelism.hint: 1argument: ‚Äúhttp://example.com:8080/metrics/my-topology/‚Äú ÊûÑÂª∫Ëá™Â∑±ÁöÑMetricÂÆö‰πâ‰∏çÂèØË¢´Â∫èÂàóÂè∑Á±ªÂûãtransientprivate transient CountMetric countMetric; ÈáçÂÜôprepare@Overridepublic void prepare(Map conf, TopologyContext context, OutputCollector collector) { // other intialization here. countMetric = new CountMetric(); context.registerMetric(‚Äúexecute_count‚Äù, countMetric, 60);} boltÁöÑexecute public void execute(Tuple input) { countMetric.incr(); // handle tuple here.} builtin_metrics.clj ‰∏∫ÂÜÖÈÉ®ÁöÑ metrics ËÆæÁΩÆ‰∫ÜÊï∞ÊçÆÁªìÊûÑÔºå‰ª•ÂèäÂÖ∂‰ªñÊ°ÜÊû∂ÁªÑ‰ª∂ÂèØ‰ª•Áî®‰∫éÊõ¥Êñ∞ÁöÑËôöÊãüÊñπÊ≥ï„ÄÇmetrics Êú¨Ë∫´ÊòØÂú®ÂõûË∞É‰ª£Á†Å‰∏≠ÂÆûÁé∞ËÆ°ÁÆóÁöÑ ‚Äì ËØ∑ÂèÇËÄÉ clj/b/s/daemon/daemon/executor.clj ‰∏≠ÁöÑ ack-spout-msg ÁöÑ‰æãÂ≠ê„ÄÇ LoggingMetricsConsumer,ÁªüËÆ°ÊåáÊ†áÂÄºÂ∞ÜËæìÂá∫Âà∞metric.logÊó•ÂøóÊñá‰ª∂‰∏≠„ÄÇ]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bashËÑöÊú¨ÂÆûÊàò]]></title>
    <url>%2F2017%2F03%2F19%2Fbash%E8%84%9A%E6%9C%AC%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[ÁªÉ‰π†È¢òÁõÆÂÜô‰∏Ä‰∏™ËÑöÊú¨getinterface.shÔºåËÑöÊú¨ÂèØ‰ª•Êé•ÂèóÂèÇÊï∞(i,I,a)ÔºåÂÆåÊàê‰ª•‰∏ã‰ªªÂä°Ôºö (1)‰ΩøÁî®‰ª•‰∏ãÂΩ¢ÂºèÔºögetinterface.sh [-i interface|-I IP|-a] (2)ÂΩìÁî®Êà∑‰ΩøÁî®-iÈÄâÈ°πÊó∂ÔºåÊòæÁ§∫ÂÖ∂ÊåáÂÆöÁΩëÂç°ÁöÑIPÂú∞ÂùÄÔºõ (3)ÂΩìÁî®Êà∑‰ΩøÁî®-IÈÄâÈ°πÊó∂ÔºåÊòæÁ§∫ÂÖ∂ÂêéÈù¢ÁöÑIPÂú∞ÂùÄÊâÄÂ±ûÁöÑÁΩëÁªúÊé•Âè£ÔºõÔºàÂ¶Ç 192.168.199.183Ôºöeth0Ôºâ (4)ÂΩìÁî®Êà∑ÂçïÁã¨‰ΩøÁî®-aÈÄâÈ°πÊó∂ÔºåÊòæÁ§∫ÊâÄÊúâÁΩëÁªúÊé•Âè£ÂèäÂÖ∂IPÂú∞ÂùÄÔºàloÈô§Â§ñÔºâ ÂàÜÊûêÂèÇÊï∞1‰∏∫param1 ÂèÇÊï∞2‰Ωçparam2Ôºà1ÔºâÂà©Áî®CAT &lt;&lt; EOF * EOFÊâìÂç∞‰ø°ÊÅØ12345678cat &lt;&lt; EOF getinterface.sh [-i interface|-I IP|-a] -i interface) show ip of the interface -I IP) show interface of the IP and IP with :; -a) list all interfaces and their IPs except lo; *) quit=================================================================EOF Ôºà2ÔºâÊ†°È™åÊé•Âè£ÊòØÂê¶Â≠òÂú®ÔºàÂèÇÊï∞2Ôºâ1234ifconfig $interface &gt;/dev/null $? -ne 1 #Â¶ÇÊûúÂ≠òÂú®ÊâìÂç∞ÁΩëÂç°ÁöÑIPifconfig $parma2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' Ôºà3ÔºâÂÖàÊâìÂç∞ÊâÄÊúâÊé•Âè£1netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125; Âπ∂Â≠òÂú®‰∏Ä‰∏™ÂèòÈáè‰∏≠ 1list=`netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125;'` ÈÅçÂéÜÈõÜÂêàÔºåÊü•ÁúãÂíåËæìÂÖ•IPÁõ∏ÂêåÁöÑÊâìÂç∞IPÂíåÊé•Âè£12345678for inter in $list; do ip_temp=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' if [ $IP == $ip_temp ];then echo "$IP : $inter" exit 0 fi done Ôºà4ÔºâÈÅçÂéÜÈõÜÂêàÔºåËøáÊª§Êé•Âè£‰∏∫loÁöÑÔºåÊâìÂç∞IPÂíåÊé•Âè£12345678for inter in $list; do ip_temp=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' if [ $inter != "lo" ];then echo "$IP : $inter" exit 0 fi done ÊúÄÁªàÁöÑËÑöÊú¨Â¶Ç‰∏ãÔºö1234567891011121314151617181920212223242526272829303132333435363738394041424344cat &lt;&lt; EOF getinterface.sh [-i interface|-I IP|-a] -i interface) show ip of the interface -I IP) show interface of the IP and IP with :; -a) list all interfaces and their IPs except lo; *) quit=================================================================EOFread -p "please choice " param1 param2list=`netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125;'`if [[ "$param1" == '-i' ]]; then ifconfig $interface &gt;/dev/null flag=$? if [ $flag -ne 1 ];then ip=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;'` echo "$param2 $ip" else echo "$parma2 is not exist" fielif [[ "$param1" == '-I' ]]; thenfor lt in $list; do ip =$(ifconfig $lt | awk -F" " '/inet.*netmask/&#123;print $2&#125;') #ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;') if [ $param2 == $ip ];then echo "$lt : $ip" fi doneelif [[ "$param1" == '-a' ]]; then for lt in $list; do ip=$(ifconfig $lt | awk -F" " '/inet.*netmask/&#123;print $2&#125;') #ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;') if [ $lt != "lo" ];then echo "$lt : $ip" exit 0 fi doneelse echo "quit" exit 0fi Ê≥®ÊÑèÂ¶Ç‰∏äÂÜÖÂÆπÊ†πÊçÆ‰Ω†‰∏™‰∫∫Êú∫Âô®ÊòæÁ§∫ÊÉÖÂÜµËÄåÂÆöÔºåÂ¶ÇÊûúÊòØÊòæÁ§∫ inet addr:127.0.0.1 Mask:255.0.0.0 Â∞±Ë¶ÅÊääÊ≠£ÂàôÊîπÊàê ip=$(ifconfig $lt | awk -F‚Äù ‚Äú ‚Äò/inet.*Mask/{print $2}‚Äô | awk -F: ‚Äò{print $2}‚Äô)]]></content>
      <categories>
        <category>bash</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH hadoopÂÆûÊàò]]></title>
    <url>%2F2017%2F03%2F14%2FCDH%20hadoop%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Ëß£ÂÜ≥ÈóÆÈ¢òÔºöÂõ†‰∏∫Êï∞ÊçÆÊâãÊú∫Âè∑‰∏ÄÈ°πÂåÖÂê´‰∏∫NULLÁöÑÊï∞ÊçÆÔºåÈúÄË¶ÅÊ∏ÖÊ¥ó„ÄÇ ÂàõÂª∫Â∑•Á®ãÔºåÊ∑ªÂä†Â¶Ç‰∏ã‰æùËµñ123456789101112131415161718192021222324&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; ÁºñÂÜômapperÁ±ª1234567891011121314151617181920212223242526package map;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * Created by Administrator on 2017/3/14. */public class UidPhoneMapper extends Mapper&lt;Text, Text, Text, Text&gt; &#123; @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; String arr[] = key.toString().split(","); if (arr.length != 2) &#123; return; &#125; if(arr[1] == null)&#123; return; &#125; context.write(new Text(arr[0]), new Text(arr[1])); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package main;import map.UidPhoneMapper;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.GenericOptionsParser;import java.io.IOException;import org.apache.hadoop.fs.Path;/** * Created by Administrator on 2017/3/14. */public class UidPhoneDropNull &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); GenericOptionsParser parser = new GenericOptionsParser(conf, args); String[] otherArgs = parser.getRemainingArgs(); if (args.length != 2) &#123; System.err.println("Usage: NewlyJoin &lt;inpath&gt; &lt;output&gt;"); System.exit(2); &#125; Job job = new Job(conf, "UidPhoneDropNull"); // ËÆæÁΩÆËøêË°åÁöÑjob job.setJarByClass(UidPhoneDropNull.class); // ËÆæÁΩÆMapÁõ∏ÂÖ≥ÂÜÖÂÆπ job.setMapperClass(UidPhoneMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setInputFormatClass(KeyValueTextInputFormat.class); //ËÆæÁΩÆÊñá‰ª∂ËæìÂÖ•Ê†ºÂºè job.setNumReduceTasks(0); //ËÆæÁΩÆReduce‰∏™Êï∞‰∏∫0 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); // ËÆæÁΩÆËæìÂÖ•ÂíåËæìÂá∫ÁöÑÁõÆÂΩï FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); // ÊâßË°åÔºåÁõ¥Âà∞ÁªìÊùüÂ∞±ÈÄÄÂá∫ System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; ËøêË°å‰∏äÈù¢ÁöÑÁ®ãÂ∫è12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[hdfs@slave3 jar]$ hadoop jar bigdataMR.jar main.UidPhoneDropNull /bigdata/data/hdfs/BD/UP/* /bigdata/data/hdfs/BD/UP2 17/03/14 17:37:54 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:803217/03/14 17:37:55 INFO input.FileInputFormat: Total input paths to process : 317/03/14 17:37:55 INFO mapreduce.JobSubmitter: number of splits:317/03/14 17:37:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_004417/03/14 17:37:55 INFO impl.YarnClientImpl: Submitted application application_1488970234114_004417/03/14 17:37:55 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0044/17/03/14 17:37:55 INFO mapreduce.Job: Running job: job_1488970234114_004417/03/14 17:38:00 INFO mapreduce.Job: Job job_1488970234114_0044 running in uber mode : false17/03/14 17:38:00 INFO mapreduce.Job: map 0% reduce 0%17/03/14 17:38:04 INFO mapreduce.Job: map 33% reduce 0%17/03/14 17:38:05 INFO mapreduce.Job: map 100% reduce 0%17/03/14 17:38:05 INFO mapreduce.Job: Job job_1488970234114_0044 completed successfully17/03/14 17:38:05 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=378748 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=13514456 HDFS: Number of bytes written=10824901 HDFS: Number of read operations=15 HDFS: Number of large read operations=0 HDFS: Number of write operations=6 Job Counters Launched map tasks=3 Data-local map tasks=3 Total time spent by all maps in occupied slots (ms)=8240 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=8240 Total vcore-seconds taken by all map tasks=8240 Total megabyte-seconds taken by all map tasks=8437760 Map-Reduce Framework Map input records=661590 Map output records=485478 Input split bytes=393 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=175 CPU time spent (ms)=3730 Physical memory (bytes) snapshot=712597504 Virtual memory (bytes) snapshot=8316772352 Total committed heap usage (bytes)=814743552 File Input Format Counters Bytes Read=13514063 File Output Format Counters Bytes Written=10824901[hdfs@slave3 jar]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP2Found 4 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7609438 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00000-rw-r--r-- 3 hdfs supergroup 3215441 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00001-rw-r--r-- 3 hdfs supergroup 22 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00002 ÊúÄÂêéÂ∞ÜÊï∞ÊçÆÂØºÂÖ•Âà∞hivehive&gt; create EXTERNAL table IF NOT EXISTS UP2 (uid STRING,phone STRING) row format delimited fields terminated by ‚Äò,‚Äô location ‚Äò/bigdata/data/hdfs/BD/UP2/‚Äò;OKTime taken: 0.026 secondshive&gt; select count(1) from UP2;Query ID = hdfs_20170314174141_1b60d560-2036-44e6-ab65-19f17efc5b1bTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=In order to limit the maximum number of reducers: set hive.exec.reducers.max=In order to set a constant number of reducers: set mapreduce.job.reduces=Starting Job = job_1488970234114_0045, Tracking URL = http://master2:8088/proxy/application_1488970234114_0045/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0045Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 17:41:33,200 Stage-1 map = 0%, reduce = 0%2017-03-14 17:41:38,371 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.54 sec2017-03-14 17:41:43,486 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.38 secMapReduce Total cumulative CPU time: 4 seconds 380 msecEnded Job = job_1488970234114_0045MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 4.38 sec HDFS Read: 10831709 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 380 msecOK485478Time taken: 16.267 seconds, Fetched: 1 row(s) ÊØîÊ∏ÖÊ¥óÂâçÊï∞ÊçÆ661590Â∞ë‰∫Ü176112Êù°]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH hive ÂÆûÊàò]]></title>
    <url>%2F2017%2F03%2F14%2FCDH%20hive%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[ÂàõÂª∫Êï∞ÊçÆÂ∫ì123456hive&gt; create database BD;OKTime taken: 0.161 secondshive&gt; use BD;OKTime taken: 0.013 seconds ÂàõÂª∫Ë°®12345678hive&gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by ',' ;OKTime taken: 0.211 secondshive&gt; load data inpath '/bigdata/data/hdfs/shuju/SO/part-m-00000' into table UP;Loading data to table BD.UPTable BD.UP stats: [numFiles=1, totalSize=898611236]OKTime taken: 0.317 seconds Êü•ËØ¢Ë°®Êï∞ÊçÆ1234567891011121314151617181920212223242526hive&gt; select count(1) from UP;Query ID = hdfs_20170314111111_d29152a3-56b3-42f9-b17b-1ddaf7451117Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0025, Tracking URL = http://master2:8088/proxy/application_1488970234114_0025/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0025Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 12017-03-14 11:11:49,267 Stage-1 map = 0%, reduce = 0%2017-03-14 11:11:55,519 Stage-1 map = 25%, reduce = 0%, Cumulative CPU 3.31 sec2017-03-14 11:11:56,542 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 16.57 sec2017-03-14 11:12:01,655 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 18.41 secMapReduce Total cumulative CPU time: 18 seconds 410 msecEnded Job = job_1488970234114_0025MapReduce Jobs Launched: Stage-Stage-1: Map: 4 Reduce: 1 Cumulative CPU: 18.41 sec HDFS Read: 899021344 HDFS Write: 8 SUCCESSTotal MapReduce CPU Time Spent: 18 seconds 410 msecOK5674200Time taken: 19.429 seconds, Fetched: 1 row(s) Âà†Èô§Ë°®Ôºö123hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.088 seconds Êü•Áúãhdfs‰∏äÁöÑÊï∞ÊçÆÔºåÂèëÁé∞Êï∞ÊçÆË¢´Âà†Èô§123[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/SOFound 1 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-13 19:16 /bigdata/data/hdfs/shuju/SO/_SUCCESS ÊµãËØï2ÔºöÈ¶ñÂÖàÊü•ËØ¢ÔºåÂèëÁé∞biaomingÊï∞ÊçÆÂ≠òÂú®1234[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/biaomingFound 2 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS-rw-r--r-- 3 hdfs supergroup 13514063 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/part-m-00000 ÂàõÂª∫Â§ñÈÉ®Ë°®Ôºö123hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' ;OKTime taken: 0.033 seconds Âä†ËΩΩÊï∞ÊçÆ12345hive&gt; load data inpath '/bigdata/data/hdfs/shuju/biaoming/part-m-00000' into table UP;Loading data to table BD.UPTable BD.UP stats: [numFiles=1, totalSize=13514063]OKTime taken: 0.201 seconds Êü•ËØ¢Êï∞ÊçÆÔºö12345678910111213141516171819202122232425hive&gt; select count(1) from UP;Query ID = hdfs_20170314115959_6004ef81-913b-4d38-99dc-1199cc6e73f2Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0027, Tracking URL = http://master2:8088/proxy/application_1488970234114_0027/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0027Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 11:59:09,331 Stage-1 map = 0%, reduce = 0%2017-03-14 11:59:14,463 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.06 sec2017-03-14 11:59:19,587 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.9 secMapReduce Total cumulative CPU time: 4 seconds 900 msecEnded Job = job_1488970234114_0027MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 4.9 sec HDFS Read: 13520683 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 900 msecOK661590Time taken: 17.707 seconds, Fetched: 1 row(s) Âà†Èô§Êï∞ÊçÆ123hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.037 seconds ÂÜçÊü•ÁúãÂÖÉÊï∞ÊçÆÔºåÂèëÁé∞‰æùÁÑ∂Ë¢´Âà†Èô§‰∫Ü123[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/biaomingFound 1 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS ÊµãËØï3ÔºöÈáçÊñ∞‰∏ä‰º†Êï∞ÊçÆÔºö1sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver --connect 'jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=BD' --table=UP --target-dir /bigdata/data/hdfs/BD/UP --split-by uid -m 3 Êü•ÁúãÊï∞ÊçÆÔºö1234567[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPFound 5 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000-rw-r--r-- 3 hdfs supergroup 2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001-rw-r--r-- 3 hdfs supergroup 3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002-rw-r--r-- 3 hdfs supergroup 22 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00003 ÂàõÂª∫Â§ñÈÉ®Ë°®Êï∞ÊçÆÔºö123456789101112131415161718192021222324252627282930hive&gt; select count(1) from UP;FAILED: SemanticException [Error 10001]: Line 1:21 Table not found 'UP'hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' location '/bigdata/data/hdfs/BD/UP/';OKTime taken: 0.03 secondshive&gt; select count(1) from UP;Query ID = hdfs_20170314141111_5be2c701-ad6a-4717-8801-513f38d64928Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0031, Tracking URL = http://master2:8088/proxy/application_1488970234114_0031/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0031Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 14:11:48,608 Stage-1 map = 0%, reduce = 0%2017-03-14 14:11:54,743 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.26 sec2017-03-14 14:11:58,833 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 5.08 secMapReduce Total cumulative CPU time: 5 seconds 80 msecEnded Job = job_1488970234114_0031MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 5.08 sec HDFS Read: 13520952 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 5 seconds 80 msecOK661590Time taken: 16.253 seconds, Fetched: 1 row(s) Ê≠§Êó∂ÂÜçÂà†Èô§Ë°®ÂêéÊï∞ÊçÆËøòÂ≠òÂú®„ÄÇ123456789hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.037 seconds[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPFound 5 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000-rw-r--r-- 3 hdfs supergroup 2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001-rw-r--r-- 3 hdfs supergroup 3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002 ÂàõÂª∫ÊôÆÈÄöË°®Ôºö123hive&gt; create table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' location '/bigdata/data/hdfs/BD/UP/';OKTime taken: 0.029 seconds Âà†Èô§Ë°®ÂêéÊï∞ÊçÆ‰∏çÂ≠òÂú®12345hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.046 seconds[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPls: `/bigdata/data/hdfs/BD/UP': No such file or directory ÊÄªÁªìÁî®loadÊñπÂºèÊó†ËÆ∫Â§ñÈÉ®Ë°®ËøòÊòØÂÜÖÈÉ®Ë°®Êï∞ÊçÆÈÉΩ‰ºöÂà†Èô§ÔºåÁî®locationÊñπÂºèÔºåÂ§ñÈÉ®Ë°®‰∏ç‰ºöÂà†Èô§Êï∞ÊçÆÔºåÂÜÖÈÉ®Ë°®‰ºöÂà†Èô§Êï∞ÊçÆ]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH sqoop1 ÂÆûÊàò-Â∏¶Êù°‰ª∂ÁöÑÂØºÂÖ•]]></title>
    <url>%2F2017%2F03%2F13%2FCDH%20sqoop1%20%E5%AE%9E%E6%88%98-%E5%B8%A6%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[ÂØºÂÖ•[hdfs@slave3 lib]$ sqoop import ‚Äìconnect jdbc:mysql://10.105.10.46:3306/shuju ‚Äìusername username ‚Äìpassword password ‚Äìquery ‚ÄúSELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1‚Äù ‚Äìfields-terminated-by ‚Äú,‚Äù ‚Äìlines-terminated-by ‚Äú\n‚Äù ‚Äìsplit-by id ‚Äìhive-import ‚Äìcreate-hive-table ‚Äìhive-table card_record ‚Äìtarget-dir /bigdata/data/hdfs/shuju/biaoming17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.017/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‚Äò$CONDITIONS‚Äô in WHERE clause. at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332) at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858) at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) ‰øÆÊîπ‰∏∫Ôºösqoop import ‚Äìconnect jdbc:mysql://10.105.10.46:3306/shuju ‚Äìusername username ‚Äìpassword password ‚Äìquery ‚ÄòSELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS‚Äô ‚Äìfields-terminated-by ‚Äú,‚Äù ‚Äìlines-terminated-by ‚Äú\n‚Äù ‚Äìsplit-by id ‚Äìhive-import ‚Äìcreate-hive-table ‚Äìhive-table card_record ‚Äìtarget-dir /bigdata/data/hdfs/shuju/biaoming Âú®ÊâßË°åÔºåÂèëÁé∞Êä•ÈîôËØ¥Êàëdatabase‰∏çÂ≠òÂú®ÔºåÂπ∂Âú®ÁõÆÂΩï‰∏ãÁîüÊàê‰∫Ü‰∏Ä‰∏™metastore_dbÔºåÂéüÊù•ÊòØÊâßË°åsqoopÁöÑÊú∫Âô®‰∏çÂØπÔºåÊç¢‰∫Ü‰∏ÄÂè∞Êú∫Âô®ÊâßË°åÊ≤°ÊúâÈóÆÈ¢òÔºåÂæàÂ•áÊÄ™ÊòéÊòéÊòØÂàÜÂ∏ÉÂºèÁöÑ‰∏∫‰ªÄ‰πà‰ºöÂá∫Áé∞Ëøô‰∏™ÈóÆÈ¢òÂë¢ ÊúÄÁªàÁöÑÂÖ∑‰ΩìÊó•ÂøóÔºö [hdfs@master1 root]$ sqoop import ‚Äìconnect jdbc:mysql://10.105.10.46:3306/shuju ‚Äìusername username ‚Äìpassword password ‚Äìquery ‚ÄòSELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS‚Äô ‚Äìfields-terminated-by ‚Äú,‚Äù ‚Äìlines-terminated-by ‚Äú\n‚Äù ‚Äìsplit-by id ‚Äìhive-import ‚Äìcreate-hive-table ‚Äìhive-table card_record ‚Äìtarget-dir /bigdata/data/hdfs/shuju/biaoming ‚Äìhive-database shujuWarning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.17/03/15 14:32:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.017/03/15 14:32:53 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/15 14:32:53 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/15 14:32:53 INFO tool.CodeGenTool: Beginning code generation17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/15 14:33:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.jar17/03/15 14:33:00 INFO mapreduce.ImportJobBase: Beginning query import.17/03/15 14:33:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/15 14:33:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/15 14:33:01 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:803217/03/15 14:33:02 INFO db.DBInputFormat: Using read commited transaction isolation17/03/15 14:33:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM (SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 1) ) AS t117/03/15 14:33:03 INFO db.IntegerSplitter: Split size: 7508318; Num splits: 4 from: 20079 to: 3005335417/03/15 14:33:03 INFO mapreduce.JobSubmitter: number of splits:417/03/15 14:33:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_005717/03/15 14:33:03 INFO impl.YarnClientImpl: Submitted application application_1488970234114_005717/03/15 14:33:03 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0057/17/03/15 14:33:03 INFO mapreduce.Job: Running job: job_1488970234114_005717/03/15 14:33:08 INFO mapreduce.Job: Job job_1488970234114_0057 running in uber mode : false17/03/15 14:33:08 INFO mapreduce.Job: map 0% reduce 0%17/03/15 14:33:15 INFO mapreduce.Job: map 75% reduce 0%17/03/15 14:33:16 INFO mapreduce.Job: map 100% reduce 0%17/03/15 14:33:16 INFO mapreduce.Job: Job job_1488970234114_0057 completed successfully17/03/15 14:33:16 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=608160 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=428 HDFS: Number of bytes written=17645206 HDFS: Number of read operations=16 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 Job Counters Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=17968 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=17968 Total vcore-seconds taken by all map tasks=17968 Total megabyte-seconds taken by all map tasks=18399232 Map-Reduce Framework Map input records=283659 Map output records=283659 Input split bytes=428 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=268 CPU time spent (ms)=12960 Physical memory (bytes) snapshot=1151381504 Virtual memory (bytes) snapshot=11157348352 Total committed heap usage (bytes)=1045954560 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=1764520617/03/15 14:33:16 INFO mapreduce.ImportJobBase: Transferred 16.8278 MB in 15.7965 seconds (1.0653 MB/sec)17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Retrieved 283659 records.17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:33:16 WARN hive.TableDefWriter: Column update_time had to be cast to a less precise type in Hive17/03/15 14:33:16 INFO hive.HiveImport: Loading uploaded data into Hive Logging initialized using configuration in jar:file:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.propertiesOKTime taken: 1.874 secondsLoading data to table shuju.card_recordTable shuju.card_record stats: [numFiles=4, totalSize=17645206]OKTime taken: 0.367 seconds]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH sqoop1 ÂÆûÊàò]]></title>
    <url>%2F2017%2F03%2F13%2FCDH%20sqoop1%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[‰∏ãËΩΩsqlserverÁöÑjdbcÈ©±Âä®ÂåÖhttps://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599Ëß£ÂéãÂ∞Üsqljdbc4.jarÊîæÂú®Ôºö/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/lib ‰∏ãËΩΩSQL Server-Hadoop ConnectorÔºösqoop-sqlserver-1.0.tar.gz http://www.microsoft.com/en-us/download/details.aspx?id=27584 ÂØºÂÖ•Êï∞ÊçÆÔºö[root@slave3 sqoop]# ./bin/sqoop import ‚Äìconnect ‚Äòjdbc: server://10.105.32.246 username=sa password=123456 database=databaseName ‚Äìtable=tableName ‚Äìtarget-dir /bigdata/data/hdfsWarning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/bin/../../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.+======================================================================+| Error: JAVA_HOME is not set |+‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-+| Please download the latest Sun JDK from the Sun Java web site || &gt; http://www.oracle.com/technetwork/java/javase/downloads || || HBase requires Java 1.7 or later. |+======================================================================+Error: JAVA_HOME is not set and could not be found. Êä•ÈîôËß£ÂÜ≥ÊñπÊ≥ïÔºö‰øÆÊîπbin‰∏ãÁöÑconfigure-sqoop Ê≥®Èáä‰ª•‰∏ã‰ª£Á†ÅÔºö123456789101112131415161718192021222324252627282930313233343536373839#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hive-hcatalog# elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hcatalog# else# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog# if [ ! -d $&#123;HCAT_HOME&#125; ]; then# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog# fi# fi#fi#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/accumulo&quot; ]; then# ACCUMULO_HOME=/usr/lib/accumulo# else# ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo# fi#fi#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hive-hcatalog# elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hcatalog# else# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog# if [ ! -d $&#123;HCAT_HOME&#125; ]; then# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog# fi# fi#fi#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/accumulo&quot; ]; then# ACCUMULO_HOME=/usr/lib/accumulo# else# ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo# fi#fi Âú®ÊâßË°åÂ¶ÇÊûúÊèêÁ§∫JAVA_HOME‰∏çÂ≠òÂú®ÔºåÊâãÂä®ÊâßË°å‰∏Ä‰∏ãexport JAVA_HOME=/usr/java/jdk1.8.0_121 ÂÜçÊâßË°åÊèêÁ§∫ÔºöERROR tool.BaseSqoopTool: Error parsing arguments for import: ./bin/sqoop import ‚Äìconnect ‚Äòjdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName‚Äô ‚Äìtable=tableName ‚Äìtarget-dir /bigdata/data/hdfs ‚Äìsplit-by order_id ‚Äìfields-terminated-by ‚Äò\t‚Äô ‚Äìm 3 ÈúÄË¶Å‰∏ãËΩΩsqoop-sqlserver‰∏ãËΩΩÂú∞ÂùÄÂ∑≤ÁªèÂ§±ÊïàÔºåÂú®CSDN‰∏äÊâæÂà∞Ôºöhttps://passport.csdn.net/account/login?from=http://download.csdn.net/download/nma_123456/9405343 Ëß£ÂéãËøõÂÖ•ÁõÆÂΩïÊâßË°åÔºö[root@slave3 sqoop-sqlserver-1.0]# export SQOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop[root@slave3 sqoop-sqlserver-1.0]# ./install.sh ÂÜçÊ¨°ÊâßË°åÔºöÊä•Â¶Ç‰∏ãÈîôËØØException in thread ‚Äúmain‚Äù java.lang.NoClassDefFoundError: org/json/JSONObject at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:43) at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:767) at org.apache.sqoop.mapreduce.JobBase.putSqoopOptionsToConfiguration(JobBase.java:388) at org.apache.sqoop.mapreduce.JobBase.createJob(JobBase.java:374) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:256) at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236)Caused by: java.lang.ClassNotFoundException: org.json.JSONObject at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ÂàÜÊûêÂéüÂõ†Áº∫Â∞ëorg/json/JSONObjectÔºå‰∏äÁΩë‰∏ãËΩΩ‰∏Ä‰∏™json.jarÂú∞ÂùÄÔºöhttp://download.csdn.net/download/haixia_12/8462933 ÊâîËøõÂéªÈáçÊñ∞ÊâßË°åÂ∞±OK‰∫Ü ÊúÄÁªàÂëΩ‰ª§Ôºösqoop import ‚Äìdriver com.microsoft.sqlserver.jdbc.SQLServerDriver ‚Äìconnect ‚Äòjdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName‚Äô ‚Äìtable=tableName ‚Äìtarget-dir /bigdata/data/hdfs/cards ‚Äìsplit-by order_id -m 3 Êü•Áúãsqlserver‰∏äÊúâ‰ªÄ‰πàÊï∞ÊçÆÔºö sqoop list-tables ‚Äìdriver com.microsoft.sqlserver.jdbc.SQLServerDriver ‚Äìconnect ‚Äòjdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName‚Äô sqlserverÂ¶ÇÊûúËøûÊé•Â§±Ë¥•ÔºåË¶ÅÁúãÊòØÂê¶ÂºÄÂêØ‰∫ÜËøúÁ®ãËÆøÈóÆÂíåTCP/UDPÁ´ØÂè£Êò†Â∞Ñ Êä•Èîô17/03/13 17:20:01 ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.17/03/13 17:20:01 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf Â¢ûÂä†Ôºöexport HADOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoopexport HIVE_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive export HIVE_CONF_DIR=/etc/hive/confexport HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib/*]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH ÂÆâË£Ö]]></title>
    <url>%2F2017%2F03%2F08%2FCDH%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[ËøêË°åÁéØÂ¢ÉÔºö ‰∏ªÊú∫IP ‰∏ªÊú∫Âêç ÂÜÖÂ≠ò 1.1.1.147 po-master1 16G 1.1.1.127 po-master2 8G 1.1.1.118 po-slave1 8G 1.1.1.92 po-slave2 8G 1.1.1.230 po-slave3 8G ÈÖçÁΩÆ‰∏ªÊú∫Âêç(ÂàÜÂà´Âú®‰∫îÂè∞Êú∫Âô®‰∏äÊâßË°å)vi /etc/sysconfig/networkhostname +‰∏ªÊú∫Âêç‰æãÂ¶ÇÔºö hostname po-master1 ÈÖçÁΩÆÊò†Â∞ÑÂÖ≥Á≥ª(Êää‰ª•‰∏ã‰∫îÊù°ÂëΩ‰ª§Âú®‰∫îÂè∞Êú∫Âô®‰∏äÊâßË°å)echo ‚Äú1.1.1.147 po-master1‚Äù&gt;&gt;/etc/hostsecho ‚Äú1.1.1.127 po-master2‚Äù&gt;&gt;/etc/hostsecho ‚Äú1.1.1.118 po-slave1‚Äù&gt;&gt;/etc/hostsecho ‚Äú1.1.1.92 po-slave2‚Äù&gt;&gt;/etc/hostsecho ‚Äú1.1.1.230 po-slave3‚Äù&gt;&gt;/etc/hosts ÂÆâË£ÖJDKÔºàÂú®po-master1‰∏äÊâßË°åÔºâ1.‰∏ãËΩΩJDKÂÆâË£ÖÂåÖÔºöjdk-8u102-linux-x64.tar.gzÊ≥®Ôºö‰ΩúËÄÖÊîæÂà∞/soft/javaÂÖ∑‰Ωì‰ΩçÁΩÆÂèØËá™Ë°åÂÆâÊéí ÂÆâË£ÖÔºö 123cd /soft/javamkdir jdk1.8.0_121rpm -ivh jdk-7u76-linux-x64.rpm --prefix=/soft/java ÂàõÂª∫ËøûÊé• 1ln -s -f jdk1.8.0_121/ jdk ÂºÄÊîæÁ´ØÂè£Ôºà‰∫îÂè∞Êú∫Âô®‰∏äÈÉΩÈúÄË¶ÅÈÖçÁΩÆÔºâ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEP/sbin/iptables -A INPUT -s 0.0.0.0/0 -p tcp --dport 22 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 22 -j ACCEPT/sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 1004 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50075 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 1006 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50070 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50470 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50495 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8485 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8480 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8032 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8030 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8031 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8033 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8088 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8040 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8042 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8041 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 10020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 13562 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 19888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60030 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8080 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8085 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9095 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9083 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 10000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 16000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 4181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8019 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 11000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 11001 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 14000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 14001 -j ACCEPT /etc/rc.d/init.d/iptables save ÂÖ≥Èó≠Á´ØÂè£ËØ¶Ëß£-ÂèÇËÄÉCDHÂÆòÁΩëhttps://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_ports_cdh5.html ÊµãËØïÔºàÂèØÂøΩÁï•Ôºâ1/etc/init.d/iptables status ‰∫îÂè∞Êú∫Âô®ÈÖçÁΩÆ‰∫íÁõ∏ÂÖçÁßòÈí•ÁôªÈôÜ1.ÂàõÂª∫sshÊñá‰ª∂Â¶ÇÊûúÂ∑≤ÁªèÂàõÂª∫‰∏çË¶ÅË¶ÜÁõñcat ~/.ssh/id_rsa.pub &gt;&gt;~/.ssh/authorized_keysÂàÜÂà´Êää‰∫îÂè∞Êú∫Âô®ÁöÑÂÖ¨Èí•Âä†ËΩΩÂà∞authorized_keys 2.vi /etc/ssh/sshd_configÊâìÂºÄÂ¶Ç‰∏ãÂÜÖÂÆπHostKey /etc/ssh/ssh_host_rsa_keyRSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys 3.ÈáçÂêØ/etc/init.d/sshd restart 4.ÊµãËØïsshssh po-master1ssh po-master2ssh po-slave1ssh po-slave2ssh po-slave3 ÂêëÂÖ∂‰ªñÊú∫Âô®ÂàÜÂèëjdkscp -rp /soft/java/ root@po-master2:/soft/javascp -rp /soft/java/ root@po-salve1:/soft/javascp -rp /soft/java/ root@po-salve2:/soft/javascp -rp /soft/java/ root@po-salve3:/soft/java ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè(ÂàÜÂà´Âú®‰∫îÂè∞Êú∫Âô®‰∏äÊâßË°å)ÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§Ôºö1234echo "export JAVA_HOME=/soft/java/jdk" &gt;&gt; /etc/profileecho "export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/" &gt;&gt; /etc/profileecho "export CLASSPATH=.:$JAVA_HOME/lib" &gt;&gt; /etc/profile. /etc/profile ÊµãËØïÔºàÂèØÂøΩÁï•Ôºâ1234[root@po-master1 java]# java -versionjava version "1.8.0_121"Java(TM) SE Runtime Environment (build 1.8.0_121-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) ÈÖçÁΩÆNTPÊúçÂä°Âô®ÂíåÂÆ¢Êà∑Á´ØÔºàÂõ†‰∏∫‰ΩøÁî®ÈòøÈáå‰∫ëÊ≠§Â§ÑÁúÅÁï•ÔºâÈÖçÁΩÆmysql1.‰∏ä‰º†mysqlÊñá‰ª∂ÔºàÂçö‰∏ªÊîæÂà∞/soft/mysqlÁõÆÂΩï‰∏ãÔºâ2.Ëß£Âéãcd /soft/mysqltar -zxvf mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz -C /usr/local3.Â∞ÜÁõÆÂΩïÈáçÂëΩÂêçcd /usr/localmv mysql-5.7.17-linux-glibc2.5-x86_64/ mysql4.ÂàõÂª∫dataÁõÆÂΩï mkdir /usr/local/mysql/data 5.ÂÆâË£ÖÊèí‰ª∂ÔºàÁΩë‰∏äÊúâ‰∫∫ËØ¥‰∏çÂÆâË£ÖÊèêÁ§∫libiaoÈîôËØØÔºåÂçö‰∏ªÁî®ÈòøÈáå‰∫ëlibaioÂ∑≤ÁªèÊòØÊúÄÊñ∞ÁâàÊú¨ÔºåÊâÄ‰ª•‰∏çÁî®ÂÆâË£ÖÔºå‰πü‰∏çÁü•ÈÅìÊ≤°ÂÆâË£ÖÊúâ‰ªÄ‰πàÂùèÂ§ÑÔºâ yum install libaio 6.ÂÆâË£Ömysqlcd /usr/local/mysql/bin./mysql_install_db ‚Äìuser=root ‚Äìbasedir=/usr/local/mysql ‚Äìdatadir=//usr/local/mysql/data 1.ÂÆòÁΩë‰∏ãËΩΩyumÊ∫êhttps://dev.mysql.com/downloads/repo/yum/2.ÂÆâË£ÖyumÊ∫êyum localinstall mysql57-community-release-el6-9.noarch.rpm3.ÂÆâË£Ömysqlyum install mysql-community-server 4.ÂàõÂª∫ÁªÑÂíåÁî®Êà∑groupadd mysqluseradd mysql -g mysql 5.‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÂºÄÂêØ‰∫åËøõÂà∂Êó•Âøóvi /etc/my.cnf ÔºàÂú®[mysqld]‰∏ãÈù¢Ê∑ªÂä†Â¶Ç‰∏ãÂÜÖÂÆπÔºâserver-id=1log-bin=/home/mysql/log/logbin.log 6.ÂºÄÂêØÊúçÂä°service mysqld start 7.Êü•ÁúãmysqlÈªòËÆ§ÁöÑÂØÜÁ†Ågrep ‚Äòtemporary password‚Äô /var/log/mysqld.log 8.Ê†πÊçÆÂØÜÁ†ÅËøõÂÖ•mysqlmysql -u root -pALTER USER ‚Äòroot‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚ÄòMyNewPass4!‚Äô; ‰æãÂ¶ÇÔºöALTER USER ‚Äòroot‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚Äòpassword‚Äô;Query OK, 0 rows affected (0.01 sec) Ê≥®ÔºöMySQL‚Äôs validate_password plugin is installed by default. This will require that passwords contain at least one upper case letter, one lower case letter, one digit, and one special character, and that the total password length is at least 8 characters. 9.ÊéàÊùÉÔºàÁªôÂÖ∂‰ªñÂõõÂè∞Êú∫Âô®ÊéàÊùÉÔºâgrant all privileges on oozie. to ‚Äòoozie‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on oozie. to ‚Äòoozie‚Äô@‚Äô10.28.92.19‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on oozie. to ‚Äòoozie‚Äô@‚Äô10.28.100.108‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on oozie. to ‚Äòoozie‚Äô@‚Äô10.28.100.212‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on oozie.* to ‚Äòoozie‚Äô@‚Äô10.28.100.254‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION; GRANT all privileges on . to ‚Äòroot‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‚Äòroot‚Äô@‚Äô10.28.92.19‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‚Äòroot‚Äô@‚Äô10.28.100.108‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‚Äòroot‚Äô@‚Äô10.28.100.212‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‚Äòroot‚Äô@‚Äô10.28.100.254‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION; GRANT ALL PRIVILEGES ON . TO ‚Äòroot‚Äô@‚Äô182.48.105.23‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION; grant all privileges on hive. to ‚Äòhive‚Äô@‚Äôlocalhost‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on hive. to ‚Äòhive‚Äô@‚Äô10.28.92.19‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on hive. to ‚Äòhive‚Äô@‚Äô10.28.100.108‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on hive. to ‚Äòhive‚Äô@‚Äô10.28.100.212‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION;grant all privileges on hive.* to ‚Äòhive‚Äô@‚Äô10.28.100.254‚Äô IDENTIFIED BY ‚Äòpassword‚Äô WITH GRANT OPTION; flush privileges; ÂÖ≥‰∫éÊñ∞ÁâàÊú¨ÁöÑË¥¶Êà∑ËØ¥ÊòéÔºöhttps://dev.mysql.com/doc/refman/5.7/en/adding-users.html 10.ÂàõÂª∫Êï∞ÊçÆÂ∫ìcreate database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; ##ÂÆâË£Öcloudera manager1.‰∏ãËΩΩÂú∞ÂùÄÔºöhttp://archive-primary.cloudera.com/cm5/cm/5/ÔºàÂçö‰∏ª‰∏ãËΩΩÁöÑÊòØcloudera-manager-wheezy-cm5.10.0_amd64.tar.gz ÊîæÂú®/soft/bigdata/clouderamanager‰∏ãÔºâcd /soft/bigdata/clouderamanagertar -xvf cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz ÊµãËØïÔºöÔºàÂèØÈÄâÔºâcat /etc/passwd ÂàõÂª∫Áî®Êà∑ÔºàÊâÄÊúâËäÇÁÇπÔºâuseradd ‚Äìsystem ‚Äìhome=/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server ‚Äìno-create-home ‚Äìshell=/bin/false ‚Äìcomment ‚ÄúCloudera SCM User‚Äù cloudera-scm ÊµãËØïÔºàÂèØÈÄâÔºâ[root@master1 cloudera-scm-server]# cat /etc/passwd‚Ä¶.cloudera-scm:x:498:498:Cloudera SCM User:/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server:/bin/false ‰øÆÊîπ‰∏ªÊú∫ÂêçÂíåÁ´ØÂè£Âè∑cd /soft/bigdata/clouderamanager/cm-5.10.0/etc/cloudera-scm-agentvi config.ini Hostname of the CM server.server_host=po-master1 Port that the CM server is listening on.server_port=7182 ‰∏ãËΩΩÈ©±Âä®ÂåÖ‰∏ãËΩΩmysql-connector-java-*.jarÔºàÂçö‰∏ª‰∏ãËΩΩÁöÑmysql-connector-java-5.1.7-bin.jarÔºâÊîæÂà∞/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib ÁõÆÂΩï‰∏ã ‰∏∫Cloudera Manager 5Âª∫Á´ãÊï∞ÊçÆÂ∫ìÔºö/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/schema/scm_prepare_database.sh mysql scm -hlocalhost -uroot -ppassword ‚Äìscm-host localhost scm password scmÊ†ºÂºèÊòØ:scm_prepare_database.sh Êï∞ÊçÆÂ∫ìÁ±ªÂûã Êï∞ÊçÆÂ∫ì ÊúçÂä°Âô® Áî®Êà∑Âêç ÂØÜÁ†Å ‚Äìscm-host Cloudera_Manager_ServerÊâÄÂú®ÁöÑÊú∫Âô®ÔºåÂêéÈù¢ÈÇ£‰∏â‰∏™‰∏çÁü•ÈÅì‰ª£Ë°®‰ªÄ‰πàÔºåÁõ¥Êé•ÁÖßÊäÑÂÆòÁΩëÁöÑ‰∫Ü„ÄÇ ÂºÄÂêØCloudera Manager 5 ServerÁ´ØÔºö Ê≥®ÊÑèscmËßÅÈù¢ÊòØ‰∏§‰∏™-ÔºåÂõ†‰∏∫ÂçöÂÆ¢ÂÖ≥Á≥ª‰∏çËÉΩÂÖ®ÈÉ®ÊòæÁ§∫ ÂêëÂÖ∂‰ªñÊú∫Âô®ÂàÜÂèëCDHscp -rp /soft/bigdata/clouderamanager root@po-master2:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave1:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave2:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave3:/soft/bigdata ÂáÜÂ§áParcelsÔºåÁî®‰ª•ÂÆâË£ÖCDH5 ÔºàÂçö‰∏ªÊîæÂú®:/soft/bigdata/clouderamanager/cloudera/parcel-repoÔºåË∑ØÂæÑÂøÖÈ°ªÂåÖÂê´cloudera/parcel-repoÔºâÂÆòÊñπÂú∞ÂùÄÔºöhttp://archive.cloudera.com/cdh5/parcelsÂçö‰∏ªÈÄâÊã©ÁöÑhttp://archive.cloudera.com/cdh5/parcels/latest/ ÈúÄË¶Å‰∏ãËΩΩ‰ª•‰∏ã‰∏§‰∏™Êñá‰ª∂‚Ä¢ CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel‚Ä¢ manifest.json ÊâìÂºÄ manifest.jsonÊâæÂà∞CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcelÁöÑhashÂÄºÈáåÁöÑÂÜÖÂÆπ‚Äúhash‚Äù: ‚Äú52f95da433f203a05c2fd33eb0f144e6a5c9d558‚Äùecho ‚Äò52f95da433f203a05c2fd33eb0f144e6a5c9d558‚Äô &gt;&gt; CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha ÊµãËØïÔºàÂèØÈÄâÔºâ[root@master1 parcel-repo]# lltotal 1466572-rw-r‚Äìr‚Äì 1 root root 1501694035 Mar 6 14:24 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel-rw-r‚Äìr‚Äì 1 root root 41 Mar 20 15:26 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha-rw-r‚Äìr‚Äì 1 root root 64807 Mar 17 17:07 manifest.json ÂêØÂä®/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server startÔºà‰∏ªËäÇÁÇπÂêØÂä®Ôºâ/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent startÔºàÊâÄÊúâËäÇÁÇπ‰∏äÂêØÂä®Ôºâ ÊµãËØïnetstat -an | grep 7182netstat -an | grep 7180 ÁôªÈôÜhttp://po-master1:7180ÈªòËÆ§Áî®Êà∑ÂØÜÁ†ÅÈÉΩÊòØadmin ÁÇπÂáªÁªßÁª≠ÈÄâÊã©ÂÖçË¥πÁöÑÁÇπÂáªÁªßÁª≠ ÂãæÈÄâÊú∫Âô® ÁÇπÂáªÊõ¥Â§öÈÄâÈ°π‰øÆÊîπparcelË∑ØÂæÑ/soft/bigdata/clouderamanager/cloudera/parcel-repoÊèíÂÖ•Âõæ5 ÈúÄË¶ÅÈáçÂêØÊâÄÊúâËäÇÁÇπÁöÑÊúçÂä°/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server restartÔºà‰∏ªËäÇÁÇπÂêØÂä®Ôºâ/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent restartÔºàÊâÄÊúâËäÇÁÇπ‰∏äÂêØÂä®Ôºâ ÈÄâÊã©Â¶Ç‰∏ãÂÜÖÂÆπÁÇπÂáªÁªßÁª≠ Á≠âÂæÖÂÆâË£Ö‚Ä¶ ÂÆâË£ÖÂÆåÊàêÔºåÁÇπÂáªÁªßÁª≠ ÂÆâË£ÖËøáÁ®ãÊúâ‰∏™Â∞èÊèêÁ§∫ÔºöÂ∑≤ÂêØÁî®ÈÄèÊòéÂ§ßÈ°µÈù¢ÂéãÁº©ÔºåÂèØËÉΩ‰ºöÂØºËá¥ÈáçÂ§ßÊÄßËÉΩÈóÆÈ¢ò„ÄÇËØ∑ËøêË°å‚Äúecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag‚Äù‰ª•Á¶ÅÁî®Ê≠§ËÆæÁΩÆÔºåÁÑ∂ÂêéÂ∞ÜÂêå‰∏ÄÂëΩ‰ª§Ê∑ªÂä†Âà∞ /etc/rc.local Á≠âÂàùÂßãËÑöÊú¨‰∏≠Ôºå‰ª•‰æøÂú®Á≥ªÁªüÈáçÂêØÊó∂‰∫à‰ª•ËÆæÁΩÆ„ÄÇ‰ª•‰∏ã‰∏ªÊú∫Â∞ÜÂèóÂà∞ÂΩ±ÂìçÔºö ÈÄâÊã©Ëá™ÂÆö‰πâÊúçÂä°ÔºåÈÄâÊã©Ëá™Â∑±ÈúÄË¶ÅÁöÑÊúçÂä° Á≠âÂæÖÂÆâË£Ö ÂÆâË£ÖËøáÁ®ã‰∏≠‰ºöÈÅáÂà∞ÈîôËØØÔºö ÊòØÁº∫Â∞ëjdbcÈ©±Âä®ÊääÊñá‰ª∂ËÄÉÂÖ•Âà∞lib‰∏ãÂç≥ÂèØ ÈÖçÁΩÆNameNode HAËøõÂÖ•HDFSÁïåÈù¢ÔºåÁÇπÂáª‚ÄúÂêØÁî®High Availability‚ÄùËæìÂÖ•NameServiceÂêçÁß∞ÔºåËøôÈáåËÆæÁΩÆ‰∏∫Ôºönameservice1ÔºåÁÇπÂáªÁªßÁª≠ÊåâÈíÆ„ÄÇÈÖçÁΩÆJourNodeÁöÑË∑ØÂæÑÔºå(Âçö‰∏ª‰øÆÊîπ‰∏∫/opt/dfs/jn) ÈîôËØØÊï¥ÁêÜ;Fatal error during KafkaServer startup. Prepare to shutdownkafka.common.InconsistentBrokerIdException: Configured broker.id 52 doesn‚Äôt match stored broker.id 102 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs). at kafka.server.KafkaServer.getBrokerId(KafkaServer.scala:648) at kafka.server.KafkaServer.startup(KafkaServer.scala:187) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:37) at kafka.Kafka$.main(Kafka.scala:67) at com.cloudera.kafka.wrap.Kafka$.main(Kafka.scala:76) at com.cloudera.kafka.wrap.Kafka.main(Kafka.scala) ËøõÂÖ•Âà∞/var/local/kafka/dataÁõÆÂΩïÊü•Áúãmeta.propertieÈáåÈù¢ÁöÑkakfa ÁöÑbroker idÊòØ‰ªÄ‰πà [main]: Metastore Thrift Server threw an exception‚Ä¶javax.jdo.JDOFatalInternalException: Error creating transactional connection factory at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)NestedThrowablesStackTrace:java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) at org.datanucleus.store.AbstractStoreManager.(AbstractStoreManager.java:240) at org.datanucleus.store.rdbms.RDBMSStoreManager.(RDBMSStoreManager.java:286) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the ‚ÄúBONECP‚Äù plugin to create a ConnectionPool gave an error : The specified datastore driver (‚Äúcom.mysql.jdbc.Driver‚Äù) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.(ConnectionFactoryImpl.java:85) ‚Ä¶ 54 moreCaused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (‚Äúcom.mysql.jdbc.Driver‚Äù) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58) at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) ‚Ä¶ 56 more ÊääÈ©±Âä®Á®ãÂ∫èÊîæÂú®/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib SERVER[po-master1] E0103: Could not load service classes, Cannot load JDBC driver class ‚Äòcom.mysql.jdbc.Driver‚Äôorg.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot load JDBC driver class ‚Äòcom.mysql.jdbc.Driver‚Äô at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)Caused by: org.apache.openjpa.persistence.PersistenceException: Cannot load JDBC driver class ‚Äòcom.mysql.jdbc.Driver‚Äô at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:102) at org.apache.openjpa.jdbc.conf.JDBCConfigurationImpl.getDBDictionaryInstance(JDBCConfigurationImpl.java:603) at org.apache.openjpa.jdbc.meta.MappingRepository.endConfiguration(MappingRepository.java:1518) at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:531) at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:456) at org.apache.openjpa.lib.conf.PluginValue.instantiate(PluginValue.java:120) at org.apache.openjpa.conf.MetaDataRepositoryValue.instantiate(MetaDataRepositoryValue.java:68) at org.apache.openjpa.lib.conf.ObjectValue.instantiate(ObjectValue.java:83) at org.apache.openjpa.conf.OpenJPAConfigurationImpl.newMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:967) at org.apache.openjpa.conf.OpenJPAConfigurationImpl.getMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:958) at org.apache.openjpa.kernel.AbstractBrokerFactory.makeReadOnly(AbstractBrokerFactory.java:644) at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:203) at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:156) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:227) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:154) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:60) at org.apache.oozie.service.JPAService.getEntityManager(JPAService.java:514) at org.apache.oozie.service.JPAService.init(JPAService.java:215) at org.apache.oozie.service.Services.setServiceInternal(Services.java:386) at org.apache.oozie.service.Services.setService(Services.java:372) at org.apache.oozie.service.Services.loadServices(Services.java:305) ‚Ä¶ 26 moreCaused by: org.apache.commons.dbcp.SQLNestedException: Cannot load JDBC driver class ‚Äòcom.mysql.jdbc.Driver‚Äô at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1429) at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1371) at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044) at org.apache.openjpa.lib.jdbc.DelegatingDataSource.getConnection(DelegatingDataSource.java:110) at org.apache.openjpa.lib.jdbc.DecoratingDataSource.getConnection(DecoratingDataSource.java:87) at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:91) ‚Ä¶ 46 moreCaused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680) at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526) at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1420) ‚Ä¶ 51 more Êäämysql-connector-java.jarÔºåmysql-connector-java-5.1.39.jarÈ©±Âä®Á®ãÂ∫èÊîæÂú®Ôºö/var/lib/oozie [main]: Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidatesRequired table missing : ‚ÄúDBS‚Äú in Catalog ‚Äú‚Äù Schema ‚Äú‚Äù. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable ‚Äúdatanucleus.autoCreateTables‚Äùorg.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : ‚ÄúDBS‚Äú in Catalog ‚Äú‚Äù Schema ‚Äú‚Äù. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable ‚Äúdatanucleus.autoCreateTables‚Äù at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:485) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3380) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:185) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.(MetaStoreDirectSql.java:136) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:340) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136) SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table &apos;oozie.VALIDATE_CONN&apos; doesn&apos;t exist) org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‚Äòoozie.VALIDATE_CONN‚Äô doesn‚Äôt exist) at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414) Êä•Ëøô‰∏™ÈîôËØØÈúÄË¶Å‰øÆÊîπhiveÁöÑÈÖçÁΩÆ„ÄÇÊêúÁ¥¢autoCreateSchema Êîπ‰∏∫true SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‚Äòoozie.VALIDATE_CONN‚Äô doesn‚Äôt exist)org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‚Äòoozie.VALIDATE_CONN‚Äô doesn‚Äôt exist) at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414) ÁÇπÂáªÁïåÈù¢‰∏äÁöÑOozie ÁÇπÂáªÊìç‰ΩúÔºåÂàõÂª∫OozieÊï∞ÊçÆÂ∫ìË°® ÊúÄÂêéÂØºÂÖ•ÁéØÂ¢ÉÂèòÈáèÂ∞±ÂèØ‰ª•ÊµãËØï‰∫Üexport ZK_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/export HBASE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hbase/export HADOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/export HIVE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/export SQOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/export OOZIE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/oozie/export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$OOZIE_HOME/bin:$ZK_HOME/bin:$HBASE_HOME/bin ÊúÄÂêéÊµãËØïÈò∂ÊÆµÔºåÂèØÂøΩÁï•ÔºåÊú¨ÊñáÂÆå„ÄÇÊµãËØïzookeeperÔºöÂú®po-slave1Ôºåpo-slave2Ôºåpo-slave3‰∏äÊâßË°åÔºàÔºâÔºö1234[root@po-slave1 data]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: leader 1234[root@po-slave2 data]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower 1234[root@po-slave3 dfs]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower 12345678910111213141516171819202122232425262728293031323334[root@po-slave3 dfs]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower[root@po-slave3 dfs]# zkCli.sh Connecting to localhost:21812017-03-21 16:05:56,829 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.10.0--1, built on 01/20/2017 20:10 GMT2017-03-21 16:05:56,832 [myid:] - INFO [main:Environment@100] - Client environment:host.name=po-slave32017-03-21 16:05:56,832 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_1212017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/soft/java/jdk/jre2017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/classes:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../src/java/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf:.:/soft/java/jdk/lib2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd642017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.version=2.6.32-642.13.1.el6.x86_642017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.name=root2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/root2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/opt/dfs2017-03-21 16:05:56,836 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@506c589eWelcome to ZooKeeper!2017-03-21 16:05:56,873 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@975] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2017-03-21 16:05:56,935 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@852] - Socket connection established, initiating session, client: /127.0.0.1:42694, server: localhost/127.0.0.1:21812017-03-21 16:05:56,941 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15aeb7f0edb054c, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0] ls /[controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, config, hbase][zk: localhost:2181(CONNECTED) 1] ÊµãËØïhdfs1234567891011121314[root@po-master1 ~]# hadoop dfs -ls /DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 3 itemsdrwxr-xr-x - hbase hbase 0 2017-03-21 10:30 /hbasedrwxrwxrwt - hdfs supergroup 0 2017-03-20 17:06 /tmpdrwxr-xr-x - hdfs supergroup 0 2017-03-20 17:06 /user[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -mkdir /data2[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -put hdfs-audit.log /data2/hdfs-audit.log[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -ls /data2Found 1 items-rw-r--r-- 3 hdfs supergroup 2908825 2017-03-21 17:28 /data2/hdfs-audit.log ÊµãËØïÁΩëÈ°µ ÊµãËØïhadoopÈ°µÈù¢http://po-master1:50030/jobtracker.jsp ÊµãËØïhive1234567891011121314151617[root@po-master1 ~]# hiveJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future releaseJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive&gt; show databases;OKdefaultTime taken: 1.836 seconds, Fetched: 1 row(s)hive&gt; create database test;OKTime taken: 0.06 secondshive&gt; drop database test;OKTime taken: 0.184 seconds ÊµãËØïhbase12345678910111213141516[root@po-master1 ~]# hbase shellJava HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/21 16:20:23 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.availableHBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.Type "exit&lt;RETURN&gt;" to leave the HBase ShellVersion 1.2.0-cdh5.10.0, rUnknown, Fri Jan 20 12:13:18 PST 2017hbase(main):001:0&gt; listTABLE 0 row(s) in 0.2020 seconds=&gt; []hbase(main):002:0&gt; create 't1','id','name'0 row(s) in 2.3540 seconds=&gt; Hbase::Table - t1hbase(main):003:0&gt; listTABLE t1 1 row(s) in 0.0100 seconds=&gt; ["t1"]hbase(main):004:0&gt; Âç∏ËΩΩÂÆâË£ÖÔºö1umount /soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-agent/process ÈóÆÈ¢òÊ±áÊÄªÔºöÂ¶ÇÊûúÂêØÂä®CDHÂêéÊó†Ê≥ïÁÇπÂáªHDFS ÁöÑWEB UIÔºåÊü•ÁúãÁ´ØÂè£ÂèàÊòØË¢´ÁõëÂê¨ÊòØÂõ†‰∏∫ÈúÄË¶Å‰øÆÊîπÈÖçÁΩÆÔºö![Ê≠§Â§ÑËæìÂÖ•ÂõæÁâáÁöÑÊèèËø∞][19] ÂêåÁêÜjob ÁöÑweb UI‰πüÈúÄË¶Å‰øÆÊîπ![Ê≠§Â§ÑËæìÂÖ•ÂõæÁâáÁöÑÊèèËø∞][20]]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn pca]]></title>
    <url>%2F2017%2F03%2F03%2Fscikit-learn%20pca%2F</url>
    <content type="text"><![CDATA[from sklearn import datasets digits = datasets.load_digits()x = digits.datay = digits.target from sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=666) print(x_train.shape) #(1347, 64)from sklearn.neighbors import KNeighborsClassifierimport timestart = time.clock()knn_clf = KNeighborsClassifier() knn_clf.fit(x_train,y_train)end = time.clock() print(end-start) #0.009107513739889835score = knn_clf.score(x_test,y_test) print(score) #0.986666666667 from sklearn.decomposition import PCA pca = PCA(n_components=2)pca.fit(x_train)X_train_reduction = pca.transform(x_train)X_test_reduction = pca.transform(x_test) start2 = time.clock()knn_clf = KNeighborsClassifier()knn_clf.fit(X_train_reduction,y_train)end2 = time.clock() print(end2-start2) #0.0019209365663966915score = knn_clf.score(X_test_reduction,y_test)print(score) #0.606666666667 print(pca.explainedvariance) pca = PCA(n_components=x_train.shape[1]) pca3 = PCA(0.95)pca3.fit(x_train)print(pca3.ncomponents) #28 start3 = time.clock()knn_clf3 = KNeighborsClassifier()X_train_reduction = pca3.transform(x_train)X_test_reduction = pca3.transform(x_test)knn_clf3.fit(X_train_reduction,y_train)end3 = time.clock() print(end3-start3) #0.006395458395239972score = knn_clf3.score(X_test_reduction,y_test)print(score) #0.98]]></content>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn KNN]]></title>
    <url>%2F2017%2F03%2F02%2Fscikit-learn%20KNN%2F</url>
    <content type="text"><![CDATA[Âú∞ÂùÄÔºöhttp://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier 1234567891011from sklearn.neighbors import KNeighborsClassifierX = [[0], [1], [2], [3]]y = [0, 0, 1, 1]neigh = KNeighborsClassifier(n_neighbors=3)neigh.fit(X, y)print(neigh.predict([[1.1]]))# [0]print(neigh.predict_proba([[0.9]]))#[[ 0.66666667 0.33333333]] ÊâãÂÜôÊï∞Â≠óÁªÉ‰π†Ôºö 12345678910111213141516171819from sklearn import datasetsdigits = datasets.load_digits()x= digits.datay= digits.targetprint(x.shape)#(1797, 64)from sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=666)from sklearn.neighbors import KNeighborsClassifierknn_clf = KNeighborsClassifier(n_neighbors=3)knn_clf.fit(x_train,y_train)score= knn_clf.score(x_test,y_test)print(score)#0.988888888889 Êü•ÊâæÊúÄ‰Ω≥Ë∂ÖÂèÇÊï∞123456789101112beat_score=0break_k =-1for k in range(1,11): knn_clf2 = KNeighborsClassifier(n_neighbors=k) knn_clf2.fit(x_train,y_train) score= knn_clf2.score(x_test,y_test) if(score &gt;beat_score): break_k =k beat_score = scoreprint("beat_score",beat_score)print("break_k",break_k) beat_score 0.991666666667break_k 4 123456789101112131415beat_score=0break_k =-1beat_method=''for method in ['uniform','distance']: for k in range(1,11): knn_clf2 = KNeighborsClassifier(n_neighbors=k,weights=method) knn_clf2.fit(x_train,y_train) score= knn_clf2.score(x_test,y_test) if(score &gt;beat_score): beat_method = method break_k =k beat_score = scoreprint("beat_score",beat_score)print("break_k",break_k) ÁΩëÊ†ºÊêúÁ¥†Ôºö123456789101112131415161718192021222324252627import timestart = time.clock()param_grid=[ &#123; 'weights':['uniform'], 'n_neighbors':[i for i in range(1,11)] &#125;, &#123; 'weights':['distance'], 'n_neighbors':[i for i in range(1,11)], 'p':[i for i in range(1,6)] &#125;]knn_clf = KNeighborsClassifier()from sklearn.model_selection import GridSearchCVgrid_search = GridSearchCV(knn_clf,param_grid)grid_search.fit(x_train,y_train)print("grid_search.best_estimator_:",grid_search.best_estimator_)print("grid_search.best_score_:",grid_search.best_score_)print("grid_search.best_params_:",grid_search.best_params_)knn_clf = grid_search.best_estimator_knn_clf_score = knn_clf.score(x_test,y_test)print(knn_clf_score)end = time.clock()print(end-start) grid_search.bestestimator: KNeighborsClassifier(algorithm=‚Äôauto‚Äô, leaf_size=30, metric=‚Äôminkowski‚Äô, metric_params=None, n_jobs=1, n_neighbors=3, p=3, weights=‚Äôdistance‚Äô)grid_search.bestscore: 0.985386221294grid_search.bestparams: {‚Äòn_neighbors‚Äô: 3, ‚Äòp‚Äô: 3, ‚Äòweights‚Äô: ‚Äòdistance‚Äô}0.983333333333296.18877317471 Â¢ûÂä†Âπ∂Ë°åÂåñÂ§ÑÁêÜÔºö1grid_search = GridSearchCV(knn_clf,param_grid,n_jobs=2,verbose=2) ÈÄüÂ∫¶Âèò‰∏∫Ôºö156.67693082041933 ÊúÄÂÄºÂΩí‰∏ÄÂåñÔºöÈÄÇÁî®‰∫éÂàÜÂ∏ÉÊúâÊòéÊòæËæπÁïåÁöÑÊÉÖÂÜµÔºåÂèóoutlierÂΩ±ÂìçËæÉÂ§ßÂùáÂÄºÊñπÂ∑ÆÂΩí‰∏ÄÂåñÔºöÈÄÇÁî®‰∫éÂàÜÂ∏ÉÊ≤°ÊúâÊòéÊòæËæπÁïåÁöÑÊÉÖÂÜµÔºåÊúâÂèØËÉΩÂ≠òÂú®ÊûÅÁ´ØÊï∞ÂÄº]]></content>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH KuduÂÆâË£ÖÂèä‰ªãÁªç]]></title>
    <url>%2F2017%2F03%2F01%2FCDH%20Kudu%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[ÂÆâË£ÖKuduÁöÑË¶ÅÊ±Ç1.Êìç‰ΩúÁ≥ªÁªüÂíåÁâàÊú¨ÊîØÊåÅCloudera„ÄÇ2.ÈÄöËøáCloudera ManagerÁÆ°ÁêÜKuduÔºåË¶ÅÊ±ÇCloudera Manager5.4.3ÊàñÊõ¥ÊîπÁöÑÁâàÊú¨„ÄÇCDH 5.4ÊàñÊõ¥È´òÁâàÊú¨ÁöÑË¶ÅÊ±Ç„ÄÇÊé®ËçêCloudera Manager5.4.7ÔºåÂõ†‰∏∫ÂÆÉÂ¢ûÂä†‰∫ÜKuduÈááÈõÜÁöÑÊåáÊ†áÊîØÊåÅ„ÄÇ3.Â¶ÇÊûúÂõ∫ÊÄÅÂ≠òÂÇ®ÊòØÂèØÁî®ÁöÑÔºåÂú®ËøôÁßçÈ´òÊÄßËÉΩÁöÑÂ™í‰ΩìÂ≠òÂÇ®Kudu WALsÂèØ‰ª•ÊòæËëóÊîπÂñÑÊó∂ÁöÑKuduÈÖçÁΩÆÈ´òËÄêÁî®ÊÄß„ÄÇ ÈÄöËøáCloudera ManagerÂÆâË£ÖKuduÊÉ≥Ë¶ÅÈÄöËøáCloudera ManagerÂÆâË£ÖKuduÔºåÈ¶ñÂÖà‰∏ãËΩΩKuduÁöÑÂÆöÂà∂ÊúçÂä°ÊèèËø∞Á¨¶ÔºàCSDÔºâÊñá‰ª∂http://archive.cloudera.com/kudu/csd/KUDU-5.10.0.jar?_ga=1.154711740.507225699.1488251539Âπ∂‰∏ä‰º†Âà∞/opt/cloudera/csd/Áî®‰ª•‰∏ãÊìç‰ΩúÁ≥ªÁªüÂëΩ‰ª§ÈáçÂêØClouderaÁÆ°ÁêÜÊúçÂä°Âô®„ÄÇ 1$ sudo service cloudera-scm-server restart ‰ªéhttp://archive.cloudera.com/kudu/parcels/ÁΩëÁ´ôÊâæÂà∞Áõ∏Â∫îÁöÑKudu parcelÔºå Âπ∂Ê∑ªÂä†Âà∞ Parcel Settings &gt; ËøúÁ®ã Parcel Â≠òÂÇ®Â∫ì URL Êé•‰∏ãÊù•ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ã‰∏§ÁßçÊñπÂºèÂÆâË£ÖUsing Parcels1.Hosts &gt; Parcels &gt; Kudu &gt; Download2.Locations &gt; Distribute &gt; Activate3.ÈáçÂêØÈõÜÁæ§4.Actions &gt; Add a Service. &gt; Kudu (Beta) &gt; Continue5.ÈÄâÊã©‰∏Ä‰∏™‰∏ªÊú∫‰Ωú‰∏∫masterÔºå‰∏Ä‰∫õ‰∏ªÊú∫‰Ωú‰∏∫tablet ÊúçÂä°ËßíËâ≤„ÄÇ‰∏Ä‰∏™‰∏ªÊú∫Âç≥ÂèØ‰ª•ÊòØmasterÂèàÂèØ‰ª•ÊòØtabletÔºå‰ΩÜÊòØÂØπ‰∫éÂ§ßÈõÜÁæ§Êù•ËØ¥‰ºöÈÄ†ÊàêÊÄßËÉΩÈóÆÈ¢ò„ÄÇKuduÁöÑmaster‰∏çÊòØ‰∏Ä‰∏™ËµÑÊ∫êÂØÜÈõÜÂûãÁöÑÔºåË¢´ÂÖ∂‰ªñÁõ∏‰ººÁöÑÂ§ÑÁêÜ‰æãÂ¶ÇÔºöHDFSÁöÑnamenodeÔºåYARNÁöÑResourceManagerÊî∂ÈõÜ„ÄÇÈÄâÊã©ÂÆå‰∏ªÊú∫ÁÇπÂáªContinue6.Âú®masters Âíå tabletÈÖçÁΩÆKuduÁöÑÂ≠òÂÇ®‰ΩçÁΩÆÂíåÈ¢ÑÂÜôÊó•ÂøóÔºàWALÔºâÊñá‰ª∂ÔºåCloudera ManagerÂ∞Ü‰ºöÂàõÂª∫Êñá‰ª∂Â§π„ÄÇ6.1 ‰Ω†ÂèØ‰ª•‰ΩøÁî®Áõ∏ÂêåÁöÑÁõÆÂΩïÂ≠òÂÇ®Êï∞ÊçÆÂíåWALs6.2 ‰Ω†‰∏çÂèØ‰ª•Â∞ÜWALs ÁõÆÂΩïËÆæÁΩÆ‰∏∫Êï∞ÊçÆÁöÑÂ≠êÁõÆÂΩï6.3 Â¶ÇÊûú‰Ω†ÁöÑ‰∏ªÊú∫ÂèäÊòØmasterÂèàÊòØtabletÔºåÈÖçÁΩÆ‰∏çÂêåÁöÑmasterÂíåtabletÊúçÂä°Ôºå‰æãÂ¶Ç /data/kudu/master and /data/kudu/tserver. 6.4 Â¶ÇÊûú‰Ω†ÈÄâÊã©ÁöÑÊñá‰ª∂Á≥ªÁªü‰∏çÊîØÊåÅÊâìÊ¥ûÊäÄÊúØ;ÊúçÂä°ÂêØÂä®Â§±Ë¥•6.4.1 ÈÄÄÂá∫ÈÖçÁΩÆÂêëÂØºÔºåÁÇπÂáªCloudera Manager Êé•Âè£‰∏äÁöÑCloudera ÂõæÊ†á6.4.2 Âà∞ Kudu (Beta) ÊúçÂä°6.4.3 Configuration &gt; Kudu (Beta) Service Advanced Configuration Snippet (Safety Valve) &gt; gflagfile5.4.4 Ê∑ªÂä†Â¶Ç‰∏ãÂÜÖÂÆπÂπ∂‰øùÂ≠òÊîπÂèò1--block_manager=file Note: The file block manager does not perform well at scale and should only be used for small-scale development and testing. 7.Â¶ÇÊûú‰Ω†‰∏çÈúÄË¶ÅÈÄÄÂá∫ÔºåÁÇπÂáªContinue„ÄÇKuduÁöÑmasterÂíåtabletÊúçÂä°Â∑≤ÁªèÂêØÂä®„ÄÇÂê¶ÂàôÁÇπÂáª Kudu (Beta) &gt; Actions &gt; Start.8.‰ΩøÁî®ÂÖ∂‰∏≠‰∏ÄÁßçÊñπÊ≥ïÈ™åËØÅÊúçÂä°Ôºö8.1 ÈÄöËøápsÂëΩ‰ª§È™åËØÅ‰∏Ä‰∏™ÊàñÂÖ®ÈÉ®kudu-masterÊàñkudu-tserverÁ®ãÂ∫èÊòØÂê¶ËøêË°å8.2 ÈÄöËøáÊâìÂºÄWebÊµèËßàÂô®‰∏≠ÁöÑURLËÆøÈóÆmasterÊàñËÄÖtablet„ÄÇmasterÁöÑURLÔºö http://:8051/tabletÁöÑURLÔºö http://:8050/9.ÈáçÂêØÁõëÊéßÊúçÂä°Âπ∂Ê£ÄÊü•KuduÁöÑÂõæË°®ÔºåÂà∞Cloudera ManagerÊúçÂä°ÁÇπÂáªService Monitor &gt; Actions &gt; Restart10.ÁÆ°ÁêÜËßíËâ≤„ÄÇKudu (Beta) ÊúçÂä° ‰ΩøÁî® Actions Êù•ÂÅúÊ≠¢ÔºåÂêØÂä®ÔºåÈáçÂêØÊàñËÄÖÂÖ∂‰ªñÁÆ°ÁêÜÊúçÂä° Using PackagesKudu ‰ªìÂ∫ì Âíå Package ÈìæÊé• Operating System Repository Package Individual Packages RHEL RHEL 6 RHEL 6 Ubuntu Trusty Trusty 1.1 ‰∏ãËΩΩKuduÁöÑyumÊ∫êÊñá‰ª∂Âà∞ÔºöRHELÔºà/etc/yum.repos.d/Ôºâ ÊàñËÄÖ UbuntuÔºà/etc/apt/sources.list.d/Ôºâ1.2 Â¶ÇÊûú‰Ω†ÈúÄË¶ÅC++ÂÆ¢Êà∑Á´ØÂºÄÂèëÂ∫ìÊàñKuduÁöÑSDKÔºåRHEL kudu-client-devel ÂåÖ Êàñ UbuntuÁöÑ libkuduclient0 and libkuduclient-dev ÂåÖ1.3 Â¶ÇÊûú‰Ω†‰ΩøÁî®Cloudera ManagerÔºå‰∏çÂÆâË£Ö kudu-master Âíå kudu-tserverÂåÖÔºåCloudera Manager‰ΩøÁî®Kudu Êèê‰æõÊìç‰ΩúÁ≥ªÁªüÂêØÂä®ËÑöÊú¨„ÄÇ 2.Áæ§ÈõÜ‰∏äÂÆâË£ÖKuduÊúçÂä°„ÄÇÂéª‰Ω†ÊÉ≥ÂÆâË£ÖKuduÊâÄÂú®ÁöÑÈõÜÁæ§„ÄÇÂçïÂáª Actions &gt; Add a Service &gt; Kudu &gt; Continue„ÄÇ 3.ÈÄâÊã©‰∏Ä‰∏™‰∏ªÊú∫‰Ωú‰∏∫masterÔºå‰∏Ä‰∫õ‰∏ªÊú∫‰Ωú‰∏∫tablet ÊúçÂä°ËßíËâ≤„ÄÇ‰∏Ä‰∏™‰∏ªÊú∫Âç≥ÂèØ‰ª•ÊòØmasterÂèàÂèØ‰ª•ÊòØtabletÔºå‰ΩÜÊòØÂØπ‰∫éÂ§ßÈõÜÁæ§Êù•ËØ¥‰ºöÈÄ†ÊàêÊÄßËÉΩÈóÆÈ¢ò„ÄÇKuduÁöÑmaster‰∏çÊòØ‰∏Ä‰∏™ËµÑÊ∫êÂØÜÈõÜÂûãÁöÑÔºåË¢´ÂÖ∂‰ªñÁõ∏‰ººÁöÑÂ§ÑÁêÜ‰æãÂ¶ÇÔºöHDFSÁöÑnamenodeÔºåYARNÁöÑResourceManagerÊî∂ÈõÜ„ÄÇÈÄâÊã©ÂÆå‰∏ªÊú∫ÁÇπÂáªContinue4.Âú®masters Âíå tabletÈÖçÁΩÆKuduÁöÑÂ≠òÂÇ®‰ΩçÁΩÆÂíåÈ¢ÑÂÜôÊó•ÂøóÔºàWALÔºâÊñá‰ª∂ÔºåCloudera ManagerÂ∞Ü‰ºöÂàõÂª∫Êñá‰ª∂Â§π„ÄÇ4.1 ‰Ω†ÂèØ‰ª•‰ΩøÁî®Áõ∏ÂêåÁöÑÁõÆÂΩïÂ≠òÂÇ®Êï∞ÊçÆÂíåWALs4.2 ‰Ω†‰∏çÂèØ‰ª•Â∞ÜWALs ÁõÆÂΩïËÆæÁΩÆ‰∏∫Êï∞ÊçÆÁöÑÂ≠êÁõÆÂΩï5.3 Â¶ÇÊûú‰Ω†ÁöÑ‰∏ªÊú∫ÂèäÊòØmasterÂèàÊòØtabletÔºåÈÖçÁΩÆ‰∏çÂêåÁöÑmasterÂíåtabletÊúçÂä°Ôºå‰æãÂ¶Ç /data/kudu/master and /data/kudu/tserver. 5 Â¶ÇÊûú‰Ω†ÈÄâÊã©ÁöÑÊñá‰ª∂Á≥ªÁªü‰∏çÊîØÊåÅÊâìÊ¥ûÊäÄÊúØ;ÊúçÂä°ÂêØÂä®Â§±Ë¥•ÁÇπÂáª Continue &gt; Kudu masters Âíå tablet ÂêØÂä®„ÄÇÂê¶ÂàôÂà∞KuduÁöÑÊúçÂä°‰∏äÁÇπÂáª Actions &gt; Start. 6.‰ΩøÁî®ÂÖ∂‰∏≠‰∏ÄÁßçÊñπÊ≥ïÈ™åËØÅÊúçÂä°Ôºö6.1 ÈÄöËøápsÂëΩ‰ª§È™åËØÅ‰∏Ä‰∏™ÊàñÂÖ®ÈÉ®kudu-masterÊàñkudu-tserverÁ®ãÂ∫èÊòØÂê¶ËøêË°å6.2 ÈÄöËøáÊâìÂºÄWebÊµèËßàÂô®‰∏≠ÁöÑURLËÆøÈóÆmasterÊàñËÄÖtablet„ÄÇmasterÁöÑURLÔºö http://:8051/tabletÁöÑURLÔºö http://:8050/ 7.ÈáçÂêØÁõëÊéßÊúçÂä°Âπ∂Ê£ÄÊü•KuduÁöÑÂõæË°®ÔºåÂà∞Cloudera ManagerÊúçÂä°ÁÇπÂáªService Monitor &gt; Actions &gt; Restart8.ÁÆ°ÁêÜËßíËâ≤„ÄÇKudu ÊúçÂä° ‰ΩøÁî® Actions Êù•ÂÅúÊ≠¢ÔºåÂêØÂä®ÔºåÈáçÂêØÊàñËÄÖÂÖ∂‰ªñÁÆ°ÁêÜÊúçÂä° Ê≠§‰πÉÂÆòÊñπÊé®ËçêÔºåÊñπÊ≥ï1ÔºåÂçö‰∏ª‰∫≤ËØïÊ≤°ÊúâÊàêÂäüÔºåÂÆâË£ÖÂÆå‰∏çÊòæÁ§∫ÊúçÂä° Êú¨ÊñáÂèÇËÄÉÈ°µÈù¢Ôºöhttps://www.cloudera.com/documentation/enterprise/latest/topics/kudu_install_cm.html]]></content>
      <categories>
        <category>cloudera</category>
      </categories>
      <tags>
        <tag>cloudera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn regression]]></title>
    <url>%2F2017%2F03%2F01%2Fscikit-learn%20regression%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122from sklearn import datasetsboston = datasets.load_boston()x= boston.datay= boston.targefrom sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=666)from sklearn import linear_modelclf = linear_model.LinearRegression()clf.fit(x_train,y_train)y_predict = clf.predict(x_test)from sklearn.metrics import mean_squared_errorfrom sklearn.metrics import mean_absolute_errorMSE = mean_squared_error(y_test,y_predict)print(MSE)MAE = mean_absolute_error(y_test,y_predict)print(MAE)]]></content>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canalÂàÜÊûêbinlog]]></title>
    <url>%2F2017%2F02%2F23%2Fcanal%E5%88%86%E6%9E%90binlog%2F</url>
    <content type="text"><![CDATA[ÁéØÂ¢É‰ªãÁªçlinuxÊúçÂä°Âô®Ôºöcentos7.1mysql : 5.7.10canal : 1.0.23 ‰∏Ä.centos7‰∏ãÂÆâË£Ömysql1.‰∏ãËΩΩmysqlÁöÑrepoÊ∫ê1wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 2.ÂÆâË£Ömysql-community-release-el7-5.noarch.rpmÂåÖ1rpm -ivh mysql-community-release-el7-5.noarch.rpm 3.ÂÆâË£Ömysql1yum install mysql-server 4.ËÆæÁΩÆÁî®Êà∑12345CREATE USER &apos;canal&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;localhost&apos; WITH GRANT OPTION;CREATE USER &apos;canal&apos;@&apos;%&apos; IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos; WITH GRANT OPTION;flush privileges; Ê≥®ÔºöcanalÁöÑÂéüÁêÜÊòØÊ®°ÊãüËá™Â∑±‰∏∫mysql slaveÔºåÊâÄ‰ª•ËøôÈáå‰∏ÄÂÆöÈúÄË¶ÅÂÅö‰∏∫mysql slaveÁöÑÁõ∏ÂÖ≥ÊùÉÈôê.CREATE USER canal IDENTIFIED BY ‚Äòcanal‚Äô;GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON . TO ‚Äòcanal‚Äô@‚Äô%‚Äô;‚Äì GRANT ALL PRIVILEGES ON . TO ‚Äòcanal‚Äô@‚Äô%‚Äô ;FLUSH PRIVILEGES; 5.‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂1234[mysqld]log-bin=mysql-bin #Ê∑ªÂä†Ëøô‰∏ÄË°åÂ∞±okbinlog-format=ROW #ÈÄâÊã©rowÊ®°Âºèserver_id=1 #ÈÖçÁΩÆmysql replactionÈúÄË¶ÅÂÆö‰πâÔºå‰∏çËÉΩÂíåcanalÁöÑslaveIdÈáçÂ§ç ‰∫å.ÂÆâË£Öcanal1.‰∏ãËΩΩcanal1wget https://github.com/alibaba/canal/releases/download/v1.0.23/canal.deployer-1.0.23.tar.gz 2.Ëß£ÂéãÁº©12mkdir /root/canaltar zxvf canal.deployer-1.0.23.tar.gz -C /root/canal 3.‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂(Â¶ÇÊûúÊòØËÆøÈóÆÊú¨Êú∫ÔºåÂπ∂‰∏îÁî®Êà∑ÂØÜÁ†ÅÈÉΩ‰∏∫canalÂàô‰∏çÈúÄË¶Å‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂)1vi /root/canal/conf/example/instance.properties 4.ÂêØÂä®1sh /root/canal/bin/startup.sh 5.Êü•ÁúãÊó•Âøó1234567[root@zhm1 ~]# cat /root/canal/logs/canal/canal.logOpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release.2017-02-22 17:40:08.901 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.2017-02-22 17:40:09.069 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.118.128:11111]2017-02-22 17:40:09.758 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... ÂÖ∑‰ΩìinstanceÁöÑÊó•ÂøóÔºö1[root@zhm1 ~]# cat /root/canal/logs/example/example.log 6.ÂÖ≥Èó≠sh /root/canal/bin/stop.sh ‰∏â.ÂÜôÂÆ¢Êà∑Á´Ø‰ª£Á†Å1.Âú®mavenÁöÑsetting.xmlÂä†ÂÖ•ÈòøÈáåÁöÑÈïúÂÉè123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;&lt;/mirror&gt; 2.ÂàõÂª∫ÂàùÂßãÈ°πÁõÆ1mvn archetype:generate -DgroupId=com.alibaba.otter -DartifactId=canal.sample 3.pomÊñá‰ª∂Â¢ûÂä†12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.0.12&lt;/version&gt;&lt;/dependency&gt; 4. ClientSample‰ª£Á†Å123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package com.alibaba.otter;/** * Created by Administrator on 2017/2/23. */import java.net.InetSocketAddress;import java.util.List;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.common.utils.AddressUtils;import com.alibaba.otter.canal.protocol.Message;import com.alibaba.otter.canal.protocol.CanalEntry.Column;import com.alibaba.otter.canal.protocol.CanalEntry.Entry;import com.alibaba.otter.canal.protocol.CanalEntry.EntryType;import com.alibaba.otter.canal.protocol.CanalEntry.EventType;import com.alibaba.otter.canal.protocol.CanalEntry.RowChange;import com.alibaba.otter.canal.protocol.CanalEntry.RowData;public class ClientSample &#123; public static void main(String args[]) &#123; // ÂàõÂª∫ÈìæÊé• CanalConnector connector = CanalConnectors.newSingleConnector( new InetSocketAddress("192.168.118.128",//AddressUtils.getHostIp(), 11111), "example", "", ""); int batchSize = 1000; int emptyCount = 0; try &#123; connector.connect(); connector.subscribe(".*\\..*"); connector.rollback(); int totalEmptyCount = 120; while (emptyCount &lt; totalEmptyCount) &#123; Message message = connector.getWithoutAck(batchSize); // Ëé∑ÂèñÊåáÂÆöÊï∞ÈáèÁöÑÊï∞ÊçÆ long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) &#123; emptyCount++; System.out.println("empty count : " + emptyCount); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; &#125; else &#123; emptyCount = 0; // System.out.printf("message[batchId=%s,size=%s] \n", batchId, size); printEntry(message.getEntries()); &#125; connector.ack(batchId); // Êèê‰∫§Á°ÆËÆ§ // connector.rollback(batchId); // Â§ÑÁêÜÂ§±Ë¥•, ÂõûÊªöÊï∞ÊçÆ &#125; System.out.println("empty too many times, exit"); &#125; finally &#123; connector.disconnect(); &#125; &#125; private static void printEntry(List&lt;Entry&gt; entrys) &#123; for (Entry entry : entrys) &#123; if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) &#123; continue; &#125; RowChange rowChage = null; try &#123; rowChage = RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException("ERROR ## parser of eromanga-event has an error, data:" + entry.toString(),e); &#125; EventType eventType = rowChage.getEventType(); System.out.println(String.format("================&gt; binlog[%s:%s] , name[%s,%s] , eventType : %s", entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); for (RowData rowData : rowChage.getRowDatasList()) &#123; if (eventType == EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); &#125; else if (eventType == EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); &#125; else &#123; System.out.println("-------&gt; before"); printColumn(rowData.getBeforeColumnsList()); System.out.println("-------&gt; after"); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;Column&gt; columns) &#123; for (Column column : columns) &#123; System.out.println(column.getName() + " : " + column.getValue() + " update=" + column.getUpdated()); &#125; &#125;&#125; 5. mysql‰∏ãÊâßË°åÊìç‰Ωú1234567891011mysql&gt; use test;Database changedmysql&gt; CREATE TABLE `xdual` ( -&gt; `ID` int(11) NOT NULL AUTO_INCREMENT, -&gt; `X` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, -&gt; PRIMARY KEY (`ID`) -&gt; ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ;Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into xdual(id,x) values(4,now());Query OK, 1 row affected (0.06 sec) ÂèØ‰ª•‰ªéÊéßÂà∂Âè∞‰∏≠ÁúãÂà∞Ôºö1234567empty count : 1empty count : 2empty count : 3empty count : 4================&gt; binlog[mysql-bin.001946:313661577] , name[test,xdual] , eventType : INSERTID : 4 update=trueX : 2017-02-23 14:20:00 update=true Âõõ.canalÈõÜÁæ§Êê≠Âª∫1.ÂÆâË£ÖzookeeperÁï• 2.‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂123456vi /root/canal/conf/canal.propertiescanal.zkServers=1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181canal.instance.global.spring.xml = classpath:spring/default-instance.xmlvi /root/canal/conf/example/instance.propertiescanal.instance.mysql.slaveId = 1234 Âè¶Â§ñ‰∏Ä‰∏™Êú∫Âô®ÊîπÊàê1235‰∏é1234‰∏çÂêåÂç≥ÂèØ ÂàÜÂà´Âú®‰∏§Âè∞Êú∫Âô®‰∏äÂêØÂä®ÔºåÂèëÁé∞Âè™Êúâ‰∏ÄÂè∞Êú∫Âô®logs‰∏ãÈù¢ÊúâexampleÁõÆÂΩïÔºåÂπ∂‰∏îÊòæÁ§∫ÂêØÂä®ÊàêÂäü 3.ËøõÂÖ•Âà∞zkClientÊü•ÁúãÁä∂ÊÄÅËé∑ÂèñÊ≠£Âú®ËøêË°åÁöÑcanal server1get /otter/canal/destinations/example/running Ëé∑ÂèñÊ≠£Âú®ËøûÊé•ÁöÑcanal client1get /otter/canal/destinations/example/1001/running Ëé∑ÂèñÂΩìÂâçÊúÄÂêé‰∏ÄÊ¨°Ê∂àË¥πËΩ¶ÊàêÂäüÁöÑbinlog1get /otter/canal/destinations/example/1001/cursor 4.ÂÆ¢Êà∑Á´Ø‰ª£Á†Å‰øÆÊîπÂ¶Ç‰∏ãÔºö1CanalConnector connector = CanalConnectors.newClusterConnector("1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181", "example", "", ""); ‰ª£Á†ÅÂêØÂä® ÂÅúÊ≠¢‰∏Ä‰∏™canal ÂÖ®ÈÉ®ÂÅúÊ≠¢canal Êü•ÁúãÂΩìÂâçcanalÊ∂àË¥πÂà∞Âì™‰∏™position[zk: localhost:2181(CONNECTED) 15] get /otter/canal/destinations/example/1001/cursor{‚Äú@type‚Äù:‚Äùcom.alibaba.otter.canal.protocol.position.LogPosition‚Äù,‚Äùidentity‚Äù:{‚ÄúslaveId‚Äù:-1,‚ÄùsourceAddress‚Äù:{‚Äúaddress‚Äù:‚Äùpo-master1‚Äù,‚Äùport‚Äù:3306}},‚Äùpostion‚Äù:{‚Äúincluded‚Äù:false,‚ÄùjournalName‚Äù:‚Äùlogbin.000004‚Äù,‚Äùposition‚Äù:6897322,‚ÄùserverId‚Äù:1,‚Äùtimestamp‚Äù:1492065268000}} Êü•ÁúãmysqlÁöÑÊ®°Âºèshow global variables like ‚Äò%binlog_format%‚Äô;Êü•Áúãbinlog‰ΩçÁΩÆshow master status ÈîôËØØÊï¥ÁêÜÔºöÂêØÂä®ÂèëÁé∞ÈîôËØØÔºö[root@slave1 canal]# cat logs/borrow/borrow.log2017-06-12 03:57:25.931 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]2017-06-12 03:57:25.938 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [borrow/instance.properties]2017-06-12 03:57:25.946 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration, e.g. on the BeanWrapper/BeanFactory!2017-06-12 03:57:25.951 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-borrow2017-06-12 03:57:25.953 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to test_ydxsb..*2017-06-12 03:57:25.953 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful‚Ä¶.2017-06-12 03:57:25.971 [destination = borrow , address = /10.105.0.191:3306 , EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::14969673010002017-06-12 03:57:26.047 [destination = borrow , address = /10.105.0.191:3306 , EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn‚Äôt find the corresponding binlog files from mysql-bin.000001 to mysql-bin.0000012017-06-12 03:57:26.050 [destination = borrow , address = /10.105.0.191:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.105.0.191:3306 has an error, retrying. caused bycom.alibaba.otter.canal.parse.exception.CanalParseException: can‚Äôt find start position for borrow2017-06-12 03:57:26.052 [destination = borrow , address = /10.105.0.191:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:borrow[com.alibaba.otter.canal.parse.exception.CanalParseException: can‚Äôt find start position for borrow ÈúÄË¶ÅÂà†ÊéâborrowÁõÆÂΩï‰∏ãÁöÑmeta.dat Â¶ÇÊûúÊèêÁ§∫ÔºöÊâæ‰∏çÂà∞Êó•ÂøóÁöÑ‰ΩçÁΩÆÂú®mysql‰∏ãÊâßË°å(ÂèØËÉΩÊòØËÆæÁΩÆÂÆåÊ†ºÂºèÔºåÊ†ºÂºè‰∏çÂØπ)set global binlog_checksum=‚ÄôNONE‚Äô]]></content>
      <categories>
        <category>canal</category>
      </categories>
      <tags>
        <tag>canal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm1.0.3ÈõÜÁæ§ÈÉ®ÁΩ≤]]></title>
    <url>%2F2017%2F02%2F15%2Fstorm1.0.3%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Êò®Êó•Â∑≤ÁªèÂèëÂ∏Éstorm1.0.3ÁâàÊú¨ÔºåÊù•Áé©‰∏ÄÁé© ÁéØÂ¢ÉÔºözookeeper3.4.9ËäÇÁÇπÔºöstorm01Ôºåstorm02Ôºåstorm03storm1.0.3 nimbusËäÇÁÇπstorm01ÔºåsupervisorËäÇÁÇπstorm02Ôºåstorm03 1234567[storm@localhost app]$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz[storm@localhost app]$ tar zxvf zookeeper-3.4.9.tar.gz[storm@localhost app]$ ln -s -f zookeeper-3.4.9 zookeeper[storm@localhost app]$ mkdir zookeeper/data[storm@localhost app]$ cd zookeeper/conf/[storm@localhost conf]$ cp zoo_sample.cfg zoo.cfg[storm@localhost conf]$ vi zoo.cfg ‰øÆÊîπdataDir1234dataDir=/home/storm/app/zookeeper/dataserver.1=storm01:2888:3888server.2=storm02:2888:3888server.3=storm03:2888:3888 Âú®storm01‰∏ãÊâßË°å12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 1 &gt; myid Âú®storm02‰∏ãÊâßË°å12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 2 &gt; myid Âú®storm03‰∏ãÊâßË°å12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 3 &gt; myid ÈÉ®ÁΩ≤storm123[storm@localhost app]$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/storm/apache-storm-1.0.3/apache-storm-1.0.3.tar.gz[storm@localhost app]$ tar zxvf apache-storm-1.0.3.tar.gz[storm@localhost app]$ ln -s -f apache-storm-1.0.3 storm ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÔºåjdkÂÆâË£ÖÊ≠§Â§ÑÁï•12345678[storm@localhost app]$ su[root@localhost app]# echo &quot;export JAVA_HOME=/home/storm/app/jdk&quot;&gt;&gt; /etc/profile[root@localhost app]# echo &quot;export ZOOKEEPER_HOME=/home/storm/app/zookeeper&quot;&gt;&gt; /etc/profile[root@localhost app]# echo &quot;export STORM_HOME=/home/storm/app/storm&quot;&gt;&gt; /etc/profile[root@localhost app]# source /etc/profile[root@localhost app]# echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;ZOOKEEPER_HOME&#125;/bin:$&#123;STORM_HOME&#125;/bin:$PATH&quot;&gt;&gt; /etc/profile[root@localhost app]# source /etc/profile[root@localhost app]# su storm ‰øÆÊîπstomÈÖçÁΩÆÊñá‰ª∂12[storm@localhost app]$ cd storm/conf[storm@localhost conf]$ vi storm.yaml 123456789101112storm.zookeeper.servers: - &quot;storm01&quot; - &quot;storm02&quot; - &quot;storm03&quot;storm.local.dir: &quot;/home/storm/app/storm/data&quot;storm.zookeeper.root: &quot;/storm&quot; nimbus.seeds: [&quot;storm01&quot;]supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 ÂêØÂä®zookeeperÔºåÂú®‰∏â‰∏™ËäÇÁÇπÂàÜÂà´ÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§1[storm@storm01 zookeeper]$ zkServer.sh start 1[storm@storm02 zookeeper]$ zkServer.sh start 1[storm@storm03 zookeeper]$ zkServer.sh start ÂêØÂä®stormÈõÜÁæ§Âú®storm01ËäÇÁÇπ‰∏äÂêØÂä®nimbusÔºåuiÔºålogviewer Ôºö123[storm@storm01 zookeeper]$ nohup storm nimbus &amp;[storm@storm01 zookeeper]$ nohup storm ui &amp;[storm@storm01 zookeeper]$ nohup storm logviewer &amp; Âú®stomr02Ôºåstorm03ËäÇÁÇπ‰∏äÂêØÂä®supervisor12[storm@storm02 zookeeper]$ nohup storm supervisor &amp;[storm@storm02 zookeeper]$ nohup storm logviewer &amp; 12[storm@storm03 zookeeper]$ nohup storm supervisor &amp;[storm@storm03 zookeeper]$ nohup storm logviewer &amp;]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017Â≠¶‰π†ËÆ°Âàí]]></title>
    <url>%2F2017%2F02%2F07%2F2017%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[Â§ßÊï∞ÊçÆÂèäÊú∫Âô®Â≠¶‰π†Â≠¶‰π†ËÆ°Âàí ÁºñÁ®ãÂü∫Á°ÄÔºöPythonhttps://cn.udacity.com/course/programming-foundations-with-python--ud036 ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂØºËÆ∫ 72Â∞èÊó∂https://cn.udacity.com/course/intro-to-computer-science--cs101 Êé®ËÆ∫ÁªüËÆ°Â≠¶ 48Â∞èÊó∂https://cn.udacity.com/course/intro-to-inferential-statistics--ud201 ÊèèËø∞ÁªüËÆ°Â≠¶ 48Â∞èÊó∂https://cn.udacity.com/course/intro-to-inferential-statistics--ud201 Êú∫Âô®Â≠¶‰π† 240Â∞èÊó∂https://cn.udacity.com/course/machine-learning--ud262 ÁªüËÆ°Â≠¶ÂÖ•Èó®https://cn.udacity.com/course/intro-to-statistics--st101 Âü∫Á°ÄÁ∫øÊÄß‰ª£Êï∞https://cn.udacity.com/course/linear-algebra-refresher-course--ud953 Êú∫Âô®Â≠¶‰π†https://cn.udacity.com/course/machine-learning-engineer-nanodegree--nd009 Apache Storm ËøõË°åÂÆûÊó∂ÂàÜÊûê 48Â∞èÊó∂https://cn.udacity.com/course/real-time-analytics-with-apache-storm--ud381 BashËÑöÊú¨ 40+Â∞èÊó∂]]></content>
      <categories>
        <category>Â≠¶‰π†ËÆ°Âàí</category>
      </categories>
      <tags>
        <tag>Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-ÊîØÊåÅÂêëÈáèÊú∫]]></title>
    <url>%2F2017%2F02%2F05%2FMathematical%20Support%20Vector%20Machine%2F</url>
    <content type="text"><![CDATA[ÊîØÊåÅÂêëÈáèÊú∫ÔºàSupport Vector MachineÔºåSVMÔºâÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºöÁÇπÂà∞Ë∂ÖÂπ≥Èù¢ÁöÑË∑ùÁ¶ªÂú®ÂàÜÁ±ª‰ªªÂä°‰∏≠Ôºå‰∏∫‰∫ÜËé∑ÂèñÁ®≥ÂÅ•ÁöÑÁ∫øÊÄßÂàÜÁ±ªÂô®Ôºå‰∏Ä‰∏™ÂæàËá™ÁÑ∂ÁöÑÊÉ≥Ê≥ïÊòØÔºåÊâæÂá∫‰∏ÄÊù°ÂàÜÂâ≤Á∫ø‰ΩøÂæó‰∏§‰æßÊ†∑Êú¨‰∏éËØ•ÂàÜÂâ≤Á∫øÁöÑÂπ≥ÂùáË∑ùÁ¶ªË∂≥Â§üÁöÑËøú„ÄÇÂú®Ê¨ßÂºèÁ©∫Èó¥‰∏≠ÔºåÂÆö‰πâ‰∏Ä‰∏™ÁÇπùíôÂà∞Áõ¥Á∫øÔºàÊàñËÄÖÈ´òÁª¥Á©∫Èó¥‰∏≠ÁöÑË∂ÖÂπ≥Èù¢Ôºâùíò^ùëá ùíô+ùëè=0ÁöÑË∑ùÁ¶ªÂÖ¨ÂºèÊòØÔºö ùëü(ùë•)= (|ùíò^ùëá ùíô+ùëè|)/(||ùíò||)Âú®ÂàÜÁ±ªÈóÆÈ¢ò‰∏≠ÔºåÂ¶ÇÊûúËøôÊ†∑ÁöÑÂàÜÂâ≤Á∫øÊàñËÄÖÂàÜÂâ≤Âπ≥Èù¢ËÉΩÂ§üÂáÜÁ°ÆÂú∞Â∞ÜÊ†∑Êú¨ÂàÜÂºÄÔºåÂØπ‰∫éÊ†∑Êú¨{ùíôùëñ,ùë¶ùëñ}‚ààùê∑, ùë¶ùëñ=¬±1 ËÄåË®ÄÔºåËã•ùë¶ùëñ=1ÔºåÂàôÊúâùíò^ùëá ùíôùíä+ùëè‚â•1ÔºåÂèç‰πãËã•ùë¶ùëñ=-1ÔºåÂàôÊúâùíò^ùëá ùíô_ùíä+ùëè‚â§‚àí1.]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm1.0.2ÂÆâË£Ö]]></title>
    <url>%2F2017%2F01%2F28%2Fstorm1.0.2%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[ÂÆâË£ÖÈÖçÁΩÆzookeeperÁï• ‰∏ãËΩΩÔºöwget http://mirrors.cnnic.cn/apache/storm/apache-storm-1.0.2/apache-storm-1.0.2.tar.gz Ëß£ÂéãÔºåÂπ∂ÂàõÂª∫Á¨¶Âè∑ÈìæÊé•‰øÆÊîπprofileÈÖçÁΩÆÊñá‰ª∂‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂Ôºövi storm.yaml Ê∑ªÂä†Â¶Ç‰∏ãÂÜÖÂÆπstorm.zookeeper.servers: - &quot;www.hadoop01.com&quot; storm.zookeeper.root: ‚Äú/storm_1.0.2‚Äùnimbus.seeds: [‚Äúwww.hadoop01.com‚Äù]supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 ÂêØÂä®ÂêØÂä®zookeeperÂêØÂä®nimbus./bin/storm nimbus &amp;ÂêØÂä®ui ./bin/storm ui &amp; ÁôªÂΩïUIÔºöhttp://www.hadoop01.com:8080/index.htmlÂêØÂä®supervisor./bin/storm supervisor &amp;ÂêØÂä®logviewer./bin/storm logviewer &amp; ÊâßË°ådemocd /home/hadoop/apps/storm/storm/examples/storm-starter storm jar storm-starter-topologies-1.0.2.jar org.apache.storm.starter.WordCountTopology first-topology Êü•ÁúãUIÁÇπÂáªspout:]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stormÂü∫Á°ÄÁü•ËØÜ]]></title>
    <url>%2F2017%2F01%2F27%2Fstorm%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[StormÂü∫Êú¨Ê¶ÇÂøµStormÊòØ‰∏Ä‰∏™ÂàÜÂ∏ÉÂºèËÆ°ÁÆóÊ°ÜÊû∂Ôºå‰∏ªË¶ÅÁî±ClojureÁºñÁ®ãËØ≠Ë®ÄÁºñÂÜô„ÄÇÊúÄÂàùÊòØÁî±Nathan MarzÂèäÂÖ∂Âõ¢ÈòüÂàõÂª∫‰∫éBackTypeÔºåËØ•È°πÁõÆÂú®Ë¢´TwitterÂèñÂæóÂêéÂºÄÊ∫ê„ÄÇÂÆÉ‰ΩøÁî®Áî®Êà∑ÂàõÂª∫ÁöÑ‚ÄúÁÆ°ÔºàspoutsÔºâ‚ÄùÂíå‚ÄúËû∫Ê†ìÔºàboltsÔºâ‚ÄùÊù•ÂÆö‰πâ‰ø°ÊÅØÊ∫êÂíåÊìç‰ΩúÊù•ÂÖÅËÆ∏ÊâπÈáè„ÄÅÂàÜÂ∏ÉÂºèÂ§ÑÁêÜÊµÅÂºèÊï∞ÊçÆ„ÄÇÊúÄÂàùÁöÑÁâàÊú¨ÂèëÂ∏É‰∫é2011Âπ¥9Êúà17Êó•„ÄÇStormÂ∫îÁî®Ë¢´ËÆæËÆ°Êàê‰∏∫‰∏Ä‰∏™ÊãìÊâëÁªìÊûÑÔºåÂÖ∂Êé•Âè£ÂàõÂª∫‰∏Ä‰∏™ËΩ¨Êç¢‚ÄúÊµÅ‚Äù„ÄÇÂÆÉÊèê‰æõ‰∏éMapReduce‰Ωú‰∏öÁ±ª‰ººÁöÑÂäüËÉΩÔºåÂΩìÈÅáÂà∞ÂºÇÂ∏∏Êó∂ËØ•ÊãìÊâëÁªìÊûÑÁêÜËÆ∫‰∏äÂ∞Ü‰∏çÁ°ÆÂÆöÂú∞ËøêË°åÔºåÁõ¥Âà∞ÂÆÉË¢´ÊâãÂä®ÁªàÊ≠¢ ‰∏ªË¶ÅÁâàÊú¨ ÁâàÊú¨ ÂèëÂ∏ÉÊó•Êúü 0.9.0.1 2013Âπ¥12Êúà8Êó• 0.9.0 2013Âπ¥12Êúà8Êó• 0.8.0 2012Âπ¥8Êúà2Êó• 0.7.0 2012Âπ¥2Êúà28Êó• 0.6.0 2011Âπ¥12Êúà15Êó• 0.5.0 2011Âπ¥9Êúà19Êó• ÁºñÁ®ãÊ®°ÂûãTopology‰∏Ä‰∏™ÂÆûÊó∂ËÆ°ÁÆóÂ∫îÁî®Á®ãÂ∫èÈÄªËæëË¢´Â∞ÅË£ÖÂú®TopologyÂØπË±°‰∏≠Ôºå Á±ª‰ººHadoop‰∏≠ÁöÑjobÔºå Topology‰ºö‰∏ÄÁõ¥ËøêË°åÁõ¥Âà∞‰Ω†ÊòæÂºèÊùÄÊ≠ªÂÆÉ ###DataSourceÂ§ñÈÉ®Êï∞ÊçÆÊ∫ê SpoutÊé•ÂèóÂ§ñÈÉ®Êï∞ÊçÆÊ∫êÁöÑÁªÑ‰ª∂ÔºåÂ∞ÜÂ§ñÈÉ®Êï∞ÊçÆÊ∫êËΩ¨ÂåñÊàêStormÂÜÖÈÉ®ÁöÑÊï∞ÊçÆÔºå‰ª•Tuple‰∏∫Âü∫Êú¨ÁöÑ‰º†ËæìÂçïÂÖÉ‰∏ãÂèëÁªôBolt BoltÊé•ÂèóSpoutÂèëÈÄÅÁöÑÊï∞ÊçÆÔºåÊàñ‰∏äÊ∏∏ÁöÑboltÁöÑÂèëÈÄÅÁöÑÊï∞ÊçÆ„ÄÇÊ†πÊçÆ‰∏öÂä°ÈÄªËæëËøõË°åÂ§ÑÁêÜ„ÄÇÂèëÈÄÅÁªô‰∏ã‰∏Ä‰∏™BoltÊàñËÄÖÊòØÂ≠òÂÇ®Âà∞ÊüêÁßç‰ªãË¥®‰∏ä„ÄÇ‰ªãË¥®ÂèØ‰ª•ÊòØRedisÂèØ‰ª•ÊòØmysqlÔºåÊàñËÄÖÂÖ∂‰ªñ„ÄÇ TupleStormÂÜÖÈÉ®‰∏≠Êï∞ÊçÆ‰º†ËæìÁöÑÂü∫Êú¨ÂçïÂÖÉÔºåÈáåÈù¢Â∞ÅË£Ö‰∫Ü‰∏Ä‰∏™ListÂØπË±°ÔºåÁî®Êù•‰øùÂ≠òÊï∞ÊçÆ„ÄÇ StreamGrouping:Êï∞ÊçÆÂàÜÁªÑÁ≠ñÁï•7ÁßçÔºöshuffleGrouping(RandomÂáΩÊï∞),NonGrouping(RandomÂáΩÊï∞,ÁõÆÂâçÂíåshuffleGrouping‰∏ÄÊ†∑),FieldGrouping(HashÂèñÊ®°),Local or ShuffleGrouping ÔºàÊú¨Âú∞ÊàñÈöèÊú∫Ôºå‰ºòÂÖàÊú¨Âú∞Ôºâ,Fields groupingÔºàÊ†πÊçÆTuple‰∏≠ÁöÑÊüê‰∏Ä‰∏™FiledÊàñËÄÖÂ§ö‰∏™FiledÁöÑÊòØÂÄºÊù•ÂàíÂàÜ„ÄÇ ÊØîÂ¶ÇStreamÊ†πÊçÆField‰∏∫user-idÊù•groupingÔºå Áõ∏Âêåuser-idÂÄºÁöÑTuple‰ºöË¢´ÂàÜÂèëÂà∞Áõ∏ÂêåÁöÑTask‰∏≠Ôºâ,Global groupingÔºàÊï¥‰∏™Stream‰ºöÈÄâÊã©‰∏Ä‰∏™Task‰Ωú‰∏∫ÂàÜÂèëÁöÑÁõÆÁöÑÂú∞Ôºå ÈÄöÂ∏∏ÊòØÊúÄÊñ∞ÁöÑÈÇ£‰∏™idÁöÑTaskÔºâDirect groupingÔºà‰∫ßÁîüÊï∞ÊçÆÁöÑSpout/BoltËá™Â∑±ÊòéÁ°ÆÂÜ≥ÂÆöËøô‰∏™TupleË¢´BoltÁöÑÈÇ£‰∫õTaskÊâÄÊ∂àË¥πÔºâ Storm‰ºòÁÇπÂÅ•Â£ÆÊÄßÂΩìWorkerÂ§±ÊïàÊàñÊú∫Âô®Âá∫Áé∞ÊïÖÈöúÊó∂Ôºå Ëá™Âä®ÂàÜÈÖçÊñ∞ÁöÑWorkerÊõøÊç¢Â§±ÊïàWorker ÂáÜÁ°ÆÊÄßÈááÁî®AckerÊú∫Âà∂Ôºå‰øùËØÅÊï∞ÊçÆ‰∏ç‰∏¢Â§±ÈááÁî®‰∫ãÂä°Êú∫Âà∂Ôºå‰øùËØÅÊï∞ÊçÆÂáÜÁ°ÆÊÄß stormÊû∂ÊûÑ StormÁöÑ‰∏ªÁ∫ø‰∏ªË¶ÅÂåÖÊã¨4Êù°Ôºönimbus, supervisor, workerÂíåtask„ÄÇ ÂØπ‰∫éstorm0.9.6ÁöÑÈÖçÁΩÆÊñá‰ª∂nimbusÈÖçÁΩÆÂèÇÊï∞ËøòÊòØnimbus.hosts Áé∞Âú®Êñ∞ÁâàÊú¨1.0.2Â∑≤Áªè‰øÆÊîπ‰∏∫nimbus.seeds,Â∑≤ÁªèÂèØ‰ª•ÊîØÊåÅHAÊ≠§Â§Ñ‰πã‰∏ÄÈÖçÁΩÆnimbus.seeds: XX„ÄÇÊ≠§Â§Ñnimbus.seeds:ÂêéÈù¢Ë¶ÅÊúâ‰∏Ä‰∏™Á©∫Ê†ºÔºåÂê¶ÂàôÂêØÂä®Êä•ÈîôstormÂêØÂä®nimbusÁöÑÊó∂ÂÄôjps‰ºöÊúâ‰∏Ä‰∏™ËøõÁ®ãconfig_valueÁÑ∂ÂêéÂèòÊàênimbus„ÄÇÂÖ∂‰ªñÂêØÂä®Âêå‰∏ä]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PythonÊú∫Âô®Â≠¶‰π†ÂáÜÂ§á]]></title>
    <url>%2F2017%2F01%2F27%2FPython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[‰∏ãËΩΩNumPyhttp://download.csdn.net/download/z1137730824/8384347Ôºànumpy 64‰ΩçÔºâhttp://download.csdn.net/detail/u010156024/9302649Ôºànumpy 32‰ΩçÔºâ ‰∏ãËΩΩmatplotlibhttps://pypi.python.org/pypi/matplotlib/ Âçö‰∏ªÈÄâÊã©matplotlib-2.0.0-cp27-cp27m-win_amd64.whl (md5)‰∏ãËΩΩÂÆåÊäämatplotlib-2.0.0-cp27-cp27m-win_amd64.whlÊîπÊàêmatplotlib-2.0.0-cp27-cp27m-win_amd64.zipËß£ÂéãÂà∞PythonÁõÆÂΩï‰∏ãÁöÑLibÊñá‰ª∂Â§π‰∏ãÁöÑsite-packagesÁõÆÂΩï ÂÆâË£ÖdateutilÁõ¥Êé•Áî®pip install python-dateutilÊàñËÄÖÂéªÁΩë‰∏ä‰∏ãËΩΩhttps://pypi.python.org/pypi/python-dateutil ÂÆâË£Öpyparsingpip install pyparsingÊàñËÄÖÂéªÁΩë‰∏ä‰∏ãËΩΩhttps://pypi.python.org/pypi/pyparsing/2.0.2http://pyparsing.wikispaces.com/Download+and+Installation ÊúÄÁªàÂÆâË£ÖÂ¶Ç‰∏ãÂÆâË£ÖÂåÖÔºönumpy, setuptools, python-dateutil, pytz, pyparsing, and cyclerÔºåfunctools32]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PythonÊò•ËäÇÁ•ùÁ¶èËØ≠Ëá™Âä®ÂõûÂ§ç]]></title>
    <url>%2F2017%2F01%2F27%2FPython%E6%98%A5%E8%8A%82%E7%A5%9D%E7%A6%8F%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%9B%9E%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[È¶ñÂÖàÂÆâË£Ö‰∏§‰∏™Â∫ìpip install itchat pillow 1234567891011121314151617ÁéØÂ¢Éwin7Ôºåpyython3ÁºñÂÜôpyÊñá‰ª∂ËæìÂÖ•‰ª•‰∏ãÂÜÖÂÆπimport itchat, time, refrom itchat.content import *@itchat.msg_register([TEXT])def text_reply(msg): match = re.search('Âπ¥', msg['Text']).span() if match: itchat.send(('È∏°Âπ¥Â§ßÂêâ'), msg['FromUserName'])@itchat.msg_register([PICTURE, RECORDING, VIDEO, SHARING])def other_reply(msg): itchat.send(('È∏°Âπ¥Â§ßÂêâ'), msg['FromUserName'])itchat.auto_login(enableCmdQR=True,hotReload=True)itchat.run() ËøêË°åÂêéÔºåÊâ´ÊèèÁîüÊàêÁöÑ‰∫åÁª¥Á†ÅÂç≥ÂèØ ÁôªÂΩïÊàêÂäüÊúâ‰ª•‰∏ãÊèêÁ§∫]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-ÂõûÂΩíÁÆóÊ≥ïÂÆû‰æã]]></title>
    <url>%2F2017%2F01%2F27%2FMathematical%20regression%20%2F</url>
    <content type="text"><![CDATA[Ê¶ÇÂøµÊ¢≥ÁêÜÔºöÊï∞Â≠¶ÊúüÊúõÔºöÂú®Ê¶ÇÁéáËÆ∫ÂíåÁªüËÆ°Â≠¶‰∏≠ÔºåÊï∞Â≠¶ÊúüÊúõ(mean)ÔºàÊàñÂùáÂÄºÔºå‰∫¶ÁÆÄÁß∞ÊúüÊúõÔºâÊòØËØïÈ™å‰∏≠ÊØèÊ¨°ÂèØËÉΩÁªìÊûúÁöÑÊ¶ÇÁéá‰πò‰ª•ÂÖ∂ÁªìÊûúÁöÑÊÄªÂíåÔºåÊòØÊúÄÂü∫Êú¨ÁöÑÊï∞Â≠¶ÁâπÂæÅ‰πã‰∏Ä„ÄÇÂÆÉÂèçÊò†ÈöèÊú∫ÂèòÈáèÂπ≥ÂùáÂèñÂÄºÁöÑÂ§ßÂ∞è ÊñπÂ∑ÆÔºöÔºàvariance)ÊòØÂú®Ê¶ÇÁéáËÆ∫ÂíåÁªüËÆ°ÊñπÂ∑ÆË°°ÈáèÈöèÊú∫ÂèòÈáèÊàñ‰∏ÄÁªÑÊï∞ÊçÆÊó∂Á¶ªÊï£Á®ãÂ∫¶ÁöÑÂ∫¶Èáè„ÄÇÊ¶ÇÁéáËÆ∫‰∏≠ÊñπÂ∑ÆÁî®Êù•Â∫¶ÈáèÈöèÊú∫ÂèòÈáèÂíåÂÖ∂Êï∞Â≠¶ÊúüÊúõÔºàÂç≥ÂùáÂÄºÔºâ‰πãÈó¥ÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶„ÄÇÁªüËÆ°‰∏≠ÁöÑÊñπÂ∑ÆÔºàÊ†∑Êú¨ÊñπÂ∑ÆÔºâÊòØÊØè‰∏™Ê†∑Êú¨ÂÄº‰∏éÂÖ®‰ΩìÊ†∑Êú¨ÂÄºÁöÑÂπ≥ÂùáÊï∞‰πãÂ∑ÆÁöÑÂπ≥ÊñπÂÄºÁöÑÂπ≥ÂùáÊï∞„ÄÇÂú®ËÆ∏Â§öÂÆûÈôÖÈóÆÈ¢ò‰∏≠ÔºåÁ†îÁ©∂ÊñπÂ∑ÆÂç≥ÂÅèÁ¶ªÁ®ãÂ∫¶ÊúâÁùÄÈáçË¶ÅÊÑè‰πâ„ÄÇ Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÔºöÂú®Êï∞Â≠¶‰∏≠ÔºåËøûÁª≠ÂûãÈöèÊú∫ÂèòÈáèÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÔºàÂú®‰∏çËá≥‰∫éÊ∑∑Ê∑ÜÊó∂ÂèØ‰ª•ÁÆÄÁß∞‰∏∫ÂØÜÂ∫¶ÂáΩÊï∞ÔºâÊòØ‰∏Ä‰∏™ÊèèËø∞Ëøô‰∏™ÈöèÊú∫ÂèòÈáèÁöÑËæìÂá∫ÂÄºÔºåÂú®Êüê‰∏™Á°ÆÂÆöÁöÑÂèñÂÄºÁÇπÈôÑËøëÁöÑÂèØËÉΩÊÄßÁöÑÂáΩÊï∞„ÄÇËÄåÈöèÊú∫ÂèòÈáèÁöÑÂèñÂÄºËêΩÂú®Êüê‰∏™Âå∫Âüü‰πãÂÜÖÁöÑÊ¶ÇÁéáÂàô‰∏∫Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Âú®Ëøô‰∏™Âå∫Âüü‰∏äÁöÑÁßØÂàÜ„ÄÇÂΩìÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Â≠òÂú®ÁöÑÊó∂ÂÄôÔºåÁ¥ØÁßØÂàÜÂ∏ÉÂáΩÊï∞ÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÁöÑÁßØÂàÜ„ÄÇÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏ÄËà¨‰ª•Â∞èÂÜôÊ†áËÆ∞Ê≠£ÊÄÅÂàÜÂ∏ÉÊòØÈáçË¶ÅÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇÂÆÉÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊòØÔºöÈöèÁùÄÂèÇÊï∞ŒºÂíåœÉÂèòÂåñÔºåÊ¶ÇÁéáÂàÜÂ∏É‰πü‰∫ßÁîüÂèòÂåñ„ÄÇÊúüÊúõÔºöŒºÊñπÂ∑ÆÔºöœÉ^2‰∏≠‰ΩçÊï∞ÔºöŒº‰ºó44o6fdeswq DFGI-Êï∞ÔºöŒºÂÅèÂ∫¶Ôºö0Â≥∞Â∫¶Ôºö3\]Ê≠£ÊÄÅÂàÜÂ∏ÉÔºöÂèàÁß∞‰∏∫Â∏∏ÊÄÅÂàÜÂ∏ÉÔºåÈ´òÊñØÂàÜÂ∏É„ÄÇËã•ÈöèÊú∫ÂèòÈáèXÊúç‰ªé‰∏Ä‰∏™Êï∞Â≠¶ÊúüÊúõ‰∏∫Œº„ÄÅÊñπÂ∑Æ‰∏∫œÉ^2ÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåËÆ∞‰∏∫N(ŒºÔºåœÉ^2)„ÄÇÂÖ∂Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏∫Ê≠£ÊÄÅÂàÜÂ∏ÉÁöÑÊúüÊúõÂÄºŒºÂÜ≥ÂÆö‰∫ÜÂÖ∂‰ΩçÁΩÆÔºåÂÖ∂Ê†áÂáÜÂ∑ÆœÉÂÜ≥ÂÆö‰∫ÜÂàÜÂ∏ÉÁöÑÂπÖÂ∫¶„ÄÇÂΩìŒº = 0,œÉ = 1Êó∂ÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉÊòØÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ Á∫øÊÄßÂõûÂΩíÔºöÁ∫øÊÄßÂõûÂΩíÊòØÂà©Áî®Êï∞ÁêÜÁªüËÆ°‰∏≠ÂõûÂΩíÂàÜÊûêÔºåÊù•Á°ÆÂÆö‰∏§ÁßçÊàñ‰∏§Áßç‰ª•‰∏äÂèòÈáèÈó¥Áõ∏‰∫í‰æùËµñÁöÑÂÆöÈáèÂÖ≥Á≥ªÁöÑ‰∏ÄÁßçÁªüËÆ°ÂàÜÊûêÊñπÊ≥ïÔºåËøêÁî®ÂçÅÂàÜÂπøÊ≥õ„ÄÇÂÖ∂Ë°®ËææÂΩ¢Âºè‰∏∫y = w‚Äôx+eÔºåe‰∏∫ËØØÂ∑ÆÊúç‰ªéÂùáÂÄº‰∏∫0ÁöÑÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ ÂõûÂΩíÊï∞ÊçÆÔºöhttp://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption Â±ûÊÄß‰ø°ÊÅØÔºö 1.dateÔºöÊ†ºÂºèdd/mm/yyyyÊó•Êúü2.timeÔºöÊ†ºÂºèHHÊó∂Èó¥ÔºöMMÔºöSS3.global_active_powerÔºöÂÆ∂Áî®ÂÖ®ÁêÉÂàÜÈíüÂπ≥ÂùáÊúâÂäüÂäüÁéáÔºàÂçÉÁì¶Ôºâ4.global_reactive_power: ÂÆ∂Áî®ÂÖ®ÁêÉÂàÜÈíüÂπ≥ÂùáÊó†ÂäüÂäüÁéáÔºàÂçÉÁì¶Ôºâ5.voltageÔºöÂàÜÈíüÂπ≥ÂùáÁîµÂéãÔºà‰ºèÁâπÔºâ6.global_intensityÔºöÂÆ∂Áî®ÂÖ®ÁêÉÂàÜÈíüÂπ≥ÂùáÁîµÊµÅÂº∫Â∫¶ÔºàÂÆâÂüπÔºâ7.sub_metering_1ÔºöËÉΩËÄóÂàÜÈ°πËÆ°Èáè1Âè∑Ôºà‰∏≠ÊúâÂäüÁîµËÉΩÁîµËÉΩÔºâ„ÄÇÂÆÉ‰∏éÂé®ÊàøÁõ∏ÂØπÂ∫îÔºå‰∏ªË¶ÅÂåÖÊã¨Ê¥óÁ¢óÊú∫„ÄÅÁÉ§ÁÆ±ÂíåÂæÆÊ≥¢ÁÇâÔºàÁÉ≠Êùø‰∏çÊòØÁîµÂä®ÁöÑÔºåËÄåÊòØÁáÉÊ∞îÈ©±Âä®ÁöÑÔºâ„ÄÇ8.sub_metering_2ÔºöËÉΩËÄóÂàÜÈ°πËÆ°Èáè2Âè∑Ôºà‰∏≠ÊúâÂäüÁîµËÉΩÁîµËÉΩÔºâ„ÄÇÂÆÉÂØπÂ∫îÊ¥óË°£ÊàøÔºåÂåÖÊã¨Ê¥óË°£Êú∫„ÄÅÊªöÁ≠íÁÉòÂπ≤Êú∫„ÄÅÂÜ∞ÁÆ±ÂíåÁÅØ„ÄÇ9.sub_metering_3ÔºöËÉΩËÄóÂàÜÈ°πËÆ°Èáè3Âè∑Ôºà‰∏≠ÊúâÂäüÁîµËÉΩÁîµËÉΩÔºâ„ÄÇÂÆÉÁõ∏ÂΩì‰∫é‰∏Ä‰∏™ÁîµÁÉ≠Ê∞¥Âô®Âíå‰∏Ä‰∏™Á©∫Ë∞É„ÄÇ 1234567891011121314151617import pandas as pdimport numpy as npimport timeimport sklearnimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitpath='C:/Users/zhanghongming/Documents/data/100.txt'names = ['Date','Time','Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']df=pd.read_csv(path,sep=';')print(df.head()) Date Time Global_active_power Global_reactive_power Voltage \ 0 16/12/2006 17:24:00 4.216 0.418 234.841 16/12/2006 17:25:00 5.360 0.436 233.632 16/12/2006 17:26:00 5.374 0.498 233.293 16/12/2006 17:27:00 5.388 0.502 233.744 16/12/2006 17:28:00 3.666 0.528 235.68 Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_30 18.4 0.0 1.0 17.01 23.0 0.0 1.0 16.02 23.0 0.0 2.0 17.03 23.0 0.0 1.0 17.04 15.8 0.0 1.0 17.0 ÁúãÊâÄÊúâÁöÑÂèòÈáèÂÄº123for i in df.columns: print(df[i].value_counts()) Name: Date, dtype: int6419:01:00 117:27:00 118:24:00 1 .. Name: Time, Length: 99, dtype: int644.230 22.912 24.218 26.072 15.412 1 .. Name: Global_active_power, Length: 96, dtype: int640.000 330.090 70.054 40.144 3 .. Name: Global_reactive_power, dtype: int64235.84 3234.20 2235.68 2233.74 2 .. Name: Voltage, Length: 90, dtype: int6412.4 713.8 515.8 5 ..Name: Global_intensity, dtype: int640.0 99Name: Sub_metering_1, dtype: int641.0 500.0 262.0 8 Name: Sub_metering_2, dtype: int6417.0 7716.0 1818.0 4 Name: Sub_metering_3, dtype: int64 12345678910111213141516171819#Á©∫ÂÄºÂ§ÑÁêÜnew_df= df.replace('?',np.nan)datas = new_df.dropna(how='any')#ÂÆö‰πâÊó∂Èó¥Ê†ºÂºèÂåñdef datae_format(dt): t = time.strptime(' '.join(dt),'%d/%m/%Y %H:%M:%S') return (t.tm_year,t.tm_mon,t.tm_mday,t.tm_hour,t.tm_min,t.tm_sec)##ÂàÜÊûêÂäüÁéáÂíåÊó∂Èó¥ÁöÑÁ∫øÊÄßÂÖ≥Á≥ª„ÄÇÂ∞ÜÊó∂Èó¥ËΩ¨Êç¢‰∏∫ËøûÁª≠ÁöÑX = datas[names[0:2]]X = X.apply(lambda x :pd.Series(datae_format(x)),axis=1)Y = datas[names[2]]print(X.head(5))print(Y.head(5)) 0 1 2 3 4 5 0 2006 12 16 17 24 01 2006 12 16 17 25 02 2006 12 16 17 26 03 2006 12 16 17 27 04 2006 12 16 17 28 00 4.2161 5.3602 5.3743 5.3884 3.666Name: Global_active_power, dtype: float64 ÂáΩÊï∞ËÆ≤Ëß£sklearn.model_selection.train_test_splitÈöèÊú∫ÂàíÂàÜËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ‰∏ÄËà¨ÂΩ¢ÂºèÔºötrain_test_splitÊòØ‰∫§ÂèâÈ™åËØÅ‰∏≠Â∏∏Áî®ÁöÑÂáΩÊï∞ÔºåÂäüËÉΩÊòØ‰ªéÊ†∑Êú¨‰∏≠ÈöèÊú∫ÁöÑÊåâÊØî‰æãÈÄâÂèñtrain dataÂíåtestdataÔºåÂΩ¢Âºè‰∏∫ÔºöX_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)ÂèÇÊï∞Ëß£ÈáäÔºötrain_dataÔºöÊâÄË¶ÅÂàíÂàÜÁöÑÊ†∑Êú¨ÁâπÂæÅÈõÜtrain_targetÔºöÊâÄË¶ÅÂàíÂàÜÁöÑÊ†∑Êú¨ÁªìÊûútest_sizeÔºöÊ†∑Êú¨Âç†ÊØîÔºåÂ¶ÇÊûúÊòØÊï¥Êï∞ÁöÑËØùÂ∞±ÊòØÊ†∑Êú¨ÁöÑÊï∞Èáèrandom_stateÔºöÊòØÈöèÊú∫Êï∞ÁöÑÁßçÂ≠ê„ÄÇÈöèÊú∫Êï∞ÁßçÂ≠êÔºöÂÖ∂ÂÆûÂ∞±ÊòØËØ•ÁªÑÈöèÊú∫Êï∞ÁöÑÁºñÂè∑ÔºåÂú®ÈúÄË¶ÅÈáçÂ§çËØïÈ™åÁöÑÊó∂ÂÄôÔºå‰øùËØÅÂæóÂà∞‰∏ÄÁªÑ‰∏ÄÊ†∑ÁöÑÈöèÊú∫Êï∞„ÄÇÊØîÂ¶Ç‰Ω†ÊØèÊ¨°ÈÉΩÂ°´1ÔºåÂÖ∂‰ªñÂèÇÊï∞‰∏ÄÊ†∑ÁöÑÊÉÖÂÜµ‰∏ã‰Ω†ÂæóÂà∞ÁöÑÈöèÊú∫Êï∞ÁªÑÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ‰ΩÜÂ°´0Êàñ‰∏çÂ°´ÔºåÊØèÊ¨°ÈÉΩ‰ºö‰∏ç‰∏ÄÊ†∑„ÄÇÈöèÊú∫Êï∞ÁöÑ‰∫ßÁîüÂèñÂÜ≥‰∫éÁßçÂ≠êÔºåÈöèÊú∫Êï∞ÂíåÁßçÂ≠ê‰πãÈó¥ÁöÑÂÖ≥Á≥ªÈÅµ‰ªé‰ª•‰∏ã‰∏§‰∏™ËßÑÂàôÔºöÁßçÂ≠ê‰∏çÂêåÔºå‰∫ßÁîü‰∏çÂêåÁöÑÈöèÊú∫Êï∞ÔºõÁßçÂ≠êÁõ∏ÂêåÔºåÂç≥‰ΩøÂÆû‰æã‰∏çÂêå‰πü‰∫ßÁîüÁõ∏ÂêåÁöÑÈöèÊú∫Êï∞„ÄÇ 1234567891011121314151617X_train,X_test,Y_train,Y_test = train_test_split( X, Y, test_size=0.2, random_state=42)## Êï∞ÊçÆÊ†áÂáÜÊç¢Ë°åss = StandardScaler()X_train = ss.fit_transform(X_train)X_test = ss.fit_transform(X_test)##ËÆ≠ÁªÉÊï∞ÊçÆlr = LinearRegression()lr.fit(X_train,Y_train)##È¢ÑÊµãYÂÄºy_predict = lr.predict(X_test)print("ÂáÜÁ°ÆÁéá:",lr.score(X_test,Y_test)) Ê†∑Êú¨Êï∞ÊçÆ100Êù°ÔºöÂáÜÁ°ÆÁéá: 0.0226499044921Ê†∑Êú¨Êï∞ÊçÆ1000Êù°Ôºö0.103073016594 Ê®°Âûã‰øùÂ≠òÂèäÂä†ËΩΩÔºö12345678from sklearn.externals import joblib## Ê®°Âûã‰øùÂ≠òÔºöjoblib.dump(ss,"data_ss.model")joblib.dump(lr,"data_lr.model")## Âä†ËΩΩÊ®°Âûãjoblib,load("data_ss.model")joblib,load("data_lr.model") plotÊñáÊ°£Ôºöhttps://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot 123456789101112## Ëß£ÂÜ≥‰∏≠ÊñáÈóÆÈ¢òmpl.rcParams['font.sans-serif'] = [u'SimHei'];mpl.rcParams['axes.unicode_minus'] = Falset=np.arange(len(X_test))plt.figure(facecolor='w')plt.plot(t,Y_test,'r--',linewidth=2,label=u'ÁúüÂÆûÂÄº')plt.plot(t,y_predict,'g--',linewidth=2,label=u'È¢ÑÊµãÂÄº')plt.legend(loc ='lower right')plt.title(u'Á∫øÊÄßÂõûÂΩíÊó∂Èó¥‰∏éÁîµÂéãÁöÑÂÖ≥Á≥ª',fontsize=20 )plt.grid(b=True)plt.show() 100Êù°Êï∞ÊçÆÔºö 1000Êù°Êï∞ÊçÆÔºö linearÂ§öÈ°πÂºè123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.grid_search import GridSearchCVmodels = [Pipeline([('Poly',PolynomialFeatures()),('Linear',LinearRegression())])]model = models[0]##Ëé∑ÂèñXÔºåYÂèòÈáèÔºåÂπ∂Â∞ÜÊó∂Èó¥ÂèòÈáèËΩ¨Êç¢‰∏∫Êï∞ÂÄºÂûãËøûÁª≠ÁöÑX = datas[names[0:2]]X = X.apply(lambda x :pd.Series(datae_format(x)),axis=1)Y = datas[names[4]]## ÂØπÊï∞ÊçÆÈõÜËøõË°åÂàíÂàÜX_train,X_test,Y_train,Y_test = train_test_split( X, Y, test_size=0.2, random_state=0)## Êï∞ÊçÆÊ†áÂáÜÂåñss = StandardScaler()X_train = ss.fit_transform(X_train)X_test = ss.fit_transform(X_test)## Ê®°ÂûãËÆ≠ÁªÉt=np.arange(len(X_test))N =5d_pool= np.arange(1,N,1)m=d_pool.sizeclrs = [] # È¢úËâ≤for c in np.linspace(16711680, 255, m,dtype='int64'): clrs.append('#%06x' % c)line_width = 3plt.figure(figsize=(12,6),facecolor='w')for i,d in enumerate(d_pool): plt.subplot(N-1,1,i+1) plt.plot(t,Y_test,'r--',label=u'ÁúüÂÆûÂÄº',ms=10,zorder=N) model.set_params(Poly__degree=d) # ËÆæÁΩÆÂ§öÈ°πÂºèÁöÑÈò∂ model.fit(X_train,Y_train) lin = model.get_params('Linear')['Linear'] output =u'%dÈò∂ÔºåÁ≥ªÊï∞‰∏∫Ôºö'%d print( output,lin.coef_.ravel()) y_hat = model.predict(X_test) s = model.score(X_test,Y_test) z=N-1 if (d==2) else 0 label=u'%dÈò∂,ÂáÜÁ°ÆÁéá=%.3f'%(d,s) plt.plot(t,y_hat,color=clrs[i],lw=line_width,alpha = 0.75,label=label,zorder=z) plt.legend(loc = 'upper left') plt.grid(True) plt.ylabel(u'%dÈò∂ÁªìÊûú'%d,fontsize=12)# È¢ÑÊµãÂÄºÂíåÁúüÂÆûÂÄºÁîªÂõæÊØîËæÉplt.legend(loc = 'lower right')plt.suptitle(u'Á∫øÊÄßÂõûÂΩíÊó∂Èó¥‰∏éÁîµÂéã‰πãÈó¥Â§öÈ°πÂºèÂÖ≥Á≥ª')plt.grid(b=True)plt.show() 1Èò∂ÔºåÁ≥ªÊï∞‰∏∫Ôºö [ 0.00000000e+00 5.55111512e-17 0.00000000e+00 0.00000000e+00 -4.22939297e-01 -4.34494704e-01 0.00000000e+00]2Èò∂ÔºåÁ≥ªÊï∞‰∏∫Ôºö [ 2.47983335e-17 1.11022302e-16 -2.22044605e-16 -1.11022302e-16 -5.05820937e-01 -3.46571423e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 -8.58357173e-01 -7.57689882e-01 0.00000000e+00 -1.60364055e-01 0.00000000e+00 0.00000000e+00]3Èò∂ÔºåÁ≥ªÊï∞‰∏∫Ôºö [ -1.69309011e-15 -2.99760217e-15 3.33066907e-16 5.55111512e-16 -4.41970713e-02 -3.57278153e-01 0.00000000e+00 2.22044605e-16 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.43644538e-01 4.86208530e-01 0.00000000e+00 3.26242425e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.48140740e-01 6.37890832e-01 0.00000000e+00 -7.45035081e-01 0.00000000e+00 0.00000000e+00 -5.02511111e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]4Èò∂ÔºåÁ≥ªÊï∞‰∏∫Ôºö [ 2.22002972e-013 -8.01497757e-013 5.37209166e-014 -4.53519167e-013 7.19933381e-003 2.05441337e-001 3.86510268e-013 -5.94066463e-013 6.66133815e-015 6.66133815e-016 -1.88737914e-015 6.27276009e-015 -5.30131494e-015 -1.01585407e-014 3.35287353e-014 8.21565038e-015 -2.55351296e-015 -2.22044605e-016 -6.31088724e-030 3.02922588e-028 1.00974196e-028 -5.04870979e-029 -4.89052469e-002 3.28220946e-001 0.00000000e+000 6.15440583e-003 0.00000000e+000 0.00000000e+000 1.26217745e-029 0.00000000e+000 -5.60519386e-044 -8.40779079e-045 -7.00649232e-045 -2.24207754e-044 -5.60519386e-045 2.80259693e-045 1.40129846e-045 0.00000000e+000 0.00000000e+000 4.97841222e-060 1.55575382e-061 -1.24460306e-060 -2.48920611e-060 0.00000000e+000 -3.11150764e-061 -1.16681536e-061 -3.11150764e-061 -4.66726146e-061 2.21085915e-075 -5.52714788e-076 -1.38178697e-076 2.76357394e-076 -1.38178697e-076 -3.45446742e-077 0.00000000e+000 1.34940134e-079 0.00000000e+000 1.91761463e-093 1.49813643e-095 -5.99254573e-095 4.49440930e-095 1.12360233e-095 0.00000000e+000 7.49068217e-096 -2.34083818e-097 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 -1.66326556e-111 4.15816391e-112 -2.07908195e-112 0.00000000e+000 -6.13741990e-002 -8.70197287e-001 8.39734513e-140 -1.48604949e+000 0.00000000e+000 0.00000000e+000 -4.67097255e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 -2.42848401e-001 -9.28403263e-001 0.00000000e+000 -8.91115491e-001 0.00000000e+000 0.00000000e+000 1.33924630e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 2.81909059e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PythonÁΩëÁªúÁà¨Ëô´]]></title>
    <url>%2F2017%2F01%2F26%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[pythonÂÆòÊñπÊèê‰æõÁöÑÁΩëÈ°µ‰∏ãËΩΩÂô®ÊòØurllib2Á¨¨‰∏âÊñπÊúâÊõ¥Âº∫Â§ßÁöÑ‰∏ãËΩΩÂô®ÊòØrequests Âú®python2.xÈáåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®urllib2.urlopen(‚Äúhttp://www.baidu.com&quot;)ÊâìÂºÄÁΩëÈ°µ‰ΩÜÊòØÂú®python3.xÈáå urllib2 ÈúÄË¶ÅÊîπÊàêurllib.request Âú®python2.xÈáåÂèØ‰ª•‰ΩøÁî®urllib.urlencodeÔºö‰æãÂ¶ÇÔºövalues = {‚Äúusername‚Äù:‚ÄùPythonÁà¨Ëô´‚Äù,‚Äùpassword‚Äù:‚Äù123456789‚Äù}data = urllib.urlencode(values)‰ΩÜÊòØÂú®python3.xÈáå urllib2 ÈúÄË¶ÅÊîπÊàêurllib.requestdata = urllib.parse.urlencode(values)ËÄå‰∏îËøòÈúÄË¶ÅÊäädataÊ†ºÂºèËΩ¨Êç¢data = data.encode(‚Äòutf-8‚Äô)Êàñdata = data.encode(encoding=‚ÄôUTF8‚Äô)Âê¶Âàô‰ºöÊèêÁ§∫TypeError: POST data should be bytes or an iterable of bytes. It cannot be of type str.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kNNÂÆûÁé∞ÊâãÂÜôÊï∞Â≠óËØÜÂà´]]></title>
    <url>%2F2017%2F01%2F25%2FkNN%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[ÈúÄÊ±ÇÂà©Áî®‰∏Ä‰∏™ÊâãÂÜôÊï∞Â≠ó‚ÄúÂÖàÈ™åÊï∞ÊçÆ‚ÄùÈõÜÔºå‰ΩøÁî®knnÁÆóÊ≥ïÊù•ÂÆûÁé∞ÂØπÊâãÂÜôÊï∞Â≠óÁöÑËá™Âä®ËØÜÂà´ÔºõÂÖàÈ™åÊï∞ÊçÆÔºàËÆ≠ÁªÉÊï∞ÊçÆÔºâÈõÜÔºö Êï∞ÊçÆÁª¥Â∫¶ÊØîËæÉÂ§ßÔºåÊ†∑Êú¨Êï∞ÊØîËæÉÂ§ö„ÄÇ Êï∞ÊçÆÈõÜÂåÖÊã¨Êï∞Â≠ó0-9ÁöÑÊâãÂÜô‰Ωì„ÄÇ ÊØè‰∏™Êï∞Â≠óÂ§ßÁ∫¶Êúâ200‰∏™Ê†∑Êú¨„ÄÇ ÊØè‰∏™Ê†∑Êú¨‰øùÊåÅÂú®‰∏Ä‰∏™txtÊñá‰ª∂‰∏≠„ÄÇ ÊâãÂÜô‰ΩìÂõæÂÉèÊú¨Ë∫´ÁöÑÂ§ßÂ∞èÊòØ32x32ÁöÑ‰∫åÂÄºÂõæÔºåËΩ¨Êç¢Âà∞txtÊñá‰ª∂‰øùÂ≠òÂêéÔºåÂÜÖÂÆπ‰πüÊòØ32x32‰∏™Êï∞Â≠óÔºå0ÊàñËÄÖ1ÔºåÂ¶Ç‰∏ãÔºö È¶ñÂÖàÂáÜÂ§áÊµãËØïÊñá‰ª∂:1934‰∏™ËÆ≠ÁªÉÊï∞ÊçÆ946‰∏™ÊµãËØïÊï∞ÊçÆ ÂàÜÊûêÔºö1„ÄÅÊâãÂÜô‰ΩìÂõ†‰∏∫ÊØè‰∏™‰∫∫ÔºåÁîöËá≥ÊØèÊ¨°ÂÜôÁöÑÂ≠óÈÉΩ‰∏ç‰ºöÂÆåÂÖ®Á≤æÁ°Æ‰∏ÄËá¥ÔºåÊâÄ‰ª•ÔºåËØÜÂà´ÊâãÂÜô‰ΩìÁöÑÂÖ≥ÈîÆÊòØ‚ÄúÁõ∏‰ººÂ∫¶‚Äù2„ÄÅÊó¢ÁÑ∂ÊòØË¶ÅÊ±ÇÊ†∑Êú¨‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÔºåÈÇ£‰πàÔºåÈ¶ñÂÖàÈúÄË¶ÅÂ∞ÜÊ†∑Êú¨ËøõË°åÊäΩË±°ÔºåÂ∞ÜÊØè‰∏™Ê†∑Êú¨ÂèòÊàê‰∏ÄÁ≥ªÂàóÁâπÂæÅÊï∞ÊçÆÔºàÂç≥ÁâπÂæÅÂêëÈáèÔºâ3„ÄÅÊâãÂÜô‰ΩìÂú®Áõ¥ËßÇ‰∏äÂ∞±ÊòØ‰∏Ä‰∏™‰∏™ÁöÑÂõæÁâáÔºåËÄåÂõæÁâáÊòØÁî±‰∏äËø∞ÂõæÁ§∫‰∏≠ÁöÑÂÉèÁ¥†ÁÇπÊù•ÊèèËø∞ÁöÑÔºåÊ†∑Êú¨ÁöÑÁõ∏‰ººÂ∫¶ÂÖ∂ÂÆûÂ∞±ÊòØÂÉèÁ¥†ÁöÑ‰ΩçÁΩÆÂíåÈ¢úËâ≤‰πãÈó¥ÁöÑÁªÑÂêàÁöÑÁõ∏‰ººÂ∫¶4„ÄÅÂõ†Ê≠§ÔºåÂ∞ÜÂõæÁâáÁöÑÂÉèÁ¥†ÊåâÁÖßÂõ∫ÂÆöÈ°∫Â∫èËØªÂèñÂà∞‰∏Ä‰∏™‰∏™ÁöÑÂêëÈáè‰∏≠ÔºåÂç≥ÂèØÂæàÂ•ΩÂú∞Ë°®Á§∫ÊâãÂÜô‰ΩìÊ†∑Êú¨5„ÄÅÊäΩË±°Âá∫‰∫ÜÊ†∑Êú¨ÂêëÈáèÔºåÂèäÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊ®°ÂûãÔºåÂç≥ÂèØÂ∫îÁî®KNNÊù•ÂÆûÁé∞ ‰ª£Á†ÅÔºö1) ‰∏Ä‰∏™Áî®Êù•ÁîüÊàêÂ∞ÜÊØè‰∏™Ê†∑Êú¨ÁöÑtxtÊñá‰ª∂ËΩ¨Êç¢‰∏∫ÂØπÂ∫îÁöÑ‰∏Ä‰∏™ÂêëÈáèÔºå2) ‰∏Ä‰∏™Áî®Êù•Âä†ËΩΩÊï¥‰∏™Êï∞ÊçÆÈõÜÔºå3) ‰∏Ä‰∏™ÂÆûÁé∞kNNÂàÜÁ±ªÁÆóÊ≥ï„ÄÇ4) ÊúÄÂêéÂ∞±ÊòØÂÆûÁé∞Âä†ËΩΩ„ÄÅÊµãËØïÁöÑÂáΩÊï∞„ÄÇ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125########################################## kNN: k Nearest Neighbors# ÂèÇÊï∞: inX: vector to compare to existing dataset (1xN)# dataSet: size m data set of known vectors (NxM)# labels: data set labels (1xM vector)# k: number of neighbors to use for comparison # ËæìÂá∫: Â§öÊï∞Á±ª#########################################from numpy import *import operatorimport os# KNNÂàÜÁ±ªÊ†∏ÂøÉÊñπÊ≥ïdef kNNClassify(newInput, dataSet, labels, k): numSamples = dataSet.shape[0] # shape[0]‰ª£Ë°®Ë°åÊï∞ ## step 1: ËÆ°ÁÆóÊ¨ßÂºèË∑ùÁ¶ª # tile(A, reps): Â∞ÜAÈáçÂ§çrepsÊ¨°Êù•ÊûÑÈÄ†‰∏Ä‰∏™Áü©Èòµ # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) - dataSet # Subtract element-wise squaredDiff = diff ** 2 # squared for the subtract squaredDist = sum(squaredDiff, axis = 1) # sum is performed by row distance = squaredDist ** 0.5 ## step 2: ÂØπË∑ùÁ¶ªÊéíÂ∫è # argsort()ËøîÂõûÊéíÂ∫èÂêéÁöÑÁ¥¢Âºï sortedDistIndices = argsort(distance) classCount = &#123;&#125; # ÂÆö‰πâ‰∏Ä‰∏™Á©∫ÁöÑÂ≠óÂÖ∏ for i in xrange(k): ## step 3: ÈÄâÊã©k‰∏™ÊúÄÂ∞èË∑ùÁ¶ª voteLabel = labels[sortedDistIndices[i]] ## step 4: ËÆ°ÁÆóÁ±ªÂà´ÁöÑÂá∫Áé∞Ê¨°Êï∞ # when the key voteLabel is not in dictionary classCount, get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ## step 5: ËøîÂõûÂá∫Áé∞Ê¨°Êï∞ÊúÄÂ§öÁöÑÁ±ªÂà´‰Ωú‰∏∫ÂàÜÁ±ªÁªìÊûú maxCount = 0 for key, value in classCount.items(): if value &gt; maxCount: maxCount = value maxIndex = key return maxIndex # Â∞ÜÂõæÁâáËΩ¨Êç¢‰∏∫ÂêëÈáèdef img2vector(filename): rows = 32 cols = 32 imgVector = zeros((1, rows * cols)) fileIn = open(filename) for row in xrange(rows): lineStr = fileIn.readline() for col in xrange(cols): imgVector[0, row * 32 + col] = int(lineStr[col]) return imgVector# Âä†ËΩΩÊï∞ÊçÆÈõÜdef loadDataSet(): ## step 1: ËØªÂèñËÆ≠ÁªÉÊï∞ÊçÆÈõÜ print "---Getting training set..." dataSetDir = 'E:/Python/ml/knn/' trainingFileList = os.listdir(dataSetDir + 'trainingDigits') # Âä†ËΩΩÊµãËØïÊï∞ÊçÆ numSamples = len(trainingFileList) train_x = zeros((numSamples, 1024)) train_y = [] for i in xrange(numSamples): filename = trainingFileList[i] # get train_x train_x[i, :] = img2vector(dataSetDir + 'trainingDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 train_y.append(label) ## step 2:ËØªÂèñÊµãËØïÊï∞ÊçÆÈõÜ print "---Getting testing set..." testingFileList = os.listdir(dataSetDir + 'testDigits') # load the testing set numSamples = len(testingFileList) test_x = zeros((numSamples, 1024)) test_y = [] for i in xrange(numSamples): filename = testingFileList[i] # get train_x test_x[i, :] = img2vector(dataSetDir + 'testDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 test_y.append(label) return train_x, train_y, test_x, test_y# ÊâãÂÜôËØÜÂà´‰∏ªÊµÅÁ®ãdef testHandWritingClass(): ## step 1: Âä†ËΩΩÊï∞ÊçÆ print "step 1: load data..." train_x, train_y, test_x, test_y = loadDataSet() ## step 2: Ê®°ÂûãËÆ≠ÁªÉ. print "step 2: training..." pass ## step 3: ÊµãËØï print "step 3: testing..." numTestSamples = test_x.shape[0] matchCount = 0 for i in xrange(numTestSamples): predict = kNNClassify(test_x[i], train_x, train_y, 3) if predict == test_y[i]: matchCount += 1 accuracy = float(matchCount) / numTestSamples ## step 4: ËæìÂá∫ÁªìÊûú print "step 4: show the result..." print 'The classify accuracy is: %.2f%%' % (accuracy * 100) ÊµãËØï12import kNNkNN.testHandWritingClass() ÊâßË°åÁªìÊûúÔºö1234567tep 1: load data...---Getting training set...---Getting testing set...step 2: training...step 3: testing...step 4: show the result...The classify accuracy is: 98.84%]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†‰πãKNNÁÆóÊ≥ïÊé®Êºî]]></title>
    <url>%2F2017%2F01%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN%E7%AE%97%E6%B3%95%E6%8E%A8%E6%BC%94%2F</url>
    <content type="text"><![CDATA[‰ªéËÆ≠ÁªÉÈõÜ‰∏≠ÊâæÂà∞ÂíåÊñ∞Êï∞ÊçÆÊúÄÊé•ËøëÁöÑkÊù°ËÆ∞ÂΩïÔºåÁÑ∂ÂêéÊ†πÊçÆÂ§öÊï∞Á±ªÊù•ÂÜ≥ÂÆöÊñ∞Êï∞ÊçÆÁ±ªÂà´„ÄÇÁÆóÊ≥ïÊ∂âÂèä3‰∏™‰∏ªË¶ÅÂõ†Á¥†Ôºö1) ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ2) Ë∑ùÁ¶ªÊàñÁõ∏‰ººÂ∫¶ÁöÑËÆ°ÁÆóË°°Èáè3) kÁöÑÂ§ßÂ∞èÁªøËâ≤ÂúÜË¶ÅË¢´ÂÜ≥ÂÆöËµã‰∫àÂì™‰∏™Á±ªÔºåÊòØÁ∫¢Ëâ≤‰∏âËßíÂΩ¢ËøòÊòØËìùËâ≤ÂõõÊñπÂΩ¢ÔºüÂ¶ÇÊûúK=3ÔºåÁî±‰∫éÁ∫¢Ëâ≤‰∏âËßíÂΩ¢ÊâÄÂç†ÊØî‰æã‰∏∫2/3ÔºåÁªøËâ≤ÂúÜÂ∞ÜË¢´Ëµã‰∫àÁ∫¢Ëâ≤‰∏âËßíÂΩ¢ÈÇ£‰∏™Á±ªÔºåÂ¶ÇÊûúK=5ÔºåÁî±‰∫éËìùËâ≤ÂõõÊñπÂΩ¢ÊØî‰æã‰∏∫3/5ÔºåÂõ†Ê≠§ÁªøËâ≤ÂúÜË¢´Ëµã‰∫àËìùËâ≤ÂõõÊñπÂΩ¢Á±ª„ÄÇ KNNÂàÜÁ±ªÁÆóÊ≥ïPythonÂÆûÊàòÊúâ‰ª•‰∏ãÂÖàÈ™åÊï∞ÊçÆÔºå‰ΩøÁî®knnÁÆóÊ≥ïÂØπÊú™Áü•Á±ªÂà´Êï∞ÊçÆÂàÜÁ±ª xËΩ¥ yËΩ¥ Á±ªÂûã 1.0 0.9 A 1.0 1.0 A 0.0 0.1 B 0.1 0.2 B Êú™Áü•Á±ªÂà´Êï∞ÊçÆ xËΩ¥ yËΩ¥ Á±ªÂûã 1.2 1.0 Ôºü 0.1 0.3 Ôºü 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960########################################## kNN: k Nearest Neighbors# ËæìÂÖ•: newInput: (1xN)ÁöÑÂæÖÂàÜÁ±ªÂêëÈáè# dataSet: (NxM)ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ# labels: ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÁ±ªÂà´Ê†áÁ≠æÂêëÈáè# k: ËøëÈÇªÊï∞ # ËæìÂá∫: ÂèØËÉΩÊÄßÊúÄÂ§ßÁöÑÂàÜÁ±ªÊ†áÁ≠æ#########################################from numpy import *import operator#ÂàõÂª∫‰∏Ä‰∏™Êï∞ÊçÆÈõÜÔºåÂåÖÂê´2‰∏™Á±ªÂà´ÂÖ±4‰∏™Ê†∑Êú¨def createDataSet(): # ÁîüÊàê‰∏Ä‰∏™Áü©ÈòµÔºåÊØèË°åË°®Á§∫‰∏Ä‰∏™Ê†∑Êú¨ group = array([[1.0, 0.9], [1.0, 1.0], [0.1, 0.2], [0.0, 0.1]]) # 4‰∏™Ê†∑Êú¨ÂàÜÂà´ÊâÄÂ±ûÁöÑÁ±ªÂà´ labels = ['A', 'A', 'B', 'B'] return group, labels# KNNÂàÜÁ±ªÁÆóÊ≥ïÂáΩÊï∞ÂÆö‰πâdef kNNClassify(newInput, dataSet, labels, k): numSamples = dataSet.shape[0] # shape[0]Ë°®Á§∫Ë°åÊï∞ ## step 1: ËÆ°ÁÆóË∑ùÁ¶ª # tile(A, reps): ÊûÑÈÄ†‰∏Ä‰∏™Áü©ÈòµÔºåÈÄöËøáAÈáçÂ§çrepsÊ¨°ÂæóÂà∞(tile(A, ÔºàrepsXÔºårepsYÔºâ)Ê≠§Â§ÑÂú®Ë°åÁöÑÊñπÂêëÈáçÂ§çrepsXÊ¨°ÔºåÂú®YÁöÑÊñπÂêëÈáçÂ§çrepsYÊ¨°) # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) - dataSet # ÊåâÂÖÉÁ¥†Ê±ÇÂ∑ÆÂÄº squaredDiff = diff ** 2 #Â∞ÜÂ∑ÆÂÄºÂπ≥Êñπ squaredDist = sum(squaredDiff, axis = 1) # ÊåâË°åÁ¥ØÂä† distance = squaredDist ** 0.5 #Â∞ÜÂ∑ÆÂÄºÂπ≥ÊñπÂíåÊ±ÇÂºÄÊñπÔºåÂç≥ÂæóË∑ùÁ¶ª ## step 2: ÂØπË∑ùÁ¶ªÊéíÂ∫è ## Ê≠§Â§ÑÊéíÂ∫èÈúÄË¶ÅÊ≥®ÊÑèÊ†∑Êú¨Ê†áÁ≠æÁöÑÈ°∫Â∫è„ÄÇÊéíÂ∫èÂêéÂ≠òÁöÑÊòØËßíÊ†áÂè∑„ÄÇÂêéÁª≠Áõ¥Êé•‰ªéÊéíÂ∫èÂêéÁöÑÊ†áÁ≠æÂ∫èÂàóÊâæ‰∏ãÊ†áÊòØÊéíÂ∫èÂÜÖÂÆπÁöÑÊï∞ÊçÆ # argsort() ËøîÂõûÊéíÂ∫èÂêéÁöÑÁ¥¢ÂºïÂÄº sortedDistIndices = argsort(distance) #Êàñdistance.argsort() classCount = &#123;&#125; # define a dictionary (can be append element) for i in xrange(k): ## step 3: ÈÄâÊã©k‰∏™ÊúÄËøëÈÇª voteLabel = labels[sortedDistIndices[i]] ## step 4: ËÆ°ÁÆók‰∏™ÊúÄËøëÈÇª‰∏≠ÂêÑÁ±ªÂà´Âá∫Áé∞ÁöÑÊ¨°Êï∞ # when the key voteLabel is not in dictionary classCount, get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ## step 5: ËøîÂõûÂá∫Áé∞Ê¨°Êï∞ÊúÄÂ§öÁöÑÁ±ªÂà´Ê†áÁ≠æ maxCount = 0 for key, value in classCount.items(): if value &gt; maxCount: maxCount = value maxIndex = key return maxIndex ##step 5 ÂèØ‰ª•ÊîπÊàêÔºö # sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True) #return sortedClassCount[0][0] ÁÑ∂ÂêéË∞ÉÁî®ÁÆóÊ≥ïËøõË°åÊµãËØïÔºö1234567891011121314import kNNfrom numpy import * #ÁîüÊàêÊï∞ÊçÆÈõÜÂíåÁ±ªÂà´Ê†áÁ≠ædataSet, labels = kNN.createDataSet()#ÂÆö‰πâ‰∏Ä‰∏™Êú™Áü•Á±ªÂà´ÁöÑÊï∞ÊçÆtestX = array([1.2, 1.0])k = 3#Ë∞ÉÁî®ÂàÜÁ±ªÂáΩÊï∞ÂØπÊú™Áü•Êï∞ÊçÆÂàÜÁ±ªoutputLabel = kNN.kNNClassify(testX, dataSet, labels, 3)print "Your input is:", testX, "and classified to class: ", outputLabeltestX = array([0.1, 0.3])outputLabel = kNN.kNNClassify(testX, dataSet, labels, 3)print "Your input is:", testX, "and classified to class: ", outputLabel ËøôÊó∂ÂÄô‰ºöËæìÂá∫12Your input is: [ 1.2 1.0] and classified to class: AYour input is: [ 0.1 0.3] and classified to class: B]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Kylin]]></title>
    <url>%2F2017%2F01%2F18%2FApache%20Kylin%2F</url>
    <content type="text"><![CDATA[kylinApache Kylin‚Ñ¢ is an open source Distributed Analytics Engine designed to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets, original contributed from eBay Inc. Apache Kylin‚Ñ¢ lets you query massive data set at sub-second latency in 3 steps. Identify a Star Schema on Hadoop. Build Cube from the identified tables. Query with ANSI-SQL and get results in sub-second, via ODBC, JDBC or RESTful API.]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Beam]]></title>
    <url>%2F2017%2F01%2F16%2FApache%20Beam%2F</url>
    <content type="text"><![CDATA[Apache BeamApache Beam provides an advanced unified programming model, allowing you to implement batch and streaming data processing jobs that can run on any execution engine. Apache BeamÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖàËøõÁöÑÁªü‰∏ÄÁºñÁ®ãÊ®°ÂûãÔºåÂèØ‰ª•ÂÆûÁé∞ÊâπÈáèÂíåÊµÅÊï∞ÊçÆÂ§ÑÁêÜÂ∑•‰ΩúÔºåÂèØ‰ª•ËøêË°åÂú®‰ªª‰ΩïÊâßË°åÂºïÊìé„ÄÇApache Beam is: UNIFIED - Use a single programming model for both batch and streaming use cases. PORTABLE - Execute pipelines on multiple execution environments, including Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow. EXTENSIBLE - Write and share new SDKs, IO connectors, and transformation libraries. Áªü‰∏ÄÁöÑ - ÂØπÊâπÂ§ÑÁêÜÂíåÊµÅÂ™í‰ΩìÁî®‰æã‰ΩøÁî®Âçï‰∏ÄÁºñÁ®ãÊ®°Âûã„ÄÇ ËΩª‰æøÁöÑ(ÂèØÁßªÊ§ç) - ‰æøÊê∫ÂºèÁÆ°ÈÅì‰∏äÊâßË°åÂ§ö‰∏™ÊâßË°åÁéØÂ¢ÉÔºåÂåÖÊã¨ApacheÁöÑÂÖàÁ´ØÔºåApacheFlinkÔºåApache SparkÔºåÂíåË∞∑Ê≠å‰∫ëÊï∞ÊçÆÊµÅ„ÄÇ ÂèØÊâ©Â±ïÁöÑ - ÂÜôÂíåÂàÜ‰∫´Êñ∞ÁöÑËΩØ‰ª∂ÂºÄÂèëÂ∑•ÂÖ∑ÂåÖÔºåIOËøûÊé•Âô®ÔºåÂíåÂä®ÊÄÅÂ∫ì„ÄÇ Get Started To use Beam for your data processing tasks, start by reading the Beam Overview and performing the steps in the Quickstart. Then dive into the Documentation section for in-depth concepts and reference materials for the Beam model, SDKs, and runners.‰ΩøÁî®Beam‰∏∫‰Ω†ÁöÑÊï∞ÊçÆÂ§ÑÁêÜ‰ªªÂä°ÔºåÂºÄÂßãÈÄöËøáÈòÖËØªBeamÊ¶ÇËø∞ÂíåÁ§∫‰æãÊâßË°å‰ª•‰∏ãÊ≠•È™§„ÄÇÁÑ∂ÂêéÊΩúÂÖ•ÊñáÊ°£ÈÉ®ÂàÜËøõË°åÊ∑±ÂÖ•ÁöÑÊ¶ÇÂøµÂíåBeamÊ®°ÂûãÔºåSDKsÔºåÂíåÊâßË°å„ÄÇ Contribute Beam is an Apache Software Foundation project, available under the Apache v2 license. Beam is an open source community and contributions are greatly appreciated! If you‚Äôd like to contribute, please see the Contribute section.Beam ÊòØApacheËΩØ‰ª∂Âü∫Èáë‰ºöÁöÑÈ°πÁõÆÔºåApache v2ËÆ∏ÂèØ‰∏ãÂèØÁî®„ÄÇBeamÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁ§æÂå∫ÂíåË¥°ÁåÆÈùûÂ∏∏ÊÑüË∞¢ÔºÅÂ¶ÇÊûú‰Ω†ÊÉ≥Ë¥°ÁåÆÔºåËØ∑ÂèÇÈòÖÊäïÁ®øÈÉ®ÂàÜ„ÄÇ Apache Beam Overview Apache Beam is an open source, unified programming model that you can use to create a data processing pipeline. You start by building a program that defines the pipeline using one of the open source Beam SDKs. The pipeline is then executed by one of Beam‚Äôs supported distributed processing back-ends, which include Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow.ApacheÁöÑBeamÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ„ÄÅÁªü‰∏ÄÁöÑÁºñÁ®ãÊ®°ÂûãÔºåÂèØ‰ª•Áî®Êù•ÂàõÂª∫‰∏Ä‰∏™Êï∞ÊçÆÂ§ÑÁêÜÁÆ°ÈÅì„ÄÇ‰Ω†ÂºÄÂßãÂª∫Á´ã‰∏Ä‰∏™Á®ãÂ∫èÔºåÂÆö‰πâ‰∫ÜÁÆ°ÈÅì‰ΩøÁî®‰∏Ä‰∏™ÂºÄÊ∫êÁöÑBeamÁöÑSDK„ÄÇÁÆ°ÈÅìÊòØÈÄöËøá‰∏ÄBeamÁöÑÊîØÊåÅÂàÜÂ∏ÉÂºèÂ§ÑÁêÜÂêéÁ´ØÊâßË°åÔºåÂåÖÊã¨ApacheÁöÑÂÖàÁ´ØÔºåApache FlinkÔºåApacheÁöÑSparkÔºåÂíåË∞∑Ê≠å‰∫ëÊï∞ÊçÆÊµÅ„ÄÇ Beam is particularly useful for Embarrassingly Parallel data processing tasks, in which the problem can be decomposed into many smaller bundles of data that can be processed independently and in parallel. You can also use Beam for Extract, Transform, and Load (ETL) tasks and pure data integration. These tasks are useful for moving data between different storage media and data sources, transforming data into a more desirable format, or loading data onto a new system.BeamÁöÑÈ´òÂ∫¶Âπ∂Ë°åÁöÑÊï∞ÊçÆÂ§ÑÁêÜ‰ªªÂä°ÊòØÁâπÂà´ÊúâÁî®ÁöÑÔºåÂÖ∂‰∏≠ÁöÑÈóÆÈ¢òÂèØ‰ª•Ë¢´ÂàÜËß£ÊàêËÆ∏Â§öËæÉÂ∞èÁöÑËÆ∏Â§öÊï∞ÊçÆÔºåÂèØ‰ª•Áã¨Á´ãÂíåÂπ∂Ë°åÂ§ÑÁêÜ„ÄÇ‰Ω†‰πüÂèØ‰ª•‰ΩøÁî®BeamÂèòÊç¢ÔºåÊèêÂèñÔºåÂíåÂä†ËΩΩÔºàETLÔºâ‰ªªÂä°ÂíåÁ∫ØÊï∞ÊçÆÈõÜÊàê„ÄÇËøô‰∫õ‰ªªÂä°Áî®‰∫éÂú®‰∏çÂêåÁöÑÂ≠òÂÇ®‰ªãË¥®ÂíåÊï∞ÊçÆÊ∫ê‰πãÈó¥ÁßªÂä®Êï∞ÊçÆÔºåÂ∞ÜÊï∞ÊçÆËΩ¨Êç¢ÊàêÊõ¥ÁêÜÊÉ≥ÁöÑÊ†ºÂºèÔºåÊàñËÄÖÂ∞ÜÊï∞ÊçÆÂä†ËΩΩÂà∞Êñ∞Á≥ªÁªü‰∏ä„ÄÇ Apache Beam SDKs The Beam SDKs provide a unified programming model that can represent and transform data sets of any size, whether the input is a finite data set from a batch data source, or an infinite data set from a streaming data source. The Beam SDKs use the same classes to represent both bounded and unbounded data, and the same transforms to operate on that data. You use the Beam SDK of your choice to build a program that defines your data processing pipeline.BeamÁöÑSDKÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÁºñÁ®ãÊ®°ÂûãÔºåÂèØ‰ª•Ë°®Á§∫ÂíåÂèòÊç¢‰ªªÊÑèÂ§ßÂ∞èÁöÑÊï∞ÊçÆÈõÜÔºåËæìÂÖ•ÊòØÂê¶ÊòØ‰∏Ä‰∏™ÊúâÈôêÁöÑÊï∞ÊçÆÈõÜ‰ªé‰∏Ä‰∏™ÊâπÂ§ÑÁêÜÁöÑÊï∞ÊçÆÊ∫êÔºåÊàñÊó†ÈôêÁöÑÊï∞ÊçÆÈõÜ‰ªé‰∏Ä‰∏™ÊµÅÁöÑÊï∞ÊçÆÊ∫ê„ÄÇBeamÁöÑSDK‰ΩøÁî®Âêå‰∏ÄÁ±ªÁöÑ‰ª£Ë°®ÊúâÁïåÂíåÊó†ÁïåÁöÑÊï∞ÊçÆÔºåÂíåÁõ∏ÂêåÁöÑÂèòÊç¢ÔºåÂØπËøô‰∫õÊï∞ÊçÆÁöÑÊìç‰Ωú„ÄÇ‰Ω†Áî®‰Ω†ÈÄâÊã©ÁöÑBeam SDKÊûÑÂª∫ÁöÑÁ®ãÂ∫èÂÆö‰πâÊï∞ÊçÆÂ§ÑÁêÜÁÆ°ÈÅì„ÄÇ Beam currently supports the following language-specific SDKs:Language SDK StatusJava Active DevelopmentPython Coming SoonOther TBD Apache Beam Pipeline Runners The Beam Pipeline Runners translate the data processing pipeline you define with your Beam program into the API compatible with the distributed processing back-end of your choice. When you run your Beam program, you‚Äôll need to specify the appropriate runner for the back-end where you want to execute your pipeline.BeamÊµÅÁÆ°ÈÅìÁøªËØëÊÇ®ÂÆö‰πâÁöÑÊï∞ÊçÆÂ§ÑÁêÜÁÆ°ÈÅì‰∏éÊÇ®ÁöÑBeamÁ®ãÂ∫èÂà∞API‰∏éÊÇ®ÈÄâÊã©ÁöÑÂàÜÂ∏ÉÂºèÂ§ÑÁêÜÂêéÁ´ØÂÖºÂÆπ„ÄÇÂΩì‰Ω†ËøêË°å‰Ω†ÁöÑBeamÊùüËÆ°ÂàíÊó∂Ôºå‰Ω†ÈúÄË¶ÅÊåáÂÆö‰Ω†ÊÉ≥Ë¶ÅÊâßË°å‰Ω†ÁöÑÁÆ°ÈÅìÁöÑÂêéÁ´ØÁöÑÂêàÈÄÇÁöÑÊâßË°å.„ÄÇ Beam currently supports Runners that work with the following distributed processing back-ends:BeamÁõÆÂâçÊîØÊåÅËøêË°å‰∏é‰∏ãÂàóÂàÜÂ∏ÉÂºèÂ§ÑÁêÜÂêéÁ´ØRunner StatusApache Apex In DevelopmentApache Flink In DevelopmentApache Spark In DevelopmentGoogle Cloud Dataflow In Development Quickstart Êù•‰∏ÄÊÆµhello WorldÂêßÁéØÂ¢ÉÂáÜÂ§áÔºöJDK1.7+„ÄÇMaven mavenÂëΩ‰ª§Ôºö12345678910$ mvn archetype:generate \ -DarchetypeRepository=https://repository.apache.org/content/groups/snapshots \ -DarchetypeGroupId=org.apache.beam \ -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples \ -DarchetypeVersion=LATEST \ -DgroupId=org.example \ -DartifactId=word-count-beam \ -Dversion="0.1" \ -Dpackage=org.apache.beam.examples \ -DinteractiveMode=false Êü•ÁúãÊñá‰ª∂12345[root@zhm1 Beam]# lsword-count-beam[root@zhm1 Beam]# cd word-count-beam/[root@zhm1 word-count-beam]# ls src/main/java/org/apache/beam/examples/common DebuggingWordCount.java MinimalWordCount.java WindowedWordCount.java WordCount.java ËøêË°åÔºö12mvn compile exec:java -Dexec.mainClass=org.apache.beam.examples.WordCount \&gt; -Dexec.args="--inputFile=pom.xml --output=counts" -Pdirect-runner ÁªìÊûúÔºö 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150[root@zhm1 word-count-beam]# ll -lh counts*-rw-r--r--. 1 root root 618 1Êúà 19 14:02 counts-00000-of-00005-rw-r--r--. 1 root root 596 1Êúà 19 14:02 counts-00001-of-00005-rw-r--r--. 1 root root 585 1Êúà 19 14:02 counts-00002-of-00005-rw-r--r--. 1 root root 581 1Êúà 19 14:02 counts-00003-of-00005-rw-r--r--. 1 root root 593 1Êúà 19 14:02 counts-00004-of-00005[root@zhm1 word-count-beam]# cat counts-00000-of-00005work: 1IS: 1versions: 1direct: 3specified: 1incubating: 1more: 1snapshots: 4submission: 1... Minimal WordCount demonstrates the basic principles involved in building a pipeline. WordCount introduces some of the more common best practices in creating re-usable and maintainable pipelines. Debugging WordCount introduces logging and debugging practices. Windowed WordCount demonstrates how you can use Beam‚Äôs programming model to handle both bounded and unbounded datasets.```java‰ªéMinimal WordCountÂàÜÊûê‰ª£Á†ÅÔºö/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * "License"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.beam.examples;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Count;import org.apache.beam.sdk.transforms.DoFn;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.ParDo;import org.apache.beam.sdk.transforms.SimpleFunction;import org.apache.beam.sdk.values.KV;/** * An example that counts words in Shakespeare. * * &lt;p&gt;This class, &#123;@link MinimalWordCount&#125;, is the first in a series of four successively more * detailed 'word count' examples. Here, for simplicity, we don't show any error-checking or * argument processing, and focus on construction of the pipeline, which chains together the * application of core transforms. * * &lt;p&gt;Next, see the &#123;@link WordCount&#125; pipeline, then the &#123;@link DebuggingWordCount&#125;, and finally the * &#123;@link WindowedWordCount&#125; pipeline, for more detailed examples that introduce additional * concepts. * * &lt;p&gt;Concepts: * * &lt;pre&gt; * 1. Reading data from text files * 2. Specifying 'inline' transforms * 3. Counting items in a PCollection * 4. Writing data to text files * &lt;/pre&gt; * * &lt;p&gt;No arguments are required to run this pipeline. It will be executed with the DirectRunner. You * can see the results in the output files in your current working directory, with names like * "wordcounts-00001-of-00005. When running on a distributed service, you would use an appropriate * file service. */public class MinimalWordCount &#123; public static void main(String[] args) &#123; // Create a PipelineOptions object. This object lets us set various execution // options for our pipeline, such as the runner you wish to use. This example // will run with the DirectRunner by default, based on the class path configured // in its dependencies. PipelineOptions options = PipelineOptionsFactory.create(); //ÈªòËÆ§ÊòØoptions.setRunner(DirectRunner.class); // Create the Pipeline object with the options we defined above. Pipeline p = Pipeline.create(options); // Apply the pipeline's transforms. // Concept #1: Apply a root transform to the pipeline; in this case, TextIO.Read to read a set // of input text files. TextIO.Read returns a PCollection where each element is one line from // the input text (a set of Shakespeare's texts). // This example reads a public data set consisting of the complete works of Shakespeare. p.apply(TextIO.Read.from("gs://apache-beam-samples/shakespeare/*")) // ËØªÂèñÊú¨Âú∞Êñá‰ª∂ÔºåÊûÑÂª∫Á¨¨‰∏Ä‰∏™PTransfor // Concept #2: Apply a ParDo transform to our PCollection of text lines. This ParDo invokes a // DoFn (defined in-line) on each element that tokenizes the text line into individual words. // The ParDo returns a PCollection&lt;String&gt;, where each element is an individual word in // Shakespeare's collected texts. .apply("ExtractWords", ParDo.of(new DoFn&lt;String, String&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; for (String word : c.element().split("[^a-zA-Z']+")) &#123; if (!word.isEmpty()) &#123; c.output(word); &#125; &#125; &#125; &#125;)) // Concept #3: Apply the Count transform to our PCollection of individual words. The Count // transform returns a new PCollection of key/value pairs, where each key represents a unique // word in the text. The associated value is the occurrence count for that word. .apply(Count.&lt;String&gt;perElement()) // Apply a MapElements transform that formats our PCollection of word counts into a printable // string, suitable for writing to an output file. .apply("FormatResults", MapElements.via(new SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Long&gt; input) &#123; return input.getKey() + ": " + input.getValue(); &#125; &#125;)) // Concept #4: Apply a write transform, TextIO.Write, at the end of the pipeline. // TextIO.Write writes the contents of a PCollection (in this case, our PCollection of // formatted strings) to a series of text files. // // By default, it will write to a set of files with names like wordcount-00001-of-00005 .apply(TextIO.Write.to("wordcounts")); // Run the pipeline. p.run().waitUntilFinish(); &#125;&#125; ÂàõÂª∫ÁÆ°ÈÅì1PipelineOptions options = PipelineOptionsFactory.create(); ‰∏ã‰∏ÄÊ≠•ÊòØ‰ΩøÁî®Êàë‰ª¨ÂàöÊâçÊûÑÂª∫ÁöÑÈÄâÈ°πÂàõÂª∫‰∏Ä‰∏™PipelineÂØπË±°„ÄÇPipelineÂØπË±°ÊûÑÂª∫Ë¶ÅÊâßË°åÁöÑÂèòÊç¢ÂõæÔºå‰∏éÁâπÂÆöÊµÅÊ∞¥Á∫øÁõ∏ÂÖ≥ËÅî„ÄÇ1Pipeline p = Pipeline.create(options); Â∫îÁî®ÁÆ°ÈÅìÂèòÊç¢ MinimalÁöÑWordCountÊµÅÊ∞¥Á∫øÂåÖÂê´Âá†‰∏™ÂèòÊç¢Ôºå‰ª•Â∞ÜÊï∞ÊçÆËØªÂÖ•ÊµÅÊ∞¥Á∫øÔºåÊìçÁ∫µÊàñ‰ª•ÂÖ∂‰ªñÊñπÂºètransformÊï∞ÊçÆÔºåÂπ∂ÂÜôÂá∫ÁªìÊûú„ÄÇÊØè‰∏™transformË°®Á§∫ÁÆ°ÈÅì‰∏≠ÁöÑÊìç‰Ωú„ÄÇ ÊØè‰∏™transformÈááÁî®ÊüêÁßçËæìÂÖ•ÔºàÊï∞ÊçÆÊàñÂÖ∂‰ªñÔºâÔºåÂπ∂‰∫ßÁîü‰∏Ä‰∫õËæìÂá∫Êï∞ÊçÆ„ÄÇËæìÂÖ•ÂíåËæìÂá∫Êï∞ÊçÆÊòØÁî±SDKÁ±ªË°®Á§∫PCollection„ÄÇPCollectionÊòØ‰∏Ä‰∏™ÁâπÊÆäÁöÑÁ±ªÔºåÁî±Beam SDKÊèê‰æõÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂÆÉÊù•Ë°®Á§∫Âá†‰πé‰ªª‰ΩïÂ§ßÂ∞èÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨Êó†ÈôêÊï∞ÊçÆÈõÜ„ÄÇ Minimal WordCountÊµÅÊ∞¥Á∫øÂåÖÂê´‰∫î‰∏™transform 1.‰∏Ä‰∏™ÊñáÊú¨Êñá‰ª∂Read transformÂ∫îÁî®‰∫éÊµÅÊ∞¥Á∫øÂØπË±°Êú¨Ë∫´ÔºåÂπ∂‰∫ßÁîüPCollection‰Ωú‰∏∫ËæìÂá∫„ÄÇËæìÂá∫PCollection‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†Ë°®Á§∫ËæìÂÖ•Êñá‰ª∂‰∏≠ÁöÑ‰∏ÄË°åÊñáÊú¨„ÄÇÊ≠§Á§∫‰æãÊÅ∞Â•Ω‰ΩøÁî®Â≠òÂÇ®Âú®ÂèØÂÖ¨ÂºÄËÆøÈóÆÁöÑGoogle Cloud StorageÂ≠òÂÇ®Ê°∂Ôºà‚ÄúgsÔºö//‚ÄùÔºâ‰∏≠ÁöÑËæìÂÖ•Êï∞ÊçÆ„ÄÇ1p.apply(TextIO.Read.from("gs://apache-beam-samples/shakespeare/*")) 2.‰∏Ä‰∏™ParDo transformÂÆÉË∞ÉÁî®DoFn‰∫ÜtokenizesÊñáÊú¨Ë°åÊàêÂçï‰∏™ÁöÑÂçïËØçÊØè‰∏™ÂÖÉÁ¥†ÔºàÂú®Á∫ø‰Ωú‰∏∫‰∏Ä‰∏™ÂåøÂêçÁ±ª‰∏≠ÂÆö‰πâÔºâ„ÄÇÂØπ‰∫éÊ≠§transformÁöÑËæìÂÖ•ÊòØPCollectionÁî±ÂÖàÂâçÁîüÊàêÁöÑÊñáÊú¨ÁöÑË°åTextIO.ReadÂèòÊç¢„ÄÇÁöÑParDoÂèòÊç¢ËæìÂá∫‰∏Ä‰∏™Êñ∞ÁöÑPCollectionÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂÖÉÁ¥†Ë°®Á§∫ÁöÑÊñáÊú¨ÁöÑÂçïËØç„ÄÇ12345678910.apply("ExtractWords", ParDo.of(new DoFn&lt;String, String&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; for (String word : c.element().split("[^a-zA-Z']+")) &#123; if (!word.isEmpty()) &#123; c.output(word); &#125; &#125; &#125; &#125;)) 3.ËØ•SDKÊèê‰æõÁöÑCount transformÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑtransformÔºåÂÆÉÊé•Âèó‰∏Ä‰∏™PCollection‰ªª‰ΩïÁ±ªÂûãÁöÑÔºåÂíåËøîÂõûPCollectionÈîÆ/ÂÄºÂØπ„ÄÇÊØè‰∏™ÈîÆË°®Á§∫ËæìÂÖ•ÈõÜÂêà‰∏≠ÁöÑÂîØ‰∏ÄÂÖÉÁ¥†ÔºåÊØè‰∏™ÂÄºË°®Á§∫ÈîÆÂú®ËæìÂÖ•ÈõÜÂêà‰∏≠Âá∫Áé∞ÁöÑÊ¨°Êï∞„ÄÇ ËøôÊù°ÁÆ°Á∫øÔºåÁî®‰∫éÂ∞ÜËæìÂÖ•CountÁöÑÊòØPCollectionÁî±‰ª•Ââç‰∫ßÁîü‰∏™Âà´ÂçïËØçParDoÔºåÂπ∂ËæìÂá∫‰∏∫‰∏Ä‰∏™PCollectionÔºåÂÖ∂‰∏≠ÊØè‰∏™ÈîÆ‰ª£Ë°®‰∏Ä‰∏™ÂîØ‰∏ÄÂ≠ó‰∏≠ÁöÑÊñáÊú¨ÂíåÁõ∏ÂÖ≥ÂÄºÊòØÂá∫Áé∞ËÆ°Êï∞ÊØè‰∏™ÈîÆ/ÂÄºÂØπ„ÄÇ1.apply(Count.&lt;String&gt;perElement()) 4.‰∏ã‰∏Ä‰∏™transformÂ∞ÜÂîØ‰∏ÄÂ≠óÂíåÂá∫Áé∞ËÆ°Êï∞ÁöÑÊØè‰∏™ÈîÆ/ÂÄºÂØπÊ†ºÂºèÂåñ‰∏∫ÈÄÇ‰∫éÂÜôÂÖ•ËæìÂá∫Êñá‰ª∂ÁöÑÂèØÊâìÂç∞Â≠óÁ¨¶‰∏≤„ÄÇ MapElementsÊòØ‰∏Ä‰∏™Êõ¥È´òÂ±ÇÊ¨°ÁöÑÂ§çÂêàËΩ¨Êç¢ÔºåÂÆÉÂ∞ÅË£Ö‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑParDo; ‰∏∫ËæìÂÖ•‰∏≠ÁöÑÊØè‰∏™ÂÖÉ‰ª∂PCollectionÔºåMapElementsÈÄÇÁî®‰∫é‰∫ßÁîüÊÅ∞Â•Ω‰∏Ä‰∏™ËæìÂá∫ÂÖÉ‰ª∂ÁöÑÂäüËÉΩ„ÄÇÂú®Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåMapElementsË∞ÉÁî®SimpleFunctionÔºàÂú®Á∫ø‰Ωú‰∏∫ÂåøÂêçÁ±ª‰∏≠ÂÆö‰πâÔºâÔºåËØ•Á°ÆÂÆûÁöÑÊ†ºÂºè„ÄÇ‰Ωú‰∏∫ËæìÂÖ•ÔºåMapElementsÂèñPCollectionÊâÄ‰∫ßÁîüÁöÑÈîÆ/ÂÄºÂØπÁöÑCountÔºåÂπ∂‰∫ßÁîü‰∏Ä‰∏™Êñ∞PCollectionÁöÑÂèØÊâìÂç∞Â≠óÁ¨¶‰∏≤„ÄÇ123456.apply("FormatResults", MapElements.via(new SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Long&gt; input) &#123; return input.getKey() + ": " + input.getValue(); &#125; &#125;)) 5.‰∏Ä‰∏™ÊñáÊú¨Êñá‰ª∂Write„ÄÇÊ≠§transformÈúÄË¶ÅÁöÑÊúÄÁªàPCollectionÊ†ºÂºèÂ≠óÁ¨¶‰∏≤‰Ωú‰∏∫ËæìÂÖ•ÂíåÊØè‰∏™ÂÖÉÁ¥†ÂÜôÂÖ•Âà∞ËæìÂá∫ÊñáÊú¨Êñá‰ª∂„ÄÇËæìÂÖ•‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†PCollectionË°®Á§∫Âú®ÊâÄ‰∫ßÁîüÁöÑËæìÂá∫Êñá‰ª∂ÁöÑÊñáÊú¨ÁöÑ‰∏Ä‰∏™Ë°å„ÄÇ1.apply(TextIO.Write.to("wordcounts")); Ê≥®ÊÑèÔºåËØ•WriteÂèòÊç¢‰∫ßÁîüÁ±ªÂûãÁöÑÁêêÁ¢éÁªìÊûúÂÄºPDoneÔºåÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãË¢´ÂøΩÁï• ËøêË°åÁÆ°ÈÅìÈÄöËøáË∞ÉÁî®ËøêË°åÁÆ°Á∫ørunÁöÑÊñπÊ≥ïÔºåÂÆÉ‰ºöÂ∞ÜÊÇ®ÁöÑÁÆ°ÈÅìÁî±ÊÇ®Âú®ÂàõÂª∫ÊÇ®ÁöÑÁÆ°ÈÅì‰∏≠ÊåáÂÆöÁöÑÁÆ°ÈÅìËøêË°åÁ®ãÂ∫èÊâßË°å„ÄÇ1p.run().waitUntilFinish(); Ê≥®ÊÑèÔºåËØ•runÊñπÊ≥ïÊòØÂºÇÊ≠•ÁöÑ„ÄÇÂØπ‰∫éÈòªÊ≠¢ÊâßË°åÔºåËÄå‰∏çÊòØÔºåËøêË°åÁÆ°Á∫øËøΩÂä†waitUntilFinishÊñπÊ≥ï„ÄÇ]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>Beam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-Ê¶ÇÂøµ]]></title>
    <url>%2F2017%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[ÁõëÁù£Â≠¶‰π†ÔºöÊàë‰ª¨ÂêëÁ≥ªÁªüËæìÂÖ•Êàë‰ª¨ÊâÄÁß∞ÁöÑ‚ÄúÂ∑≤Ê†áËÆ∞Ê†∑Êú¨‚ÄùÂ∑≤Ê†áËÆ∞Ê†∑Êú¨ÊåáÊàë‰ª¨ÁªôÁ≥ªÁªü‰∏Ä‰∫õ‰ø°ÊÅØÔºåËÆ©ÂÆÉÁî®‰∫éÁêÜËß£Êàë‰ª¨ÂêëÂÆÉËæìÂÖ•ÁöÑÊï∞ÊçÆ„ÄÇ‰æãÂ¶ÇÊàë‰ª¨ÂàõÂª∫‰∏Ä‰∏™Á≥ªÁªüÊù•ËØÜÂà´ÊàëÁöÑËÑ∏ÔºåÊàë‰ª¨Â∞ÜÁªô‰ªñ‰∏Ä‰∫õÊàëÁöÑÈù¢ÈÉ®ÁÖßÁâáÔºå‰∏Ä‰∫õÂà´‰∫∫ÁöÑÈù¢ÈÉ®ÁÖßÁâáÔºåÊàë‰ª¨‰ºöÂëäËØâÁ≥ªÁªü Ëøô‰∫õÁÖßÁâáÊòØDavidÁöÑÔºåÈÇ£‰∫õÁÖßÁâáÊòØÂà´‰∫∫ÁöÑ„ÄÇÈöèÊó∂Èó¥Êé®ÁßªÔºåÂÆÉ‰ºöÂà©Áî®Ëøô‰∫õ‰ø°ÊÅØÊèêÂçáÂÆÉÁöÑÁêÜËß£Á®ãÂ∫¶„ÄÇ‚ÄùÁõëÁù£‚ÄúË°®Á§∫‰Ω†ÊúâÂæàÂ§öÊ†∑Êú¨Ôºå‰Ω†‰∫ÜËß£Ëøô‰∫õÊ†∑Êú¨ÁöÑÊ≠£Á°ÆÁ≠îÊ°à ÈùûÁõëÁù£Â≠¶‰π†ÊØîÂ¶ÇÂú®ÂåªÁñó‰øùÂÅ•ÊñπÈù¢ÔºåÂ¶ÇÊûúÊàë‰ª¨Â∑≤Áªè‰∫ÜËß£ÊüêÁßçÁñæÁóÖÔºåÁõëÁù£Â≠¶‰π†ÂèØÁî®‰ª•Â∏ÆÂä©Á≥ªÁªüÔºåËØÜÂà´Âì™‰∫õ‰∫∫ÊúâÊÇ£ËØ•ÁóÖÁöÑÈ£éÈô©ÔºåËÄåÈùûÁõëÁù£Â≠¶‰π†ÂÆûÈôÖ‰∏äÂèØ‰ª•Ê†πÊçÆÂ∏∏ËßÅÁóáÁä∂ÁöÑÊ®°ÂºèÂ∏ÆÂä©Êàë‰ª¨ÂèëÁé∞ÁîöËá≥Êàë‰ª¨Ëøò‰∏çÁü•ÈÅìÁöÑÁñæÁóÖ„ÄÇ ËÅöÁ±ªÔºöK-MEANS Êú¥Á¥†Ë¥ùÂè∂ÊñØÔºöÊé®ÂØºËøáÁ®ãÂíåÁî®Ê≥ïÔºöhttp://scikit-learn.org/stable/modules/naive_bayes.html ËøòÊúâÈ´òÊñØÊú¥Á¥†Ë¥ùÂè∂ÊñØ pythonÊúâ‰∏Ä‰∏™Â∫ìÂè´sklearn]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êú∫Âô®Â≠¶‰π†-È¢ÑÊµãÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰πòÂÆ¢ÁîüËøòÁéá]]></title>
    <url>%2F2017%2F01%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E8%BF%98%E7%8E%87%2F</url>
    <content type="text"><![CDATA[Êú∫Âô®Â≠¶‰π†È°πÁõÆ 0: È¢ÑÊµãÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰πòÂÆ¢ÁîüËøòÁéá1912Âπ¥ÔºåÊ≥∞Âù¶Â∞ºÂÖãÂè∑Âú®Á¨¨‰∏ÄÊ¨°Ëà™Ë°å‰∏≠Â∞±‰∏éÂÜ∞Â±±Áõ∏ÊíûÊ≤âÊ≤°ÔºåÂØºËá¥‰∫ÜÂ§ßÈÉ®ÂàÜ‰πòÂÆ¢ÂíåËàπÂëòË∫´‰∫°„ÄÇÂú®Ëøô‰∏™ÂÖ•Èó®È°πÁõÆ‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢Á¥¢ÈÉ®ÂàÜÊ≥∞Âù¶Â∞ºÂÖãÂè∑ÊóÖÂÆ¢ÂêçÂçïÔºåÊù•Á°ÆÂÆöÂì™‰∫õÁâπÂæÅÂèØ‰ª•ÊúÄÂ•ΩÂú∞È¢ÑÊµã‰∏Ä‰∏™‰∫∫ÊòØÂê¶‰ºöÁîüËøò„ÄÇ‰∏∫‰∫ÜÂÆåÊàêËøô‰∏™È°πÁõÆÔºå‰Ω†Â∞ÜÈúÄË¶ÅÂÆûÁé∞Âá†‰∏™Âü∫‰∫éÊù°‰ª∂ÁöÑÈ¢ÑÊµãÂπ∂ÂõûÁ≠î‰∏ãÈù¢ÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨Â∞ÜÊ†πÊçÆ‰ª£Á†ÅÁöÑÂÆåÊàêÂ∫¶ÂíåÂØπÈóÆÈ¢òÁöÑËß£Á≠îÊù•ÂØπ‰Ω†Êèê‰∫§ÁöÑÈ°πÁõÆÁöÑËøõË°åËØÑ‰º∞„ÄÇ ÂºÄÂßãÂΩìÊàë‰ª¨ÂºÄÂßãÂ§ÑÁêÜÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰πòÂÆ¢Êï∞ÊçÆÊó∂Ôºå‰ºöÂÖàÂØºÂÖ•Êàë‰ª¨ÈúÄË¶ÅÁöÑÂäüËÉΩÊ®°Âùó‰ª•ÂèäÂ∞ÜÊï∞ÊçÆÂä†ËΩΩÂà∞ pandas DataFrame„ÄÇËøêË°å‰∏ãÈù¢Âå∫Âüü‰∏≠ÁöÑ‰ª£Á†ÅÂä†ËΩΩÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî® .head() ÂáΩÊï∞ÊòæÁ§∫ÂâçÂá†È°π‰πòÂÆ¢Êï∞ÊçÆ„ÄÇ 1234567891011121314151617import numpy as npimport pandas as pd# RMS Titanic data visualization code # Êï∞ÊçÆÂèØËßÜÂåñ‰ª£Á†Åfrom titanic_visualizations import survival_statsfrom IPython.display import display%matplotlib inline# Load the dataset # Âä†ËΩΩÊï∞ÊçÆÈõÜin_file = 'titanic_data.csv'full_data = pd.read_csv(in_file)# Print the first few entries of the RMS Titanic data # ÊòæÁ§∫Êï∞ÊçÆÂàóË°®‰∏≠ÁöÑÂâçÂá†È°π‰πòÂÆ¢Êï∞ÊçÆdisplay(full_data.head()) Â∫èÂè∑ PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th‚Ä¶ female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S ‰ªéÊ≥∞Âù¶Â∞ºÂÖãÂè∑ÁöÑÊï∞ÊçÆÊ†∑Êú¨‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞Ëàπ‰∏äÊØè‰ΩçÊóÖÂÆ¢ÁöÑÁâπÂæÅ SurvivedÔºöÊòØÂê¶Â≠òÊ¥ªÔºà0‰ª£Ë°®Âê¶Ôºå1‰ª£Ë°®ÊòØÔºâ PclassÔºöÁ§æ‰ºöÈò∂Á∫ßÔºà1‰ª£Ë°®‰∏äÂ±ÇÈò∂Á∫ßÔºå2‰ª£Ë°®‰∏≠Â±ÇÈò∂Á∫ßÔºå3‰ª£Ë°®Â∫ïÂ±ÇÈò∂Á∫ßÔºâ NameÔºöËàπ‰∏ä‰πòÂÆ¢ÁöÑÂêçÂ≠ó SexÔºöËàπ‰∏ä‰πòÂÆ¢ÁöÑÊÄßÂà´ Age:Ëàπ‰∏ä‰πòÂÆ¢ÁöÑÂπ¥ÈæÑÔºàÂèØËÉΩÂ≠òÂú® NaNÔºâ SibSpÔºö‰πòÂÆ¢Âú®Ëàπ‰∏äÁöÑÂÖÑÂºüÂßêÂ¶πÂíåÈÖçÂÅ∂ÁöÑÊï∞Èáè ParchÔºö‰πòÂÆ¢Âú®Ëàπ‰∏äÁöÑÁà∂ÊØç‰ª•ÂèäÂ∞èÂ≠©ÁöÑÊï∞Èáè TicketÔºö‰πòÂÆ¢ËàπÁ•®ÁöÑÁºñÂè∑ FareÔºö‰πòÂÆ¢‰∏∫ËàπÁ•®ÊîØ‰ªòÁöÑË¥πÁî® CabinÔºö‰πòÂÆ¢ÊâÄÂú®ËàπËà±ÁöÑÁºñÂè∑ÔºàÂèØËÉΩÂ≠òÂú® NaNÔºâ EmbarkedÔºö‰πòÂÆ¢‰∏äËàπÁöÑÊ∏ØÂè£ÔºàC ‰ª£Ë°®‰ªé Cherbourg ÁôªËàπÔºåQ ‰ª£Ë°®‰ªé Queenstown ÁôªËàπÔºåS ‰ª£Ë°®‰ªé Southampton ÁôªËàπÔºâ Âõ†‰∏∫Êàë‰ª¨ÊÑüÂÖ¥Ë∂£ÁöÑÊòØÊØè‰∏™‰πòÂÆ¢ÊàñËàπÂëòÊòØÂê¶Âú®‰∫ãÊïÖ‰∏≠Ê¥ª‰∫Ü‰∏ãÊù•„ÄÇÂèØ‰ª•Â∞Ü Survived Ëøô‰∏ÄÁâπÂæÅ‰ªéËøô‰∏™Êï∞ÊçÆÈõÜÁßªÈô§ÔºåÂπ∂‰∏îÁî®‰∏Ä‰∏™ÂçïÁã¨ÁöÑÂèòÈáè outcomes Êù•Â≠òÂÇ®„ÄÇÂÆÉ‰πüÂÅö‰∏∫Êàë‰ª¨Ë¶ÅÈ¢ÑÊµãÁöÑÁõÆÊ†á„ÄÇ ËøêË°åËØ•‰ª£Á†ÅÔºå‰ªéÊï∞ÊçÆÈõÜ‰∏≠ÁßªÈô§ Survived Ëøô‰∏™ÁâπÂæÅÔºåÂπ∂Â∞ÜÂÆÉÂ≠òÂÇ®Âú®ÂèòÈáè outcomes ‰∏≠„ÄÇ 123456789101112# Store the 'Survived' feature in a new variable and remove it from the dataset # ‰ªéÊï∞ÊçÆÈõÜ‰∏≠ÁßªÈô§ 'Survived' Ëøô‰∏™ÁâπÂæÅÔºåÂπ∂Â∞ÜÂÆÉÂ≠òÂÇ®Âú®‰∏Ä‰∏™Êñ∞ÁöÑÂèòÈáè‰∏≠„ÄÇoutcomes = full_data['Survived']data = full_data.drop('Survived', axis = 1)# Show the new dataset with 'Survived' removed# ÊòæÁ§∫Â∑≤ÁßªÈô§ 'Survived' ÁâπÂæÅÁöÑÊï∞ÊçÆÈõÜdisplay(data.head())display(outcomes[1])display(data.loc[1]) Â∫èÂè∑ PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 Cumings, Mrs. John Bradley (Florence Briggs Th‚Ä¶ female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1 PassengerId 2 Pclass 1 Name Cumings, Mrs. John Bradley (Florence Briggs Th... Sex female Age 38 SibSp 1 Parch 0 Ticket PC 17599 Fare 71.2833 Cabin C85 Embarked C Name: 1, dtype: object Ëøô‰∏™‰æãÂ≠êÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÊ≥∞Âù¶Â∞ºÂÖãÂè∑ÁöÑ Survived Êï∞ÊçÆ‰ªé DataFrame ÁßªÈô§„ÄÇÊ≥®ÊÑèÂà∞ dataÔºà‰πòÂÆ¢Êï∞ÊçÆÔºâÂíå outcomes ÔºàÊòØÂê¶Â≠òÊ¥ªÔºâÁé∞Âú®Â∑≤ÁªèÂåπÈÖçÂ•Ω„ÄÇËøôÊÑèÂë≥ÁùÄÂØπ‰∫é‰ªª‰Ωï‰πòÂÆ¢ÁöÑ data.loc[i] ÈÉΩÊúâÂØπÂ∫îÁöÑÂ≠òÊ¥ªÁöÑÁªìÊûú outcome[i]„ÄÇ ‰∏∫‰∫ÜÈ™åËØÅÊàë‰ª¨È¢ÑÊµãÁöÑÁªìÊûúÔºåÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™Ê†áÂáÜÊù•ÁªôÊàë‰ª¨ÁöÑÈ¢ÑÊµãÊâìÂàÜ„ÄÇÂõ†‰∏∫Êàë‰ª¨ÊúÄÊÑüÂÖ¥Ë∂£ÁöÑÊòØÊàë‰ª¨È¢ÑÊµãÁöÑÂáÜÁ°ÆÁéáÔºåÊó¢Ê≠£Á°ÆÈ¢ÑÊµã‰πòÂÆ¢Â≠òÊ¥ªÁöÑÊØî‰æã„ÄÇËøêË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊù•ÂàõÂª∫Êàë‰ª¨ÁöÑ accuracy_score ÂáΩÊï∞‰ª•ÂØπÂâç‰∫îÂêç‰πòÂÆ¢ÁöÑÈ¢ÑÊµãÊù•ÂÅöÊµãËØï„ÄÇ ÊÄùËÄÉÈ¢òÔºö‰ªéÁ¨¨ÂÖ≠‰∏™‰πòÂÆ¢ÁÆóËµ∑ÔºåÂ¶ÇÊûúÊàë‰ª¨È¢ÑÊµã‰ªñ‰ª¨ÂÖ®ÈÉ®ÈÉΩÂ≠òÊ¥ªÔºå‰Ω†ËßâÂæóÊàë‰ª¨È¢ÑÊµãÁöÑÂáÜÁ°ÆÁéáÊòØÂ§öÂ∞ëÔºü 123456789101112131415161718192021222324def accuracy_score(truth, pred): """ Returns accuracy score for input truth and predictions. """ # Ensure that the number of predictions matches number of outcomes # Á°Æ‰øùÈ¢ÑÊµãÁöÑÊï∞Èáè‰∏éÁªìÊûúÁöÑÊï∞Èáè‰∏ÄËá¥ if len(truth) == len(pred): # print "kaish1"# print truth# print "kaish2"# print pred# print "end" # Calculate and return the accuracy as a percent # ËÆ°ÁÆóÈ¢ÑÊµãÂáÜÁ°ÆÁéáÔºàÁôæÂàÜÊØîÔºâ return "Predictions have an accuracy of &#123;:.2f&#125;%.".format((truth == pred).mean()*100) else: return "Number of predictions does not match number of outcomes!" # Test the 'accuracy_score' function# ÊµãËØï 'accuracy_score' ÂáΩÊï∞# print np.ones(5, dtype = int)# print outcomes[:5]predictions = pd.Series(np.ones(5, dtype = int))# print accuracy_score(outcomes[:5], predictions) ÊèêÁ§∫ÔºöÂ¶ÇÊûú‰Ω†‰øùÂ≠ò iPython NotebookÔºå‰ª£Á†ÅËøêË°åÁöÑËæìÂá∫‰πüÂ∞ÜË¢´‰øùÂ≠ò„ÄÇ‰ΩÜÊòØÔºå‰∏ÄÊó¶‰Ω†ÈáçÊñ∞ÊâìÂºÄÈ°πÁõÆÔºå‰Ω†ÁöÑÂ∑•‰ΩúÂå∫Â∞Ü‰ºöË¢´ÈáçÁΩÆ„ÄÇËØ∑Á°Æ‰øùÊØèÊ¨°ÈÉΩ‰ªé‰∏äÊ¨°Á¶ªÂºÄÁöÑÂú∞ÊñπËøêË°å‰ª£Á†ÅÊù•ÈáçÊñ∞ÁîüÊàêÂèòÈáèÂíåÂáΩÊï∞„ÄÇ È¢ÑÊµãÂ¶ÇÊûúÊàë‰ª¨Ë¶ÅÈ¢ÑÊµãÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰∏äÁöÑ‰πòÂÆ¢ÊòØÂê¶Â≠òÊ¥ªÔºå‰ΩÜÊòØÊàë‰ª¨ÂèàÂØπ‰ªñ‰ª¨‰∏ÄÊó†ÊâÄÁü•ÔºåÈÇ£‰πàÊúÄÂ•ΩÁöÑÈ¢ÑÊµãÂ∞±ÊòØËàπ‰∏äÁöÑ‰∫∫Êó†‰∏ÄÂπ∏ÂÖç„ÄÇËøôÊòØÂõ†‰∏∫ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÅáÂÆöÂΩìËàπÊ≤âÊ≤°ÁöÑÊó∂ÂÄôÂ§ßÂ§öÊï∞‰πòÂÆ¢ÈÉΩÈÅáÈöæ‰∫Ü„ÄÇ‰∏ãÈù¢ÁöÑ predictions_0 ÂáΩÊï∞Â∞±È¢ÑÊµãËàπ‰∏äÁöÑ‰πòÂÆ¢ÂÖ®ÈÉ®ÈÅáÈöæ„ÄÇ 1234567891011121314151617def predictions_0(data): """ Model with no features. Always predicts a passenger did not survive. """ predictions = [] for _, passenger in data.iterrows(): # Predict the survival of 'passenger' # È¢ÑÊµã 'passenger' ÁöÑÁîüËøòÁéá predictions.append(0) # Return our predictions # ËøîÂõûÈ¢ÑÊµãÁªìÊûú return pd.Series(predictions)# Make the predictions# ËøõË°åÈ¢ÑÊµãpredictions = predictions_0(data) ÈóÆÈ¢ò1ÂØπÊØîÁúüÂÆûÁöÑÊ≥∞Âù¶Â∞ºÂÖãÂè∑ÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊûúÊàë‰ª¨ÂÅö‰∏Ä‰∏™ÊâÄÊúâ‰πòÂÆ¢ÈÉΩÊ≤°ÊúâÂ≠òÊ¥ªÁöÑÈ¢ÑÊµãÔºå‰Ω†ËÆ§‰∏∫Ëøô‰∏™È¢ÑÊµãÁöÑÂáÜÁ°ÆÁéáËÉΩËææÂà∞Â§öÂ∞ëÔºü ÊèêÁ§∫ÔºöËøêË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊù•Êü•ÁúãÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÁéá„ÄÇ 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 61.62%. ÂõûÁ≠î: ËØ∑Áî®‰∏äÈù¢Âá∫Áé∞ÁöÑÈ¢ÑÊµãÁªìÊûúÊù•ÊõøÊç¢ÊéâËøôÈáåÁöÑÊñáÂ≠ó 61.62% Êàë‰ª¨ÂèØ‰ª•‰ΩøÁî® survival_stats ÂáΩÊï∞Êù•ÁúãÁúã Sex Ëøô‰∏ÄÁâπÂæÅÂØπ‰πòÂÆ¢ÁöÑÂ≠òÊ¥ªÁéáÊúâÂ§öÂ§ßÂΩ±Âìç„ÄÇËøô‰∏™ÂáΩÊï∞ÂÆö‰πâÂú®Âêç‰∏∫ titanic_visualizations.py ÁöÑ Python ËÑöÊú¨Êñá‰ª∂‰∏≠ÔºåÊàë‰ª¨ÁöÑÈ°πÁõÆÊèê‰æõ‰∫ÜËøô‰∏™Êñá‰ª∂„ÄÇ‰º†ÈÄíÁªôÂáΩÊï∞ÁöÑÂâç‰∏§‰∏™ÂèÇÊï∞ÂàÜÂà´ÊòØÊ≥∞Âù¶Â∞ºÂÖãÂè∑ÁöÑ‰πòÂÆ¢Êï∞ÊçÆÂíå‰πòÂÆ¢ÁöÑ ÁîüËøòÁªìÊûú„ÄÇÁ¨¨‰∏â‰∏™ÂèÇÊï∞Ë°®ÊòéÊàë‰ª¨‰ºö‰æùÊçÆÂì™‰∏™ÁâπÂæÅÊù•ÁªòÂà∂ÂõæÂΩ¢„ÄÇ ËøêË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÁªòÂà∂Âá∫‰æùÊçÆ‰πòÂÆ¢ÊÄßÂà´ËÆ°ÁÆóÂ≠òÊ¥ªÁéáÁöÑÊü±ÂΩ¢Âõæ„ÄÇ 1survival_stats(data, outcomes, 'Sex') ËßÇÂØüÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰∏ä‰πòÂÆ¢Â≠òÊ¥ªÁöÑÊï∞ÊçÆÁªüËÆ°ÔºåÊàë‰ª¨ÂèØ‰ª•ÂèëÁé∞Â§ßÈÉ®ÂàÜÁî∑ÊÄß‰πòÂÆ¢Âú®ËàπÊ≤âÊ≤°ÁöÑÊó∂ÂÄôÈÉΩÈÅáÈöæ‰∫Ü„ÄÇÁõ∏ÂèçÁöÑÔºåÂ§ßÈÉ®ÂàÜÂ•≥ÊÄß‰πòÂÆ¢ÈÉΩÂú®‰∫ãÊïÖ‰∏≠ÁîüËøò„ÄÇËÆ©Êàë‰ª¨Âú®ÂÖàÂâçÊé®Êñ≠ÁöÑÂü∫Á°Ä‰∏äÁªßÁª≠ÂàõÂª∫ÔºöÂ¶ÇÊûú‰πòÂÆ¢ÊòØÁî∑ÊÄßÔºåÈÇ£‰πàÊàë‰ª¨Â∞±È¢ÑÊµã‰ªñ‰ª¨ÈÅáÈöæÔºõÂ¶ÇÊûú‰πòÂÆ¢ÊòØÂ•≥ÊÄßÔºåÈÇ£‰πàÊàë‰ª¨È¢ÑÊµã‰ªñ‰ª¨Âú®‰∫ãÊïÖ‰∏≠Ê¥ª‰∫Ü‰∏ãÊù•„ÄÇ Â∞Ü‰∏ãÈù¢ÁöÑ‰ª£Á†ÅË°•ÂÖÖÂÆåÊï¥ÔºåËÆ©ÂáΩÊï∞ÂèØ‰ª•ËøõË°åÊ≠£Á°ÆÈ¢ÑÊµã„ÄÇ ÊèêÁ§∫ÔºöÊÇ®ÂèØ‰ª•Áî®ËÆøÈóÆ dictionaryÔºàÂ≠óÂÖ∏ÔºâÁöÑÊñπÊ≥ïÊù•ËÆøÈóÆËàπ‰∏ä‰πòÂÆ¢ÁöÑÊØè‰∏™ÁâπÂæÅÂØπÂ∫îÁöÑÂÄº„ÄÇ‰æãÂ¶ÇÔºå passenger[&#39;Sex&#39;] ËøîÂõû‰πòÂÆ¢ÁöÑÊÄßÂà´„ÄÇ 123456789101112131415161718192021222324252627def predictions_1(data): """ Model with one feature: - Predict a passenger survived if they are female. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # ÁßªÈô§‰∏ãÊñπÁöÑ 'pass' Â£∞Êòé # and write your prediction conditions here # ËæìÂÖ•‰Ω†Ëá™Â∑±ÁöÑÈ¢ÑÊµãÊù°‰ª∂ # print passenger['Sex'] if(passenger['Sex'] == "male") : predictions.append(0) else : predictions.append(1) # Return our predictions # ËøîÂõûÈ¢ÑÊµãÁªìÊûú return pd.Series(predictions)# Make the predictions# ËøõË°åÈ¢ÑÊµãpredictions = predictions_1(data)# print predictions ÈóÆÈ¢ò2ÂΩìÊàë‰ª¨È¢ÑÊµãËàπ‰∏äÂ•≥ÊÄß‰πòÂÆ¢ÂÖ®ÈÉ®Â≠òÊ¥ªÔºåËÄåÂâ©‰∏ãÁöÑ‰∫∫ÂÖ®ÈÉ®ÈÅáÈöæÔºåÈÇ£‰πàÊàë‰ª¨È¢ÑÊµãÁöÑÂáÜÁ°ÆÁéá‰ºöËææÂà∞Â§öÂ∞ëÔºü ÊèêÁ§∫ÔºöËøêË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊù•Êü•ÁúãÊàë‰ª¨È¢ÑÊµãÁöÑÂáÜÁ°ÆÁéá„ÄÇ 12#print predictionsprint accuracy_score(outcomes, predictions) Predictions have an accuracy of 78.68%. ÂõûÁ≠î: Áî®‰∏äÈù¢Âá∫Áé∞ÁöÑÈ¢ÑÊµãÁªìÊûúÊù•ÊõøÊç¢ÊéâËøôÈáåÁöÑÊñáÂ≠ó 78.68%. ‰ªÖ‰ªÖ‰ΩøÁî®‰πòÂÆ¢ÊÄßÂà´ÔºàSexÔºâËøô‰∏ÄÁâπÂæÅÔºåÊàë‰ª¨È¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÂ∞±Êúâ‰∫ÜÊòéÊòæÁöÑÊèêÈ´ò„ÄÇÁé∞Âú®ÂÜçÁúã‰∏Ä‰∏ã‰ΩøÁî®È¢ùÂ§ñÁöÑÁâπÂæÅËÉΩÂê¶Êõ¥Ëøõ‰∏ÄÊ≠•ÊèêÂçáÊàë‰ª¨ÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÂ∫¶„ÄÇ‰æãÂ¶ÇÔºåÁªºÂêàËÄÉËôëÊâÄÊúâÂú®Ê≥∞Âù¶Â∞ºÂÖãÂè∑‰∏äÁöÑÁî∑ÊÄß‰πòÂÆ¢ÔºöÊàë‰ª¨ÊòØÂê¶ÊâæÂà∞Ëøô‰∫õ‰πòÂÆ¢‰∏≠ÁöÑ‰∏Ä‰∏™Â≠êÈõÜÔºå‰ªñ‰ª¨ÁöÑÂ≠òÊ¥ªÊ¶ÇÁéáËæÉÈ´ò„ÄÇËÆ©Êàë‰ª¨ÂÜçÊ¨°‰ΩøÁî® survival_stats ÂáΩÊï∞Êù•ÁúãÁúãÊØè‰ΩçÁî∑ÊÄß‰πòÂÆ¢ÁöÑÂπ¥ÈæÑÔºàAgeÔºâ„ÄÇËøô‰∏ÄÊ¨°ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®Á¨¨Âõõ‰∏™ÂèÇÊï∞Êù•ÈôêÂÆöÊü±ÂΩ¢Âõæ‰∏≠Âè™ÊúâÁî∑ÊÄß‰πòÂÆ¢„ÄÇ ËøêË°å‰∏ãÈù¢ËøôÊÆµ‰ª£Á†ÅÔºåÊääÁî∑ÊÄßÂü∫‰∫éÂπ¥ÈæÑÁöÑÁîüÂ≠òÁªìÊûúÁªòÂà∂Âá∫Êù•„ÄÇ 1survival_stats(data, outcomes, 'Age', ["Sex == 'male'"]) ‰ªîÁªÜËßÇÂØüÊ≥∞Âù¶Â∞ºÂÖãÂè∑Â≠òÊ¥ªÁöÑÊï∞ÊçÆÁªüËÆ°ÔºåÂú®ËàπÊ≤âÊ≤°ÁöÑÊó∂ÂÄôÔºåÂ§ßÈÉ®ÂàÜÂ∞è‰∫é10Â≤ÅÁöÑÁî∑Â≠©ÈÉΩÊ¥ªÁùÄÔºåËÄåÂ§ßÂ§öÊï∞10Â≤Å‰ª•‰∏äÁöÑÁî∑ÊÄßÈÉΩÈöèÁùÄËàπÁöÑÊ≤âÊ≤°ËÄåÈÅáÈöæ„ÄÇËÆ©Êàë‰ª¨ÁªßÁª≠Âú®ÂÖàÂâçÈ¢ÑÊµãÁöÑÂü∫Á°Ä‰∏äÊûÑÂª∫ÔºöÂ¶ÇÊûú‰πòÂÆ¢ÊòØÂ•≥ÊÄßÔºåÈÇ£‰πàÊàë‰ª¨Â∞±È¢ÑÊµãÂ•π‰ª¨ÂÖ®ÈÉ®Â≠òÊ¥ªÔºõÂ¶ÇÊûú‰πòÂÆ¢ÊòØÁî∑ÊÄßÂπ∂‰∏îÂ∞è‰∫é10Â≤ÅÔºåÊàë‰ª¨‰πü‰ºöÈ¢ÑÊµã‰ªñ‰ª¨ÂÖ®ÈÉ®Â≠òÊ¥ªÔºõÊâÄÊúâÂÖ∂ÂÆÉÊàë‰ª¨Â∞±È¢ÑÊµã‰ªñ‰ª¨ÈÉΩÊ≤°ÊúâÂπ∏Â≠ò„ÄÇ Â∞Ü‰∏ãÈù¢Áº∫Â§±ÁöÑ‰ª£Á†ÅË°•ÂÖÖÂÆåÊï¥ÔºåËÆ©Êàë‰ª¨ÁöÑÂáΩÊï∞ÂèØ‰ª•ÂÆûÁé∞È¢ÑÊµã„ÄÇÊèêÁ§∫: ÊÇ®ÂèØ‰ª•Áî®‰πãÂâç predictions_1 ÁöÑ‰ª£Á†Å‰Ωú‰∏∫ÂºÄÂßãÊù•‰øÆÊîπ‰ª£Á†ÅÔºåÂÆûÁé∞Êñ∞ÁöÑÈ¢ÑÊµãÂáΩÊï∞„ÄÇ 12345678910111213141516171819202122232425262728293031def predictions_2(data): """ Model with two features: - Predict a passenger survived if they are female. - Predict a passenger survived if they are male and younger than 10. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # ÁßªÈô§‰∏ãÊñπÁöÑ 'pass' Â£∞Êòé # and write your prediction conditions here # ËæìÂÖ•‰Ω†Ëá™Â∑±ÁöÑÈ¢ÑÊµãÊù°‰ª∂ if(passenger['Sex'] == "male") : if(passenger['Age'] &lt; 10) : predictions.append(1) else : predictions.append(0) else : predictions.append(1) # Return our predictions # ËøîÂõûÈ¢ÑÊµãÁªìÊûú return pd.Series(predictions)# Make the predictions# ËøõË°åÈ¢ÑÊµãpredictions = predictions_2(data)# print predictions ÈóÆÈ¢ò3ÂΩìÈ¢ÑÊµãÊâÄÊúâÂ•≥ÊÄß‰ª•ÂèäÂ∞è‰∫é10Â≤ÅÁöÑÁî∑ÊÄßÈÉΩÂ≠òÊ¥ªÁöÑÊó∂ÂÄôÔºåÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÁéá‰ºöËææÂà∞Â§öÂ∞ëÔºü ÊèêÁ§∫ÔºöËøêË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊù•Êü•ÁúãÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÁéá„ÄÇ 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 79.35%. ÂõûÁ≠î: Áî®‰∏äÈù¢Âá∫Áé∞ÁöÑÈ¢ÑÊµãÁªìÊûúÊù•ÊõøÊç¢ÊéâËøôÈáåÁöÑÊñáÂ≠ó 79.35% Ê∑ªÂä†Âπ¥ÈæÑÔºàAgeÔºâÁâπÂæÅ‰∏éÊÄßÂà´ÔºàSexÔºâÁöÑÁªìÂêàÊØîÂçïÁã¨‰ΩøÁî®ÊÄßÂà´ÔºàSexÔºâ‰πüÊèêÈ´ò‰∫Ü‰∏çÂ∞ëÂáÜÁ°ÆÂ∫¶„ÄÇÁé∞Âú®ËØ•‰Ω†Êù•ÂÅöÈ¢ÑÊµã‰∫ÜÔºöÊâæÂà∞‰∏ÄÁ≥ªÂàóÁöÑÁâπÂæÅÂíåÊù°‰ª∂Êù•ÂØπÊï∞ÊçÆËøõË°åÂàíÂàÜÔºå‰ΩøÂæóÈ¢ÑÊµãÁªìÊûúÊèêÈ´òÂà∞80%‰ª•‰∏ä„ÄÇËøôÂèØËÉΩÈúÄË¶ÅÂ§ö‰∏™ÁâπÊÄßÂíåÂ§ö‰∏™Â±ÇÊ¨°ÁöÑÊù°‰ª∂ËØ≠Âè•Êâç‰ºöÊàêÂäü„ÄÇ‰Ω†ÂèØ‰ª•Âú®‰∏çÂêåÁöÑÊù°‰ª∂‰∏ãÂ§öÊ¨°‰ΩøÁî®Áõ∏ÂêåÁöÑÁâπÂæÅ„ÄÇPclassÔºåSexÔºåAgeÔºåSibSp Âíå Parch ÊòØÂª∫ËÆÆÂ∞ùËØï‰ΩøÁî®ÁöÑÁâπÂæÅ„ÄÇ ‰ΩøÁî® survival_stats ÂáΩÊï∞Êù•ËßÇÊµãÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰∏ä‰πòÂÆ¢Â≠òÊ¥ªÁöÑÊï∞ÊçÆÁªüËÆ°„ÄÇÊèêÁ§∫: Ë¶Å‰ΩøÁî®Â§ö‰∏™ËøáÊª§Êù°‰ª∂ÔºåÊääÊØè‰∏Ä‰∏™Êù°‰ª∂ÊîæÂú®‰∏Ä‰∏™ÂàóË°®Èáå‰Ωú‰∏∫ÊúÄÂêé‰∏Ä‰∏™ÂèÇÊï∞‰º†ÈÄíËøõÂéª„ÄÇ‰æãÂ¶Ç: [&quot;Sex == &#39;male&#39;&quot;, &quot;Age &lt; 18&quot;] 1survival_stats(data, outcomes, 'Embarked', ["Sex == 'female'","Pclass == 3"]) ÂΩìÊü•ÁúãÂíåÁ†îÁ©∂‰∫ÜÂõæÂΩ¢ÂåñÁöÑÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰∏ä‰πòÂÆ¢ÁöÑÊï∞ÊçÆÁªüËÆ°ÂêéÔºåËØ∑Ë°•ÂÖ®‰∏ãÈù¢ËøôÊÆµ‰ª£Á†Å‰∏≠Áº∫Â§±ÁöÑÈÉ®ÂàÜÔºå‰ΩøÂæóÂáΩÊï∞ÂèØ‰ª•ËøîÂõû‰Ω†ÁöÑÈ¢ÑÊµã„ÄÇÂú®Âà∞ËææÊúÄÁªàÁöÑÈ¢ÑÊµãÊ®°ÂûãÂâçËØ∑Á°Æ‰øùËÆ∞ÂΩï‰Ω†Â∞ùËØïËøáÁöÑÂêÑÁßçÁâπÂæÅÂíåÊù°‰ª∂„ÄÇÊèêÁ§∫: ÊÇ®ÂèØ‰ª•Áî®‰πãÂâç predictions_2 ÁöÑ‰ª£Á†Å‰Ωú‰∏∫ÂºÄÂßãÊù•‰øÆÊîπ‰ª£Á†ÅÔºåÂÆûÁé∞Êñ∞ÁöÑÈ¢ÑÊµãÂáΩÊï∞„ÄÇ 123456789101112131415161718192021222324252627282930313233343536373839def predictions_3(data): """ Model with multiple features. Makes a prediction with an accuracy of at least 80%. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # and write your prediction conditions here if(passenger['Sex'] == "male") : if(passenger['Age'] &lt; 18) : if(passenger['Pclass'] &lt; 3 ) : predictions.append(1) else : predictions.append(0) else : predictions.append(0) # print passenger['Pclass']# if(passenger['Pclass'] &gt; 1 ) :# predictions.append(0)# else :# predictions.append(1) else : if(passenger['Pclass'] == 3 ) : if(passenger['Embarked'] == 'S' ) : predictions.append(0) else : predictions.append(1) else : predictions.append(1) # Return our predictions return pd.Series(predictions)# Make the predictionspredictions = predictions_3(data) ÁªìËÆ∫ËØ∑ÊèèËø∞‰Ω†ÂÆûÁé∞80%ÂáÜÁ°ÆÂ∫¶ÁöÑÈ¢ÑÊµãÊ®°ÂûãÊâÄÁªèÂéÜÁöÑÊ≠•È™§„ÄÇÊÇ®ËßÇÂØüËøáÂì™‰∫õÁâπÂæÅÔºüÊüê‰∫õÁâπÊÄßÊòØÂê¶ÊØîÂÖ∂‰ªñÁâπÂæÅÊõ¥ÊúâÂ∏ÆÂä©Ôºü‰Ω†Áî®‰∫Ü‰ªÄ‰πàÊù°‰ª∂Êù•È¢ÑÊµãÁîüËøòÁªìÊûúÔºü‰Ω†ÊúÄÁªàÁöÑÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÁéáÊòØÂ§öÂ∞ëÔºüÊèêÁ§∫:ËøêË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÊù•Êü•Áúã‰Ω†ÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÂ∫¶„ÄÇ 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 82.38%. ÂõûÁ≠î: Áî®‰∏äÈù¢ÈóÆÈ¢òÁöÑÁ≠îÊ°àÊù•ÊõøÊç¢ÊéâËøôÈáåÁöÑÊñáÂ≠ó ÁªìËÆ∫ÁªèËøá‰∫ÜÊï∞Ê¨°ÂØπÊï∞ÊçÆÁöÑÊé¢Á¥¢ÂíåÂàÜÁ±ªÔºå‰Ω†ÂàõÂª∫‰∫Ü‰∏Ä‰∏™È¢ÑÊµãÊ≥∞Âù¶Â∞ºÂÖãÂè∑‰πòÂÆ¢Â≠òÊ¥ªÁéáÁöÑÊúâÁî®ÁöÑÁÆóÊ≥ï„ÄÇÂú®Ëøô‰∏™È°πÁõÆ‰∏≠‰Ω†ÊâãÂä®Âú∞ÂÆûÁé∞‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊú∫Âô®Â≠¶‰π†Ê®°Âûã‚Äî‚ÄîÂÜ≥Á≠ñÊ†ëÔºàdecision treeÔºâ„ÄÇÂÜ≥Á≠ñÊ†ëÊØèÊ¨°ÊåâÁÖß‰∏Ä‰∏™ÁâπÂæÅÊääÊï∞ÊçÆÂàÜÂâ≤ÊàêË∂äÊù•Ë∂äÂ∞èÁöÑÁæ§ÁªÑÔºàË¢´Áß∞‰∏∫ nodesÔºâ„ÄÇÊØèÊ¨°Êï∞ÊçÆÁöÑ‰∏Ä‰∏™Â≠êÈõÜË¢´ÂàÜÂá∫Êù•ÔºåÂ¶ÇÊûúÂàÜÂâ≤ÁªìÊûúÁöÑÂ≠êÈõÜ‰∏≠ÁöÑÊï∞ÊçÆÊØî‰πãÂâçÊõ¥ÂêåË¥®ÔºàÂåÖÂê´Ëøë‰ººÁöÑÊ†áÁ≠æÔºâÔºåÊàë‰ª¨ÁöÑÈ¢ÑÊµã‰πüÂ∞±Êõ¥Âä†ÂáÜÁ°Æ„ÄÇÁîµËÑëÊù•Â∏ÆÂä©Êàë‰ª¨ÂÅöËøô‰ª∂‰∫ã‰ºöÊØîÊâãÂä®ÂÅöÊõ¥ÂΩªÂ∫ïÔºåÊõ¥Á≤æÁ°Æ„ÄÇËøô‰∏™ÈìæÊé•Êèê‰æõ‰∫ÜÂè¶‰∏Ä‰∏™‰ΩøÁî®ÂÜ≥Á≠ñÊ†ëÂÅöÊú∫Âô®Â≠¶‰π†ÂÖ•Èó®ÁöÑ‰æãÂ≠ê„ÄÇ ÂÜ≥Á≠ñÊ†ëÊòØËÆ∏Â§öÁõëÁù£Â≠¶‰π†ÁÆóÊ≥ï‰∏≠ÁöÑ‰∏ÄÁßç„ÄÇÂú®ÁõëÁù£Â≠¶‰π†‰∏≠ÔºåÊàë‰ª¨ÂÖ≥ÂøÉÁöÑÊòØ‰ΩøÁî®Êï∞ÊçÆÁöÑÁâπÂæÅÂπ∂Ê†πÊçÆÊï∞ÊçÆÁöÑÁªìÊûúÊ†áÁ≠æËøõË°åÈ¢ÑÊµãÊàñÂª∫Ê®°„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÊØè‰∏ÄÁªÑÊï∞ÊçÆÈÉΩÊúâ‰∏Ä‰∏™ÁúüÊ≠£ÁöÑÁªìÊûúÂÄºÔºå‰∏çËÆ∫ÊòØÂÉèÊ≥∞Âù¶Â∞ºÂÖãÂè∑ÁîüÂ≠òÊï∞ÊçÆÈõÜ‰∏ÄÊ†∑ÁöÑÊ†áÁ≠æÔºåÊàñËÄÖÊòØËøûÁª≠ÁöÑÊàø‰ª∑È¢ÑÊµã„ÄÇ ÈóÆÈ¢ò5ÊÉ≥Ë±°‰∏Ä‰∏™ÁúüÂÆû‰∏ñÁïå‰∏≠Â∫îÁî®ÁõëÁù£Â≠¶‰π†ÁöÑÂú∫ÊôØÔºå‰Ω†ÊúüÊúõÈ¢ÑÊµãÁöÑÁªìÊûúÊòØ‰ªÄ‰πàÔºü‰∏æÂá∫‰∏§‰∏™Âú®Ëøô‰∏™Âú∫ÊôØ‰∏≠ËÉΩÂ§üÂ∏ÆÂä©‰Ω†ËøõË°åÈ¢ÑÊµãÁöÑÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÁâπÂæÅ„ÄÇ]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÁÆóÊ≥ïÊé®Êºî]]></title>
    <url>%2F2017%2F01%2F10%2F%E7%AE%97%E6%B3%95%E6%8E%A8%E6%BC%94%2F</url>
    <content type="text"><![CDATA[Êù°‰ª∂Ê¶ÇÁéá‰∫ã‰ª∂AÂú®Âè¶Â§ñ‰∏Ä‰∏™‰∫ã‰ª∂BÂ∑≤ÁªèÂèëÁîüÊù°‰ª∂‰∏ãÁöÑÂèëÁîüÊ¶ÇÁéá„ÄÇÊù°‰ª∂Ê¶ÇÁéáË°®Á§∫‰∏∫PÔºàA|BÔºâÔºåËØª‰Ωú‚ÄúÂú®BÊù°‰ª∂‰∏ãAÁöÑÊ¶ÇÁéá‚Äù„ÄÇ ËÆæAÔºåB ÊòØ‰∏§‰∏™‰∫ã‰ª∂Ôºå‰∏îA‰∏çÊòØ‰∏çÂèØËÉΩ‰∫ã‰ª∂ÔºåÂàôÁß∞‰∏∫Âú®‰∫ã‰ª∂AÂèëÁîüÁöÑÊù°‰ª∂‰∏ãÔºå‰∫ã‰ª∂BÂèëÁîüÁöÑÊù°‰ª∂Ê¶ÇÁéá„ÄÇ‰∏ÄËà¨Âú∞ÔºåÔºå‰∏îÂÆÉÊª°Ë∂≥‰ª•‰∏ã‰∏âÊù°‰ª∂ÔºöÔºà1ÔºâÈùûË¥üÊÄßÔºõÔºà2ÔºâËßÑËåÉÊÄßÔºõÔºà3ÔºâÂèØÂàóÂèØÂä†ÊÄß„ÄÇ Â¶Ç‰∏äÂõæÊ±ÇÔºöÁ∫¢ÁêÉÊù•Ëá™AÁöÑÊ¶ÇÁéáPÔºàA|Á∫¢Ôºâ = PÔºàAÁ∫¢Ôºâ/PÔºàÁ∫¢Ôºâ ÂÖ®Ê¶ÇÁéáÂÖ¨ÂºèÂÖ®Ê¶ÇÁéáÂÖ¨Âºè‰∏∫Ê¶ÇÁéáËÆ∫‰∏≠ÁöÑÈáçË¶ÅÂÖ¨ÂºèÔºåÂÆÉÂ∞ÜÂØπ‰∏ÄÂ§çÊùÇ‰∫ã‰ª∂AÁöÑÊ¶ÇÁéáÊ±ÇËß£ÈóÆÈ¢òËΩ¨Âåñ‰∏∫‰∫ÜÂú®‰∏çÂêåÊÉÖÂÜµ‰∏ãÂèëÁîüÁöÑÁÆÄÂçï‰∫ã‰ª∂ÁöÑÊ¶ÇÁéáÁöÑÊ±ÇÂíåÈóÆÈ¢ò„ÄÇ ÂÖ¨ÂºèË°®Á§∫Ëã•‰∫ã‰ª∂A1ÔºåA2Ôºå‚Ä¶ÔºåAnÊûÑÊàê‰∏Ä‰∏™ÂÆåÂ§á‰∫ã‰ª∂ÁªÑ‰∏îÈÉΩÊúâÊ≠£Ê¶ÇÁéáÔºåÂàôÂØπ‰ªªÊÑè‰∏Ä‰∏™‰∫ã‰ª∂BÈÉΩÊúâÂÖ¨ÂºèÊàêÁ´ã„ÄÇ Â¶Ç‰∏äÂõæÊ±ÇÔºöÁ∫¢ÁêÉÁöÑÊ¶ÇÁéáPÔºàÁ∫¢Ôºâ = PÔºàAÁ∫¢Ôºâ+PÔºàBÁ∫¢Ôºâ Ë¥ùÂè∂ÊñØÂÆöÁêÜË¥ùÂè∂ÊñØÂÆöÁêÜÊòØÂÖ≥‰∫éÈöèÊú∫‰∫ã‰ª∂AÂíåBÁöÑÊù°‰ª∂Ê¶ÇÁéáÔºàÊàñËæπÁºòÊ¶ÇÁéáÔºâÁöÑ‰∏ÄÂàôÂÆöÁêÜ„ÄÇÂÖ∂‰∏≠P(A|B)ÊòØÂú®BÂèëÁîüÁöÑÊÉÖÂÜµ‰∏ãAÂèëÁîüÁöÑÂèØËÉΩÊÄß„ÄÇ Ê¨ßÂá†ÈáåÂæóÂÆöÁêÜ‰∫åÁª¥Á©∫Èó¥ÁöÑÂÖ¨Âºè0œÅ = sqrt( (x1-x2)^2+(y1-y2)^2 ) |x| = ‚àö( x2 + y2 )‰∏âÁª¥Á©∫Èó¥ÁöÑÂÖ¨Âºè0œÅ = ‚àö( (x1-x2)^2+(y1-y2)^2+(z1-z2)^2 ) |x| = ‚àö( x2 + y2 + z2 )nÁª¥Á©∫Èó¥ÁöÑÂÖ¨ÂºènÁª¥Ê¨ßÊ∞èÁ©∫Èó¥ÊòØ‰∏Ä‰∏™ÁÇπÈõÜ,ÂÆÉÁöÑÊØè‰∏™ÁÇπ X ÊàñÂêëÈáè x ÂèØ‰ª•Ë°®Á§∫‰∏∫ (x1Ôºåx2Ôºå‚Ä¶Ôºåx[n]) ÔºåÂÖ∂‰∏≠ xi ÊòØÂÆûÊï∞ÔºåÁß∞‰∏∫ X ÁöÑÁ¨¨i‰∏™ÂùêÊ†á„ÄÇ‰∏§‰∏™ÁÇπ A = (a1Ôºåa2Ôºå‚Ä¶Ôºåa[n]) Âíå B = (b1Ôºåb2Ôºå‚Ä¶Ôºåb[n]) ‰πãÈó¥ÁöÑË∑ùÁ¶ª œÅ(AÔºåB) ÂÆö‰πâ‰∏∫‰∏ãÈù¢ÁöÑÂÖ¨ÂºèÔºöœÅ(AÔºåB) =‚àö [ ‚àë( a[i] - b[i] )^2 ] (i = 1Ôºå2Ôºå‚Ä¶Ôºån)ÂêëÈáè x = (x1Ôºåx2Ôºå‚Ä¶Ôºåx[n]) ÁöÑËá™ÁÑ∂ÈïøÂ∫¶ |x| ÂÆö‰πâ‰∏∫‰∏ãÈù¢ÁöÑÂÖ¨ÂºèÔºö|x| = ‚àö( x1^2 + x2^2 + ‚Ä¶ + x[n]^2 ) Á∫øÊÄßÊñπÁ®ãÁ∫øÊÄßÊñπÁ®ã‰πüÁß∞‰∏ÄÊ¨°ÊñπÁ®ã„ÄÇÊåáÊú™Áü•Êï∞ÈÉΩÊòØ‰∏ÄÊ¨°ÁöÑÊñπÁ®ã„ÄÇÂÖ∂‰∏ÄËà¨ÁöÑÂΩ¢ÂºèÊòØax+by+‚Ä¶+cz+d=0„ÄÇÁ∫øÊÄßÊñπÁ®ãÁöÑÊú¨Ë¥®ÊòØÁ≠âÂºè‰∏§Ëæπ‰πò‰ª•‰ªª‰ΩïÁõ∏ÂêåÁöÑÈùûÈõ∂Êï∞ÔºåÊñπÁ®ãÁöÑÊú¨Ë¥®ÈÉΩ‰∏çÂèóÂΩ±Âìç„ÄÇax + by = cÊú∫Âô®Â≠¶‰π†ÈáåÈù¢Áî®ÈÄºËøëËÆ∫Êù•ÁÆó]]></content>
      <categories>
        <category>ÁÆóÊ≥ï</category>
      </categories>
      <tags>
        <tag>ÁÆóÊ≥ï</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python ÂàùÊé¢]]></title>
    <url>%2F2017%2F01%2F10%2Fpython%20%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[ÂàùÊé¢PythonÊòØ‰∏ÄÁßçËß£ÈáäÂûãËØ≠Ë®ÄÔºå‰∏çÁî®ÁºñËØëPythonÊòØ‰∫§‰∫íÂºèËØ≠Ë®ÄPythonÊòØÈù¢ÂêëÂØπË±°ÁöÑËØ≠Ë®Ä ‰ªéÁÆÄÂçïÁöÑÊñáÂ≠óÂ§ÑÁêÜÂà∞Áà¨Ëô´Ê∏∏Êàè pythonÊúâ2‰∏™ÂàÜÊîØÔºå‰∏Ä‰∏™ÊòØ2.x ‰∏Ä‰∏™ÊòØ3.xËØ≠Ê≥ï‰ºö‰∏çÂêå‰æãÂ¶Çprint 1 Âú®3.xÁâàÊú¨ÊòØ‰∏Ä‰∏™ÂáΩÊï∞Ë¶ÅÂÜôÊàêprint(1) Âíåjava‰∏çÂêåÁöÑÊòØÊúâ‰ª•‰∏ã‰∏§‰∏™Á±ªÂûãÂÆö‰πâÂÖÉÁ•ñÁ±ªÂûã() ÂÖÉÁªÑÊòØÂèØËØªÁöÑÂÆö‰πâÂ≠óÂÖ∏Á±ªÂûã{} ÂàùÂßãÂåña={2,3} type‰ª•‰∏ãaÁöÑÁ±ªÂûã‰∏∫set ÂºïÁî®Ê®°Âùóimport ÂåÖÂêç.Á±ªÂêçfrom ÂåÖÂêç.Á±ªÂêç import *from ÂåÖÂêç.Á±ªÂêç import ÊñπÊ≥ïÂêç as Âà´Âêç Â¶ÇÊûúÊÉ≥Âà´ÁöÑÊ®°Âùó‰∏çËÉΩÂºïÁî®Ëá™Â∑±Ê®°ÂùóÁöÑÂÜÖÂÆπÂèØ‰ª•Â¢ûÂä†if name==‚Äômain‚Äò ÊØè‰∏™ÁõÆÂΩï‰∏ãÈÉΩÊúâinit.pyÊñá‰ª∂ÔºåËøô‰∏™ÊòØÂàùÂßãÂåñÊ®°Âùó pythonÂºÄÂêØÂ§öÁ∫øÁ®ãÔºåÂèØ‰ª•‰ΩøÁî®ÂÜÖÁΩÆÂÖ≥ÈîÆÂ≠óthreadingÁßÅÊúâÂèòÈáèÂâçÈù¢Âä†__ÂÆû‰æã‰∏çËÉΩËÆøÈóÆÁßÅÊúâÂèòÈáè python‰ΩøÁî®ÂºïÁî®ËÆ°Êï∞Ê≥ïËøΩË∏™ÂÜÖÂ≠òÂπ∂ÂõûÊî∂ ÁªßÊâøchild(parent)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ÁÆóÊ≥ïÊÄùÁª¥]]></title>
    <url>%2F2017%2F01%2F04%2F%E7%AE%97%E6%B3%95%E6%80%9D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[ÁÅ´Êü¥Ê£çÊ∏∏Êàè ÁßªÂä®‰∏§Ê†πÁÅ´Êü¥ÂæóÂà∞ÁöÑÊúÄÂ§ßÂÄºÊòØÂ§öÂ∞ëÔºü ‰∏ÄËà¨‰∫∫ÁöÑÁ¨¨‰∏ÄÂèçÂ∫îÊòØÔºö Â¶ÇÊûúÊàë‰ª¨ÊÉ≥Âà∞‰∫Ü‰ΩçÊï∞Ë∂äÂ§öÔºåÊï∞Â≠óË∂äÂ§ßÔºåÂ∞±‰ºöÊÉ≥Âà∞Ôºö Ê≠§Êó∂Â¶ÇÊûúÊÉ≥Âà∞‰∫ÜÂπ≥ÊñπËÆ°ÁÆó ÈÇ£‰πà‰Ω†ÈúÄË¶ÅÁöÑÊòØÁªßÁª≠ÂèëÊï£Âõõ‰ΩçÔºåÊÉ≥Âà∞ÁßëÂ≠¶ËÆ°Êï∞Ê≥ï ‰ΩÜÊòØÊ≠§Êó∂Âπ∂‰∏çÊòØÊúÄÂ§ßÁöÑ Âè™ÊúâÂΩì‰Ω†ÊÉ≥Âà∞‰∫Ü‰ª•‰∏ãÁöÑÊï∞ÊçÆÔºåÈÇ£‰πàÊÅ≠Âñú‰Ω†ÊàêÂäü‰∫Ü ÊÄùË∑ØÂÜ≥ÂÆöÂá∫Ë∑Ø]]></content>
      <categories>
        <category>ÁÆóÊ≥ï</category>
      </categories>
      <tags>
        <tag>ÁÆóÊ≥ï</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot solr]]></title>
    <url>%2F2016%2F12%2F28%2Fspring%20boot%20solr%2F</url>
    <content type="text"><![CDATA[pomÂºïÂÖ•solrÁöÑjarÂåÖ1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-solr&lt;/artifactId&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[dockerÁâàKafkaÈõÜÁæ§]]></title>
    <url>%2F2016%2F12%2F19%2Fdocker%E7%8E%A9kafka%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[ÂÖ•Êâãcentos7È¶ñÂÖàÊâìÂºÄcentos ÂÆòÁΩë‰∏ãËΩΩhttps://www.centos.org/download/ ÈÄâÊã©DVD ISO ÊâæÂà∞Âú∞ÂùÄÂπ∂‰∏ãËΩΩhttp://101.96.8.151/isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso ÂÆâË£ÖÂèÇËÄÉÔºöhttp://blog.csdn.net/alex_my/article/details/38142229 ÂÆâË£ÖdockerÂÆâË£ÖËØ¶ÊÉÖÂèØÂèÇËÄÉÂÆòÁΩëhttps://docs.docker.com/engine/installation/linux/centos/Âçö‰∏ª‰ΩøÁî®Âè¶Â§ñ‰∏ÄÁßçÁÆÄÂçïÊñπÂºèÂÆâË£ÖÂÆâË£Ödocker1yum install docker ÂêØÂä®ÊúçÂä°ÔºàCENTOS7‰πãÂâçÁöÑÁâàÊú¨Ôºâ12service docker startchkconfig docker on Centos‰πãÂêéÁöÑÔºö12systemctl start docker.servicesystemctl enable docker.service 123456789101112131415161718docker versionClient: Version: 1.10.3 API version: 1.22 Package version: docker-common-1.10.3-59.el7.centos.x86_64 Go version: go1.6.3 Git commit: 3999ccb-unsupported Built: Thu Dec 15 17:24:43 2016 OS/Arch: linux/amd64Server: Version: 1.10.3 API version: 1.22 Package version: docker-common-1.10.3-59.el7.centos.x86_64 Go version: go1.6.3 Git commit: 3999ccb-unsupported Built: Thu Dec 15 17:24:43 2016 OS/Arch: linux/amd64 ÂÆâË£Ödocker-composeÂçö‰∏ªÂΩìÊó∂Êúâ1.7.1ÁöÑÁâàÊú¨Âú®ÂÆâË£ÖÂÆå‰πãÂêéÂêØÂä®Êä•ÈîôCannot open self /usr/bin/docker-compose or archive /usr/bin/docker-compose.pkg ÂêéÊîπÁî®Êó©‰∏ÄÁÇπÁöÑÁâàÊú¨1.7.0ÊµãËØïÊ≤°ÊúâÈóÆÈ¢ò‰∏ãËΩΩÂíåÂÆâË£ÖÂú∞ÂùÄÂèØÂèÇËÄÉÔºöhttps://github.com/docker/compose/releases?after=docs-v1.7.1-2016-05-3112345678curl -L https://github.com/docker/compose/releases/download/1.7.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composedocker-compose versiondocker-compose version 1.7.0, build 0d7bf73docker-py version: 1.8.0CPython version: 2.7.9OpenSSL version: OpenSSL 1.0.1e 11 Feb 2013 ÁºñÂÜôdockerÊñá‰ª∂ÔºåÊ≠§Â§ÑÂèÇËÄÉJasonÂ§ßÁ•ûÁöÑÔºåÂ∞äÈáçÂéüÂàõÔºåËΩ¨ËΩΩËØ∑Ê†áÊ≥®kafka.Dockerfile12345678910111213141516171819202122232425262728293031323334353637383940414243444546FROM centos:6.6RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\ mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoRUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pipRUN mkdir /opt/java &amp;&amp;\ wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/javaENV KAFKA_VERSION "0.8.2.2"RUN mkdir /opt/kafka &amp;&amp;\ wget http://apache.fayea.com/kafka/$KAFKA_VERSION/kafka_2.11-$KAFKA_VERSION.tgz -P /opt/kafkaRUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\ JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\ sed -i "/^PATH/i export JAVA_HOME=$JAVA_HOME" /root/.bash_profile &amp;&amp;\ sed -i "s%^PATH.*$%&amp;:$JAVA_HOME/bin%g" /root/.bash_profile &amp;&amp;\ source /root/.bash_profileRUN tar zxvf /opt/kafka/kafka*.tgz -C /opt/kafka &amp;&amp;\ sed -i 's/num.partitions.*$/num.partitions=3/g' /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.propertiesRUN echo "source /root/.bash_profile" &gt; /opt/kafka/start.sh &amp;&amp;\ echo "cd /opt/kafka/kafka_2.11-"$KAFKA_VERSION &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ #echo "sed -i 's%zookeeper.connect=.*$%zookeeper.connect=zookeeper:2181%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""ZOOKEEPER_CONNECT"" ] &amp;&amp; sed -i 's%.*zookeeper.connect=.*$%zookeeper.connect='$""ZOOKEEPER_CONNECT'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""BROKER_ID"" ] &amp;&amp; sed -i 's%broker.id=.*$%broker.id='$""BROKER_ID'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""BROKER_PORT"" ] &amp;&amp; sed -i 's%port=.*$%port='$""BROKER_PORT'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "sed -i 's%#advertised.host.name=.*$%advertised.host.name='$""(hostname -i)'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""ADVERTISED_HOST_NAME"" ] &amp;&amp; sed -i 's%.*advertised.host.name=.*$%advertised.host.name='$""ADVERTISED_HOST_NAME'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "sed -i 's%#host.name=.*$%host.name='$""(hostname -i)'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""HOST_NAME"" ] &amp;&amp; sed -i 's%.*host.name=.*$%host.name='$""HOST_NAME'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "delete.topic.enable=true" &gt;&gt; /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.properties &amp;&amp;\ echo "bin/kafka-server-start.sh config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ chmod a+x /opt/kafka/start.shRUN yum install -y ncEXPOSE 9092WORKDIR /opt/kafka/kafka_2.11-$KAFKA_VERSIONENTRYPOINT ["sh", "/opt/kafka/start.sh"] zookeeper.Dockerfile1234567891011121314151617181920212223242526272829303132333435363738FROM centos:6.6RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\ mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoRUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pipRUN mkdir /opt/java &amp;&amp;\ wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/javaRUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\ JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\ sed -i &quot;/^PATH/i export JAVA_HOME=$JAVA_HOME&quot; /root/.bash_profile &amp;&amp;\ sed -i &quot;s%^PATH.*$%&amp;:$JAVA_HOME/bin%g&quot; /root/.bash_profile &amp;&amp;\ source /root/.bash_profileENV ZOOKEEPER_VERSION &quot;3.4.6&quot;RUN mkdir /opt/zookeeper &amp;&amp;\ wget http://mirror.olnevhost.net/pub/apache/zookeeper/zookeeper-$ZOOKEEPER_VERSION/zookeeper-$ZOOKEEPER_VERSION.tar.gz -P /opt/zookeeperRUN tar zxvf /opt/zookeeper/zookeeper*.tar.gz -C /opt/zookeeperRUN echo &quot;source /root/.bash_profile&quot; &gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;cp /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo_sample.cfg /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_PORT&quot;&quot; ] &amp;&amp; sed -i &apos;s%.*clientPort=.*$%clientPort=&apos;$&quot;&quot;ZOOKEEPER_PORT&apos;&quot;&quot;%g&apos; /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_ID&quot;&quot; ] &amp;&amp; mkdir -p /tmp/zookeeper &amp;&amp; echo $&quot;&quot;ZOOKEEPER_ID &gt; /tmp/zookeeper/myid&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[[ ! -z $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot; ]] &amp;&amp; for server in $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot;; do echo $&quot;&quot;server&quot;&quot; &gt;&gt; /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg; done&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;/opt/zookeeper/zookeeper-$&quot;ZOOKEEPER_VERSION&quot;/bin/zkServer.sh start-foreground&quot; &gt;&gt; /opt/zookeeper/start.shRUN yum install -y ncEXPOSE 2181WORKDIR /opt/zookeeper/zookeeper-$ZOOKEEPER_VERSIONENTRYPOINT [&quot;sh&quot;, &quot;/opt/zookeeper/start.sh&quot;] docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139version: &apos;2.0&apos;services: zookeeper0: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper0 hostname: zookeeper0 ports: - &quot;2181:2181&quot; - &quot;2888:2888&quot; - &quot;3888:3888&quot; expose: - 2181 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_ID: 0 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 zookeeper1: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper1 hostname: zookeeper1 ports: - &quot;2182:2182&quot; - &quot;28881:28881&quot; - &quot;38881:38881&quot; expose: - 2182 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2182 ZOOKEEPER_ID: 1 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882# depends_on:# - zookeeper0 zookeeper2: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper2 hostname: zookeeper2 ports: - &quot;2183:2183&quot; - &quot;28882:28882&quot; - &quot;38882:38882&quot; expose: - 2183 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2183 ZOOKEEPER_ID: 2 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882# depends_on:# - zookeeper1 kafka0: build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka0 hostname: kafka0 ports: - &quot;9092:9092&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 0 BROKER_PORT: 9092 ADVERTISED_HOST_NAME: kafka0 HOST_NAME: kafka0 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9092# links:# - zookeeper kafka1: build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka1 hostname: kafka1 ports: - &quot;9093:9093&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 1 BROKER_PORT: 9093 ADVERTISED_HOST_NAME: kafka1 HOST_NAME: kafka1 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9093# links:# - zookeeper kafka2: build: . build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka2 hostname: kafka2 ports: - &quot;9094:9094&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 2 BROKER_PORT: 9094 ADVERTISED_HOST_NAME: kafka2 HOST_NAME: kafka2 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9094# links:# - zookeeper ÂêØÂä®docker-compose1[root@zhm1 docker-file]# docker-compose up -d Â¶ÇÊûúÊõæÁªèÊúâÂêØÂä®ÁöÑÈúÄË¶ÅÂÖàÂÅúÊ≠¢ÁßªÈô§Âú®ÂêØÂä®1[root@zhm1 docker-file]# docker-compose stop;docker-compose rm -f 12345678[root@zhm1 docker-file]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES891585b4767a jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 9092/tcp, 0.0.0.0:9093-&gt;9093/tcp kafka181f90ccb917a jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 9092/tcp, 0.0.0.0:9094-&gt;9094/tcp kafka22edb39ed0e97 jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 0.0.0.0:9092-&gt;9092/tcp kafka0b21ac8fb0f72 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 2181/tcp, 2888/tcp, 0.0.0.0:2183-&gt;2183/tcp, 0.0.0.0:28882-&gt;28882/tcp, 3888/tcp, 0.0.0.0:38882-&gt;38882/tcp zookeeper248f5ada24ff7 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2888-&gt;2888/tcp, 0.0.0.0:3888-&gt;3888/tcp zookeeper0d247a8ac6a30 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 2181/tcp, 2888/tcp, 0.0.0.0:2182-&gt;2182/tcp, 0.0.0.0:28881-&gt;28881/tcp, 3888/tcp, 0.0.0.0:38881-&gt;38881/tcp zookeeper1 Ê≠§Êó∂ÂèØ‰ª•ËøõÂÖ•ÂÆπÂô®ÂÜÖÊü•ÁúãÔºö123docker exec -it zookeeper0 bashsource /root/.bash_profilebin/zkCli.sh -server zookeeper0:2181 1234567[zk: zookeeper0:2182(CONNECTED) 0] ls /[zookeeper, kafka][zk: zookeeper0:2182(CONNECTED) 1] ls /kafka[admin, consumers, controller, controller_epoch, brokers, config][zk: zookeeper0:2182(CONNECTED) 2] ls /kafka/brokers/ids[0, 1, 2][zk: zookeeper0:2182(CONNECTED) 3] Ê≠§Â§ÑÊ≥®ÊÑèÔºåÂõ†‰∏∫docker-compose.ymlÊñá‰ª∂ÈáåÈÖçÁΩÆÁöÑkafkaÂºïÁî®zookeeperÊ†πÁõÆÂΩï‰∏ãÁöÑkafkaÁõÆÂΩïÔºåÊïÖÂú®ÂÜôzookeeperÁöÑÊó∂ÂÄôË¶ÅÂ¢ûÂä†/kafka123456789[root@zhm1 docker-file]# docker exec -it kafka0 bash[root@kafka0 kafka_2.11-0.8.2.2]# source /root/.bash_profile[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --create --partitions 3 --replication-factor 1Created topic "topic1".[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --describe Topic:topic1 PartitionCount:3 ReplicationFactor:1 Configs: Topic: topic1 Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: topic1 Partition: 1 Leader: 2 Replicas: 2 Isr: 2 Topic: topic1 Partition: 2 Leader: 0 Replicas: 0 Isr: 0 kafkaÊúØËØ≠TopicTopic,ÊòØKAFKAÂØπÊ∂àÊÅØÂàÜÁ±ªÁöÑ‰æùÊçÆ;‰∏ÄÊù°Ê∂àÊÅØ,ÂøÖÈ°ªÊúâ‰∏Ä‰∏™‰∏é‰πãÂØπÂ∫îÁöÑTopic;ÊØîÂ¶ÇÁé∞Âú®Âèà‰∏§‰∏™Topic,ÂàÜÂà´ÊòØTopicAÂíåTopicB,ProducerÂêëTopicAÂèëÈÄÅ‰∏Ä‰∏™Ê∂àÊÅØmessageA,ÁÑ∂ÂêéÂêëTopicBÂèëÈÄÅ‰∏Ä‰∏™Ê∂àÊÅØmessaeB;ÈÇ£‰πà,ËÆ¢ÈòÖTopicAÁöÑConsumerÂ∞±‰ºöÊî∂Âà∞Ê∂àÊÅØmessageA,ËÆ¢ÈòÖTopicBÁöÑConsumerÂ∞±‰ºöÊî∂Âà∞Ê∂àÊÅØmessaeB;(ÊØè‰∏™ConsumerÂèØ‰ª•ÂêåÊó∂ËÆ¢ÈòÖÂ§ö‰∏™Topic,‰πüÂç≥ÊòØËØ¥,ÂêåÊó∂ËÆ¢ÈòÖTopicAÂíåTopicBÁöÑConsumerÂèØ‰ª•Êî∂Âà∞messageAÂíåmessaeB)„ÄÇÂêå‰∏Ä‰∏™Group idÁöÑconsumersÂú®Âêå‰∏Ä‰∏™TopicÁöÑÂêå‰∏ÄÊù°Ê∂àÊÅØÂè™ËÉΩË¢´‰∏Ä‰∏™consumerÊ∂àË¥πÔºåÂÆûÁé∞‰∫ÜÁÇπÂØπÁÇπÊ®°ÂºèÔºå‰∏çÂêåGroup idÁöÑConsumersÂú®Âêå‰∏Ä‰∏™Topic‰∏äÁöÑÂêå‰∏ÄÊù°Ê∂àÊÅØÂèØ‰ª•ÂêåÊó∂Ê∂àË¥πÂà∞ÔºåÂàôÂÆûÁé∞‰∫ÜÂèëÂ∏ÉËÆ¢ÈòÖÊ®°Âºè„ÄÇÈÄöËøáConsumerÁöÑGroup idÂÆûÁé∞‰∫ÜJMSÁöÑÊ∂àÊÅØÊ®°Âºè PartitionÊØè‰∏Ä‰∏™TopicÂèØ‰ª•ÊúâÂ§ö‰∏™Partition,ËøôÊ†∑ÂÅöÊòØ‰∏∫‰∫ÜÊèêÈ´òKAFKAÁ≥ªÁªüÁöÑÂπ∂ÂèëËÉΩÂäõÔºåÊØè‰∏™Partition‰∏≠ÊåâÁÖßÊ∂àÊÅØÂèëÈÄÅÁöÑÈ°∫Â∫è‰øùÂ≠òÁùÄProducerÂèëÊù•ÁöÑÊ∂àÊÅØ,ÊØè‰∏™Ê∂àÊÅØÁî®IDÊ†áËØÜ,‰ª£Ë°®Ëøô‰∏™Ê∂àÊÅØÂú®ÊîπPartition‰∏≠ÁöÑÂÅèÁßªÈáè,ËøôÊ†∑,Áü•ÈÅì‰∫ÜID,Â∞±ÂèØ‰ª•Êñπ‰æøÁöÑÂÆö‰Ωç‰∏Ä‰∏™Ê∂àÊÅØ‰∫Ü;ÊØè‰∏™Êñ∞Êèê‰∫§ËøáÊù•ÁöÑÊ∂àÊÅØ,Ë¢´ËøΩÂä†Âà∞PartitionÁöÑÂ∞æÈÉ®;Â¶ÇÊûú‰∏Ä‰∏™PartitionË¢´ÂÜôÊª°‰∫Ü,Â∞±‰∏çÂÜçËøΩÂä†;(Ê≥®ÊÑè,KAFKA‰∏ç‰øùËØÅ‰∏çÂêåPartition‰πãÈó¥ÁöÑÊ∂àÊÅØÊúâÂ∫è‰øùÂ≠ò) LeaderPartition‰∏≠Ë¥üË¥£Ê∂àÊÅØËØªÂÜôÁöÑËäÇÁÇπ;LeaderÊòØ‰ªéPartitionÁöÑËäÇÁÇπ‰∏≠ÈöèÊú∫ÈÄâÂèñÁöÑ„ÄÇÊØè‰∏™PartitionÈÉΩ‰ºöÂú®ÈõÜ‰∏≠ÁöÑÂÖ∂‰∏≠‰∏ÄÂè∞ÊúçÂä°Âô®Â≠òÂú®Leader„ÄÇ‰∏Ä‰∏™TopicÂ¶ÇÊûúÊúâÂ§ö‰∏™PartitionÔºåÂàô‰ºöÊúâÂ§ö‰∏™Leader„ÄÇ ReplicationFactor‰∏Ä‰∏™Partition‰∏≠Â§çÂà∂Êï∞ÊçÆÁöÑÊâÄÊúâËäÇÁÇπ,ÂåÖÊã¨Â∑≤ÁªèÊåÇ‰∫ÜÁöÑ;Êï∞Èáè‰∏ç‰ºöË∂ÖËøáÈõÜÁæ§‰∏≠brokerÁöÑÊï∞Èáè isrReplicationFactorÁöÑÂ≠êÈõÜ,Â≠òÊ¥ªÁöÑ‰∏îÂíåLeader‰øùÊåÅÂêåÊ≠•ÁöÑËäÇÁÇπ;leade‰ºöÁª¥Êä§‰∏Ä‰∏™‰∏éÂÖ∂Âü∫Êú¨‰øùÊåÅÂêåÊ≠•ÁöÑReplicaÂàóË°®ÔºåËØ•ÂàóË°®Êàê‰∏∫ISRÔºàin-sync ReplicaÔºâ.Â¶ÇÊûú‰∏Ä‰∏™FollowerÊØîleaderËêΩÂêéÂ§™Â§öÔºåÊàñËÄÖË∂ÖËøá‰∏ÄÊÆµÊó∂Èó¥Êú™ÂèëËµ∑Êï∞ÊçÆÂ§çÂà∂ËØ∑Ê±ÇÔºàkafkaÁöÑÊï∞ÊçÆÂ§çÂà∂ÊòØfollowerÁöÑpullÂΩ¢ÂºèÔºâÔºåÂàôleaderÂ∞ÜÂÖ∂‰ªéISR‰∏≠ÁßªÈô§ balabala..ÈÇ£‰πàÂ§öÔºåÊù•‰∏™ÂÆû‰æãÁé©Áé©Âêß ÊâìÂºÄÂè¶Â§ñ‰∏Ä‰∏™ÁªàÁ´ØÔºåËøõÂÖ•kafka1ÂêØÂä®‰∏™Ê∂àË¥πËÄÖÔºåÊ≠§Â§ÑË¶ÅÊ≥®ÊÑèÊ∂àË¥πËÄÖË∑ØÂæÑ‰πüË¶ÅÊåáÂÆökafka1[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginning 1[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-console-producer.sh --broker-list kafka0:9092 --topic topic3 ËæìÂÖ•hello world Âè¶Â§ñ‰∏Ä‰∏™ÁªàÁ´ØÊòæÁ§∫12[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginninghello world Â¶ÇÊûúËÆæÁΩÆtopicÁöÑÊó∂ÂÄôËÆæÁΩÆlog.cleanup.policy‰∏∫compactÔºåÂàôÊ∂àË¥πÁöÑÊ∂àÊÅØ‰ºöË¢´ÂéãÁº©]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Áî®git‰∏ä‰º†‰ª£Á†ÅÂà∞Á†Å‰∫ë]]></title>
    <url>%2F2016%2F12%2F13%2F%E7%A0%81%E4%BA%91%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Áî®git‰∏ä‰º†‰ª£Á†ÅÂà∞Á†Å‰∫ë ‰∏ãËΩΩgitÂÆ¢Êà∑Á´ØWindow ‰∏ãÁöÑÂÆâË£Ö‰ªé http://git-scm.com/download ‰∏ä‰∏ãËΩΩwindowÁâàÁöÑÂÆ¢Êà∑Á´ØÔºåÁÑ∂Âêé‰∏ÄÁõ¥‰∏ã‰∏ÄÊ≠•‰∏ã‰∏ÄÊ≠•ÂÆâË£ÖgitÂç≥ÂèØÔºåËØ∑Ê≥®ÊÑèÔºåÂ¶ÇÊûú‰Ω†‰∏çÁÜüÊÇâÊØè‰∏™ÈÄâÈ°πÁöÑÊÑèÊÄùÔºåËØ∑‰øùÊåÅÈªòËÆ§ÁöÑÈÄâÈ°π git config ‚Äìglobal user.name Jenickgit config ‚Äìglobal user.email 258409707@qq.com ‰ª•‰∏ãÂÜÖÂÆπWieËΩ¨ËΩΩÔºö1.ÂÖàÂú®ËøúÁ®ãÂ∫ìÂàõÂª∫È°πÁõÆÔºåÁÑ∂ÂêéÂ§çÂà∂ËøúÁ®ãÂ∫ìÈáåÈù¢ÁöÑÈ°πÁõÆÁöÑÂú∞ÂùÄÔºõ 2.ÊâìÂºÄÂëΩ‰ª§Ë°åÔºàÁîµËÑë‰∏äÁî®windows+RÈîÆÔºâÔºåÂàõÂª∫‰∏Ä‰∏™Êñá‰ª∂Â§πÁî®‰∫é‰ªéËøúÁ®ãÂ∫ìÈáåÂÖã ÈöÜÊâÄË¶ÅÊõ¥ÊîπÁöÑÊñá‰ª∂ÔºöÂëΩ‰ª§‰∏∫Ôºömkdir + ‚ÄúÁõÆÂΩïÂêç‚ÄùÔºõ3.Áî®cd + ‚ÄúÁõÆÂΩïÂêç‚ÄùË∑≥ËΩ¨Âà∞ÊâÄÂàõÂª∫ÁöÑÁõÆÂΩïÂêç‰∏ãÔºåÁî®ÂëΩ‰ª§git clone +ËøúÁ®ãÂ∫ì Êñá‰ª∂Âú∞ÂùÄÔºõÊ≠§Êó∂Âà∑Êñ∞ÊâÄÂàõÂª∫ÁöÑÁõÆÂΩïÔºå‰ºöÂèëÁé∞Êñ∞Âá∫Áé∞‰∫Ü‰∏Ä‰∏™Êñá‰ª∂Â§πÔºåÈÇ£Â∞±ÊòØ ‰ªéËøúÁ®ãÂ∫ìÈáåÈù¢Â§çÂà∂ÁöÑÊñá‰ª∂ÁõÆÂΩïÔºàÈáåÈù¢ÂåÖÂê´ÊúâÂêéÁºÄ‰∏∫.gitÁöÑÊñá‰ª∂Ôºâ 4.Áî®cd ÂëΩ‰ª§Ë∑≥ËΩ¨Âà∞Êñ∞ÁöÑÊñá‰ª∂Â§πÈáåÔºåÊääÊâÄÈúÄË¶ÅÊèê‰∫§ÁöÑÊñá‰ª∂Â§çÂà∂Âà∞Ëøô‰∏™Êñá‰ª∂Â§π ÈáåÔºõ 5.Áî®git statusÂëΩ‰ª§ËÉΩÁúãÂà∞ÊâÄÊîπÂèò‰∫ÜÁöÑÊñá‰ª∂ÁõÆÂΩïÂàóË°®ÔºåÁî®git diffÂëΩ‰ª§ËÉΩÁúã Âà∞ÂÖ∑‰ΩìÊîπÂèò‰∫ÜÁöÑÂì™‰∏ÄË°å‰ª£Á†ÅÔºõ 6.Áî®ÂëΩ‰ª§git branch + ‚ÄúÂàÜÊîØÂêç‚ÄùÂàõÂª∫Êú¨Âú∞ÂàÜÊîØÔºåÁî®ÂëΩ‰ª§git branchÂèØÊü•Áúã ÁõÆÂâçÊâÄÂú®ÂàÜÊîØÔºåÁî® git checkout ‚Äú‰Ω†ÊâÄÂàõÂª∫ÁöÑÂàÜÊîØÂêç‚ÄùÔºõË∑≥ËΩ¨Âà∞‰Ω†ÊâÄÂàõÂª∫ÁöÑÂàÜ ÊîØÔºõ 7.Áî®git add+(Á©∫Ê†º)+ ‚Äú.‚ÄùÂ∞ÜÊâÄÊúâÁöÑ‰øÆÊîπËøΩÂä†Âà∞Êñá‰ª∂‰∏≠ÂéªÔºõ 8.Áî®git commit -m‚ÄúÊõ¥ÊîπÁöÑÊñá‰ª∂Â§áÊ≥®‚ÄùÂëΩ‰ª§Â∞Ü‰Ω†ÁöÑÊñá‰ª∂Êèê‰∫§Âà∞Êú¨Âú∞Â∫ìÔºõ 9.Áî®git checkout master ÂàáÊç¢ÂõûÂà∞‰∏ªÂàÜÊîØÔºõ 10.Áî®git pull origin master ÂëΩ‰ª§Â∞Ü‰∏ªÂàÜÊîØ‰ªéËøúÁ®ã‰ªìÂ∫ìÈáåÈù¢ÊãâËøáÊù•Ôºõ 11.Áî®git merge+‚Äù‰Ω†ÊâÄÂàõÂª∫ÁöÑÂàÜÊîØÂêç‚ÄùÂêàÂπ∂ÂàÜÊîØÔºõ 12.Áî®git push origin master ÂëΩ‰ª§Â∞ÜÂêàÂπ∂ÂàÜÊîØÊèê‰∫§Âà∞ËøúÁ®ãÂ∫ìÔºõ 13.Âà∑Êñ∞‰∏Ä‰∏ãËøúÁ®ãÂ∫ìÔºåÂ∞±ËÉΩÂèëÁé∞ÊâÄË¶ÅÊèê‰∫§ÁöÑÊñá‰ª∂‰∫Ü ÊñáÔºèÁéÑËñõÁÉ®ÔºàÁÆÄ‰π¶‰ΩúËÄÖÔºâÂéüÊñáÈìæÊé•Ôºöhttp://www.jianshu.com/p/edf037f921c7Ëëó‰ΩúÊùÉÂΩí‰ΩúËÄÖÊâÄÊúâÔºåËΩ¨ËΩΩËØ∑ËÅîÁ≥ª‰ΩúËÄÖËé∑ÂæóÊéàÊùÉÔºåÂπ∂Ê†áÊ≥®‚ÄúÁÆÄ‰π¶‰ΩúËÄÖ‚Äù„ÄÇ]]></content>
      <categories>
        <category>ÂÖ∂‰ªñ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka stream ÂÆûÊàò3]]></title>
    <url>%2F2016%2F12%2F07%2Fkafka%20stream%20%E5%AE%9E%E6%88%983%2F</url>
    <content type="text"><![CDATA[Âú∫ÊôØÔºö ÊØè5ÁßíËæìÂá∫ËøáÂéª1Â∞èÊó∂18Â≤ÅÂà∞35Â≤ÅÁî®Êà∑ÊâÄË¥≠‰π∞ÁöÑÂïÜÂìÅ‰∏≠ÔºåÊØèÁßçÂìÅÁ±ªÈîÄÂîÆÈ¢ùÊéíÂêçÂâçÂçÅÁöÑËÆ¢ÂçïÊ±áÊÄª‰ø°ÊÅØ„ÄÇ ‰ΩøÁî®Êï∞ÊçÆÂÜÖÁöÑÊó∂Èó¥(Event Time)‰Ωú‰∏∫timestamp ÊØè5ÁßíËæìÂá∫‰∏ÄÊ¨° ÊØèÊ¨°ËÆ°ÁÆóÂà∞ËæìÂá∫‰∏∫Ê≠¢ËøáÂéª1Â∞èÊó∂ÁöÑÊï∞ÊçÆ ÊîØÊåÅËÆ¢ÂçïËØ¶ÊÉÖÂíåÁî®Êà∑ËØ¶ÊÉÖÁöÑÊõ¥Êñ∞ÂíåÂ¢ûÂä† ËæìÂá∫Â≠óÊÆµÂåÖÂê´Êó∂Èó¥Á™óÂè£ÔºàËµ∑ÂßãÊó∂Èó¥ÔºåÁªìÊùüÊó∂Èó¥ÔºâÔºåÂìÅÁ±ªÔºàcategoryÔºâÔºåÂïÜÂìÅÂêçÔºàitem_nameÔºâÔºåÈîÄÈáèÔºàquantityÔºâÔºåÂçï‰ª∑ÔºàpriceÔºâÔºåÊÄªÈîÄÂîÆÈ¢ùÔºåËØ•ÂïÜÂìÅÂú®ËØ•ÂìÅÁ±ªÂÜÖÁöÑÈîÄÂîÆÈ¢ùÊéíÂêç ÈöèÊú∫ÂàõÂª∫Êï∞ÊçÆÁöÑÁ±ª1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.io.*;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;import java.util.Random;/** * Created by Administrator on 2016/12/30. */public class createFile &#123; //cÂàõÂª∫ËÆ¢ÂçïÊñá‰ª∂ public static void main(String[] args) throws IOException, ParseException &#123; FileOutputStream outSTr = null; FileOutputStream out = null; BufferedOutputStream Buff=null; outSTr = new FileOutputStream(new File("C:/add2.csv")); Buff=new BufferedOutputStream(outSTr); String[] a1 = &#123;"Jack", "Lily", "Mike", "Lucy", "LiLei", "HanMeimei"&#125;; Random rand = new Random(); String[] a2 = &#123;"iphone", "ipad", "iwatch", "ipod"&#125;; Calendar calendar = Calendar.getInstance(); String str="2016-11-11 00:00:00"; SimpleDateFormat sdf= new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); Date date =sdf.parse(str); calendar.setTime(date); System.out.println (calendar.getTime ()); SimpleDateFormat sdf2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); String dateStr = ""; int count = 0; while(! (count == 60*60*24))&#123;//86400 System.out.println(count); count++; System.out.println(dateStr); int num = rand.nextInt(a1.length); // System.out.println(a1[num]); int num2 = rand.nextInt(a2.length); // System.out.println(a2[num2]); calendar.add (Calendar.SECOND, 1); sdf2.format(calendar.getTime()); dateStr = sdf2.format(calendar.getTime()); // System.out.println (dateStr); Buff.write((a1[num]+","+a2[num2]+", "+dateStr+","+rand.nextInt(5)+"\r\n").getBytes()); Buff.flush(); &#125; &#125;&#125; ÂÖ∂‰ªñÊñá‰ª∂Âú®ÂÆûË∑µ1ÈáåÊúâ ÁºñÂÜôDSLÊñá‰ª∂ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426import java.io.IOException;import java.util.ArrayList;import java.util.Collection;import java.util.Collections;import java.util.Comparator;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import org.apache.kafka.streams.kstream.TimeWindows;import org.apache.kafka.streams.kstream.Windowed;import com.jasongj.kafka.stream.model.Item;import com.jasongj.kafka.stream.model.Order;import com.jasongj.kafka.stream.model.User;import com.jasongj.kafka.stream.serdes.SerdesFactory;import com.jasongj.kafka.stream.timeextractor.OrderTimestampExtractor;/** * Created by root on 17-1-5. */public class topN &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items", "items-state-store"); KStream&lt;Windowed&lt;String&gt;, GroupInfo&gt; kStream = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; (orderUser.userAddress != null &amp;&amp; (orderUser.getAge() &gt;= 18 &amp;&amp; orderUser.getAge() &lt;= 35 ))) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;String, GroupInfo&gt;pair((orderUserItem.itemType), new GroupInfo(orderUserItem.itemType, new ArrayList&lt;GroupItemInfo&gt;()&#123; private static final long serialVersionUID = 1L; &#123; add(new GroupItemInfo(orderUserItem.transactionDate, orderUserItem.itemName, orderUserItem.quantity, orderUserItem.itemPrice, (orderUserItem.quantity * orderUserItem.itemPrice))); &#125;&#125;) )) .groupByKey(Serdes.String(), SerdesFactory.serdFrom(GroupInfo.class)) .reduce((GroupInfo v1, GroupInfo v2) -&gt; &#123; GroupInfo v3 = new GroupInfo(v1.getItemType()); List&lt;GroupItemInfo&gt; newItemlist = new ArrayList&lt;GroupItemInfo&gt;(); newItemlist.addAll(v1.getItemList()); newItemlist.addAll(v2.getItemList()); v3.setItemList(newItemlist); return v3; &#125; , TimeWindows.of(1000 * 60 * 60).advanceBy(1000 * 5) , "gender-amount-state-store").toStream(); kStream.map((Windowed&lt;String&gt; window, GroupInfo groupInfo) -&gt; &#123; return new KeyValue&lt;String, String&gt;(window.key(), groupInfo.printTop10(window.window().start(), window.window().end())); &#125;).to(Serdes.String(), Serdes.String(), "gender-amount"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class GroupInfo&#123; private String itemType; private List&lt;GroupItemInfo&gt; itemList; public GroupInfo()&#123; &#125; public GroupInfo(String itemType)&#123; this.itemType = itemType; &#125; public GroupInfo(String itemType, List&lt;GroupItemInfo&gt; itemList)&#123; this.itemType = itemType; this.itemList = itemList; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public List&lt;GroupItemInfo&gt; getItemList() &#123; return itemList; &#125; public void setItemList(List&lt;GroupItemInfo&gt; itemList) &#123; this.itemList = itemList; &#125; /** * Ê†πÊçÆÈáëÈ¢ùÊ±áÊÄªÂÄíÂ∫è * @param allItems * @return */ private List&lt;GroupItemInfo&gt; sortBySumDesc(Collection&lt;GroupItemInfo&gt; allItems)&#123; List&lt;GroupItemInfo&gt; result = new ArrayList&lt;GroupItemInfo&gt;(); result.addAll(allItems); Collections.sort(result, new Comparator&lt;GroupItemInfo&gt;()&#123; @Override public int compare(GroupItemInfo o1, GroupItemInfo o2) &#123; if(o1.getSum() == o2.getSum())&#123; return 0; &#125;else if(o1.getSum() &gt; o2.getSum())&#123; return -1; &#125;else&#123; return 1; &#125; &#125; &#125;); return result; &#125; /** * ÊâæÂõûÂâç10Âêç * @param startDate * @param endDate * @return */ public String printTop10(long startDate, long endDate)&#123; double allAmount = 0.0; Map&lt;String, GroupItemInfo&gt; groupMap = new HashMap&lt;String, GroupItemInfo&gt;(); for(GroupItemInfo item : itemList)&#123; String key = item.getItemName(); allAmount += item.getSum(); if(groupMap.containsKey(key))&#123; GroupItemInfo oldItem = groupMap.get(key); oldItem.setCount(oldItem.getCount() + item.getCount()); oldItem.setSum(oldItem.getSum() + item.getSum()); &#125;else&#123; groupMap.put(key, item); &#125; &#125; List&lt;GroupItemInfo&gt; sortedResult = sortBySumDesc(groupMap.values()); StringBuffer sb = new StringBuffer(); for(int i = 1; i &lt;= 10 ; i++)&#123; if(sortedResult.size() &gt;= i &amp;&amp; sortedResult.get(i-1) != null)&#123; GroupItemInfo oneItem = sortedResult.get(i-1); sb.append(startDate).append(",").append(endDate).append(",").append(itemType).append(",").append(oneItem.getItemName()).append(",") .append(oneItem.getCount()).append(",").append(oneItem.getPrice()).append(",").append(oneItem.getSum()).append(",").append(allAmount) .append(",").append(i).append("\n"); &#125;else&#123; break; &#125; &#125; return sb.toString(); &#125; &#125; private static class GroupItemInfo&#123; private long transactionDate; private String itemName; private int count; private double price; private double sum; public GroupItemInfo()&#123; &#125; public GroupItemInfo(long transactionDate, String itemName, int count, double price, double sum) &#123; this.transactionDate = transactionDate; this.itemName = itemName; this.count = count; this.price = price; this.sum = sum; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; public double getSum() &#123; return sum; &#125; public void setSum(double sum) &#123; this.sum = sum; &#125; &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if(order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if(user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if(orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if(item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka stream ÂÆûÊàò2]]></title>
    <url>%2F2016%2F12%2F06%2Fkafka%20stream%20%E5%AE%9E%E6%88%982%2F</url>
    <content type="text"><![CDATA[Âú∫ÊôØÔºö Âú®‰∏ä‰∏Ä‰∏™Á§∫‰æãÔºàÁÆóÂá∫Áî®Êà∑‰∏éÂïÜÂìÅÂêåÂú∞ÂùÄÁöÑËÆ¢Âçï‰∏≠ÔºåÁî∑Â•≥ÂàÜÂà´ÊÄªÂÖ±Ëä±‰∫ÜÂ§öÂ∞ëÈí±ÔºâÁöÑÂü∫Á°Ä‰∏äÔºåÁÆóÂá∫‰∏çÂêåÂú∞Âå∫ÔºàÁî®Êà∑Âú∞ÂùÄÔºâÔºå‰∏çÂêåÊÄßÂà´ÁöÑËÆ¢ÂçïÊï∞ÂèäÂïÜÂìÅÊÄªÊï∞ÂíåÊÄªÈáëÈ¢ù„ÄÇËæìÂá∫ÁªìÊûúschemaÂ¶Ç‰∏ãÂú∞Âå∫ÔºàÁî®Êà∑Âú∞Âå∫ÔºåÂ¶ÇSHÔºâÔºåÊÄßÂà´ÔºåËÆ¢ÂçïÊÄªÊï∞ÔºåÂïÜÂìÅÊÄªÊï∞ÔºåÊÄªÈáëÈ¢ù Á§∫‰æãËæìÂá∫ SH, male, 3, 4, 188888.88 BJ, femail, 5, 8, 288888.88 Êé®ÊºîËøáÁ®ãÔºö ÂÆûÁé∞‰ª£Á†Å123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741package com.jasongj.kafka.stream;import java.io.IOException;import java.util.Properties;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import com.jasongj.kafka.stream.model.Item;import com.jasongj.kafka.stream.model.Order;import com.jasongj.kafka.stream.model.User;import com.jasongj.kafka.stream.serdes.SerdesFactory;import com.jasongj.kafka.stream.timeextractor.OrderTimestampExtractor;/** * Created by root on 17-1-5. */public class Di9KeZuoYe &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items", "items-state-store"); KTable&lt;AddrSex, OrderMoney&gt; kTable = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != null) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .filter((String item, OrderUserItem orderUserItem) -&gt; StringUtils.compare(orderUserItem.userAddress, orderUserItem.itemAddress) == 0) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;AddrSex, OrderMoney&gt;pair(new AddrSex(orderUserItem.userAddress, orderUserItem.gender), OrderMoney.fromItem(orderUserItem.quantity, orderUserItem.itemPrice))) .groupByKey(SerdesFactory.serdFrom(AddrSex.class), SerdesFactory.serdFrom(OrderMoney.class)) .reduce((OrderMoney v1, OrderMoney v2) -&gt; new OrderMoney(v1.orderNum + v2.orderNum, v1.itemNum + v2.itemNum, v1.TotalMoney + v2.TotalMoney), "gender-amount-state-store"); kTable .toStream() .map((AddrSex addrSex, OrderMoney orderMoney) -&gt; new KeyValue&lt;String, String&gt;(addrSex.toString(), orderMoney.toString())) .to("gender-amount"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if (order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if (user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if (orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if (item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125; public static class AddrSex &#123; private String addr; private String gender; public AddrSex() &#123; &#125; public AddrSex(String addr, String gender) &#123; this.addr = addr; this.gender = gender; &#125; public String getAddr() &#123; return addr; &#125; public void setAddr(String addr) &#123; this.addr = addr; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; AddrSex addrSex = (AddrSex) o; if (!addr.equals(addrSex.addr)) return false; return gender.equals(addrSex.gender); &#125; @Override public int hashCode() &#123; int result = addr.hashCode(); result = 31 * result + gender.hashCode(); return result; &#125; @Override public String toString() &#123; return addr + " " + gender; &#125; &#125; public static class OrderMoney &#123; private int orderNum; private int itemNum; private Double TotalMoney; public OrderMoney() &#123; &#125; public OrderMoney(int orderNum, int itemNum, Double totalMoney) &#123; this.orderNum = orderNum; this.itemNum = itemNum; TotalMoney = totalMoney; &#125; public int getOrderNum() &#123; return orderNum; &#125; public void setOrderNum(int orderNum) &#123; this.orderNum = orderNum; &#125; public int getItemNum() &#123; return itemNum; &#125; public void setItemNum(int itemNum) &#123; this.itemNum = itemNum; &#125; public Double getTotalMoney() &#123; return TotalMoney; &#125; public void setTotalMoney(Double totalMoney) &#123; TotalMoney = totalMoney; &#125; public static OrderMoney fromItem(int quantity, Double itemPrice) &#123; OrderMoney orderMoney = new OrderMoney(); orderMoney.setOrderNum(1); orderMoney.setItemNum(quantity); orderMoney.setTotalMoney((double) quantity * itemPrice); return orderMoney; &#125; @Override public String toString() &#123; return orderNum + " " + itemNum + " " + TotalMoney; &#125; &#125;&#125; ËæìÂá∫ÁªìÊûú]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka stream ÂÆûÊàò]]></title>
    <url>%2F2016%2F12%2F05%2Fkafka%20stream%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Âú∫ÊôØÔºöÁõÆÂâçÊúâ‰∏â‰∏™Ë°®Ôºö‰∫ßÂìÅË°®ÔºåÁî®Êà∑Ë°®ÔºåËÆ¢ÂçïË°®„ÄÇÊ±ÇÊÄßÂà´Âπ≥ÂùáÊ∂àË¥πÈáëÈ¢ù Êï∞ÊçÆÔºö‰∫ßÂìÅË°®1234iphone, BJ, phone, 5388.88ipad, SH, pad, 4888.88iwatch, SZ, watch, 2668.88ipod, GZ, pod, 1888.88 Áî®Êà∑Ë°®1234Jack, BJ, male, 23Lily, SH, female, 21Mike, SZ, male, 22Lucy, GZ, female, 20 ËÆ¢ÂçïË°®12345678910111213141516171819Jack, iphone, 2016-11-11 00:00:01, 3Jack, ipad, 2016-11-11 00:00:02, 4Jack, iwatch, 2016-11-11 00:00:03, 5Jack, ipod, 2016-11-11 00:00:04, 4Lily, ipad, 2016-11-11 00:00:06, 3Lily, iwatch, 2016-11-11 00:00:07, 4Lily, iphone, 2016-11-11 00:00:08, 2Lily, ipod, 2016-11-11 00:00:09, 3Mike, ipad, 2016-11-11 00:00:11, 2Mike, iwatch, 2016-11-11 00:00:12, 3Mike, iphone, 2016-11-11 00:00:13, 4Mike, ipod, 2016-11-11 00:00:14, 3Lucy, ipod, 2016-11-11 00:00:16, 3Lucy, ipad, 2016-11-11 00:00:17, 4Lucy, iwatch, 2016-11-11 00:00:18, 3Lucy, iphone, 2016-11-11 00:00:19, 5 ÂàõÂª∫ÊâìÂç∞Á±ª123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.Arrays;import java.util.Properties;import java.util.concurrent.atomic.AtomicLong;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.DoubleDeserializer;import org.apache.kafka.common.serialization.StringDeserializer;public class DemoConsumerManualCommit &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; "jenick.com:9092", "gender-amount2", "group4", "consumer2" &#125;; if (args == null || args.length != 4) &#123; System.err.println( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"); System.exit(1); &#125; String bootstrap = args[0]; String topic = args[1]; String groupid = args[2]; String clientid = args[3]; Properties props = new Properties(); props.put("bootstrap.servers", bootstrap); props.put("group.id", groupid); props.put("enable.auto.commit", "false"); props.put("key.deserializer", StringDeserializer.class.getName()); props.put("value.deserializer", DoubleDeserializer.class.getName()); props.put("max.poll.interval.ms", "300000"); props.put("max.poll.records", "500"); props.put("auto.offset.reset", "earliest"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(topic)); AtomicLong atomicLong = new AtomicLong(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); records.forEach(record -&gt; &#123; System.out.printf("client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); if (atomicLong.get() % 10 == 0) &#123;// consumer.commitSync(); &#125; &#125;); &#125; &#125;&#125; TimestampExtractor12345678910111213141516171819202122232425262728293031323334import java.time.LocalDateTime;import java.time.ZoneOffset;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.streams.processor.TimestampExtractor;import com.fasterxml.jackson.databind.JsonNode;import com.kafka.stream.model.Item;import com.kafka.stream.model.Order;import com.kafka.stream.model.User;public class OrderTimestampExtractor implements TimestampExtractor &#123; @Override public long extract(ConsumerRecord&lt;Object, Object&gt; record) &#123; Object value = record.value(); if (record.value() instanceof Order) &#123; Order order = (Order) value; return order.getTransactionDate(); &#125; if (value instanceof JsonNode) &#123; return ((JsonNode) record.value()).get("transactionDate").longValue(); &#125; if (value instanceof Item) &#123; return LocalDateTime.of(2015, 12,11,1,0,10).toEpochSecond(ZoneOffset.UTC) * 1000; &#125; if (value instanceof User) &#123; return LocalDateTime.of(2015, 12,11,0,0,10).toEpochSecond(ZoneOffset.UTC) * 1000; &#125; return LocalDateTime.of(2015, 11,10,0,0,10).toEpochSecond(ZoneOffset.UTC) * 1000;// throw new IllegalArgumentException("OrderTimestampExtractor cannot recognize the record value " + record.value()); &#125;&#125; ÂàõÂª∫streamÂ§ÑÁêÜÁ±ª123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271import java.io.IOException;import java.util.Properties;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import com.kafka.stream.model.Item;import com.kafka.stream.model.Order;import com.kafka.stream.model.User;import com.kafka.stream.serdes.SerdesFactory;import com.kafka.stream.timeextractor.OrderTimestampExtractor;public class PurchaseAnalysis &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items2", "items-state-store");// itemTable.toStream().foreach((String itemName, Item item) -&gt; System.out.printf("Item info %s-%s-%s-%s\n", item.getItemName(), item.getAddress(), item.getType(), item.getPrice())); KTable&lt;String, Double&gt; kTable = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != null) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item2") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .filter((String item, OrderUserItem orderUserItem) -&gt; StringUtils.compare(orderUserItem.userAddress, orderUserItem.itemAddress) == 0)// .foreach((String itemName, OrderUserItem orderUserItem) -&gt; System.out.printf("%s-%s-%s-%s\n", itemName, orderUserItem.itemAddress, orderUserItem.userName, orderUserItem.userAddress)) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;String, Double&gt;pair(orderUserItem.gender, (Double)(orderUserItem.quantity * orderUserItem.itemPrice))) .groupByKey(Serdes.String(), Serdes.Double()) .reduce((Double v1, Double v2) -&gt; v1 + v2, "gender-amount-state-store");// kTable.foreach((str, dou) -&gt; System.out.printf("%s-%s\n", str, dou)); kTable .toStream() .map((String gender, Double total) -&gt; new KeyValue&lt;String, String&gt;(gender, String.valueOf(total))) .to("gender-amount2"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if(order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if(user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if(orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if(item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125;&#125; HashÂàÜÂå∫123456789101112131415161718192021222324252627282930313233343536import java.util.List;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import org.apache.kafka.common.PartitionInfo;public class HashPartitioner implements Partitioner &#123; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes != null) &#123; int hashCode = 0; if (key instanceof Integer || key instanceof Long) &#123; hashCode = (int) key; &#125; else &#123; hashCode = key.hashCode(); &#125; hashCode = hashCode &amp; 0x7fffffff; return hashCode % numPartitions; &#125; else &#123; return 0; &#125; &#125; @Override public void close() &#123; &#125;&#125; Â∫èÂàóÂåñ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.io.IOException;import java.util.Map;import org.apache.kafka.common.errors.SerializationException;import org.apache.kafka.common.serialization.Deserializer;import com.fasterxml.jackson.databind.ObjectMapper;public class GenericDeserializer&lt;T&gt; implements Deserializer&lt;T&gt; &#123; private Class&lt;T&gt; type; private ObjectMapper objectMapper = new ObjectMapper(); public GenericDeserializer() &#123;&#125; public GenericDeserializer(Class&lt;T&gt; type) &#123; this.type = type; &#125; @SuppressWarnings("unchecked") @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; if(type != null) &#123; return; &#125; String typeProp = isKey ? "key.deserializer.type" : "value.deserializer.type"; String typeName = (String)configs.get(typeProp); try &#123; type = (Class&lt;T&gt;)Class.forName(typeName); &#125; catch (Exception ex) &#123; throw new SerializationException("Failed to initialize GenericDeserializer for " + typeName, ex); &#125; &#125; @Override public T deserialize(String topic, byte[] data) &#123; if (data == null) &#123; return null; &#125; try &#123; return this.objectMapper.readValue(data, type); &#125; catch (IOException ex) &#123; throw new SerializationException(ex); &#125; &#125; @Override public void close() &#123; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.io.IOException;import java.util.Map;import org.apache.kafka.common.errors.SerializationException;import org.apache.kafka.common.serialization.Serde;import org.apache.kafka.common.serialization.Serializer;import com.fasterxml.jackson.databind.ObjectMapper;import com.kafka.stream.model.User;public class GenericSerializer&lt;T&gt; implements Serializer&lt;T&gt; &#123; private Class&lt;T&gt; type; private ObjectMapper objectMapper = new ObjectMapper(); public GenericSerializer() &#123;&#125; public GenericSerializer(Class&lt;T&gt; type) &#123; this.type = type; &#125; @SuppressWarnings("unchecked") @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; if(type != null) &#123; return; &#125; String typeProp = isKey ? "key.serializer.type" : "value.serializer.type"; String typeName = (String)configs.get(typeProp); try &#123; type = (Class&lt;T&gt;)Class.forName(typeName); &#125; catch (Exception ex) &#123; throw new SerializationException("Failed to initialize GenericSerializer for " + typeName, ex); &#125; &#125; @Override public byte[] serialize(String topic, T object) &#123; if (object == null) &#123; return null; &#125; try &#123; return this.objectMapper.writerFor(type).writeValueAsBytes(object); &#125; catch (IOException ex) &#123; throw new SerializationException(ex); &#125; &#125; @Override public void close() &#123; &#125;&#125; 1234567891011121314151617import org.apache.kafka.common.serialization.Serde;import org.apache.kafka.common.serialization.Serdes;public class SerdesFactory &#123; /** * @param &lt;T&gt; The class should have a constructor without any * arguments and have setter and getter for every member variable * @param pojoClass POJO class. * @return Instance of &#123;@link Serde&#125; */ public static &lt;T&gt; Serde&lt;T&gt; serdFrom(Class&lt;T&gt; pojoClass) &#123; return Serdes.serdeFrom(new GenericSerializer&lt;T&gt;(pojoClass), new GenericDeserializer&lt;T&gt;(pojoClass)); &#125;&#125; beanÂØπË±° 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Item &#123; private String itemName; private String address; private String type; private double price; public Item() &#123;&#125; public Item(String itemName, String address, String type, double price) &#123; this.itemName = itemName; this.address = address; this.type = type; this.price = price; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getType() &#123; return type; &#125; public void setType(String type) &#123; this.type = type; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Order &#123; private String userName; private String itemName; private long transactionDate; private int quantity; public Order() &#123;&#125; public Order(String userName, String itemName, long transactionDate, int quantity) &#123; this.userName = userName; this.itemName = itemName; this.transactionDate = transactionDate; this.quantity = quantity; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class User &#123; private String name; private String address; private String gender; private int age; public User() &#123;&#125; public User(String name, String address, String gender, int age) &#123; this.name = name; this.address = address; this.gender = gender; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; Áî®Êà∑Áîü‰∫ßËÄÖ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.IOException;import java.nio.charset.Charset;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.User;import com.kafka.stream.serdes.GenericSerializer;public class UserProducer &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", User.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, User&gt; producer = new KafkaProducer&lt;String, User&gt;(props); List&lt;User&gt; users = readUser(); users.forEach((User user) -&gt; producer.send(new ProducerRecord&lt;String, User&gt;("users", user.getName(), user))); producer.close(); &#125; public static List&lt;User&gt; readUser() throws IOException &#123; List&lt;String&gt; lines = IOUtils.readLines(OrderProducer.class.getResourceAsStream("/users.csv"), Charset.forName("UTF-8")); List&lt;User&gt; users = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new User(values[0], values[1], values[2], Integer.parseInt(values[3]))) .collect(Collectors.toList()); return users; &#125;&#125; ‰∫ßÂìÅÁîü‰∫ßËÄÖ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.IOException;import java.nio.charset.Charset;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.Item;import com.kafka.stream.serdes.GenericSerializer;public class ItemProducer &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", Item.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, Item&gt; producer = new KafkaProducer&lt;String, Item&gt;(props); List&lt;Item&gt; items = readItem(); items.forEach((Item item) -&gt; producer.send(new ProducerRecord&lt;String, Item&gt;("items2", item.getItemName(), item))); producer.close(); &#125; public static List&lt;Item&gt; readItem() throws IOException &#123; List&lt;String&gt; lines = IOUtils.readLines(OrderProducer.class.getResourceAsStream("/items.csv"), Charset.forName("UTF-8")); List&lt;Item&gt; items = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new Item(values[0], values[1], values[2], Double.parseDouble(values[3]))) .collect(Collectors.toList()); return items; &#125;&#125; ËÆ¢ÂçïÁîü‰∫ßËÄÖ123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.io.IOException;import java.io.InputStream;import java.nio.charset.Charset;import java.time.LocalDateTime;import java.time.ZoneOffset;import java.time.format.DateTimeFormatter;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.Order;import com.kafka.stream.serdes.GenericSerializer;public class OrderProducer &#123; private static DateTimeFormatter dataTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", Order.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, Order&gt; producer = new KafkaProducer&lt;String, Order&gt;(props); List&lt;Order&gt; orders = readOrder(); orders.forEach((Order order) -&gt; producer.send(new ProducerRecord&lt;String, Order&gt;("orders", order.getUserName(), order))); producer.close(); &#125; public static List&lt;Order&gt; readOrder() throws IOException &#123; InputStream inputStream = OrderProducer.class.getResourceAsStream("/orders.csv"); List&lt;String&gt; lines = IOUtils.readLines(inputStream, Charset.forName("UTF-8")); List&lt;Order&gt; orders = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new Order(values[0], values[1], LocalDateTime.parse(values[2], dataTimeFormatter).toEpochSecond(ZoneOffset.UTC) * 1000, Integer.parseInt(values[3]))) .collect(Collectors.toList()); return orders; &#125;&#125; ÈúÄË¶ÅÂàõÂª∫topicÂ¶Ç‰∏ãÔºö12345/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic orders/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic items/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic users/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic gender-amount/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic orderuser-repartition-by-item È¶ñÂÖàÂêØÂä®DemoConsumerManualCommitÔºåÁÑ∂ÂêéÂêØÂä®PurchaseAnalysisÂÖàÊäätableÁöÑÊï∞ÊçÆÂàõÂª∫ËøõÂéªUserProducer„ÄÅItemProducerÊúÄÂêéÂêØÂä®streamÁöÑOrderProducer ËøêË°åÁªìÊûúÔºö 12client : consumer2 , topic: gender-amount2 , partition: 0 , offset = 0, key = male, value = 7.489721245848924E-67client : consumer2 , topic: gender-amount2 , partition: 2 , offset = 0, key = female, value = 6.008913963681483E-67]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[codis ha]]></title>
    <url>%2F2016%2F12%2F03%2Fcodis%20ha%2F</url>
    <content type="text"><![CDATA[ÂêØÂä®Â§ö‰∏™ÂÆû‰æãÔºöcd /usr/local/codis/src/github.com/CodisLabs/codis[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf ‚Äìprotected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf ‚Äìprotected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6381.conf ‚Äìprotected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6382.conf ‚Äìprotected-mode no ÂêØÂä®zookeeper[root@zhm1 codis]# zkServer.sh start ÂêØÂä®dashboard[root@zhm1 codis]# zkServer.sh startnohup bin/codis-dashboard ‚Äìncpu=2 ‚Äìconfig=dashboard.conf ‚Äìlog=dashboard.log ‚Äìlog-level=WARN &amp; ÂêØÂä®codis-proxy[root@zhm1 codis]# nohup bin/codis-proxy ‚Äìncpu=2 ‚Äìconfig=proxy.conf ‚Äìlog =proxy.log ‚Äìlog-level=WARN &amp; ÂêØÂä®codis-fe[root@zhm1 codis]# ./bin/codis-fe ‚Äìncpu=2 ‚Äìlog=fe.log ‚Äìlog-level=WARN ‚Äìdashboard-list=conf/codis.json ‚Äìlisten=192.168.110.129:18090 &amp; Â∞ÜÂÆû‰æãÊ∑ªÂä†Âà∞group ÂàÜÈÖçslot ÂêØÂä®codis-hanohup ./bin/codis-ha ‚Äìlog=ha.log ‚Äìlog-level=WARN ‚Äìdashboard=127.0.0.1:18080 &amp; [root@zhm1 codis]# ps -ef | grep codisroot 3096 2884 0 09:51 pts/0 00:00:32 bin/codis-dashboard ‚Äìncpu=2 ‚Äìconfig=dashboard.conf ‚Äìlog=dashboard.log ‚Äìlog-level=WARNroot 3114 2884 0 09:52 pts/0 00:00:04 bin/codis-proxy ‚Äìncpu=2 ‚Äìconfig=proxy.conf ‚Äìlog =proxy.log ‚Äìlog-level=WARNroot 3149 2884 0 09:53 pts/0 00:00:02 ./bin/codis-fe ‚Äìncpu=2 ‚Äìlog=fe.log ‚Äìlog-level=WARN ‚Äìdashboard-list=conf/codis.json ‚Äìlisten=192.168.110.129:18090root 3175 1 0 09:54 ? 00:00:08 ./bin/codis-server :6379root 3179 1 0 09:54 ? 00:00:08 ./bin/codis-server :6380root 3888 1 0 10:46 ? 00:00:02 ./bin/codis-server :6382root 3892 1 0 10:46 ? 00:00:02 ./bin/codis-server :6381root 4680 2884 0 11:12 pts/0 00:00:00 ./bin/codis-ha ‚Äìlog=ha.log ‚Äìlog-level=WARN ‚Äìdashboard=127.0.0.1:18080root 4712 2884 0 11:15 pts/0 00:00:00 grep ‚Äìcolor=auto codis kill6379 ÁöÑÁ´ØÂè£kill -9 3175Êü•Áúã6380 ÁöÑÊó•Âøó3179:M 23 Dec 11:19:07.739 * MASTER MODE enabled (user request from ‚Äòid=7 addr=192.168.110.129:51438 fd=8 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof‚Äô)ÁúãÂà∞6380Êàê‰∏∫‰∏ª ÂÜçÂêØÂä®6379 Âπ∂Ê≤°ÊúâÊàê‰∏∫6380ÁöÑ‰∏ªÔºåÈúÄË¶ÅÊâãÂä®ÁÇπÂáªweb‰∏äÁªøËâ≤ÁöÑÂ∑•ÂÖ∑Êâ≥ÊâãÊÄÄÁñëÂèØËÉΩÊòØÊàëÁöÑÈÖçÁΩÆÊñá‰ª∂Ê≤°ÊúâÂÜôslave ofÁöÑÂéüÂõ† ÂÜç6380ËæìÂÖ•ÂÜÖÂÆπ[root@zhm1 codis]# ./bin/redis-cli -p 6380127.0.0.1:6380&gt; set aaa 123OK127.0.0.1:6380&gt; ÂêØÂä®6379Áúã[root@zhm1 codis]# ./bin/redis-cli -p 6379127.0.0.1:6379&gt; get aaa‚Äú123‚Äù ÊÄªÁªìcodisÂíåredisÈõÜÁæ§ÁöÑÂå∫Âà´ a. Redis Cluster ÁöÑÈõÜÁæ§‰ø°ÊÅØÂ≠òÂÇ®Âú®ÊØè‰∏™ÈõÜÁæ§ËäÇÁÇπ‰∏äÔºåËÄåCodis ÈõÜÁæ§ÁöÑ‰ø°ÊÅØÂ≠òÂÇ®Âú®‰∏Ä‰∏™Áã¨Á´ãÁöÑÂ≠òÂÇ®Á≥ªÁªüÔºàZookeeperÔºâÈáå„ÄÇb. Â§ñÈÉ®ËÆøÈóÆÈõÜÁæ§ÔºöÂØπRedis ClusterÔºåÁõ¥Êé•ÈÄöËøá‰ª•ÈõÜÁæ§Ê®°ÂºèÂêØÂä®ÁöÑredis ÂÆ¢Êà∑Á´Ø ‚Äúredis-cli -c‚ÄùÊù•ËÆøÈóÆ„ÄÇÂØπ Codis ÈõÜÁæ§ÈÄöËøáÁã¨Á´ãÁöÑ codis-proxy ËäÇÁÇπÊù•ËÆøÈóÆ„ÄÇc. Redis Cluster Êúâ 16384 ‰∏™slot ÂèØ‰ª•ÂàÜÈÖçÔºåËÄå Codis ÈõÜÁæ§Âè™Êúâ1024 ‰∏™slot ÂèØ‰ª•ÂàÜÈÖç„ÄÇd. ÁõëÊéßÂíåÊìç‰ΩúÔºöCodis ÈõÜÁæ§ÊúâÂõæÂΩ¢ÂåñÁöÑ Code FE ÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºàÂèØ‰ª•ÂÆåÊàêCodis ProxyÔºåCodis Group„ÄÅCodis Server ÁöÑÊ∑ªÂä†ÂíåÂà†Èô§ÔºåÂàÜÈÖç Slot„ÄÅÊèêÂçáSlave ‰∏∫Master Á≠âÊìç‰ΩúÔºâÔºåËÄå Redis Cluster Â•ΩÂÉèÊ≤°ÊúâËøôÁ±ªÂ∑•ÂÖ∑„ÄÇ]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[codis ÈõÜÁæ§]]></title>
    <url>%2F2016%2F12%2F02%2Fcodis%20%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[ÂÆâË£ÖgoËØ≠Ë®Ähttp://www.golangtc.com/download ÈÄâÊã©1.7.3‰∏ãËΩΩ1tar -zxvf go1.7.3.linux-amd64.tar.gz -C /usr/local/ ‰∏ãËΩΩjdkÂíåzookeeperÂπ∂Ëß£Âéã ÈÖçÁΩÆzookeeper ÂàõÂª∫Êñá‰ª∂Â§π zkDataÂíåzkLog123456cp zoo_sample.cfg zoo.cfgvi zoo.cfg dataDir=/root/zookeeper/zookeeper-3.4.6/zkDatadataLogDir=/usr/bigdata/zookeeper-3.4.6/zkLogserver.1=master:2888:3888 ËøõÂÖ•zkData1echo 1 &gt; myid ‚Äì ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè12345export JAVA_HOME=/root/java/jdk1.8.0_111export GOROOT=/usr/local/goexport GOPATH=/usr/local/codisexport ZOOKEEPER_HOME=/root/zookeeper/zookeeper-3.4.6export PATH=$PATH:$JAVA_HOME/bin:$GOROOT/bin:$GOPATH/bin:$ZOOKEEPER_HOME/bin ‰∏ãËΩΩcodishttps://github.com/CodisLabs/codis/releasesÈÄâÊã©‰∏égoÁâàÊú¨‰∏ÄÊ†∑ÁöÑcodisÁâàÊú¨ 123456mkdir -p $GOPATH/src/github.com/CodisLabstar -xzf codis-3.1.0.tar.gz -C /usr/local/codis/src/github.com/CodisLabs/cd /usr/local/codis/src/github.com/CodisLabs/mv codis-3.1.0/ codiscd codis/make 1make gotest ÊâßË°åÊàêÂäüÂêéËøõÂÖ•bin ÁõÆÂΩï1cd /usr/local/codis/src/github.com/CodisLabs/codis/bin 123mkdir /etc/codiscd /usr/local/codis/src/github.com/CodisLabs/codis/externcp redis-2.8.21/redis.conf /etc/codis/redis6379.conf ÂàõÂª∫redis ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÊâÄÊåáÂÆöÁöÑÁõÆÂΩï123456789101112131415161718192021222324[root@zhm1 codis]# cd /opt[root@zhm1 opt]# mkdir -p codisapp/run[root@zhm1 opt]# mkdir -p codisapp/logs[root@zhm1 opt]# mkdir -p codisapp/data/6379 codisapp/data/6380[root@zhm1 opt]# vi /etc/codis/redis6379.confdaemonize yespidfile /opt/codisapp/run/redis6379.pidport 6379logfile "/opt/codisapp/logs/redis6379.log"dbfilename dump.rdbdir /opt/codisapp/data/6379cp /etc/codis/redis6379.conf /etc/codis/redis6380.confvi /etc/codis/redis6380.confdaemonize yespidfile /opt/codisapp/run/redis6380.pidport 6380logfile "/opt/codisapp/logs/redis6380.log"dbfilename dump.rdbdir /opt/codisapp/data/6380 ÂêØÂä®redis123[root@zhm1 opt]# cd /usr/local/codis/src/github.com/CodisLabs/codis/[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf --protected-mode no [root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf --protected-mode no Ê≠§Â§ÑÂêØÂä®Â¢ûÂä†‚Äìprotected-mode no ÂèÇÊï∞ÂèØ‰ª•Èò≤Ê≠¢ÂêéÈù¢Ê∑ªÂä†serverÁöÑÊó∂ÂÄôÊèêÁ§∫1Cause": "DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside. ÂêØÂä®dashboardËá™ÂÆö‰πâdashboard ÈÖçÁΩÆÊñá‰ª∂1./bin/codis-dashboard --default-config | tee dashboard.conf ‰øÆÊîπIP1234567891011121314151617181920[root@zhm1 codis]# vi dashboard.conf ################################################### ## Codis-Dashboard ## #################################################### Set Coordinator, only accept "zookeeper" &amp; "etcd".coordinator_name = "zookeeper"coordinator_addr = "192.168.110.129:2181"# Set Codis Product Name/Auth.product_name = "codis-demo"product_auth = ""# Set bind address for admin(rpc), tcp only.admin_addr = "192.168.110.129:18080"# Set quorum value for sentinel, default is 2.sentinel_quorum = 2 ÂêØÂä®zookeeper1234[root@zhm1 codis]# zkServer.sh startJMX enabled by defaultUsing config: /root/zookeeper/zookeeper-3.4.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTED ÂêØÂä®dashboard1[root@zhm1 codis]# nohup bin/codis-dashboard --ncpu=2 --config=dashboard.conf --log=dashboard.log --log-level=WARN &amp; Êü•ÁúãÊó•ÂøóËæìÂá∫ÔºåËøûÊé•zookeeper ÊàêÂäü1234567891011121314151617181920[root@zhm1 codis]# cat dashboard.log.2016-12-212016/12/21 11:18:34 main.go:77: [WARN] set ncpu = 22016/12/21 11:18:34 topom.go:110: [WARN] create new topom:&#123; "token": "6047b5ce9ef2826c1eb40692023b9bf4", "start_time": "2016-12-21 11:18:34.481106135 +0800 CST", "admin_addr": "192.168.110.129:18080", "product_name": "codis-demo", "pid": 66994, "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis", "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux"&#125;2016/12/21 11:18:35 main.go:124: [WARN] create topom with configcoordinator_name = "zookeeper"coordinator_addr = "192.168.110.129:2181"admin_addr = "192.168.110.129:18080"product_name = "codis-demo"product_auth = ""sentinel_quorum = 22016/12/21 11:18:36 topom.go:381: [WARN] admin start service on 192.168.110.129:18080 ÂêØÂä®codis-proxyÁºñËØëcodis-proxy ÈÖçÁΩÆÊñá‰ª∂123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[root@zhm1 codis]# ./bin/codis-proxy --default-config | tee proxy.conf################################################### ## Codis-Proxy ## #################################################### Set Codis Product Name/Auth.product_name = "codis-demo"product_auth = ""# Set bind address for admin(rpc), tcp only.admin_addr = "0.0.0.0:11080"# Set bind address for proxy, proto_type can be "tcp", "tcp4", "tcp6", "unix" or "unixpacket".proto_type = "tcp4"proxy_addr = "0.0.0.0:19000"# Set jodis address &amp; session timeout, only accept "zookeeper" &amp; "etcd".jodis_name = ""jodis_addr = ""jodis_timeout = "20s"jodis_compatible = false# Set datacenter of proxy.proxy_datacenter = ""# Set max number of alive sessions.proxy_max_clients = 1000# Set max offheap memory size. (0 to disable)proxy_max_offheap_size = "1024mb"# Set heap placeholder to reduce GC frequency.proxy_heap_placeholder = "256mb"# Proxy will ping backend redis in a predefined interval. (0 to disable)backend_ping_period = "5s"# Set backend recv buffer size &amp; timeout.backend_recv_bufsize = "128kb"backend_recv_timeout = "30s"# Set backend send buffer &amp; timeout.backend_send_bufsize = "128kb"backend_send_timeout = "30s"# Set backend pipeline buffer size.backend_max_pipeline = 1024# Set backend never read replica groups, default is falsebackend_primary_only = false# Set backend parallel connections per serverbackend_primary_parallel = 1backend_replica_parallel = 1# Set backend tcp keepalive period. (0 to disable)backend_keepalive_period = "75s"# If there is no request from client for a long time, the connection will be closed. (0 to disable)# Set session recv buffer size &amp; timeout.session_recv_bufsize = "128kb"session_recv_timeout = "30m"# Set session send buffer size &amp; timeout.session_send_bufsize = "64kb"session_send_timeout = "30s"# Make sure this is higher than the max number of requests for each pipeline request, or your client may be blocked.# Set session pipeline buffer size.session_max_pipeline = 512# Set session tcp keepalive period. (0 to disable)session_keepalive_period = "75s"# Set metrics server (such as http://localhost:28000), proxy will report json formatted metrics to specified server in a predefined period.metrics_report_server = ""metrics_report_period = "1s"# Set influxdb server (such as http://localhost:8086), proxy will report metrics to influxdb.metrics_report_influxdb_server = ""metrics_report_influxdb_period = "1s"metrics_report_influxdb_username = ""metrics_report_influxdb_password = ""metrics_report_influxdb_database = "" product_name ÈõÜÁæ§ÂêçÁß∞ÔºåÂèÇËÄÉdashboard ÂèÇÊï∞ËØ¥Êòéjodis_addr Jodis Ê≥®ÂÜåzookeeper Âú∞ÂùÄÊ≠§Â§ÑÊääIP‰øÆÊîπ‰∏Ä‰∏ãjodis_addr = ‚Äú192.168.110.129:2181‚Äù ÂêØÂä®codis-proxy 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859nohup bin/codis-proxy --ncpu=2 --config=proxy.conf --log =proxy.log --log-level=WARN &amp;[root@zhm1 codis]# cat =proxy.log.2016-12-212016/12/21 11:25:01 main.go:100: [WARN] set ncpu = 2, max-ncpu = 02016/12/21 11:25:01 proxy.go:89: [WARN] [0xc42010e840] create new proxy:&#123; "token": "673cc48d563a3614c13a3a294067f0b9", "start_time": "2016-12-21 11:25:01.70765896 +0800 CST", "admin_addr": "192.168.110.129:11080", "proto_type": "tcp4", "proxy_addr": "192.168.110.129:19000", "product_name": "codis-demo", "pid": 67097, "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis", "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux", "hostname": "zhm1.cn", "datacenter": ""&#125;2016/12/21 11:25:01 proxy.go:353: [WARN] [0xc42010e840] admin start service on [::]:110802016/12/21 11:25:01 main.go:159: [WARN] create proxy with configproto_type = "tcp4"proxy_addr = "0.0.0.0:19000"admin_addr = "0.0.0.0:11080"jodis_name = ""jodis_addr = "192.168.110.129:2181"jodis_timeout = "20s"jodis_compatible = falseproduct_name = "codis-demo"product_auth = ""proxy_datacenter = ""proxy_max_clients = 1000proxy_max_offheap_size = "1gb"proxy_heap_placeholder = "256mb"backend_ping_period = "5s"backend_recv_bufsize = "128kb"backend_recv_timeout = "30s"backend_send_bufsize = "128kb"backend_send_timeout = "30s"backend_max_pipeline = 1024backend_primary_only = falsebackend_primary_parallel = 1backend_replica_parallel = 1backend_keepalive_period = "75s"session_recv_bufsize = "128kb"session_recv_timeout = "30m"session_send_bufsize = "64kb"session_send_timeout = "30s"session_max_pipeline = 512session_keepalive_period = "75s"metrics_report_server = ""metrics_report_period = "1s"metrics_report_influxdb_server = ""metrics_report_influxdb_period = "1s"metrics_report_influxdb_username = ""metrics_report_influxdb_password = ""metrics_report_influxdb_database = ""2016/12/21 11:25:01 main.go:180: [WARN] [0xc42010e840] proxy waiting online ...2016/12/21 11:25:01 proxy.go:377: [WARN] [0xc42010e840] proxy start service on 0.0.0.0:190002016/12/21 11:25:02 main.go:180: [WARN] [0xc42010e840] proxy waiting online ... codis-proxy ÂêØÂä®ÂêéÔºåÂ§Ñ‰∫éwaiting Áä∂ÊÄÅÔºåÁõëÂê¨proxy_addr Âú∞ÂùÄÔºå‰ΩÜÊòØ‰∏ç‰ºöaccept ËøûÊé•ÔºåÊ∑ªÂä†Âà∞ÈõÜÁæ§Âπ∂ÂÆåÊàêÈõÜÁæ§Áä∂ÊÄÅÁöÑÂêåÊ≠•ÔºåÊâçËÉΩÊîπÂèòÁä∂ÊÄÅ‰∏∫online„ÄÇ Ê∑ªÂä†codis-proxy Âà∞ÈõÜÁæ§Ê∑ªÂä†ÁöÑÊñπÊ≥ïÊúâ‰ª•‰∏ã‰∏§ÁßçÔºöÈÄöËøácodis-fe Ê∑ªÂä†ÔºåÈÄöËøáAdd Proxy ÊåâÈíÆÔºåÂ∞Üadmin_addr Âä†ÂÖ•Âà∞ÈõÜÁæ§‰∏≠ÔºõÈÄöËøácodis-admin ÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑Ê∑ªÂä†Ôºö./bin/codis-admin ‚Äìdashboard=192.168.110.129:18080 ‚Äìcreate-proxy -x 192.168.110.129:11080 Êü•Áúãzookeeper Â≠òÂÇ®ÁöÑÊï∞ÊçÆ1234567891011zkCli.sh[zk: localhost:2181(CONNECTED) 0] ls /[codis3, zookeeper, kafka][zk: localhost:2181(CONNECTED) 1] ls /codis3[codis-demo][zk: localhost:2181(CONNECTED) 2] ls /codis3/codis-demo[proxy, topom][zk: localhost:2181(CONNECTED) 3] ls /codis3/codis-demo/proxy[proxy-673cc48d563a3614c13a3a294067f0b9][zk: localhost:2181(CONNECTED) 4] ls /codis3/codis-demo/topom[] ÈÖçÁΩÆÂêØÂä®Cdis FE ÈõÜÁæ§ÁÆ°ÁêÜÁïåÈù¢ÂàõÂª∫conf ÁõÆÂΩïÔºåÊîæ‰∏ãÈù¢ÁîüÊàêÁöÑcodis.json Êñá‰ª∂12345678910111213mkdir conf./bin/codis-admin --dashboard-list --zookeeper=192.168.110.129 | tee conf/codis.json2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - zkclient setup new connection to 192.168.110.1292016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Connected to 192.168.110.129:21812016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Authenticated: id=25085718467510274, timeout=40000[ &#123; "name": "codis-demo", "dashboard": "192.168.110.129:18080" &#125;]2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Recv loop terminated: err=EOF2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Send loop terminated: err=&lt;nil&gt; ÂêØÂä®codis-fe1234567./bin/codis-fe --ncpu=2 --log=fe.log --log-level=WARN --dashboard-list=conf/codis.json --listen=192.168.110.129:18090 &amp;[3] 67271[root@zhm1 codis]# cat fe.log.2016-12-21 2016/12/21 11:33:34 main.go:102: [WARN] set ncpu = 22016/12/21 11:33:34 main.go:105: [WARN] set listen = 192.168.110.129:180902016/12/21 11:33:34 main.go:117: [WARN] set assets = /usr/local/codis/src/github.com/CodisLabs/codis/bin/assets2016/12/21 11:33:34 main.go:132: [WARN] set --dashboard-list = conf/codis.json Ëá≥Ê≠§codis ÁöÑÂõæÂΩ¢ÁïåÈù¢Â∑≤ÁªèËÉΩÂ§üÊòæÁ§∫„ÄÇÊâìÂºÄÊµèËßàÂô®ËÆøÈóÆhttp://192.168.110.129:18090/ÔºåÈÄöËøáÁÆ°ÁêÜÁïåÈù¢Êìç‰Ωúcodis Â¶ÇÊûúÊòØÊú∫Âô®Â§ñËÆøÈóÆÈúÄË¶ÅÂÖ≥Èó≠Èò≤ÁÅ´Â¢ôcentos7ÁöÑÂëΩ‰ª§ÊòØ1234[root@zhm1 codis]# systemctl stop firewalld.service[root@zhm1 codis]# systemctl disable firewalld.serviceRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service. ÂàõÂª∫ÁªÑÂíåÂÆû‰æãÊìç‰ΩúÊ≠•È™§ÔºöÂú®New Group ÂêéÈù¢ËæìÂÖ•1ÔºàË°®Á§∫Â¢ûÂä†ÁºñÂè∑‰∏∫1 ÁöÑÁªÑÔºâÔºåÁÇπÂáªNew Group ÂÆåÊàêÁªÑÁöÑÂàõÂª∫ÔºõÁÑ∂ÂêéÔºåAdd ServerÔºåÂêéÈù¢ËÆæÁΩÆip ÂíåÁ´ØÂè£ÔºåÂπ∂ÊåáÂêëÁªÑÁºñÂè∑1ÔºåÁÇπÂáªAdd Server ÂÆåÊàêredis ÂÆû‰æãÁöÑÂàõÂª∫Ôºõ Ê∑ªÂä†Á¨¨2 ‰∏™ÁªÑÔºå‰ª•ÂèäÁ¨¨2 ‰∏™ÁªÑÁöÑÂÆû‰æãÔºåÊìç‰ΩúÊ≠•È™§Âêå‰∏ä„ÄÇ ÂØπslot ËøõË°åÂàÜÁªÑËæìÂÖ•slotËåÉÂõ¥ÂíåÁªÑ Âè¶Â§ñÂàÜ‰∫´‰∏Ä‰∏™github‰∏äÁöÑÂ•ΩÊñádockerÁé©codisÈõÜÁæ§https://github.com/ruo91/docker-codis]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis ÈõÜÁæ§]]></title>
    <url>%2F2016%2F12%2F02%2Fredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[ÂêåÊó∂Âú®ÊØèÂè∞Êú∫Âô®ÈÖçÁΩÆÂ¶Ç‰∏ãÂÜÖÂÆπÔºöÂàõÂª∫‰∏Ä‰∏™Á©∫ÁöÑÈõÜÁæ§Êñá‰ª∂ vi nodes-6379.conf :wq ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÔºåÂºÄÂêØÈõÜÁæ§Ê®°Âºè vi redis.conf cluster-enabled yes cluster-config-file nodes-6379.conf ÂêØÂä®Ôºö ./src/redis-server redis.conf ÁÑ∂ÂêéËæìÂÖ•Êü•ÁúãÂëΩ‰ª§1234567891011[hadoop@www.hadoop01.com ~]$ps -ef | grep redishadoop 2657 2506 0 21:59 pts/0 00:00:00 ./src/redis-server *:6379 [cluster]hadoop 2695 2670 0 22:00 pts/1 00:00:00 grep redis [hadoop@www.hadoop02.com ~]$ps -ef | grep redishadoop 2991 2662 0 10:12 pts/0 00:00:01 ./src/redis-server *:6379 [cluster]hadoop 3039 2999 0 10:21 pts/1 00:00:00 grep redis [hadoop@www.hadoop03.com ~]$ps -ef | grep redishadoop 2482 2417 0 13:41 pts/0 00:00:01 ./src/redis-server *:6379 [cluster]hadoop 2530 2490 0 13:51 pts/1 00:00:00 grep redis ÊØè‰∏™ÂêéÈù¢ÈÉΩÊúâ‰∏Ä‰∏™clusterÊ≠§Êó∂Âπ∂Ê≤°ÊúâÊääËäÇÁÇπÊ∑ªÂä†Âà∞‰∏Ä‰∏™ÈõÜÁæ§ÔºåÁî®ÂëΩ‰ª§Êü•Áúãnodes‰∏∫112345678910111213[hadoop@www.hadoop01.com redis-3.0.7]$./src/redis-cli127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:5cluster_slots_ok:5cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:1cluster_size:1cluster_current_epoch:0cluster_my_epoch:0cluster_stats_messages_sent:0cluster_stats_messages_received:0 ËøôÊó∂ÈúÄË¶ÅÊ∑ªÂä†ÈõÜÁæ§ËäÇÁÇπÔºö123456789101112131415161718127.0.0.1:6379&gt; Cluster meet www.hadoop02.com 6379(error) ERR Invalid node address specified: www.hadoop02.com:6379127.0.0.1:6379&gt; Cluster meet 192.168.247.152 6379OK127.0.0.1:6379&gt; Cluster meet 192.168.247.154 6379OK127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:5cluster_slots_ok:5cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:27cluster_stats_messages_received:26 ÂèØ‰ª•ÁúãÂà∞ËäÇÁÇπÊï∞‰∏∫3-&gt;cluster_known_nodes:3Ôºå‰ΩÜÊòØÈõÜÁæ§Áä∂ÊÄÅÊòØÂ§±Ë¥•-&gt;cluster_state:fail ÂàÜÈÖçÊßΩ‰Ωç‰∏ÄÂÖ±16384‰∏™ÊßΩ‰Ωçcluster addslots 0 1 2 ‚Ä¶ ÂÜô‰∏™ËÑöÊú¨Ôºö1234567vi fenpeicaowei.sh#bashfor i in &#123;10000..16384&#125;; do ./src/redis-cli cluster addslots $i; donechmod a+x fenpeicaowei.sh./fenpeicaowei.sh Êü•ÁúãÊúâÂì™‰∫õËäÇÁÇπ(Âõ†‰∏∫‰πãÂâçÈÖçÁΩÆ‰∫Ü‰∏ª‰ªéÔºåËØªËÄÖÂøΩÁï•slaveËäÇÁÇπÔºåÂè¶Â§ñ‰ªéËäÇÁÇπÂàÜÈÖçÊßΩ‰ΩçÊòØ‰∏ç‰ºöË¢´ÂêåÊ≠•ÁöÑ)12345./src/redis-cli -c127.0.0.1:6379&gt; cluster nodes276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481166569926 1 connectedda0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481166571948 1 connected64a2d824459edc01c7b6b205021d01da96513121 192.168.247.152:6379 myself,master - 0 0 1 connected 1584 2171 5649 9842 10000-16383 Êü•ÁúãÊßΩ‰ΩçÂàÜÈÖçÂà∞Âì™‰∫õËäÇÁÇπ1234567891011121314151617181920212223242526272829303132333435363738394041127.0.0.1:6379&gt; cluster slots1) 1) (integer) 1584 2) (integer) 1584 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63792) 1) (integer) 2171 2) (integer) 2171 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63793) 1) (integer) 5649 2) (integer) 5649 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63794) 1) (integer) 9842 2) (integer) 9842 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63795) 1) (integer) 10000 2) (integer) 16383 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 6379 123456789101112127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:6388cluster_slots_ok:6388cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:3002cluster_stats_messages_received:2999 ÂÖ®ÈÉ®ÂàÜÈÖçÂÆåÊàêÔºåÂ∞±ÂèØ‰ª•ÁúãÂà∞123456789101112127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:792cluster_stats_messages_received:798 Êü•ÁúãÊüê‰∏™keyÂØπÂ∫îÁöÑÊßΩ‰Ωç12127.0.0.1:6379&gt; cluster keyslot 3(integer) 1584 ÈáçÊñ∞ÂàÜÁâáredisÊèê‰æõ‰∏Ä‰∏™Â∑•ÂÖ∑Ôºöredis-trib.rb ‰ΩÜÊòØË¶ÅÁî®Ëøô‰∏™Â∑•ÂÖ∑ÈúÄË¶ÅÂÆâË£ÖrubyÁéØÂ¢É ‰∏ãËΩΩrepoÊñá‰ª∂wget http://mirrors.163.com/.help/CentOS6-Base-163.repo Â§á‰ªΩÂπ∂ÊõøÊç¢Á≥ªÁªüÁöÑrepoÊñá‰ª∂cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo.bakmv CentOS6-Base-163.repo CentOS-Base.repo ÊâßË°åyumÊ∫êÊõ¥Êñ∞ÂëΩ‰ª§123456789yum clean allyum makecacheyum updateyum install -y ruby* --skip-broken yum install rubygemswget https://rubygems.global.ssl.fastly.net/gems/redis-3.2.2.gemgem install redisgem install -l ./redis-3.2.2.gem Áî®redis-trib.rbÂàõÂª∫ÈõÜÁæ§./src/redis-trib.rb create ‚Äìreplicas 1 192.168.1.150:6379 192.168.1.152:6379 192.168.1.154:6379ÂèØËÉΩ‰ºöÊä•ÈîôÔºåÊòØÁâàÊú¨Â§™‰ΩéÁöÑÂéüÂõ†Ëß£ÂÜ≥ÊñπÊ°àÔºöftp://ftp.ruby-lang.org/pub/ruby/ ‰∏ãËΩΩÊúÄÊñ∞ÁöÑrubyÂÆâË£ÖÂåÖ ruby-2.3.3.tar.gzgem update ‚Äìsystem ÂçáÁ∫ßÂà∞ÊúÄÊñ∞ÁâàÊú¨2.5.1gem install redis ÂÆâË£ÖredisÊ®°Âùó ÈáçÊñ∞ÊâßË°å123[hadoop@www.hadoop02.com redis-3.0.7]$./src/redis-trib.rb create --replicas 1 192.168.247.150:6379 192.168.247.152:6379 192.168.247.154:6379&gt;&gt;&gt; Creating cluster[ERR] Node 192.168.247.150:6379 is not empty. Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0. Âõ†‰∏∫ÂàöÊâçÂ∑≤ÁªèÂàÜÈÖç‰∫ÜËäÇÁÇπ Áé∞Âú®ÂÅöÈáçÊñ∞ÂàÜÂå∫Êü•ÁúãÂàÜÂå∫ÂâçÔºö1234127.0.0.1:6379&gt; cluster nodes276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481242407249 1 connected64a2d824459edc01c7b6b205021d01da96513121 192.168.247.152:6379 master - 0 1481242405165 1 connected 0-16383da0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 myself,slave 64a2d824459edc01c7b6b205021d01da96513121 0 0 0 connected ÂàÜÂå∫Ôºö12345678910111213141516171819202122232425262728293031323334[hadoop@www.hadoop02.com redis-3.0.7]$./src/redis-trib.rb reshard 127.0.0.1:6379&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6379)M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379 slots:0-16383 (16384 slots) master 2 additional replica(s)S: 276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slots: (0 slots) slave replicates 64a2d824459edc01c7b6b205021d01da96513121S: da0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 slots: (0 slots) slave replicates 64a2d824459edc01c7b6b205021d01da96513121[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 5000What is the receiving node ID? 276a3675a706d4626a25c54892c368ca6c37002c*** The specified node is not known or not a master, please retry.What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da9651312*** The specified node is not known or not a master, please retry.What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da96513121Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs.Source node #1:allReady to move 5000 slots. Source nodes: Destination node: M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379 slots:0-16383 (16384 slots) master 2 additional replica(s) Resharding plan:Do you want to proceed with the proposed reshard plan (yes/no)? yes rehashÂè™ËÉΩÂú®‰∏ªËäÇÁÇπËøõË°åÔºåÊïÖ247.152ÂàÜÈÖçÁªô‰∫Ü247.152ÔºåÊ≤°ÊúâÂèòÂåñ]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka Stream]]></title>
    <url>%2F2016%2F12%2F02%2FKafkaStream%2F</url>
    <content type="text"><![CDATA[‰ª•‰∏ãÂÜÖÂÆπÊëòËá™ÂÆòÁΩëÔºåÁÜüÊÇâÁöÑÂêåÂ≠¶ÂèØË∑≥Ëøá Use Kafka Streams to process dataKafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the WordCountDemo example code (converted to use Java 8 lambda expressions for easy reading).Kafka StreamsÊòØKafka‰∏≠Áî®‰∫éÂÆ¢Êà∑Á´ØÁöÑÂ∫ìÔºå‰∏ªË¶ÅÁî®‰∫éËé∑ÂèñÂÆûÊó∂ÊµÅÂ§ÑÁêÜ‰ª•ÂèäÂàÜÊûêKafka brokers‰∏≠Â≠òÂÇ®ÁöÑÊï∞ÊçÆ„ÄÇËøô‰∏™‰æãÂ≠êÂ∞Ü‰ºöÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî®Ëøô‰∏™Â∫ìÊù•ËøêË°å‰∏Ä‰∏™ÊµÅÂºèÂ§ÑÁêÜÂ∫îÁî®„ÄÇËøôÈáåÊúâ‰∏Ä‰∏™WordCountDemoÁöÑ‰∏ªË¶Å‰ª£Á†ÅÔºàËΩ¨Êç¢ÊàêJava8 lambdaË°®ËææÂºèÊõ¥ÊòìËØªÔºâÔºö 123456789KTable wordCounts = textLines // Split each text line, by whitespace, into words. .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split("\\W+"))) // Ensure the words are available as record keys for the next aggregate operation. .map((key, value) -&gt; new KeyValue&lt;&gt;(value, value)) // Count the occurrences of each word (record key) and store the results into a table named "Counts". .countByKey("Counts") It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on an infinite, unbounded stream of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed ‚Äúall‚Äù the input data. ÂÆÉÂÆûÁé∞‰∫ÜWordCountÁÆóÊ≥ïÔºåËÆ°ÁÆó‰∫ÜËæìÂÖ•ÊñáÊú¨‰∏≠ÁöÑËØçÈ¢ë„ÄÇÁÑ∂ËÄåÔºåÂπ∂‰∏çÂÉèÂÖ∂‰ªñÁöÑWordCountÁöÑ‰æãÂ≠êÔºåÈÉΩÊòØËÆ°ÁÆóÂõ∫ÂÆöÂ§ßÂ∞èÁöÑÊï∞ÊçÆÔºåËøô‰∏™WordCount demoÂ∫îÁî®Á®çÂæÆÊúâÁÇπ‰∏çÂêåÔºåÂÆÉÊòØÂü∫‰∫é‰∏ç‰ºöÁªàÊ≠¢ÁöÑÊï∞ÊçÆÊµÅËÆ°ÁÆóÁöÑ„ÄÇÂíåËÆ°ÁÆóÂõ∫ÂÆöÊï∞ÊçÆÁöÑÊ®°ÂûãÊØîËæÉÂΩ¢‰ººÁöÑÊòØÔºåÂÆÉ‰πü‰ºö‰∏çÂÅúÁöÑÊõ¥Êñ∞ËØçÈ¢ëËÆ°ÁÆóÁªìÊûú„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÂÆÉÊòØÂü∫‰∫éÊ∞∏‰∏çÂÅúÊ≠¢ÁöÑÊï∞ÊçÆÊµÅÔºåÊâÄ‰ª•‰ºöÂë®ÊúüÊÄßÁöÑËæìÂá∫ÂΩìÂâçÁöÑËÆ°ÁÆóÁªìÊûúÔºå‰ªñ‰ºö‰∏çÂÅúÁöÑÂ§ÑÁêÜÊõ¥Â§öÁöÑÊï∞ÊçÆÔºåÂõ†‰∏∫ÂÆÉ‰πü‰∏çÁü•ÈÅì‰ΩïÊó∂ÂÆÉÂ§ÑÁêÜËøá‚ÄúÊâÄÊúâ‚ÄùÁöÑËæìÂÖ•Êï∞ÊçÆ„ÄÇ We will now prepare input data to a Kafka topic, which will subsequently be processed by a Kafka Streams application.Áé∞Âú®Êàë‰ª¨Â∞ÜËæìÂÖ•Êï∞ÊçÆÂØºÂÖ•Kafka topicÔºåËøô‰∫õÊï∞ÊçÆÂ∞Ü‰ºöË¢´Kafka StreamsÂ∫îÁî®Â§ÑÁêÜ 1&gt; echo -e "all streams lead to kafka\nhello kafka streams\njoin kafka summit" &gt; file-input.txt Or on Windows:123&gt; echo all streams lead to kafka&gt; file-input.txt&gt; echo hello kafka streams&gt;&gt; file-input.txt&gt; echo|set /p=join kafka summit&gt;&gt; file-input.txt Next, we send this input data to the input topic named streams-file-input using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):Êé•ÁùÄÔºåÊàë‰ª¨‰ΩøÁî®ÁªàÁ´ØproducerÊù•Â∞ÜËøô‰∫õËæìÂÖ•Êï∞ÊçÆÂèëÈÄÅÂà∞Âêç‰∏∫streams-file-inputÁöÑtopicÔºàÂú®ÂÆûË∑µ‰∏≠ÔºåÊµÅÊï∞ÊçÆ‰ºöÊåÅÁª≠‰∏çÊñ≠ÁöÑÊµÅÂÖ•kafkaÔºåÂΩìÂ∫îÁî®Â∞Ü‰ºöÂêØÂä®Âπ∂ËøêË°åÊó∂ÔºâÔºö12345&gt; bin/kafka-topics.sh --create \ --zookeeper localhost:2181 \ --replication-factor 1 \ --partitions 1 \ --topic streams-file-input 1&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input &lt; file-input.txt We can now run the WordCount demo application to process the input data:Êàë‰ª¨ÂèØ‰ª•ËøêË°åWordCount demoÂ∫îÁî®Êù•Â§ÑÁêÜËæìÂÖ•Êï∞ÊçÆ1&gt; bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo There won‚Äôt be any STDOUT output except log entries as the results are continuously written back into another topic named streams-wordcount-output in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.‰∏ç‰ºöÊúâ‰ªª‰ΩïÁöÑstdoutËæìÂá∫Èô§‰∫ÜÊó•ÂøóÊù°ÁõÆÔºåÁªìÊûú‰ºöÊåÅÁª≠‰∏çÊñ≠ÁöÑÂÜôÂõûkafka‰∏≠Âè¶‰∏Ä‰∏™Âêç‰∏∫streams-wordcount-outputÁöÑtopic„ÄÇËøô‰∏™demoÂ∞Ü‰ºöËøêË°åÊï∞ÁßíÔºå‰∏ç‰ºöÂÉèÂÖ∏ÂûãÁöÑÊµÅÂ§ÑÁêÜÂ∫îÁî®ÔºåËá™Âä®ÁªàÊ≠¢„ÄÇ We can now inspect the output of the WordCount demo application by reading from its output topic:Êàë‰ª¨Áé∞Âú®ÈÄöËøáÈòÖËØª‰∏ªÈ¢òÁöÑÊï∞Êù•Êù•Ê£ÄÊü•WordCount demoÂ∫îÁî®Á®ãÂ∫èÁöÑËæìÂá∫Ôºö 12345678&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \ --topic streams-wordcount-output \ --from-beginning \ --formatter kafka.tools.DefaultMessageFormatter \ --property print.key=true \ --property print.value=true \ --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \ --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer with the following output data being printed to the console:ÁªàÁ´Ø‰ºöÊâìÂç∞Âá∫‰ª•‰∏ãÊï∞ÊçÆÔºö12345678all 1lead 1to 1hello 1streams 2join 1kafka 3summit 1 Here, the first column is the Kafka message key, and the second column is the message value, both in in java.lang.String format. Note that the output is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word, aka record key such as ‚Äúkafka‚Äù. For multiple records with the same key, each later record is an update of the previous one.Á¨¨‰∏ÄÂàóÊòØKafkaÊ∂àÊÅØÁöÑkeyÔºåÁ¨¨‰∫åÂàóÊòØÊ∂àÊÅØvalueÔºå‰∏§ËÄÖÈÉΩÊòØjava.lang.StringÊ†ºÂºè„ÄÇÊ≥®ÊÑèÔºåËæìÂá∫ÂÆûÈôÖ‰∏äÂ∫îËØ•ÊòØÊåÅÁª≠ÁöÑÊõ¥Êñ∞Êï∞ÊçÆÊµÅÔºåÊï∞ÊçÆÊµÅ‰∏≠ÁöÑÊØè‰∏Ä‰∏™ËÆ∞ÂΩïÔºà‰æãÂ¶ÇÔºå‰∏äÈù¢ËæìÂá∫ÁöÑÊØè‰∏ÄË°åÔºâÈÉΩÊòØ‰∏Ä‰∏™ÂçïÁã¨ËØçÊ±áÁöÑÊï∞ÈáèÔºåÊàñËÄÖÊòØËÆ∞ÂΩï‰∫ÜkeyÁöÑÊï∞ÈáèÔºå‰æãÂ¶Ç‰∏äÈù¢ÁöÑ‚Äúkafka‚Äù„ÄÇÂØπ‰∫éÂ§öÊù°ËÆ∞ÂΩïÁöÑkey‰∏ÄËá¥ËøôÁßçÊÉÖÂÜµÔºåÊØè‰∏ÄÊù°ÂêéÈù¢ÁöÑËÆ∞ÂΩïÈÉΩÊòØÂØπÂâç‰∏ÄÊù°ËÆ∞ÂΩïÁöÑÊõ¥Êñ∞„ÄÇ Now you can write more input messages to the streams-file-input topic and observe additional messages added to streams-wordcount-output topic, reflecting updated word counts (e.g., using the console producer and the console consumer, as described above).Áé∞Âú®‰Ω†ÂèØ‰ª•ÂÜôÂÖ•Êõ¥Â§öÁöÑÊ∂àÊÅØÂà∞streams-file-inputËøô‰∏™topicÔºåÂèØ‰ª•ËßÇÂØüÂà∞Êõ¥Â§öÁöÑÊ∂àÊÅØ‰ºöÂèëÈÄÅÂà∞streams-wordcount-outputËøô‰∏™topicÔºåÂèçÊò†‰∫ÜÊõ¥Êñ∞‰πãÂêéÁöÑËØçÊ±áÊï∞Èáè„ÄÇ You can stop the console consumer via Ctrl-C.‰Ω†ÂèØ‰ª•‰ΩøÁî®Ctrl+CÁªìÊùüÊéßÂà∂Âè∞ÁöÑÊ∂àË¥πËÄÖ Stream ProcessingMany users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an ‚Äúarticles‚Äù topic; further processing might normalize or deduplicate this content and published the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza.ÂæàÂ§öÁî®Êà∑Â∞ÜkafkaÁî®‰ΩúÂ§öÁ∫ßÊï∞ÊçÆÂ§ÑÁêÜ‰πãÈó¥ÁöÑÊ∂àÊÅØÁÆ°ÈÅìÔºöÂéüÂßãÊï∞ÊçÆÂ≠òÊîæ‰∫éKafka‰∏çÂêåÁöÑtopics‰∏≠ÔºåÁÑ∂ÂêéÁªèËøáËÅöÂêà„ÄÅÂ¢ûÂº∫„ÄÅÊàñËÄÖÂÖ∂‰ªñÁöÑËΩ¨Êç¢‰πãÂêéÔºåÂØºÂÖ•KafkaÊñ∞ÁöÑtopics‰∏≠Ôºå‰ª•‰æõÂêéÈù¢ÁöÑÊ∂àË¥π„ÄÇ‰æãÂ¶ÇÔºåÂØπ‰∫éÊñ∞ÈóªÊé®ËçêÁöÑÂ§ÑÁêÜÊµÅÁ®ãÊù•ÊâÄÔºöÈ¶ñÂÖà‰ªéRSS‰ø°ÊÅØÊµÅ‰∏≠Ëé∑ÂèñÊñáÁ´†ÂÜÖÂÆπÔºåÁÑ∂ÂêéÂØºÂÖ•Âêç‰∏∫‚Äúarticles‚ÄùÁöÑtopic; ÂÖ∂Ê¨°ÔºåÂêéÈù¢ÁöÑÂ§ÑÁêÜÂèØËÉΩÊòØÂØπËøô‰∫õÂÜÖÂÆπËøõË°åËßÑËåÉÂåñÊàñËÄÖÁ≤æÁÆÄÊìç‰ΩúÔºåÁÑ∂ÂêéÂ∞ÜÁªèËøá‰∏äËø∞Â§ÑÁêÜÁöÑÂÜÖÂÆπÂØºÂÖ•Êñ∞ÁöÑtopic;ÊúÄÂêéÁöÑÂ§ÑÁêÜÂèØËÉΩÊòØËØïÂõæÂ∞ÜËøô‰∫õÂÜÖÂÆπÊé®ËçêÁªôÁî®Êà∑„ÄÇËøôÊ†∑ÁöÑÂ§ÑÁêÜÊµÅÁ®ãÂÆûÈôÖÂ±ïÁé∞‰∫ÜÂÆûÊó∂ÊµÅÂú®Áã¨Á´ãÁöÑtopics‰πãÈó¥ÊµÅÂä®ÁöÑÊµÅÁ®ãÂõæ„ÄÇ‰ªé0.10.0.0ÂºÄÂßãÔºåApache KafkaÊé®Âá∫‰∫Ü‰∏ÄÊ¨æÁß∞‰∏∫Kafka StreamsÁöÑÊµÅÂºèÂ§ÑÁêÜÂ∫ìÔºå‰ºòÁÇπÊòØËΩªÈáèÁ∫ßÂêåÊó∂ÊÄßËÉΩÂæàÂ•ΩÔºåÂÆÉÂèØ‰ª•ÂÆåÊàê‰∏äÈù¢ÊâÄÊèèËø∞ÁöÑÂ§öÁ∫ßÂ§ÑÁêÜ„ÄÇÈô§‰∫ÜKafka streams‰πãÂ§ñÔºåËøòÊúâ‰∏Ä‰∫õÂºÄÊ∫êÊµÅÂºèÂ§ÑÁêÜÂ∑•ÂÖ∑ÂèØ‰ª•ÈÄâÁî®ÔºåÂåÖÊã¨Apache StormÂíåSamza„ÄÇ OverviewKafka Streams is a client library for processing and analyzing data stored in Kafka and either write the resulting data back to Kafka or send the final output to an external system. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management of application state. Kafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka‚Äôs parallelism model.Kafka StreamsÊòØ‰∏Ä‰∏™ÂÆ¢Êà∑Á´ØÁ®ãÂ∫èÂ∫ìÁî®‰∫éÂ§ÑÁêÜÂíåÂàÜÊûêÂ≠òÂÇ®Âú®Kafka‰∏≠ÁöÑÊï∞ÊçÆÔºåÂπ∂Â∞ÜÂæóÂà∞ÁöÑÊï∞ÊçÆÂÜôÂÖ•KafkaÊàñÂèëÈÄÅÊúÄÁªàËæìÂá∫Âà∞Â§ñÈÉ®Á≥ªÁªü„ÄÇÂÆÉÂª∫Á´ãÂú®Â¶ÇÈÄÇÂΩìÂå∫ÂàÜ‰∫ã‰ª∂ÁöÑÊó∂Èó¥ÂíåÂä†Â∑•Êó∂Èó¥ÔºåÁ™óÂè£ÂáΩÊï∞ÁöÑÊîØÊåÅÔºåÂíåÁÆÄÂçïËÄåÈ´òÊïàÁöÑÂ∫îÁî®Á®ãÂ∫èÁä∂ÊÄÅÁÆ°ÁêÜ„ÄÇKafka StreamsÊúâ‰∏Ä‰∏™‰ΩéÈó®ÊßõËøõÂÖ•Ôºö‰Ω†ÂèØ‰ª•Âø´ÈÄüÁºñÂÜôÂíåËøêË°å‰∏Ä‰∏™Â∞èËßÑÊ®°ÁöÑÊ¶ÇÂøµËØÅÊòéÂú®‰∏ÄÂè∞Êú∫Âô®‰∏äÔºå‰Ω†Âè™ÈúÄË¶ÅËøêË°åÂú®Â§öÂè∞Êú∫Âô®‰∏äÁöÑÂ∫îÁî®Á®ãÂ∫èÁöÑÈ¢ùÂ§ñÁöÑÂÆû‰æãÊâ©Â±ïÂà∞È´òÂÆπÈáèÁöÑÁîü‰∫ßÂ∑•‰ΩúË¥üËΩΩ„ÄÇKafka Streams ÈÄèÊòéÂú∞Â§ÑÁêÜÁõ∏ÂêåÁöÑÂ∫îÁî®Á®ãÂ∫èÈÄöËøáÂà©Áî®Kafka ÁöÑÂπ∂Ë°åÊ®°ÂûãÁöÑÂ§ö‰∏™ÂÆû‰æãÁöÑË¥üËΩΩÂπ≥Ë°°„ÄÇ Some highlights of Kafka Streams:Kafka StreamsÁöÑ‰∏Ä‰∫õ‰∫ÆÁÇπ Designed as a simple and lightweight client library, which can beeasily embedded in any Java application and integrated with anyexisting packaging, deployment and operational tools that users havefor their streaming applications. ‰Ωú‰∏∫‰∏Ä‰∏™ÁÆÄÂçïËÄåËΩªÈáèÁ∫ßÁöÑÂÆ¢Êà∑Á´ØÂ∫ìÔºåÂÆÉÂèØ‰ª•Êñπ‰æøÁöÑÂµåÂÖ•‰ªª‰ΩïjavaÂ∫îÁî®ÂíåÈõÜÊàê‰ªª‰ΩïÁé∞ÊúâÁöÑÂåÖÔºåÈÉ®ÁΩ≤ÂíåËøêËê•Â∑•ÂÖ∑ÔºåÁî®Êà∑ÊúâÂØπ‰∫é‰ªñ‰ª¨ÁöÑÊµÅÂ∫îÁî®„ÄÇ Has no external dependencies on systems other than Apache Kafkaitself as the internal messaging layer; notably, it uses Kafka‚Äôspartitioning model to horizontally scale processing while maintainingstrong ordering guarantees. ÂØπÂÖ∂‰ªñÊØîApache Kafka Ê≤°ÊúâÂ§ñÈÉ®‰æùËµñÂÆÉÊú¨Ë∫´‰Ωú‰∏∫ÂÜÖÈÉ®Ê∂àÊÅØÂ±ÇÔºåÁâπÂà´ÊòØÔºåÂÆÉ‰ΩøÁî®KafkaÁöÑ ÂàÜÂâ≤Ê®°ÂûãÂú®‰øùÊåÅÂêåÊó∂ËøõË°åÊ∞¥Âπ≥Áº©ÊîæÂ§ÑÁêÜÁöÑÂàÜÂå∫Ê®°ÂûãÂº∫ÊéíÂ∫è‰øùËØÅ„ÄÇ Supports fault-tolerant local state, which enables very fast andefficient stateful operations like joins and windowed aggregations. ÊîØÊåÅÂÆπÈîôÁöÑÊú¨Âú∞Áä∂ÊÄÅÔºå‰ΩøÈùûÂ∏∏Âø´ÈÄüÂíåÈ´òÊïàÁöÑÁä∂ÊÄÅÊìç‰ΩúÁöÑÂä†ÂÖ•ÂíåÁ™óÂè£ËÅöÈõÜ„ÄÇ Employs one-record-at-a-time processing to achieve low processinglatency, and supports event-time based windowing operations. ÈááÁî®Âêå‰∏ÄÊó∂ÂàªÂè™Êúâ‰∏ÄÊù°ËÆ∞ÂΩïÂ§ÑÁêÜÂÆûÁé∞‰ΩéÁöÑÂ§ÑÁêÜÂª∂ËøüÔºåÂπ∂ÊîØÊåÅÂü∫‰∫éÊó∂Èó¥‰∫ã‰ª∂ÁöÑÁ™óÂè£Êìç‰Ωú„ÄÇ Offers necessary stream processing primitives, along with ahigh-level Streams DSL and a low-level Processor API. Êèê‰æõÂøÖË¶ÅÁöÑÊµÅÂ§ÑÁêÜÂü∫ÂÖÉÔºå‰ª•Âèä high-level Streams DSLÂíå low-level Processor API„ÄÇ Developer Guide ÂºÄÂèëËÄÖÊåáÂçóThere is a quickstart example that provides how to run a stream processing program coded in the Kafka Streams library. This section focuses on how to write, configure, and execute a Kafka Streams application. Êúâ‰∏Ä‰∏™Âø´ÈÄüÂÖ•Èó®Á§∫‰æãÊèê‰æõ‰∫ÜÂ¶Ç‰ΩïËøêË°å‰∏Ä‰∏™ÊµÅÂ§ÑÁêÜÁ®ãÂ∫èÂú®Âç°Â§´Âç°ÊµÅÁöÑÂ∫ì‰ª£Á†Å„ÄÇÊú¨ËäÇÈáçÁÇπ‰ªãÁªçÂ¶Ç‰ΩïÁºñÂÜô„ÄÅÈÖçÁΩÆÂíåÊâßË°åÂç°Â§´Âç°ÊµÅÂ∫îÁî®Á®ãÂ∫è„ÄÇ Core Concepts Ê†∏ÂøÉÊ¶ÇÂøµWe first summarize the key concepts of Kafka Streams.Êàë‰ª¨È¶ñÂÖàÊÄªÁªì‰∫ÜKafka StreamsÁöÑÂÖ≥ÈîÆÊ¶ÇÂøµ„ÄÇ Stream Processing Topology ÊµÅÂ§ÑÁêÜTopology A stream is the most important abstraction provided by Kafka Streams:it represents an unbounded, continuously updating data set. A streamis an ordered, replayable, and fault-tolerant sequence of immutabledata records, where a data record is defined as a key-value pair. ÊµÅÊòØKafka StreamsÊèê‰æõÁöÑÊúÄÈáçË¶ÅÁöÑÊäΩË±°ÔºöÂÆÉË°®Á§∫‰∏Ä‰∏™Êó†ÁïåÁöÑÔºå‰∏çÊñ≠Êõ¥Êñ∞ÁöÑÊï∞ÊçÆÈõÜ„ÄÇ‰∏Ä‰∏™ÊµÅÊòØ‰∏Ä‰∏™ÊúâÂ∫èÁöÑ„ÄÅÂèØÈáçÂ§çÁöÑÔºåÂíå‰∏çÂèòÁöÑÂÆπÈîôÂ∫èÂàóÊï∞ÊçÆËÆ∞ÂΩïÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Êï∞ÊçÆËÆ∞ÂΩïË¢´ÂÆö‰πâ‰∏∫‰∏Ä‰∏™ÈîÆÂÄºÂØπ„ÄÇ A stream processing application written in Kafka Streams defines itscomputational logic through one or more processor topologies, where aprocessor topology is a graph of stream processors (nodes) that areconnected by streams (edges).Âú®Kafka Streams‰∏≠ÂÜôÁöÑÊµÅÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫èÂÆö‰πâ‰∫ÜÂÆÉÁöÑËÆ°ÁÆóÈÄªËæëÈÄöËøá‰∏Ä‰∏™ÊàñÂ§ö‰∏™Â§ÑÁêÜÂô®ÁöÑtopologiesÔºåÂÖ∂‰∏≠Â§ÑÁêÜÂô®ÁöÑtopologyÊòØ‰∏Ä‰∏™ÊµÅÂ§ÑÁêÜÂô®ÔºàËäÇÁÇπÔºâÁöÑÂõæÂΩ¢Áî±ÊµÅËøûÊé•ÔºàËæπÁºòÔºâ„ÄÇ A stream processor is a node in the processor topology; it representsa processing step to transform data in streams by receiving one inputrecord at a time from its upstream processors in the topology,applying its operation to it, and may subsequently producing one ormore output records to its downstream processors. ÊµÅÂ§ÑÁêÜÂô®ÊòØÂ§ÑÁêÜÂô®topology‰∏≠ÁöÑ‰∏Ä‰∏™ËäÇÁÇπÔºõÂÆÉË°®Á§∫ÈÄöËøáÊé•Êî∂‰∏Ä‰∏™ËæìÂÖ•Êù•ÂèòÊç¢ÊµÅ‰∏≠ÁöÑÊï∞ÊçÆÁöÑÂ§ÑÁêÜÊ≠•È™§Âú®topology‰∏≠ÁöÑ‰∏äÊ∏∏Â§ÑÁêÜÂô®‰∏äËÆ∞ÂΩïÁöÑÊó∂Èó¥ÔºåÂ∫îÁî®ÂÆÉÁöÑÊìç‰ΩúÔºåÂπ∂ÂèØËÉΩÈöèÂêé‰∫ßÁîü‰∏Ä‰∏™ÊàñÂêë‰∏ãÊ∏∏Â§ÑÁêÜÂô®ÁöÑÊõ¥Â§öËæìÂá∫ËÆ∞ÂΩï„ÄÇ Kafka Streams offers two ways to define the stream processing topology: the Kafka Streams DSL provides the most common data transformation operations such as map and filter; the lower-level Processor API allows developers define and connect custom processors as well as to interact with state stores.Kafka Streams Êèê‰æõ‰∫Ü‰∏§ÁßçÊñπÂºèÊù•ÂÆö‰πâÊµÅÂ§ÑÁêÜtopologyÔºöKafka Streams DSLÊèê‰æõ‰∫ÜÊúÄÂ∏∏Áî®ÁöÑÊï∞ÊçÆËΩ¨Êç¢Êìç‰ΩúÔºåÂ¶ÇmapÂíåfilterÔºõlower-level Processor APIÂÖÅËÆ∏ÂºÄÂèëËÄÖÂÆö‰πâÂíåËøûÊé•ÂÆöÂà∂Â§ÑÁêÜÂô®‰ª•ÂèäÂ≠òÂÇ®‰∫§‰∫íÁöÑÁä∂ÊÄÅ„ÄÇ Time Êó∂Èó¥A critical aspect in stream processing is the notion of time, and how it is modeled and integrated. For example, some operations such as windowing are defined based on time boundaries.ÊµÅÂ§ÑÁêÜ‰∏≠ÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆÊñπÈù¢ÊòØÊó∂Èó¥ÁöÑÊ¶ÇÂøµÔºå‰ª•ÂèäÂÆÉÊòØÂ¶Ç‰ΩïÂª∫Ê®°ÂíåÈõÜÊàê„ÄÇ‰æãÂ¶ÇÔºå‰∏Ä‰∫õÊìç‰ΩúÂ¶ÇÁ™óÂè£ÊòØÂü∫‰∫éÊó∂Èó¥ËæπÁïåÁöÑÂÆö‰πâ„ÄÇ Common notions of time in streams are:ÊµÅ‰∏≠ÁöÑÊó∂Èó¥ÁöÑÂÖ±ÂêåÊ¶ÇÂøµÊòØÔºö Event time - The point in time when an event or data record occurred,i.e. was originally created ‚Äúat the source‚Äù. ‰∫ã‰ª∂Êó∂Èó¥ - ÂΩìÂèëÁîü‰∫ã‰ª∂ÊàñÊï∞ÊçÆËÆ∞ÂΩïÊó∂ÁöÑÊó∂Èó¥ÁÇπÔºåÂç≥ÊúÄÂàùÂàõÂª∫ÁöÑ‚ÄúÂú®Ê∫êÂ§¥‰∏ä‚Äù„ÄÇ Processing time - The point in time when the event or data recordhappens to be processed by the stream processing application, i.e.when the record is being consumed. The processing time may bemilliseconds, hours, or days etc. later than the original event time. Â§ÑÁêÜÊó∂Èó¥ - ‰∫ã‰ª∂ÊàñÊï∞ÊçÆËÆ∞ÂΩïÁöÑÊó∂Èó¥ÁÇπÁ¢∞Â∑ßË¢´ÊµÅÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫èÂ§ÑÁêÜÔºåÂç≥ÂΩìËÆ∞ÂΩïË¢´Ê∂àËÄó„ÄÇÂ§ÑÁêÜÊó∂Èó¥ÂèØËÉΩÊòØÊØîÂéüÂßã‰∫ã‰ª∂Êó∂Èó¥ÊôöÁöÑÊØ´ÁßíÊï∞„ÄÅÂ∞èÊó∂ÊàñÊï∞Â§©Á≠â„ÄÇ Ingestion time - The point in time when an event or data record isstored in a topic partition by a Kafka broker. The difference toevent time is that this ingestion timestamp is generated when therecord is appended to the target topic by the Kafka broker, not whenthe record is created ‚Äúat the source‚Äù. The difference to processingtime is that processing time is when the stream processingapplication processes the record. For example, if a record is neverprocessed, there is no notion of processing time for it, but it stillhas an ingestion time. ÊëÑÂèñÊó∂Èó¥ - ÂΩì‰∏Ä‰∏™‰∫ã‰ª∂ÊàñÊï∞ÊçÆËÆ∞ÂΩïÁöÑÊó∂Èó¥ÁÇπÂ≠òÂÇ®Âú®Kafka brokerÁöÑ‰∏ªÈ¢òÂàÜÂå∫‰∏≠„ÄÇ‰∫ã‰ª∂Êó∂Èó¥‰∏çÂêåÁöÑÊòØÔºåËøôÁßçÊëÑÂèñÊó∂Èó¥Êà≥Êó∂‰∫ßÁîüÁöÑËÆ∞ÂΩïËøΩÂä†Âà∞ÁõÆÊ†á‰∏ªÈ¢òÁî±Kafka broker ÔºåËÄå‰∏çÊòØÂΩìËÆ∞ÂΩïÊòØÂú®‚ÄúÊ∫ê‚ÄùÂàõÂª∫ÁöÑ„ÄÇÂ§ÑÁêÜÂ∑ÆÂºÇÊó∂Èó¥ÊòØÂ§ÑÁêÜÊó∂Èó¥ÁöÑÊó∂ÂÄôÊòØÊµÅÂ§ÑÁêÜÁöÑÂ∫îÁî®Á®ãÂ∫èÂ§ÑÁêÜËÆ∞ÂΩï„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûú‰∏Ä‰∏™ËÆ∞ÂΩïÊòØ‰ªéÊù•Ê≤°ÊúâÂ§ÑÁêÜÔºåÊ≤°ÊúâÂ§ÑÁêÜÊó∂Èó¥ÁöÑÊ¶ÇÂøµÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂Êúâ‰∏Ä‰∏™ÊëÑÂèñÊó∂Èó¥ The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka‚Äôs configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps.‰∫ã‰ª∂ÁöÑÊó∂Èó¥ÂíåÊëÑÂèñÊó∂Èó¥‰πãÈó¥ÁöÑÈÄâÊã©ÂÆûÈôÖ‰∏äÊòØÈÄöËøáKafkaÁöÑÈÖçÁΩÆÔºà‰∏çÊòØKafka StreamsÔºâÔºö‰ªéKafka0.10.xËµ∑ÔºåÊó∂Èó¥Êà≥Ë¢´Ëá™Âä®ÂµåÂÖ•Âà∞KafkaÁöÑÊ∂àÊÅØ‰∏≠„ÄÇÊ†πÊçÆKafkaÁöÑÈÖçÁΩÆËøô‰∫õÊó∂Èó¥Êà≥Ë°®Á§∫‰∫ã‰ª∂Êó∂Èó¥ÊàñÊëÑÂèñÊó∂Èó¥„ÄÇÂêÑËá™ÁöÑKafkaÈÖçÁΩÆËÆæÁΩÆÂèØ‰ª•Âú®brokerÁ∫ßÂà´ÊàñÊØè‰∏™‰∏ªÈ¢ò‰∏äÊåáÂÆö„ÄÇÂú®Kafka StreamsÁöÑÈªòËÆ§Êó∂Èó¥Êà≥ÊèêÂèñÂô®Â∞ÜÊ£ÄÁ¥¢Ëøô‰∫õÂµåÂÖ•Êó∂Èó¥Êà≥as-is„ÄÇÂõ†Ê≠§ÔºåÊÇ®ÁöÑÂ∫îÁî®Á®ãÂ∫èÁöÑÊúâÊïàÊó∂Èó¥ËØ≠‰πâ‰æùËµñ‰∫éÊúâÊïàÁöÑKafkaÈÖçÁΩÆËøô‰∫õÂµåÂÖ•Êó∂Èó¥Êà≥„ÄÇ Kafka Streams assigns a timestamp to every data record via the TimestampExtractor interface. Concrete implementations of this interface may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce different notions of time depending on their business needs. For example, per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins.Kafka StreamsÂàÜÈÖç‰∏Ä‰∏™Êó∂Èó¥Êà≥ÁöÑÊØè‰∏Ä‰∏™Êï∞ÊçÆËÆ∞ÂΩïÈÄöËøáTimestampExtractorÊé•Âè£„ÄÇËøô‰∏™Êé•Âè£ÁöÑÂÖ∑‰ΩìÂÆûÁé∞ÂèØ‰ª•Ê£ÄÁ¥¢ÊàñËÆ°ÁÆóÂü∫‰∫éÊï∞ÊçÆËÆ∞ÂΩïÂ¶ÇÂµåÂÖ•Êó∂Èó¥Êà≥Â≠óÊÆµÊèê‰æõ‰∫ã‰ª∂Êó∂Èó¥ËØ≠‰πâÂÜÖÂÆπÁöÑÊó∂Èó¥Êà≥ÔºåÊàñ‰ΩøÁî®‰ªª‰ΩïÂÖ∂‰ªñÁöÑÊñπÊ≥ïÔºåÂ¶ÇÂä†Â∑•Êó∂ËøîÂõûÂΩìÂâçÊó∂ÈíüÊó∂Èó¥Ôºå‰ªéËÄå‰∫ßÁîüËØ≠‰πâÊµÅÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥„ÄÇÂõ†Ê≠§ÔºåÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•ÊâßË°å‰∏çÂêåÁöÑÊó∂Èó¥Ê¶ÇÂøµÔºåËøôÂèñÂÜ≥‰∫é‰ªñ‰ª¨ÁöÑ‰∏öÂä°ÈúÄÊ±Ç„ÄÇ‰æãÂ¶ÇÔºåÊØè‰∏™ËÆ∞ÂΩïÊó∂Èó¥Êà≥ÁöÑÊèèËø∞ÂÖ≥‰∫éÊµÅÊó∂Èó¥ÁöÑËøõÂ±ïÔºàËôΩÁÑ∂ËÆ∞ÂΩïÂèØËÉΩ‰ºöË∂ÖÂá∫ÊµÅÔºâÂíå‰øÉ‰ΩøÊó∂Èó¥‰æùËµñÊìç‰Ωú‰æãÂ¶Ç joins„ÄÇ Finally, whenever a Kafka Streams application writes records to Kafka, then it will also assign timestamps to these new records. The way the timestamps are assigned depends on the context:ÊúÄÂêéÔºåÂΩìKafkaËÆ∞ÂΩïÂÜôÂÖ•Âà∞Kafka StreamsÂ∫îÁî®ÔºåÈÇ£‰πàÂÆÉ‰πü‰ºöÂØπÊñ∞Á∫™ÂΩïÊåáÂÆöÊó∂Èó¥Êà≥„ÄÇÊó∂Èó¥Êà≥ÊòØÂàÜÈÖçÊñπÂºèÂèñÂÜ≥‰∫écontextÔºö When new output records are generated via processing some inputrecord, for example, context.forward() triggered in the process()function call, output record timestamps are inherited from inputrecord timestamps directly. ÂΩìÈÄöËøáÂ§ÑÁêÜ‰∏Ä‰∫õËæìÂÖ•ËÄå‰∫ßÁîüÊñ∞ÁöÑËæìÂá∫ËÆ∞ÂΩïÊó∂ËÆ∞ÂΩïÔºå‰æãÂ¶ÇÔºåcontext.forward()ÂºïÂèëÁöÑprocess()ÂáΩÊï∞Ë∞ÉÁî®ÔºåËæìÂá∫ËÆ∞ÂΩïÁöÑÊó∂Èó¥Êà≥ÊòØÁªßÊâøËá™ËæìÂÖ•Áõ¥Êé•ËÆ∞ÂΩïÊó∂Èó¥Êà≥„ÄÇ When new output records are generated via periodic functions such aspunctuate(), the output record timestamp is defined as the currentinternal time (obtained through context.timestamp()) of the streamtask. ÂΩìÊñ∞ÁöÑËæìÂá∫ËÆ∞ÂΩïÈÄöËøáËØ∏Â¶Çpunctuate()Âë®ÊúüÂáΩÊï∞ÁîüÊàêÁöÑËæìÂá∫ËÆ∞ÂΩïÊó∂Èó¥Êà≥ÂÆö‰πâ‰∏∫ÂΩìÂâçÁöÑÂÜÖÈÉ®Êó∂Èó¥Ôºàcontext.timestamp()Ëé∑ÂæóÔºâÁöÑÊµÅ‰ªªÂä°„ÄÇ For aggregations, the timestamp of a resulting aggregate updaterecord will be that of the latest arrived input record that triggeredthe update. ÂØπ‰∫é‰∏Ä‰∏™ËÅöÂêàÔºåÂΩ¢ÊàêÁöÑËÅöÂêàÊõ¥Êñ∞ËÆ∞ÂΩïÁöÑÊó∂Èó¥Êà≥Â∞ÜÊúÄÊñ∞Âà∞ËææÁöÑËæìÂÖ•ËÆ∞ÂΩïËß¶ÂèëÊõ¥Êñ∞„ÄÇ States Áä∂ÊÄÅSome stream processing applications don‚Äôt require state, which means the processing of a message is independent from the processing of all other messages. However, being able to maintain state opens up many possibilities for sophisticated stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.‰∏Ä‰∫õÊµÅÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫è‰∏çÈúÄË¶ÅÁä∂ÊÄÅÔºåËøôÊÑèÂë≥ÁùÄÊ∂àÊÅØÁöÑÂ§ÑÁêÜÊòØÁã¨Á´ã‰∫éÊâÄÊúâÂÖ∂‰ªñÊ∂àÊÅØÁöÑÂ§ÑÁêÜ„ÄÇÁÑ∂ËÄåÔºåËÉΩÂ§ü‰øùÊåÅÁä∂ÊÄÅ‰∏∫Â§çÊùÇÁöÑÊµÅÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫èÊâìÂºÄ‰∫ÜËÆ∏Â§öÂèØËÉΩÊÄßÔºöÊÇ®ÂèØ‰ª•Âä†ÂÖ•ËæìÂÖ•ÊµÅÔºåÊàñÁªÑÂíåÊ±áÊÄªÊï∞ÊçÆËÆ∞ÂΩï„ÄÇËÆ∏Â§öËøôÊ†∑ÁöÑÁä∂ÊÄÅÁöÑÊìç‰ΩúÔºåÁî±Kafka Streams DSLÊèê‰æõ„ÄÇ Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data. This is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a persistent key-value store, an in-memory hashmap, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.Kafka StreamsÊèê‰æõ‰∫ÜÊâÄË∞ìÁöÑÁä∂ÊÄÅÂ≠òÂÇ®ÔºåÂÆÉÂèØ‰ª•Áî®‰∫éÊµÅÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫èÊù•Â≠òÂÇ®ÂíåÊü•ËØ¢Êï∞ÊçÆ„ÄÇÂΩìÂÆûÊñΩÁä∂ÊÄÅÊìç‰ΩúÊó∂ËøôÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑËÉΩÂäõ„ÄÇÂú®Kafka StreamsÁöÑÊØè‰∏ÄÈ°π‰ªªÂä°ÁöÑ‰∏Ä‰∏™ÊàñÂ§ö‰∏™Áä∂ÊÄÅÂ≠òÂÇ®Â∞ÜÂèØ‰ª•ÈÄöËøáAPIÊù•Â≠òÂÇ®ÂíåÊü•ËØ¢Â§ÑÁêÜÊâÄÈúÄÁöÑÊï∞ÊçÆËÆøÈóÆ„ÄÇËøô‰∫õÁä∂ÊÄÅÂ≠òÂÇ®ÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÊåÅÁª≠ÁöÑÈîÆÂÄºÂ≠òÂÇ®ÔºåÂÜÖÂ≠ò‰∏≠ÁöÑHashMapÔºåÊàñÂè¶‰∏Ä‰∏™Êñπ‰æøÁöÑÊï∞ÊçÆÁªìÊûÑ„ÄÇKafka StreamsÊèê‰æõ‰∫ÜÂÆπÈîôÂíåÊú¨Âú∞Áä∂ÊÄÅÂ≠òÂÇ®ÁöÑËá™Âä®ÊÅ¢Â§ç„ÄÇ Kafka Streams allows direct read-only queries of the state stores by methods, threads, processes or applications external to the stream processing application that created the state stores. This is provided through a feature called Interactive Queries. All stores are named and Interactive Queries exposes only the read operations of the underlying implementation.Kafka StreamsÂÖÅËÆ∏ÈÄöËøáÊñπÊ≥ïÔºåÁ∫øÁ®ãÔºåËøõÁ®ãÊàñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ñÈÉ®ÁöÑÊµÅÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫èÂàõÂª∫ÁöÑÁä∂ÊÄÅÂ≠òÂÇ®ÁöÑÁä∂ÊÄÅÂ≠òÂÇ®ÁöÑÁõ¥Êé•Âè™ËØªÊü•ËØ¢„ÄÇËøôÊòØÈÄöËøá‰∏Ä‰∏™Ë¢´Áß∞‰∏∫‰∫§‰∫íÂºèÊü•ËØ¢ÁöÑÂäüËÉΩ„ÄÇÊâÄÊúâÁöÑÂ≠òÂÇ®ÈÉΩË¢´ÂëΩÂêçÂíå‰∫§‰∫íÊü•ËØ¢Âè™ÂÖ¨ÂºÄÂ∫ïÂ±ÇÂÆûÁé∞ÁöÑËØªÊìç‰Ωú„ÄÇ As we have mentioned above, the computational logic of a Kafka Streams application is defined as a processor topology. Currently Kafka Streams provides two sets of APIs to define the processor topology, which will be described in the subsequent sections.Ê≠£Â¶ÇÊàë‰ª¨‰∏äÊñáÊâÄÊèêÂà∞ÁöÑÔºåKafka StreamsÂ∫îÁî®Á®ãÂ∫èÁöÑËÆ°ÁÆóÈÄªËæëË¢´ÂÆö‰πâ‰∏∫‰∏Ä‰∏™Â§ÑÁêÜtopology„ÄÇÁõÆÂâçÔºåKafka StreamsÊèê‰æõ‰∫Ü‰∏§ÁªÑÁöÑÊé•Âè£Êù•ÂÆö‰πâÂ§ÑÁêÜÁöÑtopologyÔºåËøôÂ∞ÜÂú®ÈöèÂêéÁöÑÁ´†ËäÇ‰∏≠ÊèèËø∞„ÄÇ Low-Level Processor APIProcessorDevelopers can define their customized processing logic by implementing the Processor interface, which provides process and punctuate methods. The process method is performed on each of the received record; and the punctuate method is performed periodically based on elapsed time. In addition, the processor can maintain the current ProcessorContext instance variable initialized in the init method, and use the context to schedule the punctuation period (context().schedule), to forward the modified / new key-value pair to downstream processors (context().forward), to commit the current processing progress (context().commit), etc.ÂºÄÂèëËÄÖÂèØ‰ª•ÈÄöËøáÂ§ÑÁêÜÂô®Êé•Âè£ÂÆö‰πâËá™Â∑±ÁöÑÂÆöÂà∂ÁöÑÂ§ÑÁêÜÈÄªËæëÔºåÂÆÉÊèê‰æõ‰∫ÜÊñπÊ≥ïÂíåÊ†áÁÇπÁöÑÊñπÊ≥ï„ÄÇprocessÊñπÊ≥ïÊòØÂØπÊØè‰∏™Êé•Êî∂ÁöÑËÆ∞ÂΩïÊâßË°åÔºõÂíåÊ†áÁÇπÊ≥ïÊòØÂü∫‰∫éÊó∂Èó¥ËøõË°åÂÆöÊúü„ÄÇÊ≠§Â§ñÔºåËØ•Â§ÑÁêÜÂô®ÂèØ‰ª•Áª¥ÊåÅÁõÆÂâçÁöÑProcessorContextÂÆû‰æãÂèòÈáèÂú®initÊñπÊ≥ïÂàùÂßãÂåñÔºåÂπ∂‰ΩøÁî®contextÂÆâÊéíÊ†áÁÇπÁ¨¶Âè∑Âë®ÊúüÔºàcontext().scheduleÔºâÔºåÊèêÂá∫‰øÆÊîπ/Êñ∞ÁöÑÈîÆÂÄºÂØπ‰∏ãÊ∏∏Â§ÑÁêÜÂô®Ôºàcontext().forwardÔºâÔºåÊääÂΩìÂâçÁöÑÂ§ÑÁêÜËøõÂ∫¶Ôºàcontext().commitÔºâÔºåÁ≠â„ÄÇ 1234567891011121314151617181920212223242526272829303132333435363738public class MyProcessor extends Processor &#123; private ProcessorContext context; private KeyValueStore kvStore; @Override @SuppressWarnings("unchecked") public void init(ProcessorContext context) &#123; this.context = context; this.context.schedule(1000); this.kvStore = (KeyValueStore) context.getStateStore("Counts"); &#125; @Override public void process(String dummy, String line) &#123; String[] words = line.toLowerCase().split(" "); for (String word : words) &#123; Integer oldValue = this.kvStore.get(word); if (oldValue == null) &#123; this.kvStore.put(word, 1); &#125; else &#123; this.kvStore.put(word, oldValue + 1); &#125; &#125; &#125; @Override public void punctuate(long timestamp) &#123; KeyValueIterator iter = this.kvStore.all(); while (iter.hasNext()) &#123; KeyValue entry = iter.next(); context.forward(entry.key, entry.value.toString()); &#125; iter.close(); context.commit(); &#125; @Override public void close() &#123; this.kvStore.close(); &#125; &#125;; In the above implementation, the following actions are performed:Âú®‰∏äÈù¢ÁöÑÂÆûÁé∞‰∏≠ÔºåÊâßË°å‰ª•‰∏ãÊìç‰ΩúÔºö In the init method, schedule the punctuation every 1 second andretrieve the local state store by its name ‚ÄúCounts‚Äù. Âú®initÊñπÊ≥ïÔºåscheduleÊØè1ÁßíÂíåÊ†áÁÇπÁ¨¶Âè∑Ê£ÄÁ¥¢Êú¨Âú∞Áä∂ÊÄÅÂ≠òÂÇ®Áî±ÂÆÉÁöÑÂêçÁß∞‚ÄúËÆ°Êï∞‚Äù„ÄÇ In the process method, upon each received record, split the valuestring into words, and update their counts into the state store (wewill talk about this feature later in the section). Âú®Â§ÑÁêÜÊñπÊ≥ï‰∏≠ÔºåÂú®ÊØè‰∏™Êé•Êî∂Âà∞ÁöÑËÆ∞ÂΩï‰∏≠ÔºåÂ∞ÜÂÄºÂ≠óÁ¨¶‰∏≤ÂàÜÂâ≤ÊàêÂçïËØçÔºåÂπ∂Êõ¥Êñ∞‰ªñ‰ª¨ÁöÑËÆ°Êï∞Âà∞Áä∂ÊÄÅÂ≠òÂÇ®Âå∫ÔºàÊàë‰ª¨Â∞ÜÂú®Êú¨ËäÇ‰∏≠ËÆ®ËÆ∫Ëøô‰∏™ÂäüËÉΩÔºâ„ÄÇ In the punctuate method, iterate the local state store and send theaggregated counts to the downstream processor, and commit the currentstream state. Âú®Ê†áÁÇπÊ≥ïÔºåËø≠‰ª£Â±ÄÈÉ®Áä∂ÊÄÅÂ≠òÂÇ®ÂíåÂèëÈÄÅÊ±áÊÄªËÆ°Êï∞Âà∞‰∏ãÊ∏∏ÁöÑÂ§ÑÁêÜÂô®ÔºåÂπ∂ÊâøËØ∫ÁõÆÂâçÊµÅÁä∂ÊÄÅ„ÄÇ Processor TopologyWith the customized processors defined in the Processor API, developers can use the TopologyBuilder to build a processor topology by connecting these processors together:‰∏éÂÆöÂà∂ÁöÑÂ§ÑÁêÜÂô®Âú®Processor APIÔºåÂºÄÂèëËÄÖÂèØ‰ª•‰ΩøÁî®TopologyBuilderÊù•ËøûÊé•Ëøô‰∫õÂ§ÑÁêÜÂô®‰∏ÄËµ∑Âª∫Á´ã‰∏Ä‰∏™topologyÔºö12345678TopologyBuilder builder = new TopologyBuilder();builder.addSource("SOURCE", "src-topic") .addProcessor("PROCESS1", MyProcessor1::new /* the ProcessorSupplier that can generate MyProcessor1 */, "SOURCE") .addProcessor("PROCESS2", MyProcessor2::new /* the ProcessorSupplier that can generate MyProcessor2 */, "PROCESS1") .addProcessor("PROCESS3", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") .addSink("SINK1", "sink-topic1", "PROCESS1") .addSink("SINK2", "sink-topic2", "PROCESS2") .addSink("SINK3", "sink-topic3", "PROCESS3"); There are several steps in the above code to build the topology, and here is a quick walk through:Âú®‰∏äÈù¢ÁöÑ‰ª£Á†Å‰∏≠ÊúâÂá†‰∏™Ê≠•È™§Êù•ÊûÑÂª∫topologyÔºåËøôÈáåÊòØ‰∏Ä‰∏™Âø´ÈÄüÁöÑÊ≠•Ë°åÈÄöËøáÔºö First of all a source node named ‚ÄúSOURCE‚Äù is added to the topologyusing the addSource method, with one Kafka topic ‚Äúsrc-topic‚Äù fed toit. È¶ñÂÖàÔºåÊ∫êËäÇÁÇπÂëΩÂêç‰∏∫‚ÄúÊ∫ê‚ÄùÊ∑ªÂä†Âà∞topology‰ΩøÁî®addSourceÊñπÊ≥ïÔºå‰ΩøÁî®‚Äúsrc-topic‚ÄùËøô‰∏™Kafka ‰∏ªÈ¢ò„ÄÇ Three processor nodes are then added using the addProcessor method;here the first processor is a child of the ‚ÄúSOURCE‚Äù node, but is theparent of the other two processors. ‰∏â‰∏™processorËäÇÁÇπÔºåÁÑ∂Âêé‰ΩøÁî®addProcessorÊñπÊ≥ïÊ∑ªÂä†ÔºõËøôÈáåÁ¨¨‰∏Ä‰∏™processorÊòØ‰∏Ä‰∏™‚ÄúÊ∫ê‚ÄùËäÇÁÇπÁöÑÂ≠êËäÇÁÇπÔºå‰ΩÜÊòØÂÖ∂‰ªñ‰∏§Â§Ñprocessors Finally three sink nodes are added to complete the topology using theaddSink method, each piping from a different parent processor nodeand writing to a separate topic. ÊúÄÂêéÔºåÊ∑ªÂä†‰∏â‰∏™Ê±áËÅöËäÇÁÇπÊù•ÂÆåÊàê‰ΩøÁî®ÁöÑtopology addSinkÊñπÊ≥ïÔºåÊØè‰∏™ÁÆ°ÈÅì‰ªé‰∏çÂêåÁöÑÁà∂ËäÇÁÇπÁöÑprocessorÂíåÂÜôÂà∞‰∏Ä‰∏™ÂçïÁã¨ÁöÑ‰∏ªÈ¢ò„ÄÇ Local State StoreÊú¨Âú∞Áä∂ÊÄÅÂ≠òÂÇ®Note that the Processor API is not limited to only accessing the current records as they arrive, but can also maintain local state stores that keep recently arrived records to use in stateful processing operations such as aggregation or windowed joins. To take advantage of this local states, developers can use the TopologyBuilder.addStateStore method when building the processor topology to create the local state and associate it with the processor nodes that needs to access it; or they can connect a created local state store with the existing processor nodes through TopologyBuilder.connectProcessorAndStateStores.Ê≥®ÊÑèProcessor API‰∏ç‰ªÖÈôê‰∫éËÆøÈóÆÂΩìÂâçËÆ∞ÂΩïÔºåËøòÂèØ‰ª•Áª¥ÊåÅÊú¨Âú∞Áä∂ÊÄÅÊùëÁ≤óÔºå‰ΩøËÆ∞ÂΩï‰ΩøÁî®Áä∂ÊÄÅÁöÑÂ§ÑÁêÜÊìç‰ΩúÔºåÂ¶ÇËÅöÈõÜÊàñÁ™óÂè£ÁöÑÂä†ÂÖ•„ÄÇÂà©Áî®Â±ÄÈÉ®Áä∂ÊÄÅÔºåÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•‰ΩøÁî®TopologyBuilder.addStateStoreÊñπÊ≥ïÂΩìÊê≠Âª∫processor topologyÂàõÂª∫Êú¨Âú∞Áä∂ÊÄÅÔºåÂÆÉ‰∏éprocessorËäÇÁÇπÈúÄË¶ÅËÆøÈóÆÂÆÉÁöÑËÅîÊÉ≥ÔºõÊàñËÄÖ‰ªñ‰ª¨ÂèØ‰ª•ËøûÊé•ÂàõÂª∫ÁöÑÂ±ÄÈÉ®Áä∂ÊÄÅÂ≠òÂÇ®‰∏éÁé∞ÊúâÁöÑÂ§ÑÁêÜÂô®ËäÇÁÇπÈÄöËøáTopologyBuilder.connectProcessorAndStateStores.123456789101112TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", "src-topic") .addProcessor("PROCESS1", MyProcessor1::new, "SOURCE") // create the in-memory state store "COUNTS" associated with processor "PROCESS1" .addStateStore(Stores.create("COUNTS").withStringKeys().withStringValues().inMemory().build(), "PROCESS1") .addProcessor("PROCESS2", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") .addProcessor("PROCESS3", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") // connect the state store "COUNTS" with processor "PROCESS2" .connectProcessorAndStateStores("PROCESS2", "COUNTS"); .addSink("SINK1", "sink-topic1", "PROCESS1") .addSink("SINK2", "sink-topic2", "PROCESS2") .addSink("SINK3", "sink-topic3", "PROCESS3"); ‰æãÂ≠êÔºöÊê≠Âª∫kafka_2.11-0.10.1.0 ÈõÜÁæ§ÔºöËÆæÁΩÆserver.properties1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#brokerÁöÑÂÖ®Â±ÄÂîØ‰∏ÄÁºñÂè∑Ôºå‰∏çËÉΩÈáçÂ§çbroker.id=0#Áî®Êù•ÁõëÂê¨ÈìæÊé•ÁöÑÁ´ØÂè£ÔºåproducerÊàñconsumerÂ∞ÜÂú®Ê≠§Á´ØÂè£Âª∫Á´ãËøûÊé•port=9092#Â§ÑÁêÜÁΩëÁªúËØ∑Ê±ÇÁöÑÁ∫øÁ®ãÊï∞Èáènum.network.threads=3#Áî®Êù•Â§ÑÁêÜÁ£ÅÁõòIOÁöÑÁ∫øÁ®ãÊï∞Èáènum.io.threads=8#ÂèëÈÄÅÂ•óÊé•Â≠óÁöÑÁºìÂÜ≤Âå∫Â§ßÂ∞èsocket.send.buffer.bytes=102400#Êé•ÂèóÂ•óÊé•Â≠óÁöÑÁºìÂÜ≤Âå∫Â§ßÂ∞èsocket.receive.buffer.bytes=102400#ËØ∑Ê±ÇÂ•óÊé•Â≠óÁöÑÁºìÂÜ≤Âå∫Â§ßÂ∞èsocket.request.max.bytes=104857600#kafkaËøêË°åÊó•ÂøóÂ≠òÊîæÁöÑË∑ØÂæÑlog.dirs=/home/hadoop/apps/kafka_2.11-0.10.1.0/logs/kafka#topicÂú®ÂΩìÂâçbroker‰∏äÁöÑÂàÜÁâá‰∏™Êï∞num.partitions=2#Áî®Êù•ÊÅ¢Â§çÂíåÊ∏ÖÁêÜdata‰∏ãÊï∞ÊçÆÁöÑÁ∫øÁ®ãÊï∞Èáènum.recovery.threads.per.data.dir=1#segmentÊñá‰ª∂‰øùÁïôÁöÑÊúÄÈïøÊó∂Èó¥ÔºåË∂ÖÊó∂Â∞ÜË¢´Âà†Èô§log.retention.hours=168#ÊªöÂä®ÁîüÊàêÊñ∞ÁöÑsegmentÊñá‰ª∂ÁöÑÊúÄÂ§ßÊó∂Èó¥log.roll.hours=168#Êó•ÂøóÊñá‰ª∂‰∏≠ÊØè‰∏™segmentÁöÑÂ§ßÂ∞èÔºåÈªòËÆ§‰∏∫1Glog.segment.bytes=1073741824#Âë®ÊúüÊÄßÊ£ÄÊü•Êñá‰ª∂Â§ßÂ∞èÁöÑÊó∂Èó¥log.retention.check.interval.ms=300000#Êó•ÂøóÊ∏ÖÁêÜÊòØÂê¶ÊâìÂºÄlog.cleaner.enable=true#brokerÈúÄË¶Å‰ΩøÁî®zookeeper‰øùÂ≠òmetaÊï∞ÊçÆzookeeper.connect=www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka0.10.1.0#zookeeperÈìæÊé•Ë∂ÖÊó∂Êó∂Èó¥zookeeper.connection.timeout.ms=6000#partion buffer‰∏≠ÔºåÊ∂àÊÅØÁöÑÊù°Êï∞ËææÂà∞ÈòàÂÄºÔºåÂ∞ÜËß¶ÂèëflushÂà∞Á£ÅÁõòlog.flush.interval.messages=10000#Ê∂àÊÅØbufferÁöÑÊó∂Èó¥ÔºåËææÂà∞ÈòàÂÄºÔºåÂ∞ÜËß¶ÂèëflushÂà∞Á£ÅÁõòlog.flush.interval.ms=3000#Âà†Èô§topicÈúÄË¶Åserver.properties‰∏≠ËÆæÁΩÆdelete.topic.enable=trueÂê¶ÂàôÂè™ÊòØÊ†áËÆ∞Âà†Èô§delete.topic.enable=true#Ê≠§Â§ÑÁöÑhost.name‰∏∫Êú¨Êú∫IP(ÈáçË¶Å),Â¶ÇÊûú‰∏çÊîπ,ÂàôÂÆ¢Êà∑Á´Ø‰ºöÊäõÂá∫:Producer connection to localhost:9092 unsuccessful ÈîôËØØ!host.name=www.hadoop01.com ÂàÜÂèëÂà∞ÂÖ∂‰ªñÊú∫Âô®12scp -rp kafka_2.11-0.10.1.0/ hadoop@www.hadoop02.com:/home/hadoop/apps/scp -rp kafka_2.11-0.10.1.0/ hadoop@www.hadoop03.com:/home/hadoop/apps/ ‰øÆÊîπserver.properties ‰ª•‰∏ã‰∏§‰∏™ÂèÇÊï∞12broker.id=0host.name=www.hadoop01.com zookeeperËÆæÁΩÆÔºåÂêØÂä®Áï•„ÄÇ ÂàõÂª∫‰∏ªÈ¢ò 12345678910[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --replication-factor 1 --partitions 1 --topic wordsCreated topic "words".[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --replication-factor 1 --partitions 1 --topic countsCreated topic "counts".[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --describe --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --topic words Topic:words PartitionCount:1 ReplicationFactor:1 Configs: Topic: words Partition: 0 Leader: 2 Replicas: 2 Isr: 2[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --describe --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --topic counts Topic:counts PartitionCount:1 ReplicationFactor:1 Configs: Topic: counts Partition: 0 Leader: 2 Replicas: 2 Isr: 2 ‰æùÊ¨°ÂêØÂä®Ôºö 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.Arrays;import java.util.Properties;import java.util.concurrent.atomic.AtomicLong;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.DoubleDeserializer;import org.apache.kafka.common.serialization.IntegerDeserializer;import org.apache.kafka.common.serialization.StringDeserializer;public class DemoConsumerManualCommit &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; "www.hadoop01.com:9092", "gender-amount", "group4", "consumer2" &#125;; if (args == null || args.length != 4) &#123; System.err.println( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"); System.exit(1); &#125; String bootstrap = args[0]; String topic = args[1]; String groupid = args[2]; String clientid = args[3]; Properties props = new Properties(); props.put("bootstrap.servers", bootstrap); props.put("group.id", groupid); props.put("enable.auto.commit", "false"); props.put("key.deserializer", StringDeserializer.class.getName()); //props.put("value.deserializer", DoubleDeserializer.class.getName()); props.put("value.deserializer", IntegerDeserializer.class.getName()); props.put("max.poll.interval.ms", "300000"); props.put("max.poll.records", "500"); props.put("auto.offset.reset", "earliest"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(topic)); AtomicLong atomicLong = new AtomicLong(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); records.forEach(record -&gt; &#123; System.out.printf("client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); if (atomicLong.get() % 10 == 0) &#123;// consumer.commitSync(); &#125; &#125;); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839import java.io.IOException;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.IntegerSerializer;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.kafka.common.serialization.StringSerializer;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.TopologyBuilder;import org.apache.kafka.streams.state.Stores;public class WordCountTopology &#123; public static void main(String[] args) throws IOException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount-processor"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "www.hadoop01.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "www.hadoop01.com:2181/kafka0.10.1.0"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", new StringDeserializer(), new StringDeserializer(), "words") .addProcessor("WordCountProcessor", WordCountProcessor::new, "SOURCE") .addStateStore(Stores.create("Counts").withStringKeys().withIntegerValues().inMemory().build(), "WordCountProcessor")// .connectProcessorAndStateStores("WordCountProcessor", "Counts") .addSink("SINK", "count", new StringSerializer(), new IntegerSerializer(), "WordCountProcessor"); KafkaStreams stream = new KafkaStreams(builder, props); stream.start(); System.in.read(); stream.close(); stream.cleanUp(); &#125;&#125; ÂêØÂä®producer12kafka-console-producer.sh --broker-list www.hadoop01.com:9092 --topic wordshello apache hello kafka Êä•ÈîôÔºö123456Exception in thread "StreamThread-1" org.apache.kafka.streams.errors.StreamsException: Extracted timestamp value is negative, which is not allowed. at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:111) at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117) at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:144) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:415) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:242) ‰∏∫Êó∂Èó¥Êà≥ÁöÑÂéüÂõ†kafka 18MayÁöÑÊó∂ÂÄô‰ΩïÂ¶Ç‰∫ÜÊó∂Èó¥Êà≥ÁöÑ‰∏ú‰∏úÂêéÊù•Êõ¥Êîπ‰∫Ü‰∏ªÈ¢òÔºö12bin/kafka-topics.sh --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --create --topic word --partitions 1 --replication-factor 1 --config message.timestamp.type=LogAppendTimebin/kafka-topics.sh --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --create --topic count --partitions 1 --replication-factor 1 --config message.timestamp.type=LogAppendTime ÂÖàÂêØÂä®DemoConsumerManualCommitÂÜçÂêØÂä®WordCountTopology ÂëΩ‰ª§Ë°åËæìÂÖ•12kafka-console-producer.sh --broker-list www.hadoop01.com:9092 --topic word_testhello apache kafka hello apache spark hello storm ÊúÄÂêéÊéßÂà∂Âè∞ËæìÂá∫ Â∫îËØ•ÊòØ12345client : consumer2 , topic: count , partition: 0 , offset = 0, key = apache, value = 2client : consumer2 , topic: count , partition: 0 , offset = 1, key = hello, value = 3client : consumer2 , topic: count , partition: 0 , offset = 2, key = kafka, value = 1client : consumer2 , topic: count , partition: 0 , offset = 3, key = spark, value = 1client : consumer2 , topic: count , partition: 0 , offset = 4, key = storm, value = 1 Êü•ÁúãÊâÄÊúâ‰∏ªÈ¢ò1bin/kafka-topics.sh --list --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 ÂèëÁé∞Â§ö‰∫Ü‰∏Ä‰∏™streams-wordcount-processor-Counts-changelog In the next section we present another way to build the processor topology: the Kafka Streams DSL. High-Level Streams DSLTo build a processor topology using the Streams DSL, developers can apply the KStreamBuilder class, which is extended from the TopologyBuilder. A simple example is included with the source code for Kafka in the streams/examples package. The rest of this section will walk through some code to demonstrate the key steps in creating a topology using the Streams DSL, but we recommend developers to read the full example source codes for details.‰ΩøÁî®DSL StreamsÂàõÂª∫topologyÔºåÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•Â∫îÁî®KStreamBuilderÁ±ªÔºåËøôÊòØ‰ªéTopologyBuilderÂª∂‰º∏„ÄÇ‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰æãÂ≠êÊòØÂåÖÂê´‰∫Üstreams/examples packageÁöÑÊ∫ê‰ª£Á†Å„ÄÇÊú¨ËäÇÁöÑÂÖ∂‰ΩôÈÉ®ÂàÜÂ∞ÜÈÄöËøá‰∏Ä‰∫õ‰ª£Á†ÅÊù•ÊºîÁ§∫‰ΩøÁî®ÊµÅDSLÂàõÂª∫topologyÁöÑÂÖ≥ÈîÆÊ≠•È™§Ôºå‰ΩÜÊàë‰ª¨Âª∫ËÆÆÂºÄÂèëËÄÖÈòÖËØªÂÆåÊï¥ÁöÑÁ§∫‰æãÊ∫ê‰ª£Á†ÅÁöÑÁªÜËäÇ„ÄÇ KStream and KTable The DSL uses two main abstractions. A KStream is an abstraction of a record stream, where each data record represents a self-contained datum in the unbounded data set. A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesn‚Äôt exist yet, the update will be considered a create). To illustrate the difference between KStreams and KTables, let‚Äôs imagine the following two data records are being sent to the stream: (‚Äúalice‚Äù, 1) ‚Äì&gt; (‚Äúalice‚Äù, 3). If these records a KStream and the stream processing application were to sum the values it would return 4. If these records were a KTable, the return would be 3, since the last record would be considered as an update. DSL‰ΩøÁî®ÁöÑ‰∏§Áßç‰∏ªË¶ÅÁöÑÊäΩË±°„ÄÇ‰∏Ä‰∏™KStreamÊòØËÆ∞ÂΩïÊµÅÁöÑ‰∏Ä‰∏™ÊäΩË±°ÁöÑÊ¶ÇÂøµÔºåÂÖ∂‰∏≠ÊØè‰∏™Êï∞ÊçÆËÆ∞ÂΩï‰ª£Ë°®‰∏Ä‰∏™Áã¨Á´ãÁöÑÊï∞ÊçÆÁöÑÊï∞ÊçÆÈõÜ„ÄÇ‰∏Ä‰∏™KTableÊòØ‰∏Ä‰∏™ÂèòÊõ¥ÁöÑÊµÅÁöÑ‰∏Ä‰∏™ÊäΩË±°ÁöÑÊ¶ÇÂøµÔºåÂÖ∂‰∏≠ÊØè‰∏™Êï∞ÊçÆËÆ∞ÂΩïÁöÑÊõ¥Êñ∞„ÄÇÊõ¥Á≤æÁ°ÆÂú∞ËØ¥ÔºåÊï∞ÊçÆËÆ∞ÂΩï‰∏≠ÁöÑÂÄºË¢´ËÆ§‰∏∫ÊòØÂêå‰∏Ä‰∏™ËÆ∞ÂΩïÈîÆÁöÑÊúÄÂêé‰∏Ä‰∏™ÂÄºÁöÑÊõ¥Êñ∞ÔºåÂ¶ÇÊûúÊúâÔºàÂ¶ÇÊûú‰∏Ä‰∏™Áõ∏Â∫îÁöÑkey‰∏çÂ≠òÂú®ÔºåÂàôËØ•Êõ¥Êñ∞Â∞ÜË¢´ËßÜ‰∏∫‰∏Ä‰∏™ÂàõÂª∫Ôºâ„ÄÇËØ¥ÊòékstreamsÂíåKTables‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåËÆ©Êàë‰ª¨ÊÉ≥Ë±°‰∏Ä‰∏ã‰ª•‰∏ã‰∏§‰∏™Êï∞ÊçÆËÆ∞ÂΩïË¢´ÂèëÈÄÅÂà∞ÊµÅÔºö(‚Äúalice‚Äù, 1)‚Äî‚Äî&gt;(‚Äúalice‚Äù, 3)„ÄÇÂ¶ÇÊûúËøô‰∫õËÆ∞ÂΩïKStreamÂíåÊµÅÂ§ÑÁêÜÂ∫îÁî®ËøõË°åÊÄªÁªìÁöÑÂÄºÂ∞ÜËøîÂõû4„ÄÇÂ¶ÇÊûúËøô‰∫õËÆ∞ÂΩïÊòØ‰∏Ä‰∏™KTableÔºåËøîÂõûÁöÑËøîÂõûÁöÑÂ∞ÜÊòØ3ÔºåÂõ†‰∏∫ËøáÂéªÁöÑËÆ∞ÂΩïÂ∞ÜË¢´ËßÜ‰∏∫‰∏ÄÁßçÊõ¥Êñ∞„ÄÇ Create Source Streams from KafkaÂàõÂª∫Kafka Streams Either a record stream (defined as KStream) or a changelog stream (defined as KTable) can be created as a source stream from one or more Kafka topics (for KTable you can only create the source stream from a single topic).‰∏Ä‰∏™ËÆ∞ÂΩïÊµÅÔºàÂÆö‰πâ‰∏∫KStreamÔºâÊàñÊõ¥Êñ∞ÊµÅÔºàÂÆö‰πâ‰∏∫KTableÔºâÂèØ‰ª•ÂàõÂª∫‰ªé‰∏Ä‰∏™ÊàñÂ§ö‰∏™Kafka‰∏ªÈ¢òÁöÑÊ∫êÊµÅÔºà‰∏∫KTable‰Ω†Âè™ËÉΩ‰ªé‰∏Ä‰∏™Âçï‰∏ÄÁöÑ‰∏ªÈ¢òÂàõÂª∫Ê∫êÊµÅÔºâ„ÄÇ 123KStreamBuilder builder = new KStreamBuilder();KStream source1 = builder.stream("topic1", "topic2");KTable source2 = builder.table("topic3", "stateStoreName"); Windowing a stream Á™óÂè£ÊµÅA stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for join and aggregation operations, etc. Kafka Streams currently defines the following types of windows: ÊµÅÂ§ÑÁêÜÂô®ÂèØËÉΩÈúÄË¶ÅÂ∞ÜÊï∞ÊçÆËÆ∞ÂΩïÂàíÂàÜÊàêÊó∂Èó¥Ê°∂ÔºåÂç≥ÈÄöËøáÊó∂Èó¥Á™óÂè£ÁöÑÊµÅ„ÄÇËøôÈÄöÂ∏∏ÊòØÈúÄË¶ÅÁöÑËøûÊé•ÂíåËÅöÂêàÊìç‰ΩúÁ≠â„ÄÇKafkaÊµÅÁõÆÂâçÂÆö‰πâ‰∫Ü‰ª•‰∏ãÁ±ªÂûãÁöÑÁ™óÂè£Ôºö Hopping time windows are windows based on time intervals. They modelfixed-sized, (possibly) overlapping windows. A hopping window isdefined by two properties: the window‚Äôs size and its advance interval(aka ‚Äúhop‚Äù). The advance interval specifies by how much a windowmoves forward relative to the previous one. For example, you canconfigure a hopping window with a size 5 minutes and an advanceinterval of 1 minute. Since hopping windows can overlap a data recordmay belong to more than one such windows.Ë∑≥Ë∑ÉÁöÑÊó∂Èó¥ÔºàË∑≥Êó∂?ÔºâÁ™óÂè£ÊòØÂü∫‰∫éÊó∂Èó¥Èó¥ÈöîÁöÑÁ™óÂè£„ÄÇ‰ªñ‰ª¨Ê®°ÂûãÂõ∫ÂÆöÂ§ßÂ∞èÁöÑÔºåÔºàÂèØËÉΩÔºâÈáçÂè†ÁöÑÁ™óÂè£„ÄÇË∑≥Ë∑ÉÁ™óÂè£ÊòØÁî±‰∏§‰∏™Â±ûÊÄßÂÆö‰πâÁöÑÔºöÁ™óÂè£ÁöÑÂ§ßÂ∞èÂíåÂÆÉÁöÑÊèêÂâçÈó¥ÈöîÔºàÂèàÂêç‚Äùhop‚ÄùÔºâ„ÄÇÊèêÂâçÈó¥ÈöîÁî±‰∏Ä‰∏™Á™óÂè£Áõ∏ÂØπ‰∫éÂâç‰∏Ä‰∏™Á™óÂè£ÁßªÂä®ÁöÑÂ§öÂ∞ëÊù•ÊåáÂÆö„ÄÇ‰æãÂ¶ÇÔºåÊÇ®ÂèØ‰ª•ÈÖçÁΩÆ‰∏Ä‰∏™ÂÖ∑ÊúâÂ§ßÂ∞è5ÂàÜÈíüÂíå‰∏Ä‰∏™ÊèêÂâçÈó¥Èöî1ÂàÜÈíüÁöÑË∑≥Ë∑ÉÁ™óÂè£„ÄÇÁî±‰∫éË∑≥Ë∑ÉÁ™óÂè£ÂèØ‰ª•ÈáçÂè†Êï∞ÊçÆËÆ∞ÂΩïÔºåÂèØËÉΩÂ±û‰∫éÂ§ö‰∏™ËøôÊ†∑ÁöÑÁ™óÂè£„ÄÇ Tumbling time windows are a special case of hopping time windows and,like the latter, are windows based on time intervals. They modelfixed-size, non-overlapping, gap-less windows. A tumbling window isdefined by a single property: the window‚Äôs size. A tumbling window isa hopping window whose window size is equal to its advance interval.Since tumbling windows never overlap, a data record will belong toone and only one window.ÁøªÊªöÊó∂Èó¥Á™óÂè£ÊòØ‰∏Ä‰∏™ÁâπÊÆäÁöÑÊÉÖÂÜµ‰∏ãÁöÑË∑≥Ë∑ÉÊó∂Èó¥Á™óÂè£ÔºåÂíåÂêéËÄÖ‰∏ÄÊ†∑ÔºåÊòØÂü∫‰∫éÊó∂Èó¥Èó¥ÈöîÁöÑÁ™óÂè£„ÄÇ‰ªñ‰ª¨Ê®°ÂûãÂõ∫ÂÆöÁöÑÂ§ßÂ∞èÔºå‰∏çÈáçÂè†ÔºåÊó†ÁºùÈöôÁöÑÁ™óÂè£„ÄÇ‰∏Ä‰∏™ÁøªÊªöÁ™óÂè£ÊòØÁî±‰∏Ä‰∏™Âçï‰∏ÄÂ±ûÊÄßÂÆö‰πâÁöÑÔºöÁ™óÂè£ÁöÑÂ§ßÂ∞è„ÄÇ‰∏Ä‰∏™ÁøªÊªöÁ™óÂè£ÊòØ‰∏Ä‰∏™Ë∑≥Ë∑ÉÁöÑÁ™óÂè£ÔºåÂÆÉÁöÑÁ™óÂè£Â§ßÂ∞èÁ≠â‰∫éÂÆÉÁöÑÈ¢ÑÂÖàÈó¥Èöî„ÄÇÁî±‰∫éÁøªÊªöÁöÑÁ™óÂè£Ê∞∏Ëøú‰∏ç‰ºöÈáçÂè†ÔºåÊï∞ÊçÆËÆ∞ÂΩïÂ∞ÜÂ±û‰∫é‰∏Ä‰∏™Âπ∂‰∏îÂè™Êúâ‰∏Ä‰∏™Á™óÂè£„ÄÇ Sliding windows model a fixed-size window that slides continuouslyover the time axis; here, two data records are said to be included inthe same window if the difference of their timestamps is within thewindow size. Thus, sliding windows are not aligned to the epoch, buton the data record timestamps. In Kafka Streams, sliding windows areused only for join operations, and can be specified through theJoinWindows class.ÊªëÂä®Á™óÂè£Ê®°Âûã‰∏Ä‰∏™Âõ∫ÂÆöÂ§ßÂ∞èÁöÑÁ™óÂè£ÔºåÂπªÁÅØÁâá‰∏çÊñ≠Âú®Êó∂Èó¥ËΩ¥‰∏äÔºõÂú®ËøôÈáåÔºå‰∏§‰∏™Êï∞ÊçÆËÆ∞ÂΩïÔºåËØ¥ÊòØÂ¶ÇÊûú‰ªñ‰ª¨ÁöÑÊó∂Èó¥Â∑ÆÂºÇÊòØÂú®Á™óÂè£ÁöÑÂ§ßÂ∞èÔºåÂåÖÊã¨Âú®Âêå‰∏Ä‰∏™Á™óÂè£„ÄÇÂõ†Ê≠§ÔºåÊªëÂä®Á™óÂè£‰∏ç‰∏ÄËá¥ÁöÑÊó∂‰ª£Ôºå‰ΩÜÂú®Êï∞ÊçÆËÆ∞ÂΩïÁöÑÊó∂Èó¥Êà≥„ÄÇKafkaÊµÅÔºåÊªëÂä®Á™óÂè£Âè™Áî®‰∫éËøûÊé•Êìç‰ΩúÔºåÂπ∂ÂèØ‰ª•ÈÄöËøáJoinWindowsÁ±ªÊåáÂÆö„ÄÇ JoinsA join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely. In Kafka Streams, you may perform the following join operations:‰∏Ä‰∏™ËøûÊé•Êìç‰ΩúÂü∫‰∫éÂÆÉ‰ª¨ÁöÑÊï∞ÊçÆËÆ∞ÂΩïÁöÑÈîÆÂêàÂπ∂‰∏§‰∏™ÊµÅÔºåÂπ∂‰∫ßÁîü‰∏Ä‰∏™Êñ∞ÁöÑÊµÅ„ÄÇÂú®ËÆ∞ÂΩïÊµÅÁöÑÂä†ÂÖ•ÈÄöÂ∏∏ÈúÄË¶ÅÊâßË°åÂú®ËßÜÁ™óÂü∫Á°ÄÂê¶ÂàôËÆ∞ÂΩïÂøÖÈ°ª‰øùÊåÅÂ±•Ë°åÂä†ÂÖ•ÁöÑÊï∞ÈáèÂèØ‰ª•Êó†ÈôêÂ¢ûÈïø„ÄÇÂú®Kafka Streams‰∏≠ÔºåÊÇ®ÂèØ‰ª•ÊâßË°å‰ª•‰∏ãËøûÊé•Êìç‰ΩúÔºö KStream-to-KStream Joins are always windowed joins, since otherwisethe memory and state required to compute the join would growinfinitely in size. Here, a newly received record from one of thestreams is joined with the other stream‚Äôs records within thespecified window interval to produce one result for each matchingpair based on user-provided ValueJoiner. A new KStream instancerepresenting the result stream of the join is returned from thisoperator.KStream-to-KStream JoinsÊÄªÊòØÁ™óÂè£ËøûÊé•ÔºåÂê¶ÂàôÂÜÖÂ≠òÂíåËÆ°ÁÆóÊâÄÈúÄÁöÑjoin‰ºöÊó†ÈôêÂ¢ûÈïøÁöÑÂ§ßÂ∞è„ÄÇÂú®ËøôÈáåÔºåÊñ∞Êé•Êî∂ÁöÑËÆ∞ÂΩï‰ªé‰∏ÄÊù°Êï∞ÊçÆÊµÅ‰∏éÂÖ∂‰ªñÊµÅÁöÑËÆ∞ÂΩïÂú®ÊåáÂÆöÁöÑÁ™óÂè£Èó¥Èöî‰∏∫ÂØπÊØè‰∏Ä‰∏™ÂåπÈÖçÁöÑÂü∫‰∫éÁî®Êà∑Êèê‰æõÁöÑValueJoiner‰∫ßÁîüÁöÑ‰∏Ä‰∏™ÁªìÊûú„ÄÇ‰∏Ä‰∏™Êñ∞ÁöÑKStreamÂÆû‰æãË°®Á§∫ÁöÑÂä†ÂÖ•ÂØºËá¥ÊµÅ‰ªéËøô‰∏™Êìç‰ΩúÁ¨¶ËøîÂõû„ÄÇ KTable-to-KTable Joins are join operations designed to be consistentwith the ones in relational databases. Here, both changelog streamsare materialized into local state stores first. When a new record isreceived from one of the streams, it is joined with the otherstream‚Äôs materialized state stores to produce one result for eachmatching pair based on user-provided ValueJoiner. A new KTableinstance representing the result stream of the join, which is also achangelog stream of the represented table, is returned from thisoperator.KTable-to-KTable Joins Êìç‰ΩúËÆæËÆ°‰∏éÂÖ≥Á≥ªÊï∞ÊçÆÂ∫ì‰∏≠ÁöÑ‰∏ÄËá¥Ë°åÂä®„ÄÇÂú®ËøôÈáåÔºåÊó†ËÆ∫ÊòØ‰øÆÊîπÊµÅÁâ©ÂåñÂú®ÂΩìÂú∞ÂïÜÂ∫ó„ÄÇÂΩì‰∏Ä‰∏™Êñ∞ÁöÑËÆ∞ÂΩï‰ªé‰∏Ä‰∏™ÊµÅÁöÑÊé•Êî∂ÔºåÂÆÉ‰∏éÂÖ∂‰ªñÊµÅÁöÑÁâ©ÂåñÁä∂ÊÄÅÂ≠òÂÇ®‰∏∫ÂØπÊØè‰∏Ä‰∏™ÂåπÈÖçÁöÑÂü∫‰∫éÁî®Êà∑Êèê‰æõÁöÑValueJoiner‰∫ßÁîüÁöÑ‰∏Ä‰∏™ÁªìÊûú„ÄÇ‰∏Ä‰∏™Êñ∞ÁöÑKTableÂÆû‰æã‰ª£Ë°®‰∫ÜËøûÊé•ÊµÅÔºåËøô‰πüÊòØ‰∏Ä‰∏™‰ª£Ë°®Ë°®Êõ¥Êñ∞ÊµÅÔºåÊòØ‰ªéËøô‰∏™Êìç‰ΩúÁ¨¶ËøîÂõû„ÄÇ KStream-to-KTable Joins allow you to perform table lookups against achangelog stream (KTable) upon receiving a new record from anotherrecord stream (KStream). An example use case would be to enrich astream of user activities (KStream) with the latest user profileinformation (KTable). Only records received from the record streamwill trigger the join and produce results via ValueJoiner, not viceversa (i.e., records received from the changelog stream will be usedonly to update the materialized state store). A new KStream instancerepresenting the result stream of the join is returned from thisoperator.KStream-to-KTable JoinsÂÖÅËÆ∏‰Ω†ÊâßË°åË°®Êü•ÊâæÂíå‰øÆÊîπÊµÅÔºàktableÔºâÂú®‰ªéÂè¶‰∏Ä‰∏™ËÆ∞ÂΩïÊµÅÊé•Êî∂‰∏Ä‰∏™Êñ∞ÁöÑËÆ∞ÂΩïÔºàKStreamÔºâ„ÄÇ‰∏Ä‰∏™‰æãÂ≠ê‰ΩøÁî®Ê°à‰æãÂ∞Ü‰∏∞ÂØåÁî®Êà∑Ê¥ªÂä®ÊµÅÔºàKStreamÔºâÊúÄÊñ∞ÁöÑÁî®Êà∑ÈÖçÁΩÆÊñá‰ª∂‰ø°ÊÅØÔºàKTableÔºâ„ÄÇÂè™ËÆ∞ÂΩïÊî∂Âà∞ÁöÑËÆ∞ÂΩïÊµÅ‰ºöËß¶ÂèëËøûÊé•Âπ∂ÈÄöËøáValueJoiner‰∫ßÁîüÁªìÊûúÔºåÂèç‰πã‰∫¶ÁÑ∂ÔºàÂç≥ËÆ∞ÂΩïÊî∂Âà∞Êõ¥Êñ∞ÊµÅÂè™‰ºöË¢´Áî®Êù•Êõ¥Êñ∞Áâ©ÂåñÁä∂ÊÄÅÂ≠òÂÇ®Ôºâ„ÄÇ‰∏Ä‰∏™Êñ∞ÁöÑKStreamÂÆû‰æãË°®Á§∫ÁöÑÂä†ÂÖ•ÂØºËá¥ÊµÅ‰ªéËøô‰∏™Êìç‰ΩúÁ¨¶ËøîÂõû„ÄÇ Depending on the operands the following join operations are supported: inner joins, outer joins and left joins. Their semantics are similar to the corresponding operators in relational databases. aTransform a stream There is a list of transformation operations provided for KStream and KTable respectively. Each of these operations may generate either one or more KStream and KTable objects and can be translated into one or more connected processors into the underlying processor topology. All these transformation methods can be chained together to compose a complex processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types. Among these transformations, filter, map, mapValues, etc, are stateless transformation operations and can be applied to both KStream and KTable, where users can usually pass a customized function to these functions as a parameter, such as Predicate for filter, KeyValueMapper for map, etc:12// written in Java 8+, using lambda expressionsKStream mapped = source1.mapValue(record -&gt; record.get("category")); Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor; Stateful transformations, on the other hand, require accessing an associated state for processing and producing outputs. For example, in join and aggregate operations, a windowing state is usually used to store all the received records within the defined window boundary so far. The operators can then access these accumulated records in the store and compute based on them.1234567891011// written in Java 8+, using lambda expressionsKTable, Long&gt; counts = source1.groupByKey().aggregate( () -&gt; 0L, // initial value (aggKey, value, aggregate) -&gt; aggregate + 1L, // aggregating value TimeWindows.of("counts", 5000L).advanceBy(1000L), // intervals in milliseconds Serdes.Long() // serde for aggregated value);KStream joined = source1.leftJoin(source2, (record1, record2) -&gt; record1.get("user") + "-" + record2.get("region");); Write streams back to Kafka At the end of the processing, users can choose to (continuously) write the final resulted streams back to a Kafka topic through KStream.to and KTable.to.1joined.to("topic4"); If your application needs to continue reading and processing the records after they have been materialized to a topic via to above, one option is to construct a new stream that reads from the output topic; Kafka Streams provides a convenience method called through:12345// equivalent to//// joined.to("topic4");// materialized = builder.stream("topic4");KStream materialized = joined.through("topic4");]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hbase regionÂ§ÑÁêÜ]]></title>
    <url>%2F2016%2F12%2F01%2FRegionServer%2F</url>
    <content type="text"><![CDATA[ÂáèÂ∞ëRegionServerËäÇÁÇπÈÄÄÂΩπÔºöÂú®Âà∂ÂÆöËäÇÁÇπ‰∏äËøêË°åÔºåÂÅúÊ≠¢ÁöÑËäÇÁÇπ‰∏äÁöÑRegionServer1./bin/hbase-daemon.sh stop regionserver Â¶ÇÊûúÂú®ÂÖ≥Èó≠Êó∂HbaseÂ≠òÂú®Ê≠£Âú®ËøêË°åÁöÑË¥üËΩΩÂùáË°°ÔºåÈÇ£‰πàMaserÈíàÂØπRegionÁöÑÊÅ¢Â§çÊìç‰ΩúÂíåË¥üËΩΩÂùáË°°‰πãÂâçÂèØËÉΩ‰ºö‰∫ßÁîüÁ´û‰∫âÔºåÂ∞§ÂÖ∂ÊòØÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÔºåÁ¢∞Âà∞ÁöÑÂá†ÁéáÊõ¥Â§ßÔºåÊâÄ‰ª•Âª∫ËÆÆÁé∞ÈáëÁî®Ë¥üËΩΩÂùáË°° 12./hbase shell./hbase(main):001:0&gt;blance_switch false ÊªöÂä®ÈáçÂêØ1./bin/graceful_stop slave2 Â¶ÇÊûúÂêåÊó∂ÈÄÄÂΩπÂ§ö‰∏™ËäÇÁÇπÔºå‰∏∫‰∫ÜÈò≤Ê≠¢Ê≠£Âú®ÈÄÄÂΩπÁöÑËäÇÁÇπÊääregion moveÂà∞Âç≥Â∞ÜÈÄÄÂΩπÁöÑËäÇÁÇπ‰∏äÂèØ‰ª•Âà©Áî®zookeeperÔºåÂú®Hbase_root/training znode‰∏≠ÂàõÂª∫Ë¶ÅÈÄÄÂΩπÁöÑRegionServerÁöÑentryÊªöÂä®ÈáçÂêØ‰∏âÁßçÊñπÂºèÔºö‰ΩøÁî®rolling-restart.shËÑöÊú¨ÊâãÂ∑•ËøõË°åÊªöÂä®ÈáçÂêØÁºñÂÜôËá™ÂÆö‰πâÁöÑÊªöÂä®ÈáçÂêØËÑöÊú¨ Â§áMaster‰∏∫ÈÅøÂÖçMasterÊïÖÈöúÔºåÊ∑ªÂä†Â§áÁî®MasterÂú®ÊâÄÊúâËäÇÁÇπÁöÑconfÁõÆÂΩï‰∏ãÂàõÂª∫Êñá‰ª∂backup-mastersÂú®ÈáåÈù¢Ê∑ªÂä†Â§á‰ªΩ‰∏ªÊú∫Âêç ÂêØÂä®Ôºö1./bin/hbase-daemon.sh start master Â¶ÇÊûúÂèëÁé∞Á´ØÂè£ÂÜ≤Á™ÅÔºåÈúÄË¶Å‰øÆÊîπhbase-site.xml1234&lt;property&gt;&lt;name&gt;hbase.regionserver.port&lt;/name&gt;&lt;value&gt;16012&lt;/value&gt;&lt;/property&gt; Êü•ÁúãËøõÁ®ãÂç†Áî®Á´ØÂè£Âè∑Ôºö1netstat -an -p | grep -i PID Â¢ûÂä†ËäÇÁÇπÂú®conf/regionserversÂ¢ûÂä†ËäÇÁÇπÔºåÁõ¥Êé•start‰ºöË∑≥ËøáÂ∑≤ÁªèÂêØÂä®ÁöÑregionserver HbaseÂÜ∑Â§áÈúÄË¶ÅÂÅúÊ≠¢hbasedistcpËØ•ÂëΩ‰ª§‰Ωç‰∫éhdoopÁöÑtoolsÂåÖ‰∏≠hadoop distcp /hbase /hbasebackup hbase-site.xmlÈáåÈù¢Êúâhbase.rootdirÂèÇÊï∞ÔºåÂèØ‰ª•‰øÆÊîπ‰∏∫Â§á‰ªΩÂêéÁöÑË∑ØÂæÑÊù•ÊÅ¢Â§çÊï∞ÊçÆ Êìç‰ΩúÊµÅÁ®ãÔºö123456789101112131415161718192021222324./bin/hbase shellcreate 'testbackup' 'columnfamily'put 'testbackup','row1','columnfamily:column1','value1'scan 'testbackup'./bin/stop-hbase.shhadoop distcp /hbae /hbasebackup./bin/start-hbase.sh./bin/hbase shelldisable 'testbackup'drop 'testbackup'list./bin/stop-hbase.shhdfs dfs -mv /hbase /hbase_tmphadoop distcp -overwrite /hbasebackupd /hbase./bin/start-hbase.sh./bin/hbase shelllistscan 'testbackup' HbaseÁÉ≠Â§á‰∏çÈúÄË¶ÅÂÅúÊ≠¢hbaseÊñπÂºè‰∏ÄÔºöCopyTbleÊñπÂºè‰∫åÔºöExportÊñπÂºè‰∏âÔºöÈõÜÁæ§Â§çÂà∂ ÊñπÂºè‰∫å12345678910111213./bin/hbase org.apache.hadoop.hbase.mapreduce.Export testbackup /tmp/test./bin/hbase org.apache.hadoop.hbase.mapreduce.Export testbackup hdfs://www.hadoop01.com/test./bin/hbase shelldisbale testbackupdrop testbackupcreate 'testbackup' 'columnfamily'scan testbackup.Import testbackup /tmp/test./bin/hbase shellscan testbackup ÊñπÂºè‰∏ÄÔºö12345create 'mubiao' 'columnfamily'./bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=mubiao testbackup./hbase shellscan 'mubiao']]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker_install]]></title>
    <url>%2F2016%2F11%2F27%2Fdocker-install%2F</url>
    <content type="text"><![CDATA[1.Â¢ûÂä†yumÊ∫êÂú®ÂëΩ‰ª§Ë°åËæìÂÖ•Ôºö12345678cat &gt;/etc/yum.repos.d/docker.repo &lt;&lt;-EOF[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/6enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF ÈÄöËøáyumÂä†ËΩΩdocker1yum install docker-engine 2.Á¶ÅÁî®selinux#####2.1.Êü•ÁúãselinuxÁä∂ÊÄÅ [root@localhost ~]# cat /etc/selinux/config12345678910111213141516171819202122232425# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=enforcing# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted``` 2.2.‰øÆÊîπËØ•ÈÖçÁΩÆÊñá‰ª∂‰∏≠Â∞ÜenforcingÊõøÊç¢‰∏∫disabled [root@localhost ~]# cat /etc/selinux/config```js# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted 2.3.Docker‰ΩøÁî®ÈùûrootÁî®Êà∑Â∞ÜÂΩìÂâçÁî®Êà∑Âä†ÂÖ•dockerÁªÑsudo gpasswd -a ${USER} docker 2.4.ÁÑ∂ÂêéÈáçÂêØrestart 3.ÂêØÂä®docker123456789101112131415161718/etc/init.d/docker start[hadoop@www.hadoop02.com ~]$ps -ef | grep docker root 2398 1 0 19:26 ? 00:00:00 /usr/bin/docker -dhadoop 2817 2761 0 19:27 pts/0 00:00:00 grep docker[hadoop@www.hadoop02.com ~]$docker versionClient version: 1.7.1Client API version: 1.19Go version (client): go1.4.2Git commit (client): 786b29dOS/Arch (client): linux/amd64Get http:///var/run/docker.sock/v1.19/version: dial unix /var/run/docker.sock: permission denied. Are you trying to connect to a TLS-enabled daemon without TLS?[hadoop@www.hadoop02.com ~]$docker -d FATA[0000] Error starting daemon: open /var/run/docker.pid: permission denied Ê∑ªÂä†hadoopÁî®Êà∑Âà∞dockerÁªÑ12[hadoop@www.hadoop02.com run]$sudo gpasswd -a hadoop dockerAdding user hadoop to group docker Êü•Áúãdocker‰ø°ÊÅØ12345678910111213141516171819202122232425262728[hadoop@www.hadoop02.com ~]$docker infoContainers: 0Images: 0Storage Driver: devicemapper Pool Name: docker-253:0-798129-pool Pool Blocksize: 65.54 kB Backing Filesystem: extfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 305.7 MB Data Space Total: 107.4 GB Data Space Available: 3.967 GB Metadata Space Used: 729.1 kB Metadata Space Total: 2.147 GB Metadata Space Available: 2.147 GB Udev Sync Supported: true Deferred Removal Enabled: false Data loop file: /var/lib/docker/devicemapper/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata Library Version: 1.02.95-RHEL6 (2015-04-15)Execution Driver: native-0.2Logging Driver: json-fileKernel Version: 2.6.32-573.el6.x86_64Operating System: &lt;unknown&gt;CPUs: 1Total Memory: 1.82 GiBName: www.hadoop02.comID: XL2K:2DEZ:GF27:JL4P:AHWJ:HEGF:QNQ7:HVT3:3SLS:75GJ:LRQM:E7OJ ÂèØ‰ª•Áî®ip addrÁúãdocker0ÁΩëÊ°•ÂàÜÈÖç‰∫Ü‰∏Ä‰∏™ÁßÅÊúâÁΩëÊÆµ12345docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN link/ether 26:75:34:2e:06:26 brd ff:ff:ff:ff:ff:ff inet 172.17.42.1/16 scope global docker0 inet6 fe80::2475:34ff:fe2e:626/64 scope link valid_lft forever preferred_lft forever ‰∏ä‰º†dockerÊñá‰ª∂buildÊñá‰ª∂ÔºåÊ≥®ÊÑèÊúÄÂêéÊúâ‰∏™ÁÇπdocker build -t hadoop/zookeeper:3.4.6 -f zookeeper.Dockerfile .ÂèëÁé∞Êä•ÈîôÔºåÂéüÂõ†ÊòØÊ≤°ÊúâÂÆâË£ÖREPL 12345wget http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpmrpm -ivh epel-release-6-8.noarch.rpmyum repolistyum makecache ‰∏çÊâßË°åÊ≠§Âè•‰πüÂèØ ÊâßË°ådocker search centosÂèØ‰ª•ÁúãÂà∞‰ø°ÊÅØ123456789101112131415161718192021222324252627docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 2757 [OK] jdeathe/centos-ssh CentOS-6 6.8 x86_64 / CentOS-7 7.2.1511 x8... 42 [OK]jdeathe/centos-ssh-apache-php CentOS-6 6.8 x86_64 / Apache / PHP / PHP M... 21 [OK]nimmis/java-centos This is docker images of CentOS 7 with dif... 16 [OK]gluster/gluster-centos Official GlusterFS Image [ CentOS7 + Glus... 12 [OK]million12/centos-supervisor Base CentOS-7 with supervisord launcher, h... 12 [OK]torusware/speedus-centos Always updated official CentOS docker imag... 8 [OK]nickistre/centos-lamp LAMP on centos setup 7 [OK]kinogmt/centos-ssh CentOS with SSH 6 [OK]nathonfowlie/centos-jre Latest CentOS image with the JRE pre-insta... 4 [OK]centos/mariadb55-centos7 3 [OK]consol/sakuli-centos-xfce Sakuli JavaScript based end-2-end testing ... 2 [OK]timhughes/centos Centos with systemd installed and running 1 [OK]blacklabelops/centos CentOS Base Image! Built and Updates Daily! 1 [OK]darksheer/centos Base Centos Image -- Updated hourly 1 [OK]harisekhon/centos-scala Scala + CentOS (OpenJDK tags 2.10-jre7 - 2... 1 [OK]harisekhon/centos-java Java on CentOS (OpenJDK, tags jre/jdk7-8) 1 [OK]repositoryjp/centos Docker Image for CentOS. 0 [OK]labengine/centos Centos image base 0 [OK]smartentry/centos centos with smartentry 0 [OK]januswel/centos yum update-ed CentOS image 0 [OK]vcatechnology/centos A CentOS Image which is updated daily 0 [OK]grayzone/centos auto build for centos. 0 [OK]ustclug/centos USTC centos 0 [OK]kz8s/centos Official CentOS plus epel-release 0 [OK] ÈáçÊñ∞ÊâßË°ådocker build 12345678[hadoop@www.hadoop02.com ~]$docker run -itd --name zookeeper -h zookeeper -p 2181:2181 jason/zookeeper:3.4.6 bashb023a24424772462cc8d63e79dc3c3fe9e06dd0825ad617c65006e1ef331656d[hadoop@www.hadoop02.com ~]$docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb023a2442477 jason/zookeeper:3.4.6 "sh /opt/zookeeper/s 11 seconds ago Up 8 seconds 0.0.0.0:2181-&gt;2181/tcp zookeeper docker build -t hadoop/kafka:0.8.2.2 -f zookeeper.Dockerfile .]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka_high_level_apiÊµãËØï]]></title>
    <url>%2F2016%2F11%2F25%2Fkafka_high_level_api%2F</url>
    <content type="text"><![CDATA[1.Êü•ÁúãÂÆòÁΩëAPIKafka API Automatic Offset Committing1234567891011121314Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("foo", "bar")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; Manual Offset Control123456789101112131415161718192021Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "false"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("foo", "bar")); final int minBatchSize = 200; List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = new ArrayList&lt;&gt;(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; buffer.add(record); &#125; if (buffer.size() &gt;= minBatchSize) &#123; insertIntoDb(buffer); consumer.commitSync(); buffer.clear(); &#125; &#125; Âè¶Â§ñauto.offset.resetËøô‰∏™ÂèÇÊï∞ÂèØ‰ª•ËÆæÁΩÆ‰∏∫smallestÂíålargestlargestË°®Á§∫Êé•ÂèóÊé•Êî∂ÊúÄÂ§ßÁöÑoffset(Âç≥ÊúÄÊñ∞Ê∂àÊÅØ),smallestË°®Á§∫ÊúÄÂ∞èoffset,Âç≥‰ªétopicÁöÑÂºÄÂßã‰ΩçÁΩÆÊ∂àË¥πÊâÄÊúâÊ∂àÊÅØ. Name Description Type Default Valid Values Importance auto.offset.reset What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted) earliest: automatically reset the offset to the earliest offset latest: automatically reset the offset to the latest offset. none: throw exception to the consumer if no previous offset is found for the consumer‚Äôs group anything else: throw exception to the consumer. string latest [latest, earliest, none] medium kafka high level APIÊèê‰∫§offsetÈ¶ñÂÖàÂà§Êñ≠config.offsetsStorage == ‚Äúzookeeper‚ÄùÂ¶ÇÊûúÊòØzkÔºåÈÄöËøáforeachÊèê‰∫§ÊâÄÊ∂àË¥πÁöÑÊâÄÊúâÁöÑtopicÁöÑÊâÄÊúâÁöÑpartitionÁöÑoffset„ÄÇÊèê‰∫§ÁöÑÊñπÂºèÊòØcommitOffsetToZooKeeper-&gt;updatePersistentPathÂ¶ÇÊûúÊòØkafka‰ºöÈÄöËøáNIOÁöÑoffsetchannelÊèê‰∫§ÂΩìÂâçÁöÑoffsetÂà∞broker kafka high level APIÊµãËØïÂêØÂä®kafka1bin/kafka-server-start.sh config/server.properties &amp; nohup kafka-server-start.sh /home/hadoop/apps/kafka_2.11-0.9.0.1/config/server.properties &amp; ÂàõÂª∫topic_high_level_api_test1bin/kafka-topics.sh --create --zookeeper www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka --replication-factor 1 --partitions 3 --topic topic_high_level_api_test ÂêØÂä®kafka high level demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package kafka.javaapi.consumer;import kafka.consumer.Consumer;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.message.MessageAndMetadata;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;/** * Created by zhanghongming on 2016/11/26. */public class DemoHighLevelConsumer &#123; public static void main(String[] args) &#123; args = new String[]&#123;"www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka", "topic_high_level_api_test", "group1", "consumer1"&#125;; if (args == null || args.length != 4) &#123; System.err.print( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;zookeeper_list&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;consumer_id&#125;"); System.exit(1); &#125; String zk = args[0]; String topic = args[1]; String groupid = args[2]; String consumerid = args[3]; Properties props = new Properties(); props.put("zookeeper.connect", zk); props.put("zookeeper.session.timeout.ms", "3600000"); props.put("group.id", groupid); props.put("client.id", "test"); props.put("consumer.id", consumerid); props.put("auto.offset.reset","smallest");// "largest"); props.put("auto.commit.enable", "true"); props.put("auto.commit.interval.ms", "60000"); ConsumerConfig consumerConfig = new ConsumerConfig(props); ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(consumerConfig); Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(topic, 1); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumerConnector.createMessageStreams(topicCountMap); KafkaStream&lt;byte[], byte[]&gt; stream1 = consumerMap.get(topic).get(0); ConsumerIterator&lt;byte[], byte[]&gt; it1 = stream1.iterator(); while (it1.hasNext()) &#123; MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it1.next(); String message = String.format("Consumer ID:%s, Topic:%s, GroupID:%s, PartitionID:%s, Offset:%s, Message Key:%s, Message Payload: %s", consumerid, messageAndMetadata.topic(), groupid, messageAndMetadata.partition(), messageAndMetadata.offset(), new String(messageAndMetadata.key()),new String(messageAndMetadata.message())); System.out.println(message); &#125; &#125;&#125; ÂêØÂä®Áîü‰∫ßËÄÖÁî®ÈöèÊú∫ÁÆóÊ≥ïÁîü‰∫ßÊï∞ÊçÆ12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package kafka;import java.util.ArrayList;import java.util.List;import java.util.Properties;import java.util.Scanner;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import kafka.serializer.StringEncoder;public class ProducerDemo &#123; static private final String TOPIC = "topic_high_level_api_test"; static private final String ZOOKEEPER = "www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka"; static private final String BROKER_LIST = "www.hadoop01.com:9092,www.hadoop02.com:9092,www.hadoop03.com:9092";// static private final int PARTITIONS = TopicAdmin.partitionNum(ZOOKEEPER, TOPIC); static private final int PARTITIONS = 3; public static void main(String[] args) throws Exception &#123; Producer&lt;String, String&gt; producer = initProducer(); System.out.print("1111"); sendOne(producer, TOPIC); &#125; private static Producer&lt;String, String&gt; initProducer() &#123; Properties props = new Properties(); props.put("metadata.broker.list", BROKER_LIST); // props.put("serializer.class", "kafka.serializer.StringEncoder"); props.put("serializer.class", StringEncoder.class.getName()); props.put("partitioner.class", RoundRobinPartitioner.class.getName()); // props.put("partitioner.class", "kafka.producer.DefaultPartitioner");// props.put("compression.codec", "0"); props.put("producer.type", "async"); props.put("batch.num.messages", "3"); props.put("queue.buffer.max.ms", "10000000"); props.put("queue.buffering.max.messages", "1000000"); props.put("queue.enqueue.timeout.ms", "20000000"); ProducerConfig config = new ProducerConfig(props); Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config); return producer; &#125; public static void sendOne(Producer&lt;String, String&gt; producer, String topic) throws InterruptedException &#123; KeyedMessage&lt;String, String&gt; message1 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 31"); producer.send(message1); KeyedMessage&lt;String, String&gt; message2 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 32"); producer.send(message2); KeyedMessage&lt;String, String&gt; message3 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 33"); producer.send(message3); KeyedMessage&lt;String, String&gt; message4 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 34"); producer.send(message4); KeyedMessage&lt;String, String&gt; message5 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 35"); producer.send(message5); KeyedMessage&lt;String, String&gt; message6 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 36"); producer.send(message6); KeyedMessage&lt;String, String&gt; message7 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 37"); producer.send(message7); KeyedMessage&lt;String, String&gt; message8 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 38"); producer.send(message8); producer.close(); &#125;&#125; Êü•Áúãkafka high level demoÊâìÂç∞ÁöÑ‰ø°ÊÅØ1234567Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:0, Message Key:31, Message Payload: test 32Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:0, Message Key:31, Message Payload: test 31Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:0, Offset:0, Message Key:31, Message Payload: test 33Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:1, Message Key:31, Message Payload: test 35Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:2, Message Key:31, Message Payload: test 38Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:1, Message Key:31, Message Payload: test 34Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:2, Message Key:31, Message Payload: test 37 Êü•ÁúãzookeeperÈáåÁöÑ‰ø°ÊÅØ 123456789101112131415161718192021222324252627282930313233343536373839[zk: localhost:2181(CONNECTED) 92] get /kafka/consumers/group1/offsets/topic_high_level_api_test/13cZxid = 0x36000000f4ctime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f7mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000f4cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0[zk: localhost:2181(CONNECTED) 93] get /kafka/consumers/group1/offsets/topic_high_level_api_test/23cZxid = 0x36000000f1ctime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f6mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000f1cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0[zk: localhost:2181(CONNECTED) 94] get /kafka/consumers/group1/offsets/topic_high_level_api_test/01cZxid = 0x36000000eectime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f5mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000eecversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0 ÂèØ‰ª•ÂØπÂ∫îÁöÑ‰∏äÔºåÂàÜÂå∫0Ê∂àË¥π‰∫Ü1Êù°Êï∞ÊçÆÔºåÂàÜÂå∫2Âíå3Ê∂àË¥π‰∫Ü3Êù°Êï∞ÊçÆ Â¶ÇÊûúÊàë‰ª¨Èòü‰∏äËø∞Êï∞ÊçÆÂÅöÂ§ÑÁêÜÔºåkafka high level demoÈáåÁöÑauto.commit.enableËÆæÁΩÆ‰∏∫false ÂêØÂä®kafka high level demoÔºåÂèëÈÄÅÊï∞ÊçÆÔºåÂÜçÈáçÊñ∞ÂêØÂä®kafka high level demo Ëøò‰ºöÊ∂àË¥π‰πãÂâçÂèëÈÄÅÁöÑÊï∞ÊçÆÔºåÊòØÂõ†‰∏∫offsetÊ≤°ÊúâÊõ¥Êîπ Ê≠§Êó∂Êàë‰ª¨ÂèØ‰ª•Âú®‰ª£Á†ÅÈáåÂ¢ûÂä†ÊâãÂ∑•Êèê‰∫§ÁöÑ‰ª£Á†Å1consumerConnector.commitOffsets();]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sparkÁÆóÂ≠ê]]></title>
    <url>%2F2016%2F04%2F05%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[sparkÁöÑÁÆóÂ≠êÂàÜ‰∏∫‰∏§Â§ßÁ±ªTransformationsÂíåActionsTransformationsÔºö map(func) :ËøîÂõû‰∏Ä‰∏™Êñ∞ÁöÑÂàÜÂ∏ÉÂºèÊï∞ÊçÆÈõÜÔºåÁî±ÊØè‰∏™ÂéüÂÖÉÁ¥†ÁªèËøáfuncÂáΩÊï∞ËΩ¨Êç¢ÂêéÁªÑÊàê filter(func) : ËøîÂõû‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÊçÆÈõÜÔºåÁî±ÁªèËøáfuncÂáΩÊï∞ÂêéËøîÂõûÂÄº‰∏∫trueÁöÑÂéüÂÖÉÁ¥†ÁªÑÊàê flatMap(func) : Á±ª‰ºº‰∫émapÔºå‰ΩÜÊòØÊØè‰∏Ä‰∏™ËæìÂÖ•ÂÖÉÁ¥†Ôºå‰ºöË¢´Êò†Â∞Ñ‰∏∫0Âà∞Â§ö‰∏™ËæìÂá∫ÂÖÉÁ¥†ÔºàÂõ†Ê≠§ÔºåfuncÂáΩÊï∞ÁöÑËøîÂõûÂÄºÊòØ‰∏Ä‰∏™SeqÔºåËÄå‰∏çÊòØÂçï‰∏ÄÂÖÉÁ¥†Ôºâ mapPartitions(func) Á±ª‰ºº‰∫émapÔºå‰ΩÜÊòØËøêË°åÂú®‰∏çÂêåÁöÑRDDÂàÜÂå∫ÔºåfuncÂáΩÊï∞ËøêË°åÂú®‰∏Ä‰∏™RDD‰∏äÂøÖÈ°ªÊòØ Iterator =&gt; Iterator Á±ª mapPartitionsWithIndex(func) Á±ª‰ºº‰∫émapPartitions, ËÄå‰∏îËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ë°®Á§∫ÂàÜÂå∫Á¥¢ÂºïÁöÑÊï¥Êï∞ÂÄº, funcÂáΩÊï∞ËøêË°åÂú®‰∏Ä‰∏™RDD‰∏äÂøÖÈ°ªÊòØ (Int, Iterator) =&gt; Iterator Á±ªÂûã. sample(withReplacement, frac, seed) : Ê†πÊçÆÁªôÂÆöÁöÑÈöèÊú∫ÁßçÂ≠êseedÔºåÈöèÊú∫ÊäΩÊ†∑Âá∫Êï∞Èáè‰∏∫fracÁöÑÊï∞ÊçÆ union(otherDataset) : ËøîÂõû‰∏Ä‰∏™Êñ∞ÁöÑÊï∞ÊçÆÈõÜÔºåÁî±ÂéüÊï∞ÊçÆÈõÜÂíåÂèÇÊï∞ËÅîÂêàËÄåÊàê intersection(otherDataset) Return a new RDD that contains the intersection of elements in the source dataset and the argument. distinct([numTasks])) Return a new dataset that contains the distinct elements of the source dataset. groupByKey([numTasks]) : Âú®‰∏Ä‰∏™Áî±ÔºàK,VÔºâÂØπÁªÑÊàêÁöÑÊï∞ÊçÆÈõÜ‰∏äË∞ÉÁî®ÔºåËøîÂõû‰∏Ä‰∏™ÔºàKÔºåSeq[V])ÂØπÁöÑÊï∞ÊçÆÈõÜ„ÄÇÊ≥®ÊÑèÔºöÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî®8‰∏™Âπ∂Ë°å‰ªªÂä°ËøõË°åÂàÜÁªÑÔºå‰Ω†ÂèØ‰ª•‰º†ÂÖ•numTaskÂèØÈÄâÂèÇÊï∞ÔºåÊ†πÊçÆÊï∞ÊçÆÈáèËÆæÁΩÆ‰∏çÂêåÊï∞ÁõÆÁöÑTask reduceByKey(func, [numTasks]) : Âú®‰∏Ä‰∏™ÔºàKÔºåV)ÂØπÁöÑÊï∞ÊçÆÈõÜ‰∏ä‰ΩøÁî®ÔºåËøîÂõû‰∏Ä‰∏™ÔºàKÔºåVÔºâÂØπÁöÑÊï∞ÊçÆÈõÜÔºåkeyÁõ∏ÂêåÁöÑÂÄºÔºåÈÉΩË¢´‰ΩøÁî®ÊåáÂÆöÁöÑreduceÂáΩÊï∞ËÅöÂêàÂà∞‰∏ÄËµ∑„ÄÇÂíågroupbykeyÁ±ª‰ººÔºå‰ªªÂä°ÁöÑ‰∏™Êï∞ÊòØÂèØ‰ª•ÈÄöËøáÁ¨¨‰∫å‰∏™ÂèØÈÄâÂèÇÊï∞Êù•ÈÖçÁΩÆÁöÑ„ÄÇ aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral ‚Äúzero‚Äù value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. sortByKey([ascending], [numTasks]) When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.-join(otherDataset, [numTasks]) : Âú®Á±ªÂûã‰∏∫ÔºàK,V)ÂíåÔºàK,W)Á±ªÂûãÁöÑÊï∞ÊçÆÈõÜ‰∏äË∞ÉÁî®ÔºåËøîÂõû‰∏Ä‰∏™ÔºàK,(V,W))ÂØπÔºåÊØè‰∏™key‰∏≠ÁöÑÊâÄÊúâÂÖÉÁ¥†ÈÉΩÂú®‰∏ÄËµ∑ÁöÑÊï∞ÊçÆÈõÜ groupWith(otherDataset, [numTasks]) : Âú®Á±ªÂûã‰∏∫ÔºàK,V)Âíå(K,W)Á±ªÂûãÁöÑÊï∞ÊçÆÈõÜ‰∏äË∞ÉÁî®ÔºåËøîÂõû‰∏Ä‰∏™Êï∞ÊçÆÈõÜÔºåÁªÑÊàêÂÖÉÁ¥†‰∏∫ÔºàK, Seq[V], Seq[W]) Tuples„ÄÇËøô‰∏™Êìç‰ΩúÂú®ÂÖ∂ÂÆÉÊ°ÜÊû∂ÔºåÁß∞‰∏∫CoGroup cogroup(otherDataset, [numTasks]) When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable, Iterable)) tuples. This operation is also called groupWith. cartesian(otherDataset) : Á¨õÂç°Â∞îÁßØ„ÄÇ‰ΩÜÂú®Êï∞ÊçÆÈõÜTÂíåU‰∏äË∞ÉÁî®Êó∂ÔºåËøîÂõû‰∏Ä‰∏™(TÔºåUÔºâÂØπÁöÑÊï∞ÊçÆÈõÜÔºåÊâÄÊúâÂÖÉÁ¥†‰∫§‰∫íËøõË°åÁ¨õÂç°Â∞îÁßØ„ÄÇ pipe(command, [envVars]) Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process‚Äôs stdin and lines output to its stdout are returned as an RDD of strings. coalesce(numPartitions) Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset. repartition(numPartitions) Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. repartitionAndSortWithinPartitions(partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery. ‰ª•‰∏äTransformationÁÆóÂ≠êËØ¶ÁªÜÂèàÂàÜ‰∏∫Â¶Ç‰∏ã‰∏§Á±ªÔºö1.ValueÊï∞ÊçÆÁ±ªÂûãÁöÑTransformationÁÆóÂ≠ê„ÄÇËæìÂÖ•ÂàÜÂå∫‰∏éËæìÂá∫ÂàÜÂå∫‰∏ÄÂØπ‰∏ÄÂûã map„ÄÅflatMap„ÄÅmapPartitionsËæìÂÖ•ÂàÜÂå∫‰∏éËæìÂá∫ÂàÜÂå∫Â§öÂØπ‰∏ÄÂûã union„ÄÅcartesianËæìÂÖ•ÂàÜÂå∫‰∏éËæìÂá∫ÂàÜÂå∫Â§öÂØπÂ§öÂûã groupByËæìÂá∫ÂàÜÂå∫‰∏∫ËæìÂÖ•ÂàÜÂå∫Â≠êÈõÜÂûã filter„ÄÅdistinct„ÄÅsubtract„ÄÅsample„ÄÅtakeSampleCacheÂûã cache„ÄÅpersist 2.Key-ValueÊï∞ÊçÆÁ±ªÂûãÁöÑTransfromationÁÆóÂ≠ê„ÄÇÁ±ªÂûã ÁÆóÂ≠êËæìÂÖ•ÂàÜÂå∫‰∏éËæìÂá∫ÂàÜÂå∫‰∏ÄÂØπ‰∏Ä mapValuesÂØπÂçï‰∏™RDD combineByKey„ÄÅreduceByKey„ÄÅpartitionBy‰∏§‰∏™RDDËÅöÈõÜ CogroupËøûÊé• join„ÄÅleftOutJoin„ÄÅrightOutJoin ActionsÔºö reduce(func) : ÈÄöËøáÂáΩÊï∞funcËÅöÈõÜÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊâÄÊúâÂÖÉÁ¥†„ÄÇFuncÂáΩÊï∞Êé•Âèó2‰∏™ÂèÇÊï∞ÔºåËøîÂõû‰∏Ä‰∏™ÂÄº„ÄÇËøô‰∏™ÂáΩÊï∞ÂøÖÈ°ªÊòØÂÖ≥ËÅîÊÄßÁöÑÔºåÁ°Æ‰øùÂèØ‰ª•Ë¢´Ê≠£Á°ÆÁöÑÂπ∂ÂèëÊâßË°å collect() : Âú®DriverÁöÑÁ®ãÂ∫è‰∏≠Ôºå‰ª•Êï∞ÁªÑÁöÑÂΩ¢ÂºèÔºåËøîÂõûÊï∞ÊçÆÈõÜÁöÑÊâÄÊúâÂÖÉÁ¥†„ÄÇËøôÈÄöÂ∏∏‰ºöÂú®‰ΩøÁî®filterÊàñËÄÖÂÖ∂ÂÆÉÊìç‰ΩúÂêéÔºåËøîÂõû‰∏Ä‰∏™Ë∂≥Â§üÂ∞èÁöÑÊï∞ÊçÆÂ≠êÈõÜÂÜç‰ΩøÁî®ÔºåÁõ¥Êé•Â∞ÜÊï¥‰∏™RDDÈõÜCollectËøîÂõûÔºåÂæàÂèØËÉΩ‰ºöËÆ©DriverÁ®ãÂ∫èOOM count() : ËøîÂõûÊï∞ÊçÆÈõÜÁöÑÂÖÉÁ¥†‰∏™Êï∞ take(n) : ËøîÂõû‰∏Ä‰∏™Êï∞ÁªÑÔºåÁî±Êï∞ÊçÆÈõÜÁöÑÂâçn‰∏™ÂÖÉÁ¥†ÁªÑÊàê„ÄÇÊ≥®ÊÑèÔºåËøô‰∏™Êìç‰ΩúÁõÆÂâçÂπ∂ÈùûÂú®Â§ö‰∏™ËäÇÁÇπ‰∏äÔºåÂπ∂Ë°åÊâßË°åÔºåËÄåÊòØDriverÁ®ãÂ∫èÊâÄÂú®Êú∫Âô®ÔºåÂçïÊú∫ËÆ°ÁÆóÊâÄÊúâÁöÑÂÖÉÁ¥†(GatewayÁöÑÂÜÖÂ≠òÂéãÂäõ‰ºöÂ¢ûÂ§ßÔºåÈúÄË¶ÅË∞®ÊÖé‰ΩøÁî®Ôºâ first() : ËøîÂõûÊï∞ÊçÆÈõÜÁöÑÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÔºàÁ±ª‰ºº‰∫étake(1)Ôºâ take(n) Return an array with the first n elements of the dataset. takeSample(withReplacement, num, [seed]) Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. takeOrdered(n, [ordering]) Return the first n elements of the RDD using either their natural order or a custom comparator. saveAsTextFile(path) : Â∞ÜÊï∞ÊçÆÈõÜÁöÑÂÖÉÁ¥†Ôºå‰ª•textfileÁöÑÂΩ¢ÂºèÔºå‰øùÂ≠òÂà∞Êú¨Âú∞Êñá‰ª∂Á≥ªÁªüÔºåhdfsÊàñËÄÖ‰ªª‰ΩïÂÖ∂ÂÆÉhadoopÊîØÊåÅÁöÑÊñá‰ª∂Á≥ªÁªü„ÄÇSparkÂ∞Ü‰ºöË∞ÉÁî®ÊØè‰∏™ÂÖÉÁ¥†ÁöÑtoStringÊñπÊ≥ïÔºåÂπ∂Â∞ÜÂÆÉËΩ¨Êç¢‰∏∫Êñá‰ª∂‰∏≠ÁöÑ‰∏ÄË°åÊñáÊú¨ saveAsSequenceFile(path) : Â∞ÜÊï∞ÊçÆÈõÜÁöÑÂÖÉÁ¥†Ôºå‰ª•sequencefileÁöÑÊ†ºÂºèÔºå‰øùÂ≠òÂà∞ÊåáÂÆöÁöÑÁõÆÂΩï‰∏ãÔºåÊú¨Âú∞Á≥ªÁªüÔºåhdfsÊàñËÄÖ‰ªª‰ΩïÂÖ∂ÂÆÉhadoopÊîØÊåÅÁöÑÊñá‰ª∂Á≥ªÁªü„ÄÇRDDÁöÑÂÖÉÁ¥†ÂøÖÈ°ªÁî±key-valueÂØπÁªÑÊàêÔºåÂπ∂ÈÉΩÂÆûÁé∞‰∫ÜHadoopÁöÑWritableÊé•Âè£ÔºåÊàñÈöêÂºèÂèØ‰ª•ËΩ¨Êç¢‰∏∫WritableÔºàSparkÂåÖÊã¨‰∫ÜÂü∫Êú¨Á±ªÂûãÁöÑËΩ¨Êç¢Ôºå‰æãÂ¶ÇIntÔºåDoubleÔºåStringÁ≠âÁ≠âÔºâ saveAsObjectFile(path)(Java and Scala) Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile(). countByKey() Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. foreach(func) : Âú®Êï∞ÊçÆÈõÜÁöÑÊØè‰∏Ä‰∏™ÂÖÉÁ¥†‰∏äÔºåËøêË°åÂáΩÊï∞func„ÄÇËøôÈÄöÂ∏∏Áî®‰∫éÊõ¥Êñ∞‰∏Ä‰∏™Á¥ØÂä†Âô®ÂèòÈáèÔºåÊàñËÄÖÂíåÂ§ñÈÉ®Â≠òÂÇ®Á≥ªÁªüÂÅö‰∫§‰∫í]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windwos‰∏ãÂêØÂä®spark]]></title>
    <url>%2F2016%2F04%2F02%2Fwindwos%E4%B8%8B%E5%90%AF%E5%8A%A8spark%2F</url>
    <content type="text"><![CDATA[ÂÆâË£ÖscalaÔºàÁï•Ôºâ‰∏äÊñáÂ∑≤ÁªèÂÆâË£ÖÂ•Ω‰∫Ühadoop‰∏ãÈù¢ÂÆâË£Öspark ‰∏ãËΩΩÂíåhadoopÂåπÈÖçÁâàÊú¨ÁöÑsparkhttp://spark.apache.org/downloads.html Âçö‰∏ª‰∏ãËΩΩÁöÑspark-2.1.0-bin-hadoop2.6.tgzËß£ËçØÂà∞Ê∑ªÂä†ÁéØÂ¢ÉÂèòÈáèD:\tool\spark-2.1.0-bin-hadoop2.6\bin Âà∞path‰∏ã Â∞èÊèêÁ§∫ÔºöÂ¶ÇÊûúËøêË°åspark‰ª£Á†ÅÁöÑÊó∂ÂÄôÊèêÁ§∫Could not locate executable null\bin\winutils.exe in the Hadoop binaries.Ëß£ÂÜ≥ÊñπÊ≥ïÔºöSystem.setProperty(‚Äúhadoop.home.dir‚Äù, ‚ÄúD:\tool\hadoop-2.6.0‚Äù); Â¶ÇÊûúÂíåwinutilsÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÂèØËÉΩËøòÈúÄË¶Å‰∏ãËΩΩwinutilsÂú∞ÂùÄÔºöhttps://github.com/steveloughran/winutilsÊääÂØπÂ∫îhadoopÁâàÊú¨ÁöÑwinutilsÊîæÂú®hadoopÁöÑbinÁõÆÂΩï‰∏ã Áõ¥Êé•Áî®spark-shellÂú®cmdÈáåÂêØÂä®Âç≥ÂèØ ÊµãËØïÔºöÂä†ËΩΩÊñá‰ª∂val testlog=sc.textFile(‚Äúinst.ini‚Äù)Ëé∑ÂèñÁ¨¨‰∏ÄË°åÊï∞ÊçÆ„ÄÅtestlog.firstËé∑ÂèñË°åÊï∞testlog.countËøáÊª§ÂåÖÂê´GeneralÁöÑË°åval linesWithGeneral=testlog.filter(line =&gt; line.contains(‚ÄúGeneral‚Äù))ËøáÊª§‰∏çÂåÖÂê´GeneralÁöÑË°åval linesNoWithGeneral=testlog.filter(line =&gt; !line.contains(‚ÄúGeneral‚Äù))Âæ™ÁéØÊâìÂç∞linesNoWithGeneral.collect.foreach(println)Ê±ÇÂçïËØçÊúÄÂ§öÁöÑË°åÂçïËØçÊï∞testlog.map(line =&gt; line.split(‚Äú ‚Äú).size).reduce((a, b) =&gt; if (a &gt; b) a else b)‰πüÂèØ‰ª•ÂºïÁî®Êï∞Â≠¶Â∑•ÂÖ∑ÔºåÂÜôÊàêtextFile.map(line =&gt; line.split(‚Äú ‚Äú).size).reduce((a, b) =&gt; Math.max(a, b))ËÆ°ÁÆówordcountÔºàÊ≠§Â§Ñ‰ΩøÁî®reduceByKey‰ºöÁîüÊàêShuffledRDDÔºâval wordCounts = textFile.flatMap(line =&gt; line.split(‚Äú ‚Äú)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b) ÂÜô‰∏™Á®ãÂ∫èÂú®Êú¨Âú∞Ë∑ë‰∏Ä‰∏ã12345678910111213141516171819202122package cn.zwjf.test/** * Created by Administrator on 2017/4/5. */import org.apache.spark.SparkContextimport org.apache.spark.SparkConfobject SimpleApp &#123; def main(args: Array[String]) &#123; System.setProperty("hadoop.home.dir", "D:\\tool\\hadoop-2.6.0"); val logFile = "D:\\tool\\spark-2.1.0-bin-hadoop2.6\\README.md" // Should be some file on your system val conf = new SparkConf().setAppName("Simple Application").setMaster("local[2]") val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line =&gt; line.contains("Spark")).count() val numBs = logData.filter(line =&gt; line.contains("http")).count() println(s"Lines with Spark: $numAs, Lines with http: $numBs") sc.stop() &#125;&#125; ËæìÂá∫Lines with a: 20, Lines with b: 10]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windwos‰∏ãÂêØÂä®hadoop]]></title>
    <url>%2F2016%2F04%2F02%2Fwindwos%E4%B8%8B%E5%90%AF%E5%8A%A8hadoop%2F</url>
    <content type="text"><![CDATA[ÂÆâË£ÖJDKÔºàÁï•Ôºâ ‰∏ãËΩΩhadoop2.6.0Âú∞ÂùÄÔºöhttps://archive.apache.org/dist/hadoop/common/hadoop-2.6.0/ÈÄâÊã©ÁºñËØëÂêéÁöÑÊñá‰ª∂hadoop-2.6.0.tar.gz ‰∏ãËΩΩËß£ÂéãÔºåÂçö‰∏ªËß£ÂéãÂà∞D:\tool‰∏ã ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºöHADOOP_HOMED:\tool\hadoop-2.6.0 ‰øÆÊîπhadoopÈÖçÁΩÆÊñá‰ª∂core-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xmlÊ≤°ÊúâÂ∞±Â∞Ümapred-site.xml.templateÈáçÂëΩÂêç‰∏∫mapred-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;hdfs://localhost:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011&lt;configuration&gt; &lt;!-- Ëøô‰∏™ÂèÇÊï∞ËÆæÁΩÆ‰∏∫1ÔºåÂõ†‰∏∫ÊòØÂçïÊú∫Áâàhadoop --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop-env.cmd Ê≥®ÈáäÂéüÊù•ÁöÑJAVA_HOMEÔºåÂ¢ûÂä†‰Ω†Êú¨Âú∞ÁöÑË∑ØÂæÑÔºàÊ≥®ÊÑèJDKË∑ØÂæÑ‰∏∫C:\Program Files\Java\jdk1.8.0_74Ôºâ123@rem set JAVA_HOME=%JAVA_HOME%set JAVA_HOME=C:\Progra~1\Java\jdk1.8.0_74 Ê†ºÂºèÂåñhdfs namenode -format Âà∞hadoopÁöÑsbinÁõÆÂΩïÊâßË°å‚Äústart-all.cmd‚ÄùÔºåÂÆÉÂ∞Ü‰ºöÂêØÂä®‰ª•‰∏ãËøõÁ®ã„ÄÇ ÊâìÂºÄhttp://localhost:8088/cluster]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ</category>
      </categories>
      <tags>
        <tag>Â§ßÊï∞ÊçÆ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËÆæÁΩÆÊ®°Âºè-ËΩØ‰ª∂ËÆæËÆ°ÂéüÂàô]]></title>
    <url>%2F2016%2F03%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99(1)%2F</url>
    <content type="text"><![CDATA[ÂºÄÈó≠ÂéüÂàôSoftware entities should be open for extension,but closed for modificationfrom „ÄäObject Oriented Software Construction„Äã by Bertrand Meyer at 1988 entities:ÂÆû‰ΩìÂØπÂÉèextension:Êâ©Â±ï ÈÄö‰øóÊù•ËÆ≤Âú®ËÆæËÆ°‰∏Ä‰∏™Ê®°ÂùóÁöÑÊó∂ÂÄô,Â∫îÂΩì‰ΩøËøô‰∏™Ê®°ÂùóÂèØ‰ª•Âú®‰∏çË¢´‰øÆÊîπÁöÑÂâçÊèê‰∏ãË¢´Êâ©Â±ï‰æãÂ¶ÇÔºöspringÂú®Ë£ÖËΩΩ‰∏Ä‰∏™Á±ªÂêéÔºåÊÉ≥Ë¶ÅÂ¢ûÂä†Êñ∞ÂäüËÉΩÊâ©Â±ï‰∏Ä‰∏™Á±ªÔºåspringÂ∞±‰ºöËá™Âä®Ë£ÖËΩΩËøô‰∏™Á±ª ÂíåÈù¢ÂêëÊé•Âè£ÁºñÁ®ãÁªü‰∏ÄËµ∑Êù• ÂêàÊàê/ËÅöÂêàÂ§çÁî®ÂéüÂàô‰ΩøÁî®ËÅöÂêàÔºåÁªÑÂêà‰ª£ÊõøÁªßÊâø butÔºå what is ËÅöÂêàÁªÑÂêàÔºüe.g.class Driver { //‰ΩøÁî®ÊàêÂëòÂèòÈáèÂΩ¢ÂºèÂÆûÁé∞ËÅöÂêàÂÖ≥Á≥ª Car mycar; public void drive(){ mycar.run(); }}spring Ë£ÖÈÖçÂç≥ÊòØËøô‰∏™ÂéüÂàô but, when use ÁªßÊâøÔºüÂΩìÂ≠êÁ±ªÊòØÁà∂Á±ªÁöÑ‰∏ÄÁßçÁâπ‰æãÔºå‰ªéËØ≠‰πâÊù•ËØ¥ÊòØ‰∏Ä‰∏™ÂçïÁã¨ÁöÑÁà∂Á±ªÔºåËÄå‰∏çÊòØ‰∏∫‰∫ÜÂ§çÁî®‰ª£Á†Å 23ÁßçËÆæËÆ°Ê®°ÂºèÂºïÁî®Ôºö‰ª£ÁêÜÊ®°ÂºèÔºöÁúüÂÆû‰∏ªÈ¢ò‰Ωú‰∏∫‰ª£ÁêÜÁ±ªÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ‰æãÂ¶ÇÊó•ÂøóÔºåÂÆâÂÖ®ÔºåÁºìÂ≠òÂèØ‰ª•ÊîæÂú®‰ª£ÁêÜÁ±ªÈáåÔºåÈúÄË¶ÅË∞ÉÁî®ÁúüÂÆû‰∏ªÈ¢òÁöÑÊó∂ÂÄôÊâç‰ºöÂéªË∞ÉÁî®spring ÁöÑAOPÂ∞±ÊòØ‰ª£ÁêÜÊ®°Âºè ÈáåÊ∞è‰ª£Êç¢ÂéüÂàôÂ≠êÁ±ªÂèØ‰ª•Âú®ÊâÄÊúâÂú∫Âêà‰ª£ÊõøÁà∂Á±ªÈÄö‰øóÊù•ËÆ≤ÔºöÂ≠êÁ±ªÂèØ‰ª•Êâ©Â±ïÁà∂Á±ªÁöÑÂäüËÉΩÔºå‰ΩÜ‰∏çËÉΩÊîπÂèòÁà∂Á±ªÂéüÊúâÁöÑÂäüËÉΩ 23ÁßçËÆæËÆ°Ê®°ÂºèÂºïÁî®ÔºöÊ®°ÊùøÊñπÊ≥ïÔºödo not call us,we will call you ‰æùËµñÂÄíÁΩÆÂéüÂàôAbstractions should not depend upon details. Details should depend upon abstractions.ÊäΩË±°‰∏çÂ∫îÂΩì‰æùËµñ‰∫éÁªÜËäÇ,ÁªÜËäÇÂ∫îÂΩì‰æùËµñ‰∫éÊäΩË±° e.g. ÂÆ¢Êà∑Á´Ø‰æùËµñÊé•Âè£ÔºåÊé•Âè£Êúâ‰∏çÂêåÁöÑÂÆûÁé∞„ÄÇÂÆ¢Êà∑Á´Ø‰∏ç‰æùËµñÂÆûÁé∞Áà±‰∫∫‰æùËµñ‰∫∫Ëøô‰∏™ÂÄüÂè£Ôºå‰∫∫ÂÄüÂè£‰∏ãÈù¢ÊúâÈ≤ÅÂõΩ‰∫∫ÔºåÁß¶ÂõΩ‰∫∫Á≠âÂÆûÁé∞ Êé•Âè£ÈöîÁ¶ªÂéüÂàôClients should not be forced to depend upon interfaces that they don‚Äôt use‚Äî‚ÄîÂÆ¢Êà∑Á´Ø‰∏çÂ∫îËØ•‰æùËµñÂÆÉ‰∏çÈúÄÁî®ÁöÑÊé•Âè£„ÄÇThe dependency of one class to another one should depend on the smallest possible interface‚Äî‚ÄîÁ±ªÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÂ∫îËØ•Âª∫Á´ãÂú®ÊúÄÂ∞èÁöÑÊé•Âè£‰∏ä Ëø™Á±≥ÁâπÊ≥ïÂàô1ÔºâÂè™‰∏é‰Ω†Áõ¥Êé•ÁöÑÊúãÂèã‰ª¨ÈÄö‰ø°„ÄÇ2Ôºâ‰∏çË¶ÅË∑ü‚ÄúÈôåÁîü‰∫∫‚ÄùËØ¥ËØù„ÄÇ3ÔºâÊØè‰∏Ä‰∏™ËΩØ‰ª∂Âçï‰ΩçÂØπÂÖ∂‰ªñÁöÑÂçï‰ΩçÈÉΩÂè™ÊúâÊúÄÂ∞ëÁöÑÁü•ËØÜÔºåËÄå‰∏îÂ±ÄÈôê‰∫éÈÇ£‰∫õ‰∏éÊú¨Âçï‰ΩçÂØÜÂàáÁõ∏ÂÖ≥ÁöÑËΩØ‰ª∂Âçï‰Ωç„ÄÇ ‰ΩÜÂπ∂‰∏çÊòØÁªùÂØπÁöÑ‰æãÂ¶ÇStringBuffer ÁöÑappendÊñπÊ≥ïÂèØ‰ª•‰∏ÄÁõ¥append‰∏ãÂéªÔºå‰ΩÜÊòØÊØèÊ¨°ËøîÂõûÁöÑÈÉΩÊòØÂÆÉÊú¨Ë∫´ÔºåËøôÊ†∑‰∏ç‰ºöÈÄ†ÊàêÁ©∫ÊåáÈíàÁ≠âÂºÇÂ∏∏ 23ÁßçËÆæËÆ°Ê®°ÂºèÂºïÁî®ÔºöÈó®Èù¢ÔºàÂ§ñËßÇÔºâÊ®°ÂºèÂíåË∞ÉÂÅúËÄÖÔºà‰∏≠‰ªãËÄÖÔºâÊ®°ÂºèÂÆûÈôÖ‰∏äÂ∞±ÊòØËø™Á±≥ÁâπÊ≥ïÂàôÁöÑÂÖ∑‰ΩìÂ∫îÁî®„ÄÇ Âçï‰∏ÄËÅåË¥£ÂéüÂàô‰∏Ä‰∏™Á±ªÔºåÂ∫îËØ•Âè™Êúâ‰∏Ä‰∏™ËÅåË¥£„ÄÇÊØè‰∏Ä‰∏™ËÅåË¥£ÈÉΩÊòØÂèòÂåñÁöÑ‰∏Ä‰∏™ËΩ¥Á∫øÔºåÂ¶ÇÊûú‰∏Ä‰∏™Á±ªÊúâ‰∏Ä‰∏™‰ª•‰∏äÁöÑËÅåË¥£ÔºåËøô‰∫õËÅåË¥£Â∞±ËÄ¶ÂêàÂú®‰∫Ü‰∏ÄËµ∑„ÄÇËøô‰ºöÂØºËá¥ËÑÜÂº±ÁöÑËÆæËÆ°„ÄÇÂΩì‰∏Ä‰∏™ËÅåË¥£ÂèëÁîüÂèòÂåñÊó∂ÔºåÂèØËÉΩ‰ºöÂΩ±ÂìçÂÖ∂ÂÆÉÁöÑËÅåË¥£„ÄÇÂè¶Â§ñÔºåÂ§ö‰∏™ËÅåË¥£ËÄ¶ÂêàÂú®‰∏ÄËµ∑Ôºå‰ºöÂΩ±ÂìçÂ§çÁî®ÊÄß„ÄÇÊàë‰ª¨ÂèØËÉΩÂè™ÈúÄË¶ÅÂ§çÁî®ËØ•Á±ªÁöÑÊüê‰∏Ä‰∏™ËÅåË¥£Ôºå‰ΩÜËøô‰∏™ËÅåË¥£Ë∑üÂÖ∂ÂÆÉËÅåË¥£ËÄ¶ÂêàÂú®‰∫Ü‰∏ÄËµ∑ÔºåÂæàÈöæÂàÜÁ¶ªÂá∫Êù•]]></content>
      <categories>
        <category>ËÆæËÆ°Ê®°Âºè</category>
      </categories>
      <tags>
        <tag>ËÆæËÆ°Ê®°Âºè</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-MLlib-Êï∞ÊçÆÁ±ªÂûãÂÆûÊàò]]></title>
    <url>%2F2016%2F03%2F30%2Fspark-MLlib-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[ÁõëÁù£Â≠¶‰π†ÔºöÁªôÂÆöÊï∞ÊçÆÈõÜÔºåÁü•ÈÅìË¶ÅÈ¢ÑÊµã‰ªÄ‰πà ÈùûÁõëÁù£Â≠¶‰π†Ôºö‰∏çÂëäËØâËÆ°ÁÆóÊú∫Ë¶ÅÂÅö‰ªÄ‰πàÔºåËÆ©‰ªñËá™Â∑±ÂéªÂèëÁé∞Êï∞ÊçÆÁöÑÂÜÖÈÉ®Êú∫ÊûÑ ÂçäÁõëÁù£Â≠¶‰π†ÔºöËÄÉËôëÂ¶Ç‰ΩïÁî®Â∞ëÈáèÁöÑÊ†áÊ≥®Ê†∑Êú¨ÂíåÂ§ßÈáèÁöÑÊú™Ê†áÊ≥®Ê†∑Êú¨ËøõË°åËÆ≠ÁªÉÂíåÂàÜÁ±ª Âº∫ÂåñÂ≠¶‰π†ÔºöÂº∫ÂåñÂ≠¶‰π†ÈÄöËøáËßÇÂØüÊù•Â≠¶‰π†Âä®‰ΩúÁöÑÂÆåÊàêÔºåÊØè‰∏™Âä®‰ΩúÈÉΩ‰ºöÂØπÁéØÂ¢ÉÊúâÊâÄÂΩ±ÂìçÔºåÂ≠¶‰π†ÂØπË±°Ê†πÊçÆËßÇÂØüÂà∞ÁöÑÂë®Âõ¥ÁéØÂ¢ÉÁöÑÂèçÈ¶àÊù•ÂÅöÂá∫Âà§Êñ≠„ÄÇÂú®ËøôÁßçÂ≠¶‰π†Ê®°Âºè‰∏ãÔºåËæìÂÖ•Êï∞ÊçÆ‰Ωú‰∏∫ÂØπÊ®°ÂûãÁöÑÂèçÈ¶à ÂÆûÊàòÁªÉ‰π†ÔºöÊú¨Âú∞ÂêëÈáè// ÂàõÂª∫‰∏Ä‰∏™ÂØÜÈõÜÂêëÈáè (1.0, 0.0, 3.0).val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)// ÂàõÂª∫‰∏Ä‰∏™Á®ÄÁñèÂêëÈáè (1.0, 0.0, 3.0) ÈÄöËøáÂà∂ÂÆöÂØπÂ∫î‰∫éÈùûÈõ∂È°πÁöÑÁ¥¢ÂºïÂíåÂÄºval sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))// ÂàõÂª∫Á®ÄÁñèÂêëÈáè (1.0, 0.0, 3.0) ÈÄöËøáÊåáÂÆöÈùûÈõ∂È°πval sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0))) Ë°åÁü©ÈòµÈúÄË¶ÅÂºïÂÖ•Â¶Ç‰∏ãÂåÖ123456789101112131415import org.apache.spark.mllib.linalg.&#123;Vector, Vectors&#125;import org.apache.spark.mllib.linalg.distributed.RowMatriximport org.apache.spark.rdd.RDD‰ª£Á†ÅÔºöval dv1 : Vector = Vectors.dense(1.0,2.0,3.0)val dv2 : Vector = Vectors.dense(2.0,3.0,4.0)val rows : RDD[Vector] = sc.parallelize(Array(dv1,dv2))val mat: RowMatrix = new RowMatrix(rows)// Get its size.val m = mat.numRows()val n = mat.numCols()print(m) //2print(n) //3 Êàë‰ª¨ÂèØ‰ª•ÈÄöËøáÂÖ∂Ëá™Â∏¶ÁöÑcomputeColumnSummaryStatistics()ÊñπÊ≥ïËé∑ÂèñËØ•Áü©ÈòµÁöÑ‰∏Ä‰∫õÁªüËÆ°ÊëòË¶Å‰ø°ÊÅØÔºåÂπ∂ÂèØ‰ª•ÂØπÂÖ∂ËøõË°åQRÂàÜËß£ÔºåSVDÂàÜËß£ÂíåPCAÂàÜËß£1234567891011val summary = mat.computeColumnSummaryStatistics()val count = summary.countprintln(count) //2val max = summary.maxprintln(max) //[2.0,3.0,4.0]val variance = summary.varianceprintln(variance) //[0.5,0.5,0.5]val mean = summary.meanprintln(mean) //[1.5,2.5,3.5]val normL1 = summary.normL1println(normL1) //[3.0,5.0,7.0] Á¥¢ÂºïË°åÁü©ÈòµÈúÄË¶ÅÂºïÂÖ•12import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.distributed.&#123;IndexedRow, IndexedRowMatrix&#125; ‰ª£Á†Å12345val idxr1 = IndexedRow(1,dv1)val idxr2 = IndexedRow(2,dv2)val idxrows = sc.parallelize(Array(idxr1,idxr2))val idxmat: IndexedRowMatrix = new IndexedRowMatrix(idxrows)idxmat.rows.foreach(println) IndexedRow(2,[2.0,3.0,4.0])IndexedRow(1,[1.0,2.0,3.0]) ÂùêÊ†áÁü©ÈòµMatrixEntryÁöÑÂèÇÊï∞ÊÑè‰πâÔºö(i: Long, j: Long, value: Double)ÔºåÂÖ∂‰∏≠iÊòØË°åÁ¥¢ÂºïÔºåjÊòØÂàóÁ¥¢ÂºïÔºåvalueÊòØËØ•‰ΩçÁΩÆÁöÑÂÄº„ÄÇÂùêÊ†áÁü©Èòµ‰∏ÄËà¨Âú®Áü©ÈòµÁöÑ‰∏§‰∏™Áª¥Â∫¶ÈÉΩÂæàÂ§ßÔºå‰∏îÁü©ÈòµÈùûÂ∏∏Á®ÄÁñèÁöÑÊó∂ÂÄô‰ΩøÁî®ÈúÄË¶ÅÂºïÂÖ•Ôºö1import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125; ‰ª£Á†ÅÔºö1234567val conf = new SparkConf().setAppName("Matrix").setMaster("local[2]")val sc = new SparkContext(conf)var ent1 = new MatrixEntry(0,1,0.5)var ent2 = new MatrixEntry(2,2,1.8)val entries : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2))val coordMat: CoordinateMatrix = new CoordinateMatrix(entries)coordMat.entries.foreach(println) MatrixEntry(2,2,1.8)MatrixEntry(0,1,0.5) 12val transMat: CoordinateMatrix = coordMat.transpose() //ËΩ¨ÁΩÆtransMat.entries.foreach(println) MatrixEntry(1,0,0.5)MatrixEntry(2,2,1.8) 12val indexedRowMatrix = transMat.toIndexedRowMatrix() //ËΩ¨Êç¢ÊàêÁ¥¢ÂºïË°åÁü©ÈòµindexedRowMatrix.rows.foreach(println) IndexedRow(2,(3,[2],[1.8]))IndexedRow(1,(3,[0],[0.5])) ÂàÜÂùóÁü©ÈòµÂàÜÂùóÁü©ÈòµÊòØÂü∫‰∫éÁü©ÈòµÂùóMatrixBlockÊûÑÊàêÁöÑRDDÁöÑÂàÜÂ∏ÉÂºèÁü©ÈòµÔºåÂÖ∂‰∏≠ÊØè‰∏Ä‰∏™Áü©ÈòµÂùóMatrixBlockÈÉΩÊòØ‰∏Ä‰∏™ÂÖÉÁªÑ((Int, Int), Matrix)ÔºåÂÖ∂‰∏≠(Int, Int)ÊòØÂùóÁöÑÁ¥¢ÂºïÔºåËÄåMatrixÂàôÊòØÂú®ÂØπÂ∫î‰ΩçÁΩÆÁöÑÂ≠êÁü©ÈòµÔºàsub-matrixÔºâÔºåÂÖ∂Â∞∫ÂØ∏Áî±rowsPerBlockÂíåcolsPerBlockÂÜ≥ÂÆöÔºåÈªòËÆ§ÂÄºÂùá‰∏∫1024„ÄÇÂàÜÂùóÁü©ÈòµÊîØÊåÅÂíåÂè¶‰∏Ä‰∏™ÂàÜÂùóÁü©ÈòµËøõË°åÂä†Ê≥ïÊìç‰ΩúÂíå‰πòÊ≥ïÊìç‰ΩúÔºåÂπ∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊîØÊåÅÊñπÊ≥ïvalidate()Êù•Á°ÆËÆ§ÂàÜÂùóÁü©ÈòµÊòØÂê¶ÂàõÂª∫ÊàêÂäü„ÄÇÂàÜÂùóÁü©ÈòµÂèØÁî±Á¥¢ÂºïË°åÁü©ÈòµIndexedRowMatrixÊàñÂùêÊ†áÁü©ÈòµCoordinateMatrixË∞ÉÁî®toBlockMatrix()ÊñπÊ≥ïÊù•ËøõË°åËΩ¨Êç¢ÔºåËØ•ÊñπÊ≥ïÂ∞ÜÁü©ÈòµÂàíÂàÜÊàêÂ∞∫ÂØ∏ÈªòËÆ§‰∏∫1024x1024ÁöÑÂàÜÂùóÔºåÂèØ‰ª•Âú®Ë∞ÉÁî®toBlockMatrix(rowsPerBlock, colsPerBlock)ÊñπÊ≥ïÊó∂‰º†ÂÖ•ÂèÇÊï∞Êù•Ë∞ÉÊï¥ÂàÜÂùóÁöÑÂ∞∫ÂØ∏„ÄÇ‰∏ãÈù¢‰ª•Áü©ÈòµAÔºàÂ¶ÇÂõæÔºâ‰∏∫‰æãÔºåÂÖàÂà©Áî®Áü©ÈòµÈ°πMatrixEntryÂ∞ÜÂÖ∂ÊûÑÈÄ†ÊàêÂùêÊ†áÁü©ÈòµÔºåÂÜçËΩ¨ÂåñÊàêÂ¶ÇÂõæÊâÄÁ§∫ÁöÑ4‰∏™ÂàÜÂùóÁü©ÈòµÔºåÊúÄÂêéÂØπÁü©ÈòµA‰∏éÂÖ∂ËΩ¨ÁΩÆËøõË°å‰πòÊ≥ïËøêÁÆóÔºö ÈúÄË¶ÅÂºïÂÖ•12import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.BlockMatrix 123456789101112131415161718192021222324252627282930313233343536373839ent1 = new MatrixEntry(0,0,1)ent2 = new MatrixEntry(1,1,1)val ent3 = new MatrixEntry(2,0,-1)val ent4 = new MatrixEntry(2,1,2)val ent5 = new MatrixEntry(2,2,1)val ent6 = new MatrixEntry(3,0,1)val ent7 = new MatrixEntry(3,1,1)val ent8 = new MatrixEntry(3,3,1)val entries2 : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2,ent3,ent4,ent5,ent6,ent7,ent8))val coordMat2: CoordinateMatrix = new CoordinateMatrix(entries2)val matA: BlockMatrix = coordMat2.toBlockMatrix(2,2).cache()//Â∞ÜÂùêÊ†áÁü©ÈòµËΩ¨Êç¢Êàê2x2ÁöÑÂàÜÂùóÁü©ÈòµÂπ∂Â≠òÂÇ®matA.validate()println("ÂùóÁü©Èòµ")val localMatrix: Matrix = matA.toLocalMatrixprintln(localMatrix)/*1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */val numColBlocks = matA.numColBlocks //2println(numColBlocks)val numRowBlocks = matA.numColBlocks //2println(numRowBlocks)val ata = matA.transpose.multiply(matA)//ËÆ°ÁÆóÂÖ∂ËΩ¨ÁΩÆÁü©ÈòµÂíåÁü©ÈòµÁöÑÁßØÁü©Èòµval matrix = ata.toLocalMatrixprintln(matrix)/*3.0 -1.0 -1.0 1.0 -1.0 6.0 2.0 1.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ Êï¥‰ΩìÊµãËØï‰ª£Á†ÅÔºö123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package mllibimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.&#123;Matrix, Vector, Vectors&#125;import org.apache.spark.mllib.linalg.distributed.RowMatriximport org.apache.spark.rdd.RDDimport spark_streaming.LoggerLevelsimport org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.&#123;IndexedRow, IndexedRowMatrix&#125;import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.BlockMatrixclass VectorTets&#123;&#125;/** * Created by zhanghongming on 2017/4/8. */object VectorTets &#123; def main(args: Array[String]) &#123; val dv:Vector =Vectors.dense(1.0,2.0,3.0) /*val rows: RDD[Vector] = print(dv)*/ LoggerLevels.setStreamingLogLevels() //StreamingContext val conf = new SparkConf().setAppName("Matrix").setMaster("local[2]") val sc = new SparkContext(conf) val dv1 : Vector = Vectors.dense(1.0,2.0,3.0) val dv2 : Vector = Vectors.dense(2.0,3.0,4.0) val rows : RDD[Vector] = sc.parallelize(Array(dv1,dv2)) val mat: RowMatrix = new RowMatrix(rows) // Get its size. val m = mat.numRows() val n = mat.numCols() println(m) //2 println(n) //3 mat.rows.foreach(println) /* [2.0,3.0,4.0] [1.0,2.0,3.0] * */ val summary = mat.computeColumnSummaryStatistics() val count = summary.count println(count) //2 val max = summary.max println(max) //[2.0,3.0,4.0] val variance = summary.variance println(variance) //[0.5,0.5,0.5] val mean = summary.mean println(mean) //[1.5,2.5,3.5] val normL1 = summary.normL1 println(normL1) //[3.0,5.0,7.0] /////Á¥¢ÂºïË°åÁü©Èòµ val idxr1 = IndexedRow(1,dv1) val idxr2 = IndexedRow(2,dv2) val idxrows = sc.parallelize(Array(idxr1,idxr2)) val idxmat: IndexedRowMatrix = new IndexedRowMatrix(idxrows) idxmat.rows.foreach(println) /////ÂùêÊ†áÁü©Èòµ var ent1 = new MatrixEntry(0, 1, 0.5) var ent2 = new MatrixEntry(2, 2, 1.8) var entries : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2)) val coordMat: CoordinateMatrix = new CoordinateMatrix(entries) coordMat.entries.foreach(println) val transMat: CoordinateMatrix = coordMat.transpose() //ËΩ¨ÁΩÆ transMat.entries.foreach(println) val indexedRowMatrix = transMat.toIndexedRowMatrix() //ËΩ¨Êç¢ÊàêÁ¥¢ÂºïË°åÁü©Èòµ indexedRowMatrix.rows.foreach(println) //ÂùóÁü©Èòµ ent1 = new MatrixEntry(0,0,1) ent2 = new MatrixEntry(1,1,1) val ent3 = new MatrixEntry(2,0,-1) val ent4 = new MatrixEntry(2,1,2) val ent5 = new MatrixEntry(2,2,1) val ent6 = new MatrixEntry(3,0,1) val ent7 = new MatrixEntry(3,1,1) val ent8 = new MatrixEntry(3,3,1) val entries2 : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2,ent3,ent4,ent5,ent6,ent7,ent8)) val coordMat2: CoordinateMatrix = new CoordinateMatrix(entries2) val matA: BlockMatrix = coordMat2.toBlockMatrix(2,2).cache()//Â∞ÜÂùêÊ†áÁü©ÈòµËΩ¨Êç¢Êàê2x2ÁöÑÂàÜÂùóÁü©ÈòµÂπ∂Â≠òÂÇ® matA.validate() println("ÂùóÁü©Èòµ") val localMatrix: Matrix = matA.toLocalMatrix println(localMatrix) /* 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ val numColBlocks = matA.numColBlocks//2 println(numColBlocks) val numRowBlocks = matA.numColBlocks//2 println(numRowBlocks) println("ËÆ°ÁÆóÂÖ∂ËΩ¨ÁΩÆÁü©Èòµ") println( matA.transpose.toLocalMatrix()) println("ËÆ°ÁÆóÂÖ∂ËΩ¨ÁΩÆÁü©ÈòµÂíåÁü©ÈòµÁöÑÁßØÁü©Èòµ") val ata = matA.transpose.multiply(matA)//ËÆ°ÁÆóÂÖ∂ËΩ¨ÁΩÆÁü©ÈòµÂíåÁü©ÈòµÁöÑÁßØÁü©Èòµ val matrix = ata.toLocalMatrix println(matrix) /* 3.0 -1.0 -1.0 1.0 -1.0 6.0 2.0 1.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ &#125;&#125; pomÊñá‰ª∂1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.10.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-Á∫øÁ®ã]]></title>
    <url>%2F2016%2F03%2F08%2Fjava%20thread%2F</url>
    <content type="text"><![CDATA[ThreadLocalÔºöÁ∫øÁ®ãÊú¨Âú∞ÂèòÈáèÔºàThreadLocal‰∏∫ÂèòÈáèÂú®ÊØè‰∏™Á∫øÁ®ã‰∏≠ÈÉΩÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂâØÊú¨Ôºâ ÂÖà‰∫ÜËß£‰∏Ä‰∏ãThreadLocalÁ±ªÊèê‰æõÁöÑÂá†‰∏™ÊñπÊ≥ïÔºö public T get() { }public void set(T value) { }public void remove() { }protected T initialValue() { } get()ÊñπÊ≥ïÊòØÁî®Êù•Ëé∑ÂèñThreadLocalÂú®ÂΩìÂâçÁ∫øÁ®ã‰∏≠‰øùÂ≠òÁöÑÂèòÈáèÂâØÊú¨ÔºåÈÄöËøá‰ªñËé∑ÂèñThreadLocalMapÔºàÂΩìÂâçÁ∫øÁ®ãt‰∏≠ÁöÑ‰∏Ä‰∏™ÊàêÂëòÂèòÈáèthreadLocals„ÄÇÔºâThreadLocalMapÁöÑEntryÁªßÊâø‰∫ÜWeakReferenceset()Áî®Êù•ËÆæÁΩÆÂΩìÂâçÁ∫øÁ®ã‰∏≠ÂèòÈáèÁöÑÂâØÊú¨Ôºåremove()Áî®Êù•ÁßªÈô§ÂΩìÂâçÁ∫øÁ®ã‰∏≠ÂèòÈáèÁöÑÂâØÊú¨initialValue()ÊòØ‰∏Ä‰∏™protectedÊñπÊ≥ï,ÁõÆÂâçËøîÂõûnull ‰ªéÂ¶Ç‰∏äÂºïÂá∫‰∏Ä‰∏™ÈóÆÈ¢òÔºöWeakReferenceÔºöÂΩì‰∏Ä‰∏™ÂØπË±°oË¢´ÂàõÂª∫Êó∂, ÂÆÉË¢´ÊîæÂú®HeapÈáå. ÂΩìGCËøêË°åÁöÑÊó∂ÂÄô, Â¶ÇÊûúÂèëÁé∞Ê≤°Êúâ‰ªª‰ΩïÂºïÁî®ÊåáÂêëo, oÂ∞±‰ºöË¢´ÂõûÊî∂‰ª•ËÖæÂá∫ÂÜÖÂ≠òÁ©∫Èó¥. ÊàñËÄÖÊç¢Âè•ËØùËØ¥, ‰∏Ä‰∏™ÂØπË±°Ë¢´ÂõûÊî∂, ÂøÖÈ°ªÊª°Ë∂≥‰∏§‰∏™Êù°‰ª∂: 1)Ê≤°Êúâ‰ªª‰ΩïÂºïÁî®ÊåáÂêëÂÆÉ 2)GCË¢´ËøêË°å. ÂΩì‰∏Ä‰∏™ÂØπË±°‰ªÖ‰ªÖË¢´weak referenceÊåáÂêë, ËÄåÊ≤°Êúâ‰ªª‰ΩïÂÖ∂‰ªñstrong referenceÊåáÂêëÁöÑÊó∂ÂÄô, Â¶ÇÊûúGCËøêË°å, ÈÇ£‰πàËøô‰∏™ÂØπË±°Â∞±‰ºöË¢´ÂõûÊî∂. Â§öÁ∫øÁ®ãÁöÑÂá†‰∏™ÊñπÊ≥ïÔºöyield()ÔºöÂÅöÁöÑÊòØËÆ©ÂΩìÂâçËøêË°åÁ∫øÁ®ãÂõûÂà∞ÂèØËøêË°åÁä∂ÊÄÅÔºå‰ª•ÂÖÅËÆ∏ÂÖ∑ÊúâÁõ∏Âêå‰ºòÂÖàÁ∫ßÁöÑÂÖ∂‰ªñÁ∫øÁ®ãËé∑ÂæóËøêË°åÊú∫‰ºö„ÄÇ Âõ†Ê≠§Ôºå‰ΩøÁî®yield()ÁöÑÁõÆÁöÑÊòØËÆ©Áõ∏Âêå‰ºòÂÖàÁ∫ßÁöÑÁ∫øÁ®ã‰πãÈó¥ËÉΩÈÄÇÂΩìÁöÑËΩÆËΩ¨ÊâßË°å„ÄÇ‰ΩÜÊòØÔºåÂÆûÈôÖ‰∏≠Êó†Ê≥ï‰øùËØÅyield()ËææÂà∞ËÆ©Ê≠•ÁõÆÁöÑÔºåÂõ†‰∏∫ËÆ©Ê≠•ÁöÑÁ∫øÁ®ãËøòÊúâÂèØËÉΩË¢´Á∫øÁ®ãË∞ÉÂ∫¶Á®ãÂ∫èÂÜçÊ¨°ÈÄâ‰∏≠„ÄÇ join(): ÊñπÊ≥ïÁöÑ‰∏ªË¶Å‰ΩúÁî®Â∞±ÊòØÂêåÊ≠•ÔºåÂÆÉÂèØ‰ª•‰ΩøÂæóÁ∫øÁ®ã‰πãÈó¥ÁöÑÂπ∂Ë°åÊâßË°åÂèò‰∏∫‰∏≤Ë°åÊâßË°å„ÄÇÂéüÁêÜÊòØÔºöjoinÊñπÊ≥ïÊòØÈÄöËøáË∞ÉÁî®Á∫øÁ®ãÁöÑwaitÊñπÊ≥ïÊù•ËææÂà∞ÂêåÊ≠•ÁöÑÁõÆÁöÑÁöÑ„ÄÇ‰æãÂ¶ÇÔºåAÁ∫øÁ®ã‰∏≠Ë∞ÉÁî®‰∫ÜBÁ∫øÁ®ãÁöÑjoinÊñπÊ≥ïÔºåÂàôÁõ∏ÂΩì‰∫éAÁ∫øÁ®ãË∞ÉÁî®‰∫ÜBÁ∫øÁ®ãÁöÑwaitÊñπÊ≥ïÔºåÂú®Ë∞ÉÁî®‰∫ÜBÁ∫øÁ®ãÁöÑwaitÊñπÊ≥ïÂêéÔºåAÁ∫øÁ®ãÂ∞±‰ºöËøõÂÖ•ÈòªÂ°ûÁä∂ÊÄÅ]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂØºËÆ∫]]></title>
    <url>%2F2016%2F02%2F07%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[ËÆ∞ÂΩï‰∏∫‰ªÄ‰πàË¶ÅÁî®Êú∫Âô®ËØ≠Ë®ÄËÆ©ËÆ°ÁÆóÊú∫Êù•ÊâßË°åÊåá‰ª§ËÄå‰∏çÁî®Êàë‰ª¨ÊâÄÂ≠¶ÁöÑËã±ËØ≠ÊàñÂÖ∂‰ªñËØ≠Ë®ÄÂë¢Ôºü‰∏æ‰∏™‰æãÂ≠êÔºöÂ¶ÇÊûú‰Ω†ËÆ©Êú∫Âô®ÂéªÊâßË°åbiweekly„ÄÇËÆæÂÆöÂèëÂ∑•ËµÑÊòØÊåâÁÖßbiweeklyÊù•ÊâßË°åÔºåÈÇ£‰πàÊú∫Âô®‰ºöÂéªÊåâÁÖßÊØè‰∏§Âë®Âèë‰∏ÄÊ¨°ÔºåËøòÊòØÊØèÂë®Âèë‰∏§Ê¨°Âë¢ÔºüËá™ÁÑ∂ËØ≠Ë®ÄÁöÑËØ≠‰πâÂ§öÁßçÂ§öÊ†∑„ÄÇ python‰∏çËÉΩ‰ΩøÁî®Â≠óÁ¨¶‰∏≤+Êï∞Â≠óÔºå‰ΩÜÊòØÂèØ‰ª•‰ΩøÁî®Â≠óÁ¨¶‰∏≤*Êï∞Â≠ó]]></content>
      <categories>
        <category>ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂØºËÆ∫</category>
      </categories>
      <tags>
        <tag>Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAPÂéüÁêÜ]]></title>
    <url>%2F2015%2F03%2F03%2FCAP%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Consistency ‰∏ÄËá¥ÊÄß√ò ÈÄöËøáÊüê‰∏™ËäÇÁÇπÁöÑÂÜôÊìç‰ΩúÁªìÊûúÂØπÂêéÈù¢ÈÄöËøáÂÖ∂ÂÆÉËäÇÁÇπÁöÑËØªÊìç‰ΩúÂèØËßÅ√ò Â¶ÇÊûúÊõ¥Êñ∞Êï∞ÊçÆÂêéÔºåÂπ∂ÂèëËÆøÈóÆÊÉÖÂÜµ‰∏ãÂèØÁ´ãÂç≥ÊÑüÁü•ËØ•Êõ¥Êñ∞ÔºåÁß∞‰∏∫Âº∫‰∏ÄËá¥ÊÄß√ò Â¶ÇÊûúÂÖÅËÆ∏‰πãÂêéÈÉ®ÂàÜÊàñËÄÖÂÖ®ÈÉ®ÊÑüÁü•‰∏çÂà∞ËØ•Êõ¥Êñ∞ÔºåÁß∞‰∏∫Âº±‰∏ÄËá¥ÊÄß√ò Ëã•Âú®‰πãÂêéÁöÑ‰∏ÄÊÆµÊó∂Èó¥ÔºàÈÄöÂ∏∏ËØ•Êó∂Èó¥‰∏çÂõ∫ÂÆöÔºâÂêéÔºå‰∏ÄÂÆöÂèØ‰ª•ÊÑüÁü•ËØ•Êõ¥Êñ∞ÔºåÁß∞‰∏∫ÊúÄÁªà‰∏ÄËá¥ÊÄß Availability ÂèØÁî®ÊÄß√ò ‰ªª‰Ωï‰∏Ä‰∏™Ê≤°ÊúâÂèëÁîüÊïÖÈöúÁöÑËäÇÁÇπÂøÖÈ°ªÂú®ÊúâÈôêÁöÑÊó∂Èó¥ÂÜÖËøîÂõûÂêàÁêÜÁöÑÁªìÊûú Partition tolerance ÂàÜÂå∫ÂÆπÂøçÊÄß√ò ÈÉ®ÂàÜËäÇÁÇπÂÆïÊú∫ÊàñËÄÖÊó†Ê≥ï‰∏éÂÖ∂ÂÆÉËäÇÁÇπÈÄö‰ø°Êó∂ÔºåÂêÑÂàÜÂå∫Èó¥ËøòÂèØ‰øùÊåÅÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÂäüËÉΩ ÁªìËÆ∫ÔºöÂàÜÂ∏ÉÂºèÂ≠òÂÇ®Á≥ªÁªü‰∏≠Ôºå‰∏ÄËá¥ÊÄßÔºåÂèØÁî®ÊÄßÔºåÂàÜÂå∫ÂÆπÂøçÊÄßÂè™ËÉΩÊª°Ë∂≥‰∏§‰∏™Ôºå‰∏çËÉΩÊª°Ë∂≥‰∏â‰∏™„ÄÇÁî±‰∫éÂΩìÂâçÁöÑÁΩëÁªúÁ°¨‰ª∂ËÇØÂÆö‰ºöÂá∫Áé∞Âª∂Ëøü‰∏¢ÂåÖÁ≠âÈóÆÈ¢òÔºåÊâÄ‰ª•ÂàÜÂå∫ÂÆπÂøçÊÄßÊòØÊàë‰ª¨ÂøÖÈ°ªÈúÄË¶ÅÂÆûÁé∞ÁöÑ„ÄÇÂõ†Ê≠§ÂæàÂ§öÊó∂ÂÄôÊòØÂú®ÂèØÁî®ÊÄßÂíå‰∏ÄËá¥ÊÄß‰πãÈó¥ÊùÉË°°„ÄÇ„ÄÇ]]></content>
      <categories>
        <category>ÂéüÁêÜ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jdk8 Optional]]></title>
    <url>%2F2014%2F12%2F04%2Fjdk8-Optional%2F</url>
    <content type="text"><![CDATA[java 8 Optional1.ofÊñπÊ≥ïÔºö‰∏∫ÈùûnullÁöÑÂÄºÂàõÂª∫‰∏Ä‰∏™Optional„ÄÇ ofÊñπÊ≥ïÈÄöËøáÂ∑•ÂéÇÊñπÊ≥ïÂàõÂª∫OptionalÁ±ª„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÂàõÂª∫ÂØπË±°Êó∂‰º†ÂÖ•ÁöÑÂèÇÊï∞‰∏çËÉΩ‰∏∫null„ÄÇÂ¶ÇÊûú‰º†ÂÖ•ÂèÇÊï∞‰∏∫nullÔºåÂàôÊäõÂá∫NullPointerException „ÄÇ //Ë∞ÉÁî®Â∑•ÂéÇÊñπÊ≥ïÂàõÂª∫OptionalÂÆû‰æãOptional name = Optional.of(‚ÄúSanaulla‚Äù);//ok//‰º†ÂÖ•ÂèÇÊï∞‰∏∫nullÔºåÊäõÂá∫NullPointerException.Optional someNull = Optional.of(null);//java.lang.NullPointerException ofNullable‰∏∫ÊåáÂÆöÁöÑÂÄºÂàõÂª∫‰∏Ä‰∏™OptionalÔºåÂ¶ÇÊûúÊåáÂÆöÁöÑÂÄº‰∏∫nullÔºåÂàôËøîÂõû‰∏Ä‰∏™Á©∫ÁöÑOptional„ÄÇ ofNullable‰∏éofÊñπÊ≥ïÁõ∏‰ººÔºåÂîØ‰∏ÄÁöÑÂå∫Âà´ÊòØÂèØ‰ª•Êé•ÂèóÂèÇÊï∞‰∏∫nullÁöÑÊÉÖÂÜµ„ÄÇÁ§∫‰æãÂ¶Ç‰∏ãÔºö Optional empty = Optional.ofNullable(null);isPresentÂ¶ÇÊûúÂÄºÂ≠òÂú®ËøîÂõûtrueÔºåÂê¶ÂàôËøîÂõûfalse„ÄÇ Optional name = Optional.of(‚Äúname‚Äù);Optional name2 = Optional.ofNullable(null);if (name.isPresent()) { //Âú®OptionalÂÆû‰æãÂÜÖË∞ÉÁî®get()ËøîÂõûÂ∑≤Â≠òÂú®ÁöÑÂÄº System.out.println(name.get());//names} orElseÂ¶ÇÊûúÊúâÂÄºÂàôÂ∞ÜÂÖ∂ËøîÂõûÔºåÂê¶ÂàôËøîÂõûÊåáÂÆöÁöÑÂÖ∂ÂÆÉÂÄº„ÄÇ Optional name = Optional.ofNullable(null);System.out.println(name.orElse(‚ÄúThere is some value!‚Äù)); orElseGetorElseGet‰∏éorElseÊñπÊ≥ïÁ±ª‰ººÔºåÂå∫Âà´Âú®‰∫éÂæóÂà∞ÁöÑÈªòËÆ§ÂÄº„ÄÇorElseÊñπÊ≥ïÂ∞Ü‰º†ÂÖ•ÁöÑÂ≠óÁ¨¶‰∏≤‰Ωú‰∏∫ÈªòËÆ§ System.out.println(name.orElseGet(() -&gt; ‚ÄúDefault Value‚Äù));]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jdk8 ÂºïÁî®ÊñπÊ≥ï]]></title>
    <url>%2F2014%2F12%2F03%2Fjdk8-%E5%BC%95%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[java 8 ÂºïÁî®ÊñπÊ≥ïÊàë‰ª¨ÈÄöÂ∏∏‰ΩøÁî®lambdaË°®ËææÂºèÊù•ÂàõÂª∫ÂåøÂêçÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÊúâÊó∂ÂÄôÊàë‰ª¨‰ªÖ‰ªÖÊòØË∞ÉÁî®‰∫Ü‰∏Ä‰∏™Â∑≤Â≠òÂú®ÁöÑÊñπÊ≥ï„ÄÇÂ¶Ç‰∏ã: Arrays.sort(stringsArray,(s1,s2)-&gt;s1.compareToIgnoreCase(s2)); Âú®Java8‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•Áõ¥Êé•ÈÄöËøáÊñπÊ≥ïÂºïÁî®Êù•ÁÆÄÂÜôlambdaË°®ËææÂºè‰∏≠Â∑≤ÁªèÂ≠òÂú®ÁöÑÊñπÊ≥ï„ÄÇ Arrays.sort(stringsArray, String::compareToIgnoreCase); Person [] persons = new Person[10]; //‰ΩøÁî®ÂåøÂêçÁ±ªArrays.sort(persons, new Comparator() { @Override public int compare(Person o1, Person o2) { return o1.birthday.compareTo(o2.birthday); } }); //‰ΩøÁî®lambdaË°®ËææÂºèArrays.sort(persons, (o1, o2) -&gt; o1.birthday.compareTo(o2.birthday)); //‰ΩøÁî®lambdaË°®ËææÂºèÂíåÁ±ªÁöÑÈùôÊÄÅÊñπÊ≥ïArrays.sort(persons, (o1, o2) -&gt; Person.compareByAge(o1,o2)); //‰ΩøÁî®ÊñπÊ≥ïÂºïÁî®//ÂºïÁî®ÁöÑÊòØÁ±ªÁöÑÈùôÊÄÅÊñπÊ≥ïArrays.sort(persons, Person::compareByAge);]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ëß£ÂØÜJDK8 Êûö‰∏æ]]></title>
    <url>%2F2014%2F12%2F02%2Fjdk8-enum%2F</url>
    <content type="text"><![CDATA[ÂÜô‰∏Ä‰∏™Êûö‰∏æÁ±ª123456public enum Season &#123; SPRING, SUMMER, AUTUMN, WINTER&#125; ÁÑ∂ÂêéÊàë‰ª¨‰ΩøÁî®javacÁºñËØë‰∏äÈù¢ÁöÑÁ±ª,ÂæóÂà∞classÊñá‰ª∂ÁÑ∂Âêé,Êàë‰ª¨Âà©Áî®ÂèçÁºñËØëÁöÑÊñπÊ≥ïÊù•ÁúãÁúãÂ≠óËäÇÁ†ÅÊñá‰ª∂Á©∂Á´üÊòØ‰ªÄ‰πà.ËøôÈáå‰ΩøÁî®ÁöÑÂ∑•ÂÖ∑ÊòØjavapÁöÑÁÆÄÂçïÂëΩ‰ª§,ÂÖàÂàó‰∏æ‰∏Ä‰∏ãËøô‰∏™Season‰∏ãÁöÑÂÖ®ÈÉ®ÂÖÉÁ¥†. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190Classfile /C:/Season.class Last modified 2016-12-8; size 956 bytes MD5 checksum 7f6dfb988d182327a1a73ee986a9d3fa Compiled from "Season.java"public final class cn.redis.model.Season extends java.lang.Enum&lt;cn.redis.model.Season&gt; minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_FINAL, ACC_SUPER, ACC_ENUMConstant pool: #1 = Fieldref #4.#38 // cn/redis/model/Season.$VALUES:[Lcn/redis/model/Season; #2 = Methodref #39.#40 // "[Lcn/redis/model/Season;".clone:()Ljava/lang/Object; #3 = Class #23 // "[Lcn/redis/model/Season;" #4 = Class #41 // cn/redis/model/Season #5 = Methodref #16.#42 // java/lang/Enum.valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; #6 = Methodref #16.#43 // java/lang/Enum."&lt;init&gt;":(Ljava/lang/String;I)V #7 = String #17 // SPRING #8 = Methodref #4.#43 // cn/redis/model/Season."&lt;init&gt;":(Ljava/lang/String;I)V #9 = Fieldref #4.#44 // cn/redis/model/Season.SPRING:Lcn/redis/model/Season; #10 = String #19 // SUMMER #11 = Fieldref #4.#45 // cn/redis/model/Season.SUMMER:Lcn/redis/model/Season; #12 = String #20 // AUTUMN #13 = Fieldref #4.#46 // cn/redis/model/Season.AUTUMN:Lcn/redis/model/Season; #14 = String #21 // WINTER #15 = Fieldref #4.#47 // cn/redis/model/Season.WINTER:Lcn/redis/model/Season; #16 = Class #48 // java/lang/Enum #17 = Utf8 SPRING #18 = Utf8 Lcn/redis/model/Season; #19 = Utf8 SUMMER #20 = Utf8 AUTUMN #21 = Utf8 WINTER #22 = Utf8 $VALUES #23 = Utf8 [Lcn/redis/model/Season; #24 = Utf8 values #25 = Utf8 ()[Lcn/redis/model/Season; #26 = Utf8 Code #27 = Utf8 LineNumberTable #28 = Utf8 valueOf #29 = Utf8 (Ljava/lang/String;)Lcn/redis/model/Season; #30 = Utf8 &lt;init&gt; #31 = Utf8 (Ljava/lang/String;I)V #32 = Utf8 Signature #33 = Utf8 ()V #34 = Utf8 &lt;clinit&gt; #35 = Utf8 Ljava/lang/Enum&lt;Lcn/redis/model/Season;&gt;; #36 = Utf8 SourceFile #37 = Utf8 Season.java #38 = NameAndType #22:#23 // $VALUES:[Lcn/redis/model/Season; #39 = Class #23 // "[Lcn/redis/model/Season;" #40 = NameAndType #49:#50 // clone:()Ljava/lang/Object; #41 = Utf8 cn/redis/model/Season #42 = NameAndType #28:#51 // valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; #43 = NameAndType #30:#31 // "&lt;init&gt;":(Ljava/lang/String;I)V #44 = NameAndType #17:#18 // SPRING:Lcn/redis/model/Season; #45 = NameAndType #19:#18 // SUMMER:Lcn/redis/model/Season; #46 = NameAndType #20:#18 // AUTUMN:Lcn/redis/model/Season; #47 = NameAndType #21:#18 // WINTER:Lcn/redis/model/Season; #48 = Utf8 java/lang/Enum #49 = Utf8 clone #50 = Utf8 ()Ljava/lang/Object; #51 = Utf8 (Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum;&#123; public static final cn.redis.model.Season SPRING; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season SUMMER; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season AUTUMN; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season WINTER; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static cn.redis.model.Season[] values(); descriptor: ()[Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=0, args_size=0 0: getstatic #1 // Field $VALUES:[Lcn/redis/model/Season; 3: invokevirtual #2 // Method "[Lcn/redis/model/Season;".clone:()Ljava/lang/Object; 6: checkcast #3 // class "[Lcn/redis/model/Season;" 9: areturn LineNumberTable: line 3: 0 public static cn.redis.model.Season valueOf(java.lang.String); descriptor: (Ljava/lang/String;)Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=1, args_size=1 0: ldc #4 // class cn/redis/model/Season 2: aload_0 3: invokestatic #5 // Method java/lang/Enum.valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; 6: checkcast #4 // class cn/redis/model/Season 9: areturn LineNumberTable: line 3: 0 static &#123;&#125;; descriptor: ()V flags: ACC_STATIC Code: stack=4, locals=0, args_size=0 0: new #4 // class cn/redis/model/Season 3: dup 4: ldc #7 // String SPRING 6: iconst_0 7: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 10: putstatic #9 // Field SPRING:Lcn/redis/model/Season; 13: new #4 // class cn/redis/model/Season 16: dup 17: ldc #10 // String SUMMER 19: iconst_1 20: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 23: putstatic #11 // Field SUMMER:Lcn/redis/model/Se ason; 26: new #4 // class cn/redis/model/Season 29: dup 30: ldc #12 // String AUTUMN 32: iconst_2 33: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 36: putstatic #13 // Field AUTUMN:Lcn/redis/model/Season; 39: new #4 // class cn/redis/model/Season 42: dup 43: ldc #14 // String WINTER 45: iconst_3 46: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 49: putstatic #15 // Field WINTER:Lcn/redis/model/Season; 52: iconst_4 53: anewarray #4 // class cn/redis/model/Season 56: dup 57: iconst_0 58: getstatic #9 // Field SPRING:Lcn/redis/model/Season; 61: aastore 62: dup 63: iconst_1 64: getstatic #11 // Field SUMMER:Lcn/redis/model/Season; 67: aastore 68: dup 69: iconst_2 70: getstatic #13 // Field AUTUMN:Lcn/redis/model/Season; 73: aastore 74: dup 75: iconst_3 76: getstatic #15 // Field WINTER:Lcn/redis/model/Season; 79: aastore 80: putstatic #1 // Field $VALUES:[Lcn/redis/model/Season; 83: return LineNumberTable: line 4: 0 line 5: 13 line 6: 26 line 7: 39 line 3: 52 &#125;Signature: #35 // Ljava/lang/Enum&lt;Lcn/redis/model/Season;&gt;;SourceFile: "Season.java" ‰ªé‰∏äÂèçÁºñËØëÁªìÊûúÂèØÁü• java‰ª£Á†Å‰∏≠ÁöÑSeasonËΩ¨Êç¢Êàê‰∫ÜÁªßÊâøËá™ÁöÑjava.lang.enumÁöÑÁ±ª Êó¢ÁÑ∂ÈöêÂºèÁªßÊâøËá™java.lang.enum,‰πüÂ∞±ÊÑèÂë≥java‰ª£Á†Å‰∏≠,Season‰∏çËÉΩÂÜçÁªßÊâøÂÖ∂‰ªñÁöÑÁ±ª SeasonË¢´Ê†áËÆ∞Êàê‰∫Üfinal,ÊÑèÂë≥ÁùÄÂÆÉ‰∏çËÉΩË¢´ÁªßÊâø ÈùôÊÄÅÂùóstatic {}; 0~52‰∏∫ÂÆû‰æãÂåñSPRING, SUMMER, AUTUMN, WINTER 53~83‰∏∫ÂàõÂª∫Season[]Êï∞ÁªÑ$VALUES,Âπ∂Â∞Ü‰∏äÈù¢ÁöÑÂõõ‰∏™ÂØπË±°ÊîæÂÖ•Êï∞ÁªÑÁöÑÊìç‰Ωú. 8: invokevirtual #4 Âú®switch-case‰∏≠,ËøòÊòØÂ∞ÜEnumËΩ¨Êàê‰∫ÜintÂÄº(ÈÄöËøáË∞ÉÁî®Enum.oridinal()ÊñπÊ≥ï)]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2014%2F11%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Ê¨¢ËøéÂ§ßÂÆ∂Êù•Âà∞ÊàëÁöÑÂçöÂÆ¢ More info: Deployment]]></content>
      <categories>
        <category>ÂÖ∂‰ªñ</category>
      </categories>
  </entry>
</search>
