<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spring eureka]]></title>
    <url>%2F2020%2F06%2F27%2Fspring%20eureka%2F</url>
    <content type="text"><![CDATA[创建eureka-server 子项目 创建application.properties123456# 是否将自己实例注册到Eureka Server中eureka.client.register-with-eureka=false# 是否应从Eureka Server获取Eureka注册表信息eureka.client.fetch-registry=falsespring.profiles.active=master 创建application-master.properties1234server.port=8761# 注册到从节点eureka.client.serviceUrl.defaultZone=http://localhost:8762/eureka/ 创建application-slave.properties1234server.port=8762# 注册到主节点eureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/ 创建EurekaServerApplication.java1234567891011121314151617181920package spring.cloud.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;/** * Created by zhanghongming on 2020/6/27. */@EnableEurekaServer@SpringBootApplicationpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 打开 http://localhost:8761/ 改成spring.profiles.active=slave启动 eureka client 如何注册？创建子项目：eureka-client-provider 创建：application.properties 1234567server.port=8081spring.application.name=eureka-client-providereureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/# 自定义实例ID格式eureka.instance.instance-id=$&#123;spring.application.name&#125;:$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; #eureka.client.healthcheck.enabled=true 创建EurekaClientProviderApplication123456789101112131415161718package spring.cloud.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;/** * Created by zhanghongming on 2020/6/27. */@EnableDiscoveryClient@SpringBootApplicationpublic class EurekaClientProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaClientProviderApplication.class, args); &#125;&#125; 创建CustomHealthIndicator123456789101112131415161718192021222324package spring.cloud.eureka;import org.springframework.boot.actuate.health.AbstractHealthIndicator;import org.springframework.boot.actuate.health.Health;import org.springframework.stereotype.Component;/** * Created by zhanghongming on 2020/6/27. */@Componentpublic class CustomHealthIndicator extends AbstractHealthIndicator &#123; private boolean status = true; public void setStatus(boolean status) &#123; this.status = status; &#125; @Override protected void doHealthCheck(Health.Builder builder) throws Exception &#123; if (status) &#123; builder.up(); &#125; else &#123; builder.down(); &#125; &#125;&#125; 创建TestController1234567891011121314151617181920212223package spring.cloud.eureka;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;/** * Created by zhanghongming on 2020/6/27. */@RestControllerpublic class TestController &#123; @Autowired private CustomHealthIndicator customHealthIndicator; @GetMapping("/updateStatus") public String updateStatus(boolean status) &#123; customHealthIndicator.setStatus(status); return "success"; &#125;&#125; 启动 那么消费者如何消费呢？ 创建子项目eureka-client-consumer 创建application.properties123456server.port=8082spring.application.name=eureka-client-consumereureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/# 自定义实例ID格式eureka.instance.instance-id=$&#123;spring.application.name&#125;:$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; 创建EurekaConsumerApplication12345678910111213141516package spring.cloud.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;/** * Created by zhanghongming on 2020/6/27. */@SpringBootApplicationpublic class EurekaConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaConsumerApplication.class, args); &#125;&#125; 创建EurekaController.java12345678910111213141516171819202122232425262728293031323334package spring.cloud.eureka.controller;import java.util.List;import java.util.stream.Collectors;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.cloud.client.ServiceInstance;import org.springframework.cloud.client.discovery.DiscoveryClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;/** * Created by zhanghongming on 2020/6/27. */@RestControllerpublic class EurekaController &#123; @Autowired private DiscoveryClient discoveryClient; /** * 获取Eureka Server中所有服务实例信息 * @return */ @GetMapping("/instances") public List&lt;ServiceInstance&gt; getApplications() &#123; List&lt;ServiceInstance&gt; instances = discoveryClient.getServices().stream() .map(sid -&gt; discoveryClient.getInstances(sid)) .collect(Collectors.toList()) .stream().flatMap(list -&gt; list.stream()).collect(Collectors.toList()); return instances; &#125;&#125; 启动打开http://localhost:8082/instances 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145[&#123; "host": "zhm-pc", "port": 8081, "metadata": &#123; "management.port": "8081" &#125;, "secure": false, "serviceId": "EUREKA-CLIENT-PROVIDER", "uri": "http://zhm-pc:8081", "instanceInfo": &#123; "instanceId": "eureka-client-provider:192.168.1.3:8081", "app": "EUREKA-CLIENT-PROVIDER", "appGroupName": null, "ipAddr": "192.168.1.3", "sid": "na", "homePageUrl": "http://zhm-pc:8081/", "statusPageUrl": "http://zhm-pc:8081/actuator/info", "healthCheckUrl": "http://zhm-pc:8081/actuator/health", "secureHealthCheckUrl": null, "vipAddress": "eureka-client-provider", "secureVipAddress": "eureka-client-provider", "countryId": 1, "dataCenterInfo": &#123; "@class": "com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo", "name": "MyOwn" &#125;, "hostName": "zhm-pc", "status": "UP", "overriddenStatus": "UNKNOWN", "leaseInfo": &#123; "renewalIntervalInSecs": 30, "durationInSecs": 90, "registrationTimestamp": 1593256140359, "lastRenewalTimestamp": 1593256620293, "evictionTimestamp": 0, "serviceUpTimestamp": 1593256140360 &#125;, "isCoordinatingDiscoveryServer": false, "metadata": &#123; "management.port": "8081" &#125;, "lastUpdatedTimestamp": 1593256140360, "lastDirtyTimestamp": 1593256140191, "actionType": "ADDED", "asgName": null &#125;, "instanceId": "eureka-client-provider:192.168.1.3:8081", "scheme": null&#125;, &#123; "host": "zhm-pc", "port": 8080, "metadata": &#123; "management.port": "8080" &#125;, "secure": false, "serviceId": "UNKNOWN", "uri": "http://zhm-pc:8080", "instanceInfo": &#123; "instanceId": "zhm-pc", "app": "UNKNOWN", "appGroupName": null, "ipAddr": "192.168.1.3", "sid": "na", "homePageUrl": "http://zhm-pc:8080/", "statusPageUrl": "http://zhm-pc:8080/actuator/info", "healthCheckUrl": "http://zhm-pc:8080/actuator/health", "secureHealthCheckUrl": null, "vipAddress": "unknown", "secureVipAddress": "unknown", "countryId": 1, "dataCenterInfo": &#123; "@class": "com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo", "name": "MyOwn" &#125;, "hostName": "zhm-pc", "status": "UP", "overriddenStatus": "UNKNOWN", "leaseInfo": &#123; "renewalIntervalInSecs": 30, "durationInSecs": 90, "registrationTimestamp": 1593255452890, "lastRenewalTimestamp": 1593256622230, "evictionTimestamp": 0, "serviceUpTimestamp": 1593255452891 &#125;, "isCoordinatingDiscoveryServer": false, "metadata": &#123; "management.port": "8080" &#125;, "lastUpdatedTimestamp": 1593255452892, "lastDirtyTimestamp": 1593255452440, "actionType": "ADDED", "asgName": null &#125;, "instanceId": "zhm-pc", "scheme": null&#125;, &#123; "host": "zhm-pc", "port": 8082, "metadata": &#123; "management.port": "8082" &#125;, "secure": false, "serviceId": "EUREKA-CLIENT-CONSUMER", "uri": "http://zhm-pc:8082", "instanceInfo": &#123; "instanceId": "eureka-client-consumer:192.168.1.3:8082", "app": "EUREKA-CLIENT-CONSUMER", "appGroupName": null, "ipAddr": "192.168.1.3", "sid": "na", "homePageUrl": "http://zhm-pc:8082/", "statusPageUrl": "http://zhm-pc:8082/actuator/info", "healthCheckUrl": "http://zhm-pc:8082/actuator/health", "secureHealthCheckUrl": null, "vipAddress": "eureka-client-consumer", "secureVipAddress": "eureka-client-consumer", "countryId": 1, "dataCenterInfo": &#123; "@class": "com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo", "name": "MyOwn" &#125;, "hostName": "zhm-pc", "status": "UP", "overriddenStatus": "UNKNOWN", "leaseInfo": &#123; "renewalIntervalInSecs": 30, "durationInSecs": 90, "registrationTimestamp": 1593256562280, "lastRenewalTimestamp": 1593256562280, "evictionTimestamp": 0, "serviceUpTimestamp": 1593256562280 &#125;, "isCoordinatingDiscoveryServer": false, "metadata": &#123; "management.port": "8082" &#125;, "lastUpdatedTimestamp": 1593256562280, "lastDirtyTimestamp": 1593256562192, "actionType": "ADDED", "asgName": null &#125;, "instanceId": "eureka-client-consumer:192.168.1.3:8082", "scheme": null&#125;] 源码跟踪AbstractInstanceRegistry]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink back pressure]]></title>
    <url>%2F2019%2F09%2F20%2Fflink%20back%20pressure%2F</url>
    <content type="text"><![CDATA[flink 背压: Flink 在运行时主要由 operators 和 streams 两大组件构成。每个 operator 会消费中间态的流，并在流上进行转换，然后生成新的流。对于 Flink 的网络机制一种形象的类比是，Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样 一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。Flink 解决反压的方案就是这种感觉。 在 Flink 中，这些分布式阻塞队列就是这些逻辑流，而队列容量是通过缓冲池来（LocalBufferPool）实现的。每个被生产和被消费的流都会被分配一个缓冲池。缓冲池管理着一组缓冲(Buffer)，缓冲在被消费后可以被回收循环利用。这很好理解：你从池子中拿走一个缓冲，填上数据，在数据消费完之后，又把缓冲还给池子，之后你可以再次使用它。 背压监测通过反复采集运行任务的堆栈跟踪样本来工作。JobManager为作业的任务触发对Thread.getStackTrace（）的重复调用。默认情况下，作业管理器为每个任务每50毫秒触发100个堆栈跟踪（也就是说一次反压检测至少要等待5秒钟），以确定背压。你在Web界面中看到的比率告诉你，这些堆栈跟踪中有多少被困在内部方法调用中，例如0.01表示只有100的1被困在该方法中。 配置：web.backpressure.refresh-interval: 不推荐使用可用统计信息并需要刷新的时间 (默认: 60000, 1 分钟).web.backpressure.num-samples: 测定背压所需的堆栈跟踪样本数 (默认: 100).web.backpressure.delay-between-samples: 测定背压的堆栈跟踪样本之间的延迟 (默认: 50, 50 ms). 参考官方文档：https://ci.apache.org/projects/flink/flink-docs-release-1.9/monitoring/back_pressure.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink AllowLateness]]></title>
    <url>%2F2019%2F08%2F29%2Fflink%20AllowLateness%2F</url>
    <content type="text"><![CDATA[AllowLateness：为了处理乱序问题而产生的概念默认情况下，当watermark通过end-of-window之后，再有之前的数据到达时，这些数据会被删除。 为了避免有些迟到的数据被删除，因此产生了allowedLateness的概念。 简单来讲，allowedLateness就是针对event time而言，对于watermark超过end-of-window之后，还允许有一段时间（也是以event time来衡量）来等待之前的数据到达，以便再次处理这些数据。 工具类：123456789101112131415161718192021222324252627282930313233343536package com.opensourceteams.module.bigdata.flink.window.streaming;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import java.time.Instant;import java.time.ZoneId;import java.time.format.DateTimeFormatter;/** * Created by zhanghongming on 2019/8/18. */@Data@AllArgsConstructor@NoArgsConstructorpublic class Order&#123; private long timestamp; private long userid; private long itemid; private long amount; private long price; private static final DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); @Override public String toString() &#123; String time = Instant.ofEpochMilli(timestamp).atZone(ZoneId.systemDefault()).format(timeFormatter); return "Order&#123;" + "timestamp=" + time + ", userid=" + userid + ", itemid=" + itemid + ", amount=" + amount + ", price=" + price + '&#125;'; &#125;&#125; 数据类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.opensourceteams.module.bigdata.flink.window.streaming;import com.opensourceteams.module.bigdata.flink.window.ThrottledIterator;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import java.io.Serializable;import java.time.Instant;import java.time.LocalDateTime;import java.time.ZoneId;import java.time.ZoneOffset;import java.time.format.DateTimeFormatter;import java.util.Date;import java.util.Iterator;import java.util.Random;@SuppressWarnings("serial")public class WindowData &#123; static final Long[] USERID = &#123;101L,100L&#125;; static final Long[] AMOUNT = &#123;10L,15L,20L,25L,30L&#125;; static final Long[] PRICE = &#123;1000L,2000L,3000L,4000L,5000L&#125;; static final DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); /** * Continuously generates (name, grade). */ public static class GradeSource implements Iterator&lt;Tuple5&lt;Long,Long, Long, Long, Long&gt;&gt;, Serializable &#123; private final Random rnd = new Random(hashCode()); private volatile long itemid = 0L; @Override public boolean hasNext() &#123; return true; &#125; @Override public Tuple5&lt;Long, Long, Long, Long, Long&gt; next() &#123; long time = System.currentTimeMillis(); if(itemid !=0 &amp;&amp; itemid%6 ==0)&#123; time = time-6000; &#125;else if(itemid !=0 &amp;&amp; itemid%10 ==0)&#123; time = time-8000; &#125; itemid++; Tuple5&lt;Long, Long, Long, Long, Long&gt; longLongLongLongLongTuple5 = new Tuple5&lt;&gt;(time , USERID[rnd.nextInt(USERID.length)], itemid , AMOUNT[rnd.nextInt(AMOUNT.length)], PRICE[rnd.nextInt(PRICE.length)]); System.out.println(Instant.ofEpochMilli(time).atZone(ZoneId.systemDefault()).format(timeFormatter)+"=="+longLongLongLongLongTuple5); return longLongLongLongLongTuple5; &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125; public static DataStream&lt;Tuple5&lt;Long,Long, Long, Long, Long&gt;&gt; getSource(StreamExecutionEnvironment env, long rate) &#123; return env.fromCollection(new ThrottledIterator&lt;&gt;(new GradeSource(), rate), TypeInformation.of(new TypeHint&lt;Tuple5&lt;Long,Long, Long, Long, Long&gt;&gt;()&#123;&#125;)); &#125; &#125;&#125; 工具类速率控制类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package com.opensourceteams.module.bigdata.flink.window;import java.io.Serializable;import java.util.Iterator;import static java.util.Objects.requireNonNull;public class ThrottledIterator&lt;T&gt; implements Iterator&lt;T&gt;, Serializable &#123; private static final long serialVersionUID = 1L; @SuppressWarnings("NonSerializableFieldInSerializableClass") private final Iterator&lt;T&gt; source; private final long sleepBatchSize; private final long sleepBatchTime; private long lastBatchCheckTime; private long num; public ThrottledIterator(Iterator&lt;T&gt; source, long elementsPerSecond) &#123; this.source = requireNonNull(source); if (!(source instanceof Serializable)) &#123; throw new IllegalArgumentException("source must be java.io.Serializable"); &#125; if (elementsPerSecond &gt;= 1) &#123; // how long does element take this.sleepBatchSize = 1; this.sleepBatchTime = 3000 / elementsPerSecond; &#125; else &#123; throw new IllegalArgumentException("'elements per second' must be positive and not zero"); &#125; &#125; @Override public boolean hasNext() &#123; return source.hasNext(); &#125; @Override public T next() &#123; // delay if necessary if (lastBatchCheckTime &gt; 0) &#123; if (++num &gt;= sleepBatchSize) &#123; num = 0; final long now = System.currentTimeMillis(); final long elapsed = now - lastBatchCheckTime; if (elapsed &lt; sleepBatchTime /2 ) &#123; try &#123; Thread.sleep(sleepBatchTime - elapsed); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); &#125; &#125;else &#123; try &#123; if (elapsed &lt; sleepBatchTime) &#123; Thread.sleep(sleepBatchTime - (sleepBatchTime-elapsed)); &#125;else if(elapsed == sleepBatchTime)&#123; Thread.sleep(sleepBatchTime); &#125;else&#123; Thread.sleep(sleepBatchTime - (elapsed - sleepBatchTime)); &#125; &#125; catch (InterruptedException e) &#123; // restore interrupt flag and proceed Thread.currentThread().interrupt(); &#125; &#125; lastBatchCheckTime = now; &#125; &#125; else &#123; lastBatchCheckTime = System.currentTimeMillis(); &#125; return source.next(); &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125;&#125; AllowLateness实现类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package com.opensourceteams.module.bigdata.flink.window.streaming;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;import org.apache.flink.streaming.api.functions.windowing.WindowFunction;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import org.apache.flink.util.OutputTag;import java.time.Instant;import java.time.ZoneId;import java.time.format.DateTimeFormatter;/** * Created by zhanghongming on 2019/8/17. */public class StreamingJobAllowLateness &#123; public static void main(String[] args) throws Exception &#123; Configuration configuration= new Configuration()&#123; &#123; setInteger("rest.port",9191); setBoolean("local.start-webserver",true); &#125; &#125;; final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration); env.setParallelism(3).setStreamTimeCharacteristic(TimeCharacteristic.EventTime); OutputTag&lt;Order&gt; lateOutputTag = new OutputTag&lt;Order&gt;("late-data")&#123;&#125;; final ParameterTool params = ParameterTool.fromArgs(args); final long rate = params.getLong("rate", 1L); DataStream&lt;Tuple5&lt;Long, Long, Long, Long, Long&gt;&gt; source = WindowData.GradeSource.getSource(env, rate); SingleOutputStreamOperator&lt;Tuple5&lt;Long, Long, Long, Long, Long&gt;&gt; outputStreamOperator = source.assignTimestampsAndWatermarks( new BoundedOutOfOrdernessTimestampExtractor&lt;Tuple5&lt;Long, Long, Long, Long, Long&gt;&gt;(Time.seconds(5)) &#123; @Override public long extractTimestamp(Tuple5&lt;Long, Long, Long, Long, Long&gt; longLongLongLongLongTuple5) &#123; return longLongLongLongLongTuple5.f0; &#125; &#125;); SingleOutputStreamOperator&lt;OrderSummary&gt; result = outputStreamOperator.keyBy(1).timeWindow(Time.seconds(5)) .allowedLateness(Time.seconds(2)) .aggregate(new AggregateFunction&lt;Tuple5&lt;Long, Long, Long, Long, Long&gt;, Tuple4&lt;Long, Long, Long, Long&gt;, Tuple4&lt;Long, Long, Long, Long&gt;&gt;() &#123; @Override public Tuple4&lt;Long, Long, Long, Long&gt; createAccumulator() &#123; return Tuple4.of(0L, 0L, 0L, 0L); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; add(Tuple5&lt;Long, Long, Long, Long, Long&gt; value, Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; return Tuple4.of(value.f1, accumulator.f1 + 1, accumulator.f2 + value.f2, accumulator.f3 + value.f3 * value.f4); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; getResult(Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; return accumulator; &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; merge(Tuple4&lt;Long, Long, Long, Long&gt; a, Tuple4&lt;Long, Long, Long, Long&gt; b) &#123; return Tuple4.of(a.f0 + b.f0, a.f1 + b.f1, a.f2 + b.f2, a.f3 + b.f3); &#125; &#125;, new WindowFunction&lt;Tuple4&lt;Long, Long, Long, Long&gt;, OrderSummary, Tuple, TimeWindow&gt;() &#123; @Override public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple4&lt;Long, Long, Long, Long&gt;&gt; input, Collector&lt;OrderSummary&gt; out) throws Exception &#123; input.forEach(record -&gt; out.collect(new OrderSummary(window.getStart(), window.getEnd(), record.f0, record.f1, record.f2, record.f3)) ); &#125; &#125;); result.print(); //result.getSideOutput(lateOutputTag).print(); env.execute("Flink StreamingJobEventTime"); &#125; private static class NameKeySelector implements KeySelector&lt;Tuple5&lt;Long,Long,Long,Long,Long&gt;, Long&gt; &#123; @Override public Long getKey(Tuple5&lt;Long, Long, Long, Long, Long&gt; longLongLongLongLongTuple5) throws Exception &#123; return longLongLongLongLongTuple5.f1; &#125; &#125; @Data @AllArgsConstructor @NoArgsConstructor public static class OrderSummary&#123; private static final DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); public OrderSummary(long start,long end,long userid,long orderNum,long amountNum,long total)&#123; this.windowStart = Instant.ofEpochMilli(start).atZone(ZoneId.systemDefault()).format(timeFormatter); this.windowEnd = Instant.ofEpochMilli(end).atZone(ZoneId.systemDefault()).format(timeFormatter); this.userid = userid; this.orderNum = orderNum; this.amountNum = amountNum; this.total = total; &#125; private String windowStart; private String windowEnd; private long userid; private long orderNum; private long amountNum; private long total; &#125;&#125; 延迟的数据会被收集到窗口中： assignTimestampsAndWatermarks 为五秒故23:03:20~23:03:25 秒的窗口的数据再23:03:25+5 =23:03:30秒的数据到来时触发了。 同理：23:03:35~23:03:40的窗口的数据再23:03:45秒后的第一个数据23:03:45被触发了计算由于allowedLateness(Time.seconds(2)) 允许2秒的延迟数据：因此在watermark &lt; end-of-window + allowedLateness23:03:47秒内到达的数据都被计算了]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事件时间]]></title>
    <url>%2F2019%2F08%2F17%2Fflink-Event_Time%2F</url>
    <content type="text"><![CDATA[精简介绍：Processing time:Processing time refers to the system time of the machine that is executing the respective operation.Processing time 是指执行相应操作的机器的系统时间。 Event time: : Event time is the time that each individual event occurred on its producing device. Event time programs must specify how to generate Event Time Watermarks, which is the mechanism that signals progress in event time. 事件时间是每个事件在其生产设备上发生的时间。 事件时间程序必须指定如何生成事件时间水印，这是事件时间中表示进度的机制 Ingestion time:Ingestion time is the time that events enter Flink. At the source operator each record gets the source’s current time as a timestamp, and time-based operations (like time windows) refer to that timestamp.摄取时间是事件进入燧石的时间。在源操作符中，每个记录都将源的当前时间作为时间戳，基于时间的操作（如时间窗口）引用该时间戳。 官方详细介绍：中文翻译： 处理时间：处理时间是指执行相应 算子操作的机器的系统时间。当流程序在处理时间运行时，所有基于时间的 算子操作（如时间窗口）将使用运行相应算子的机器的系统时钟。每小时处理时间窗口将包括在系统时钟指示整个小时之间到达特定算子的所有记录。例如，如果应用程序在上午9:15开始运行，则第一个每小时处理时间窗口将包括在上午9:15到上午10:00之间处理的事件，下一个窗口将包括在上午10:00到11:00之间处理的事件，因此上。 处理时间是最简单的时间概念，不需要流和机器之间的协调。它提供最佳性能和最低延迟。但是，在分布式和异步环境中，处理时间不提供确定性，因为它容易受到记录到达系统的速度（例如从消息队列）到记录在系统内的算子之间流动的速度的影响。和停电（预定或其他）。 事件时间：事件时间是每个事件在其生产设备上发生的时间。此时间通常在进入Flink之前嵌入记录中，并且 可以从每个记录中提取该事件时间戳。在事件时间，时间的进展取决于数据，而不是任何挂钟。事件时间程序必须指定如何生成事件时间水印，这是表示事件时间进度的机制。该水印机制在下面的后面部分中描述。在一个完美的世界中，事件时间处理将产生完全一致和确定的结果，无论事件何时到达，或者它们的排序。但是，除非事件已知按顺序到达（按时间戳），否则事件时间处理会在等待无序事件时产生一些延迟。由于只能等待一段有限的时间，因此限制了确定性事件时间应用程序的可能性。 假设所有数据都已到达，事件时间 算子操作将按预期运行，即使在处理无序或延迟事件或重新处理历史数据时也会产生正确且一致的结果。例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何，或者何时处理它们。（有关更多信息，请参阅有关迟发事件的部分。） 请注意，有时当事件时间程序实时处理实时数据时，它们将使用一些处理时间 算子操作，以确保它们及时进行。 摄取时间：摄取时间是事件进入Flink的时间。在源算子处，每个记录将源的当前时间作为时间戳，并且基于时间的 算子操作（如时间窗口）引用该时间戳。摄取时间在概念上位于事件时间和处理时间之间。与处理时间相比 ，它稍贵一些，但可以提供更可预测的结果。因为 摄取时间使用稳定的时间戳（在源处分配一次），所以对记录的不同窗口 算子操作将引用相同的时间戳，而在处理时间中，每个窗口算子可以将记录分配给不同的窗口（基于本地系统时钟和任何运输延误）。 与事件时间相比，摄取时间程序无法处理任何无序事件或后期数据，但程序不必指定如何生成水印。 在内部，摄取时间与事件时间非常相似，但具有自动时间戳分配和自动水印生成函数。 JobEventTime例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117package org.apache.flink.streaming.examples.wordcount.util;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;import org.apache.flink.streaming.api.functions.windowing.WindowFunction;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import java.time.Instant;import java.time.ZoneId;import java.time.format.DateTimeFormatter;/** * Created by zhanghongming on 2019/8/17. */public class StreamingJobEventTime &#123; public static void main(String[] args) throws Exception &#123; Configuration configuration= new Configuration()&#123; &#123; setInteger("rest.port",9191); setBoolean("local.start-webserver",true); &#125; &#125;; final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration); env.setParallelism(3).setStreamTimeCharacteristic(TimeCharacteristic.EventTime); env.fromElements( new Order(1218182888000L,100L,1001L,10L,1000L), new Order(1218182889000L,101L,1002L,15L,2000L), new Order(1218182890000L,100L,1003L,20L,3000L), new Order(1218182891000L,101L,1004L,25L,4000L), new Order(1218182892000L,100L,1005L,30L,5000L), new Order(1218182893000L,101L,1006L,35L,6000L) ).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;Order&gt;(Time.seconds(5)) &#123; @Override public long extractTimestamp(Order element) &#123; return element.getTimestamp(); &#125; &#125;).keyBy("userid") .timeWindow(Time.seconds(5)) .aggregate(new AggregateFunction&lt;Order, Tuple4&lt;Long, Long, Long, Long&gt;, Tuple4&lt;Long, Long, Long, Long&gt;&gt;() &#123; @Override public Tuple4&lt;Long, Long, Long, Long&gt; createAccumulator() &#123; return Tuple4.of(0L, 0L, 0L, 0L); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; add(Order value, Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; return Tuple4.of(value.userid, accumulator.f1 + 1, accumulator.f2 + value.amount, accumulator.f3 + value.amount * value.price); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; getResult(Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; return accumulator; &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; merge(Tuple4&lt;Long, Long, Long, Long&gt; a, Tuple4&lt;Long, Long, Long, Long&gt; b) &#123; return Tuple4.of(a.f0 + b.f0, a.f1 + b.f1, a.f2 + b.f2, a.f3 + b.f3); &#125; &#125;, new WindowFunction&lt;Tuple4&lt;Long,Long,Long,Long&gt;, OrderSummary, Tuple, TimeWindow&gt;() &#123; @Override public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple4&lt;Long, Long, Long, Long&gt;&gt; input, Collector&lt;OrderSummary&gt; out) throws Exception &#123; input.forEach(record -&gt; out.collect(new OrderSummary(window.getStart(),window.getEnd(),record.f0,record.f1,record.f2,record.f3)) ); &#125; &#125;).print(); env.execute("Flink StreamingJobEventTime"); &#125; @Data @AllArgsConstructor @NoArgsConstructor public static class Order&#123; private long timestamp; private long userid; private long itemid; private long amount; private long price; &#125; @Data @AllArgsConstructor @NoArgsConstructor public static class OrderSummary&#123; private static final DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); public OrderSummary(long start,long end,long userid,long orderNum,long amountNum,long total)&#123; this.windowStart = Instant.ofEpochMilli(start).atZone(ZoneId.systemDefault()).format(timeFormatter); this.windowEnd = Instant.ofEpochMilli(end).atZone(ZoneId.systemDefault()).format(timeFormatter); this.userid = userid; this.orderNum = orderNum; this.amountNum = amountNum; this.total = total; &#125; private String windowStart; private String windowEnd; private long userid; private long orderNum; private long amountNum; private long total; &#125;&#125; 执行结果: 12342&gt; StreamingJobEventTime.OrderSummary(windowStart=2008-08-08 16:08:05, windowEnd=2008-08-08 16:08:10, userid=100, orderNum=1, amountNum=10, total=10000)2&gt; StreamingJobEventTime.OrderSummary(windowStart=2008-08-08 16:08:10, windowEnd=2008-08-08 16:08:15, userid=100, orderNum=2, amountNum=50, total=210000)1&gt; StreamingJobEventTime.OrderSummary(windowStart=2008-08-08 16:08:05, windowEnd=2008-08-08 16:08:10, userid=101, orderNum=1, amountNum=15, total=30000)1&gt; StreamingJobEventTime.OrderSummary(windowStart=2008-08-08 16:08:10, windowEnd=2008-08-08 16:08:15, userid=101, orderNum=2, amountNum=60, total=310000) ProcessingTime例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117package org.apache.flink.streaming.examples.wordcount.util;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;import org.apache.flink.streaming.api.functions.windowing.WindowFunction;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import java.time.Instant;import java.time.ZoneId;import java.time.format.DateTimeFormatter;/** * Created by zhanghongming on 2019/8/17. */public class StreamingJobProcessingTime &#123; public static void main(String[] args) throws Exception &#123; Configuration configuration= new Configuration()&#123; &#123; setInteger("rest.port",9191); setBoolean("local.start-webserver",true); &#125; &#125;; final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration); env.setParallelism(1).setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); env.fromElements( new Order(1218182888000L,100L,1001L,10L,1000L), new Order(1218182889000L,101L,1002L,15L,2000L), new Order(1218182890000L,100L,1003L,20L,3000L), new Order(1218182891000L,101L,1004L,25L,4000L), new Order(1218182892000L,100L,1005L,30L,5000L), new Order(1218182893000L,101L,1006L,35L,6000L) ).keyBy("userid") .timeWindow(Time.seconds(5)) .aggregate(new AggregateFunction&lt;Order, Tuple4&lt;Long, Long, Long, Long&gt;, Tuple4&lt;Long, Long, Long, Long&gt;&gt;() &#123; @Override public Tuple4&lt;Long, Long, Long, Long&gt; createAccumulator() &#123; return Tuple4.of(0L, 0L, 0L, 0L); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; add(Order value, Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return Tuple4.of(value.userid, accumulator.f1 + 1, accumulator.f2 + value.amount, accumulator.f3 + value.amount * value.price); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; getResult(Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; return accumulator; &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; merge(Tuple4&lt;Long, Long, Long, Long&gt; a, Tuple4&lt;Long, Long, Long, Long&gt; b) &#123; return Tuple4.of(a.f0 + b.f0, a.f1 + b.f1, a.f2 + b.f2, a.f3 + b.f3); &#125; &#125;, new WindowFunction&lt;Tuple4&lt;Long,Long,Long,Long&gt;, OrderSummary, Tuple, TimeWindow&gt;() &#123; @Override public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple4&lt;Long, Long, Long, Long&gt;&gt; input, Collector&lt;OrderSummary&gt; out) throws Exception &#123; input.forEach(record -&gt; out.collect(new OrderSummary(window.getStart(),window.getEnd(),record.f0,record.f1,record.f2,record.f3)) ); &#125; &#125;).print(); env.execute("Flink StreamingJobEventTime"); &#125; @Data @AllArgsConstructor @NoArgsConstructor public static class Order&#123; private long timestamp; private long userid; private long itemid; private long amount; private long price; &#125; @Data @AllArgsConstructor @NoArgsConstructor public static class OrderSummary&#123; private static final DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); public OrderSummary(long start,long end,long userid,long orderNum,long amountNum,long total)&#123; this.windowStart = Instant.ofEpochMilli(start).atZone(ZoneId.systemDefault()).format(timeFormatter); this.windowEnd = Instant.ofEpochMilli(end).atZone(ZoneId.systemDefault()).format(timeFormatter); this.userid = userid; this.orderNum = orderNum; this.amountNum = amountNum; this.total = total; &#125; private String windowStart; private String windowEnd; private long userid; private long orderNum; private long amountNum; private long total; &#125;&#125; 执行结果: 123456StreamingJobProcessingTime.OrderSummary(windowStart=2019-08-17 22:03:35, windowEnd=2019-08-17 22:03:40, userid=100, orderNum=1, amountNum=10, total=10000)StreamingJobProcessingTime.OrderSummary(windowStart=2019-08-17 22:03:40, windowEnd=2019-08-17 22:03:45, userid=101, orderNum=1, amountNum=15, total=30000)StreamingJobProcessingTime.OrderSummary(windowStart=2019-08-17 22:03:45, windowEnd=2019-08-17 22:03:50, userid=100, orderNum=1, amountNum=20, total=60000)StreamingJobProcessingTime.OrderSummary(windowStart=2019-08-17 22:03:45, windowEnd=2019-08-17 22:03:50, userid=101, orderNum=1, amountNum=25, total=100000)StreamingJobProcessingTime.OrderSummary(windowStart=2019-08-17 22:03:50, windowEnd=2019-08-17 22:03:55, userid=100, orderNum=1, amountNum=30, total=150000)StreamingJobProcessingTime.OrderSummary(windowStart=2019-08-17 22:03:50, windowEnd=2019-08-17 22:03:55, userid=101, orderNum=1, amountNum=35, total=210000) WatermarksFlink中测量事件时间进度的机制是水印。水印作为数据流的一部分流动并带有时间戳t。水印（t）声明事件时间已达到该流中的时间t，这意味着时间戳t’&lt;=t（即时间戳较旧或等于水印的事件）的流中不应存在更多元素。 水印对于无序流至关重要，如下图所示，其中事件不是按时间戳排序的。一般来说，水印是一种声明，在流中的该点之前，到某个时间戳之前的所有事件都应该到达。一旦operator抵达水印处，operator可以将其内部事件时钟提前到水印的值。 Allowed Lateness:在处理事件时间窗口时，可能会发生元素到达晚的情况，即Flink用来跟踪事件时间进度的水印已经超过了元素所属窗口的结束时间戳。有关Flink如何处理事件时间的更深入讨论，请参阅事件时间，尤其是后期元素。 默认情况下，当水印超过窗口末尾时，将删除后期元素。然而，Flink允许为窗口操作符指定最大允许延迟。allowed lateness指定在删除元素之前可以延迟多少时间，其默认值为0。在水印通过窗口结束之后，但在它通过窗口结束之前，再加上允许的延迟时间到达的元素仍会添加到窗口中。根据使用的触发器，延迟但未丢弃的元素可能会导致窗口再次触发。这就是EventTimeTrigger的情况。 Allowed Lateness例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package org.apache.flink.streaming.examples.wordcount.util;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import org.apache.flink.api.common.functions.AggregateFunction;import org.apache.flink.api.java.tuple.Tuple;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;import org.apache.flink.streaming.api.functions.windowing.WindowFunction;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.util.Collector;import org.apache.flink.util.OutputTag;import java.time.Instant;import java.time.ZoneId;import java.time.format.DateTimeFormatter;/** * Created by zhanghongming on 2019/8/17. */public class StreamingJobAllowLateness &#123; public static void main(String[] args) throws Exception &#123; Configuration configuration= new Configuration()&#123; &#123; setInteger("rest.port",9191); setBoolean("local.start-webserver",true); &#125; &#125;; final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration); env.setParallelism(3).setStreamTimeCharacteristic(TimeCharacteristic.EventTime); final OutputTag&lt;Order&gt; lateOutputTag = new OutputTag&lt;Order&gt;("late-data")&#123;&#125;; SingleOutputStreamOperator&lt;OrderSummary&gt; result = env.fromElements( new Order(1218182888000L, 100L, 1001L, 10L, 1000L), new Order(1218182889000L, 101L, 1002L, 15L, 2000L), new Order(1218182890000L, 100L, 1003L, 20L, 3000L), new Order(1218182891000L, 101L, 1004L, 25L, 4000L), new Order(1218182892000L, 100L, 1005L, 30L, 5000L), new Order(1218182893000L, 101L, 1006L, 35L, 6000L), new Order(1218182896000L, 101L, 1006L, 35L, 6000L), new Order(1218182888000L, 100L, 1007L, 40L, 7000L) ).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;Order&gt;(Time.seconds(5)) &#123; @Override public long extractTimestamp(Order element) &#123; return element.getTimestamp(); &#125; &#125;).keyBy("userid") .timeWindow(Time.seconds(5)) .sideOutputLateData(lateOutputTag) .allowedLateness(Time.seconds(2)) .aggregate(new AggregateFunction&lt;Order, Tuple4&lt;Long, Long, Long, Long&gt;, Tuple4&lt;Long, Long, Long, Long&gt;&gt;() &#123; @Override public Tuple4&lt;Long, Long, Long, Long&gt; createAccumulator() &#123; return Tuple4.of(0L, 0L, 0L, 0L); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; add(Order value, Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; return Tuple4.of(value.userid, accumulator.f1 + 1, accumulator.f2 + value.amount, accumulator.f3 + value.amount * value.price); &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; getResult(Tuple4&lt;Long, Long, Long, Long&gt; accumulator) &#123; return accumulator; &#125; @Override public Tuple4&lt;Long, Long, Long, Long&gt; merge(Tuple4&lt;Long, Long, Long, Long&gt; a, Tuple4&lt;Long, Long, Long, Long&gt; b) &#123; return Tuple4.of(a.f0 + b.f0, a.f1 + b.f1, a.f2 + b.f2, a.f3 + b.f3); &#125; &#125;, new WindowFunction&lt;Tuple4&lt;Long, Long, Long, Long&gt;, OrderSummary, Tuple, TimeWindow&gt;() &#123; @Override public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple4&lt;Long, Long, Long, Long&gt;&gt; input, Collector&lt;OrderSummary&gt; out) throws Exception &#123; input.forEach(record -&gt; out.collect(new OrderSummary(window.getStart(), window.getEnd(), record.f0, record.f1, record.f2, record.f3)) ); &#125; &#125;); result.print(); result.getSideOutput(lateOutputTag).print(); env.execute("Flink StreamingJobEventTime"); &#125; @Data @AllArgsConstructor @NoArgsConstructor public static class Order&#123; private long timestamp; private long userid; private long itemid; private long amount; private long price; &#125; @Data @AllArgsConstructor @NoArgsConstructor public static class OrderSummary&#123; private static final DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); public OrderSummary(long start,long end,long userid,long orderNum,long amountNum,long total)&#123; this.windowStart = Instant.ofEpochMilli(start).atZone(ZoneId.systemDefault()).format(timeFormatter); this.windowEnd = Instant.ofEpochMilli(end).atZone(ZoneId.systemDefault()).format(timeFormatter); this.userid = userid; this.orderNum = orderNum; this.amountNum = amountNum; this.total = total; &#125; private String windowStart; private String windowEnd; private long userid; private long orderNum; private long amountNum; private long total; &#125;&#125; 黑色为输出的晚到数据]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink sliding window join]]></title>
    <url>%2F2019%2F08%2F13%2Fflink%20sliding%20window%20join%2F</url>
    <content type="text"><![CDATA[接上篇文章flink operator 具体分析下window join官网地址：https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/operators/joining.html 根据官网例子从demo实现： 工具类：模拟source：GreenSource 发送：（”tom”,0）,（”jerry”,1） , （”alice”,2） , （”tom”,3）, （”tom”,4）OrangeSource 发送：（”tom”,0）,（”tom”,1） , （”tom”,2） , （”tom”,3）, （”tom”,4）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.opensourceteams.module.bigdata.flink.window;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import java.io.Serializable;import java.util.Iterator;import java.util.Random;@SuppressWarnings("serial")public class WindowJoinSampleData &#123; static final String[] KEY1 = &#123;"tom", "jerry", "alice", "tom", "tom"&#125;; static final String[] KEY2 = &#123;"tom", "tom", "tom", "tom", "tom"&#125;; static final int[] VALUE = &#123;0,1,2,3,4&#125;; public static class GreenSource implements Iterator&lt;Tuple2&lt;String, Integer&gt;&gt;, Serializable &#123; static int GRADE_COUNT = 0; private final Random rnd = new Random(hashCode()); @Override public boolean hasNext() &#123; return GRADE_COUNT == 5 ? false :true; &#125; @Override public Tuple2&lt;String, Integer&gt; next() &#123; Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = new Tuple2&lt;&gt;(KEY1[GRADE_COUNT], VALUE[GRADE_COUNT]); GRADE_COUNT ++; return stringIntegerTuple2; &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125; public static DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; getSource(StreamExecutionEnvironment env, long rate) &#123; return env.fromCollection(new ThrottledIterator&lt;&gt;(new GreenSource(), rate), TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;()&#123;&#125;)); &#125; &#125; /** * Continuously generates (name, salary). */ public static class OrangeSource implements Iterator&lt;Tuple2&lt;String, Integer&gt;&gt;, Serializable &#123; static int GRADE_COUNT = 0; private final Random rnd = new Random(hashCode()); @Override public boolean hasNext() &#123; return GRADE_COUNT == 5 ? false :true; &#125; @Override public Tuple2&lt;String, Integer&gt; next() &#123; Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = new Tuple2&lt;&gt;(KEY2[GRADE_COUNT], VALUE[GRADE_COUNT]); GRADE_COUNT ++; return stringIntegerTuple2; &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125; public static DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; getSource(StreamExecutionEnvironment env, long rate) &#123; return env.fromCollection(new ThrottledIterator&lt;&gt;(new OrangeSource(), rate), TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;()&#123;&#125;)); &#125; &#125;&#125; 模拟频率发送，每隔五秒发送一次数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package com.opensourceteams.module.bigdata.flink.window;import java.io.Serializable;import java.util.Iterator;import static java.util.Objects.requireNonNull;public class ThrottledIterator&lt;T&gt; implements Iterator&lt;T&gt;, Serializable &#123; private static final long serialVersionUID = 1L; @SuppressWarnings("NonSerializableFieldInSerializableClass") private final Iterator&lt;T&gt; source; private final long sleepBatchSize; private final long sleepBatchTime; private long lastBatchCheckTime; private long num; public ThrottledIterator(Iterator&lt;T&gt; source, long elementsPerSecond) &#123; this.source = requireNonNull(source); if (!(source instanceof Serializable)) &#123; throw new IllegalArgumentException("source must be java.io.Serializable"); &#125; if (elementsPerSecond &gt;= 1) &#123; // how long does element take this.sleepBatchSize = 1; this.sleepBatchTime = 5000 / elementsPerSecond; &#125; else &#123; throw new IllegalArgumentException("'elements per second' must be positive and not zero"); &#125; &#125; @Override public boolean hasNext() &#123; return source.hasNext(); &#125; @Override public T next() &#123; // delay if necessary if (lastBatchCheckTime &gt; 0) &#123; if (++num &gt;= sleepBatchSize) &#123; num = 0; final long now = System.currentTimeMillis(); final long elapsed = now - lastBatchCheckTime; if (elapsed &lt; sleepBatchTime /2 ) &#123; try &#123; Thread.sleep(sleepBatchTime - elapsed); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); &#125; &#125;else &#123; try &#123; if (elapsed &lt; sleepBatchTime) &#123; Thread.sleep(sleepBatchTime - (sleepBatchTime-elapsed)); &#125;else if(elapsed == sleepBatchTime)&#123; Thread.sleep(sleepBatchTime); &#125;else&#123; Thread.sleep(sleepBatchTime - (elapsed - sleepBatchTime)); &#125; &#125; catch (InterruptedException e) &#123; // restore interrupt flag and proceed Thread.currentThread().interrupt(); &#125; &#125; lastBatchCheckTime = now; &#125; &#125; else &#123; lastBatchCheckTime = System.currentTimeMillis(); &#125; return source.next(); &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125;&#125; 主函数：为观察方便，将时间扩大5000倍：window滑动窗口5秒滑动一次，窗口大小为10秒12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.opensourceteams.module.bigdata.flink.window;import org.apache.flink.api.common.functions.JoinFunction;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;@SuppressWarnings("serial")public class SlingdingWindowJoin &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final long windowSize = params.getLong("windowSize", 10000); final long windowSlide = params.getLong("windowSize", 5000); final long rate = params.getLong("rate", 1L); // obtain execution environment, run this example in "ingestion time" StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); // make parameters available in the web interface env.getConfig().setGlobalJobParameters(params); // create the data sources for both grades and salaries DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; orangeStream = WindowJoinSampleData.OrangeSource.getSource(env, rate); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; greenStream = WindowJoinSampleData.GreenSource.getSource(env, rate); // run the actual window join program // for testability, this functionality is in a separate method. DataStream&lt;String&gt; joinedStream = runWindowJoin(orangeStream, greenStream, windowSize,windowSlide); // print the results with a single thread, rather than in parallel joinedStream.print().setParallelism(1); // execute program env.execute("Slingding Window Join Example"); &#125; public static DataStream&lt;String&gt; runWindowJoin( DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades, DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries, long windowSize,long windowSlide) &#123; return grades.join(salaries) .where(new NameKeySelector()) .equalTo(new NameKeySelector()) .window(SlidingEventTimeWindows.of(Time.milliseconds(windowSize), Time.milliseconds(windowSlide ))) .apply(new JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, String&gt;() &#123; @Override public String join( Tuple2&lt;String, Integer&gt; first, Tuple2&lt;String, Integer&gt; second) &#123; return first.f1 + "," + second.f1; &#125; &#125;); &#125; private static class NameKeySelector implements KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt; &#123; @Override public String getKey(Tuple2&lt;String, Integer&gt; value) &#123; return value.f0; &#125; &#125;&#125; 预期结果： 运行结果：]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink operator]]></title>
    <url>%2F2019%2F08%2F10%2Fflink%20operator%2F</url>
    <content type="text"><![CDATA[接源码分析二。详细分析下Operator Transformation:1.DataStream -&gt; DataStream map flatMap filter map:1234567DataStream&lt;Integer&gt; dataStream = //...dataStream.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer value) throws Exception &#123; return 2 * value; &#125;&#125;); flatMap:123456789dataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for(String word: value.split(" "))&#123; out.collect(word); &#125; &#125;&#125;); filter123456dataStream.filter(new FilterFunction&lt;Integer&gt;() &#123; @Override public boolean filter(Integer value) throws Exception &#123; return value != 0; &#125;&#125;); 2.DataStream -&gt; KeyedDataStream keyBy 12dataStream.keyBy("someKey") // Key by field "someKey"dataStream.keyBy(0) // Key by the first element of a Tuple 3.KeyedDataStream -&gt; DataStream sum reduce fold min/minBy max/maxBy reduce:1234567keyedStream.reduce(new ReduceFunction&lt;Integer&gt;() &#123; @Override public Integer reduce(Integer value1, Integer value2) throws Exception &#123; return value1 + value2; &#125;&#125;); flod:1234567DataStream&lt;String&gt; result = keyedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String current, Integer value) &#123; return current + "-" + value; &#125; &#125;); aggregations: 12345678910keyedStream.sum(0);keyedStream.sum("key");keyedStream.min(0);keyedStream.min("key");keyedStream.max(0);keyedStream.max("key");keyedStream.minBy(0);keyedStream.minBy("key");keyedStream.maxBy(0);keyedStream.maxBy("key"); 4.WindowedStream -&gt; DataStream sum reduce fold min/minBy max/maxBy apply 1dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data apply:12345678910111213141516171819202122232425windowedStream.apply (new WindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Tuple, Window&gt;() &#123; public void apply (Tuple tuple, Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;);// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply (new AllWindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Window&gt;() &#123; public void apply (Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;); reduce:12345windowedStream.reduce (new ReduceFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123; public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception &#123; return new Tuple2&lt;String,Integer&gt;(value1.f0, value1.f1 + value2.f1); &#125;&#125;); fold:12345windowedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; public String fold(String current, Integer value) &#123; return current + "-" + value; &#125;&#125;) aggregations:12345678910windowedStream.sum(0);windowedStream.sum("key");windowedStream.min(0);windowedStream.min("key");windowedStream.max(0);windowedStream.max("key");windowedStream.minBy(0);windowedStream.minBy("key");windowedStream.maxBy(0);windowedStream.maxBy("key"); window下aggregate 会在整个window内aggregate。非windows会执行rolling aggregate 5.DataStream,DataStream -&gt; ConnectedStream connect 6.DataStream -&gt; SplitStream split 12345678910111213SplitStream&lt;Integer&gt; split = someDataStream.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) &#123; output.add("even"); &#125; else &#123; output.add("odd"); &#125; return output; &#125;&#125;); 7.SplitStream -&gt; DataStream select 1234SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select("even");DataStream&lt;Integer&gt; odd = split.select("odd");DataStream&lt;Integer&gt; all = split.select("even","odd"); Flink JoinTumbling Window Join 12345678910111213141516171819import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time; ...DataStream&lt;Integer&gt; orangeStream = ...DataStream&lt;Integer&gt; greenStream = ...orangeStream.join(greenStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(TumblingEventTimeWindows.of(Time.milliseconds(2))) .apply (new JoinFunction&lt;Integer, Integer, String&gt; ()&#123; @Override public String join(Integer first, Integer second) &#123; return first + "," + second; &#125; &#125;); Sliding Window Join 12345678910111213141516171819import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;...DataStream&lt;Integer&gt; orangeStream = ...DataStream&lt;Integer&gt; greenStream = ...orangeStream.join(greenStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(SlidingEventTimeWindows.of(Time.milliseconds(2) /* size */, Time.milliseconds(1) /* slide */)) .apply (new JoinFunction&lt;Integer, Integer, String&gt; ()&#123; @Override public String join(Integer first, Integer second) &#123; return first + "," + second; &#125; &#125;); Session Window Join 12345678910111213141516171819import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows;import org.apache.flink.streaming.api.windowing.time.Time; ...DataStream&lt;Integer&gt; orangeStream = ...DataStream&lt;Integer&gt; greenStream = ...orangeStream.join(greenStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(EventTimeSessionWindows.withGap(Time.milliseconds(1))) .apply (new JoinFunction&lt;Integer, Integer, String&gt; ()&#123; @Override public String join(Integer first, Integer second) &#123; return first + "," + second; &#125; &#125;); Interval Join 1234567891011121314151617181920import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;import org.apache.flink.streaming.api.windowing.time.Time;... DataStream&lt;Integer&gt; orangeStream = ... DataStream&lt;Integer&gt; greenStream = ... orangeStream .keyBy(&lt;KeySelector&gt;) .intervalJoin(greenStream.keyBy(&lt;KeySelector&gt;)) .between(Time.milliseconds(-2), Time.milliseconds(1)) .process (new ProcessJoinFunction&lt;Integer, Integer, String()&#123; @Override public void processElement(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out) &#123; out.collect(first + "," + second); &#125; &#125;); Async I/O API: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// This example implements the asynchronous request and callback with Futures that have the// interface of Java 8's futures (which is the same one followed by Flink's Future)/** * An implementation of the 'AsyncFunction' that sends requests and sets the callback. */class AsyncDatabaseRequest extends RichAsyncFunction&lt;String, Tuple2&lt;String, String&gt;&gt; &#123; /** The database specific client that can issue concurrent requests with callbacks */ private transient DatabaseClient client; @Override public void open(Configuration parameters) throws Exception &#123; client = new DatabaseClient(host, post, credentials); &#125; @Override public void close() throws Exception &#123; client.close(); &#125; @Override public void asyncInvoke(String key, final ResultFuture&lt;Tuple2&lt;String, String&gt;&gt; resultFuture) throws Exception &#123; // issue the asynchronous request, receive a future for result final Future&lt;String&gt; result = client.query(key); // set the callback to be executed once the request by the client is complete // the callback simply forwards the result to the result future CompletableFuture.supplyAsync(new Supplier&lt;String&gt;() &#123; @Override public String get() &#123; try &#123; return result.get(); &#125; catch (InterruptedException | ExecutionException e) &#123; // Normally handled explicitly. return null; &#125; &#125; &#125;).thenAccept( (String dbResult) -&gt; &#123; resultFuture.complete(Collections.singleton(new Tuple2&lt;&gt;(key, dbResult))); &#125;); &#125;&#125;// create the original streamDataStream&lt;String&gt; stream = ...;// apply the async I/O transformationDataStream&lt;Tuple2&lt;String, String&gt;&gt; resultStream = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100);]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink源码3--StreamGraph->JobGraph->ExecutionGraph]]></title>
    <url>%2F2019%2F08%2F04%2Fflink%E6%BA%90%E7%A0%813--StreamGraph--JobGraph--ExecutionGraph%2F</url>
    <content type="text"><![CDATA[我们接着分析StreamExecutionEnvironment这个类的实现类：即我们在调用env.execute(“Flink StreamingChainingDemo”);此处最新版的flink代码和以前的结构不同。先是调用的StreamExecutionEnvironment的execute代码 public JobExecutionResult execute(String jobName) throws Exception { Preconditions.checkNotNull(jobName, &quot;Streaming Job name should not be null.&quot;); return execute(getStreamGraph(jobName)); } @Internal public StreamGraph getStreamGraph(String jobName) { return getStreamGraphGenerator().setJobName(jobName).generate(); } private StreamGraphGenerator getStreamGraphGenerator() { if (transformations.size() &lt;= 0) { throw new IllegalStateException(&quot;No operators defined in streaming topology. Cannot execute.&quot;); } return new StreamGraphGenerator(transformations, config, checkpointCfg) .setStateBackend(defaultStateBackend)//null .setChaining(isChainingEnabled)//true .setUserArtifacts(cacheFile) .setTimeCharacteristic(timeCharacteristic)//DEFAULT_TIME_CHARACTERISTIC = TimeCharacteristic.ProcessingTime; //IngestionTime .setDefaultBufferTimeout(bufferTimeout);//100 } 根据transformations, config, checkpointCfg初始化StreamGraphGenerator JobGraph:接着会进入到LocalStreamEnvironment 这个实现类里面 public JobExecutionResult execute(StreamGraph streamGraph) throws Exception { JobGraph jobGraph = streamGraph.getJobGraph(); jobGraph.setAllowQueuedScheduling(true); Configuration configuration = new Configuration(); configuration.addAll(jobGraph.getJobConfiguration()); configuration.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, &quot;0&quot;); // add (and override) the settings with what the user defined configuration.addAll(this.configuration); if (!configuration.contains(RestOptions.BIND_PORT)) { configuration.setString(RestOptions.BIND_PORT, &quot;0&quot;); } int numSlotsPerTaskManager = configuration.getInteger(TaskManagerOptions.NUM_TASK_SLOTS, jobGraph.getMaximumParallelism()); MiniClusterConfiguration cfg = new MiniClusterConfiguration.Builder() .setConfiguration(configuration) .setNumSlotsPerTaskManager(numSlotsPerTaskManager) .build(); if (LOG.isInfoEnabled()) { LOG.info(&quot;Running job on local embedded Flink mini cluster&quot;); } MiniCluster miniCluster = new MiniCluster(cfg); try { miniCluster.start(); configuration.setInteger(RestOptions.PORT, miniCluster.getRestAddress().get().getPort()); return miniCluster.executeJobBlocking(jobGraph); } finally { transformations.clear(); miniCluster.close(); } } streamGraph.getJobGraph 里 @SuppressWarnings(&quot;deprecation&quot;) @Override public JobGraph getJobGraph(@Nullable JobID jobID) { // temporarily forbid checkpointing for iterative jobs if (isIterative() &amp;&amp; checkpointConfig.isCheckpointingEnabled() &amp;&amp; !checkpointConfig.isForceCheckpointing()) { throw new UnsupportedOperationException( &quot;Checkpointing is currently not supported by default for iterative jobs, as we cannot guarantee exactly once semantics. &quot; + &quot;State checkpoints happen normally, but records in-transit during the snapshot will be lost upon failure. &quot; + &quot;\nThe user can force enable state checkpoints with the reduced guarantees by calling: env.enableCheckpointing(interval,true)&quot;); } return StreamingJobGraphGenerator.createJobGraph(this, jobID); } public static JobGraph createJobGraph(StreamGraph streamGraph, @Nullable JobID jobID) { return new StreamingJobGraphGenerator(streamGraph, jobID).createJobGraph(); } private JobGraph createJobGraph() { // make sure that all vertices start immediately jobGraph.setScheduleMode(streamGraph.getScheduleMode()); // Generate deterministic hashes for the nodes in order to identify them across // submission iff they didn&apos;t change. Map&lt;Integer, byte[]&gt; hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph); // Generate legacy version hashes for backwards compatibility List&lt;Map&lt;Integer, byte[]&gt;&gt; legacyHashes = new ArrayList&lt;&gt;(legacyStreamGraphHashers.size()); for (StreamGraphHasher hasher : legacyStreamGraphHashers) { legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph)); } Map&lt;Integer, List&lt;Tuple2&lt;byte[], byte[]&gt;&gt;&gt; chainedOperatorHashes = new HashMap&lt;&gt;(); setChaining(hashes, legacyHashes, chainedOperatorHashes); setPhysicalEdges(); setSlotSharingAndCoLocation(); configureCheckpointing(); JobGraphGenerator.addUserArtifactEntries(streamGraph.getUserArtifacts(), jobGraph); // set the ExecutionConfig last when it has been finalized try { jobGraph.setExecutionConfig(streamGraph.getExecutionConfig()); } catch (IOException e) { throw new IllegalConfigurationException(&quot;Could not serialize the ExecutionConfig.&quot; + &quot;This indicates that non-serializable types (like custom serializers) were registered&quot;); } return jobGraph; } 上面setChaining(hashes, legacyHashes, chainedOperatorHashes);其核心代码为：createChain的isChainable public static boolean isChainable(StreamEdge edge, StreamGraph streamGraph) { StreamNode upStreamVertex = streamGraph.getSourceVertex(edge);//获取StreamEdge的源和目标StreamNode StreamNode downStreamVertex = streamGraph.getTargetVertex(edge); StreamOperatorFactory&lt;?&gt; headOperator = upStreamVertex.getOperatorFactory();//获取源和目标StreamNode中的StreamOperator StreamOperatorFactory&lt;?&gt; outOperator = downStreamVertex.getOperatorFactory(); //可以chaining的条件： return downStreamVertex.getInEdges().size() == 1//下游节点只有一个输入 &amp;&amp; outOperator != null//下游节点的操作符不为null &amp;&amp; headOperator != null//上游节点的操作符不为null &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex)//上下游节点在一个槽位共享组内 &amp;&amp; outOperator.getChainingStrategy() == ChainingStrategy.ALWAYS//下游节点的连接策略是 ALWAYS &amp;&amp; (headOperator.getChainingStrategy() == ChainingStrategy.HEAD ||//上游节点的连接策略是 HEAD 或者 ALWAYS headOperator.getChainingStrategy() == ChainingStrategy.ALWAYS) &amp;&amp; (edge.getPartitioner() instanceof ForwardPartitioner)//edge 的分区函数是 ForwardPartitioner 的实例 &amp;&amp; edge.getShuffleMode() != ShuffleMode.BATCH//边的shuffle模式为BATCH &amp;&amp; upStreamVertex.getParallelism() == downStreamVertex.getParallelism()//上下游节点的并行度相等 &amp;&amp; streamGraph.isChainingEnabled();//isChainingEnabled为true，默认为true } 只有上述的10个条件都同时满足时，才能说明两个StreamEdge的源和目标StreamNode是可以链接在一起执行的 private List&lt;StreamEdge&gt; createChain( Integer startNodeId, Integer currentNodeId, Map&lt;Integer, byte[]&gt; hashes, List&lt;Map&lt;Integer, byte[]&gt;&gt; legacyHashes, int chainIndex, Map&lt;Integer, List&lt;Tuple2&lt;byte[], byte[]&gt;&gt;&gt; chainedOperatorHashes) { if (!builtVertices.contains(startNodeId)) { List&lt;StreamEdge&gt; transitiveOutEdges = new ArrayList&lt;StreamEdge&gt;(); List&lt;StreamEdge&gt; chainableOutputs = new ArrayList&lt;StreamEdge&gt;(); List&lt;StreamEdge&gt; nonChainableOutputs = new ArrayList&lt;StreamEdge&gt;(); StreamNode currentNode = streamGraph.getStreamNode(currentNodeId); for (StreamEdge outEdge : currentNode.getOutEdges()) { if (isChainable(outEdge, streamGraph)) { chainableOutputs.add(outEdge); } else { nonChainableOutputs.add(outEdge); } } for (StreamEdge chainable : chainableOutputs) { transitiveOutEdges.addAll( createChain(startNodeId, chainable.getTargetId(), hashes, legacyHashes, chainIndex + 1, chainedOperatorHashes)); } for (StreamEdge nonChainable : nonChainableOutputs) { transitiveOutEdges.add(nonChainable);//不可连接的StreamEdge,输出StreamEdge放入transitiveOutEdges createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes, legacyHashes, 0, chainedOperatorHashes);//继续遍历可chaining的节点 } //获取头节点散列值，没有初始化为空链表 List&lt;Tuple2&lt;byte[], byte[]&gt;&gt; operatorHashes = chainedOperatorHashes.computeIfAbsent(startNodeId, k -&gt; new ArrayList&lt;&gt;()); //获取当前节点散列值 byte[] primaryHashBytes = hashes.get(currentNodeId); OperatorID currentOperatorId = new OperatorID(primaryHashBytes); //遍历legacyHashes 与primaryHashBytes组成二元数组，添加到链表中 for (Map&lt;Integer, byte[]&gt; legacyHash : legacyHashes) { operatorHashes.add(new Tuple2&lt;&gt;(primaryHashBytes, legacyHash.get(currentNodeId))); } //通过-&gt; 拼接chaining名称 chainedNames.put(currentNodeId, createChainedName(currentNodeId, chainableOutputs)); chainedMinResources.put(currentNodeId, createChainedMinResources(currentNodeId, chainableOutputs)); chainedPreferredResources.put(currentNodeId, createChainedPreferredResources(currentNodeId, chainableOutputs)); if (currentNode.getInputFormat() != null) { getOrCreateFormatContainer(startNodeId).addInputFormat(currentOperatorId, currentNode.getInputFormat()); } if (currentNode.getOutputFormat() != null) { getOrCreateFormatContainer(startNodeId).addOutputFormat(currentOperatorId, currentNode.getOutputFormat()); } //创建jobVertex并设置并行度返回StreamConfig实例 StreamConfig config = currentNodeId.equals(startNodeId) ? createJobVertex(startNodeId, hashes, legacyHashes, chainedOperatorHashes) : new StreamConfig(new Configuration()); //设置序列化器，StreamOperator，checkpoint（默认AT_LEAST_ONCE） setVertexConfig(currentNodeId, config, chainableOutputs, nonChainableOutputs); if (currentNodeId.equals(startNodeId)) { config.setChainStart(); config.setChainIndex(0); config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName()); config.setOutEdgesInOrder(transitiveOutEdges); config.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges()); for (StreamEdge edge : transitiveOutEdges) { connect(startNodeId, edge);//将JobVertex和JobEdge相连 } //将chain中所有子节点的StreamConfig写入到 headOfChain 节点的 chainedTaskConfig_ 配置中 config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId)); } else { chainedConfigs.computeIfAbsent(startNodeId, k -&gt; new HashMap&lt;Integer, StreamConfig&gt;()); config.setChainIndex(chainIndex); StreamNode node = streamGraph.getStreamNode(currentNodeId); config.setOperatorName(node.getOperatorName()); chainedConfigs.get(startNodeId).put(currentNodeId, config); } config.setOperatorID(currentOperatorId); if (chainableOutputs.isEmpty()) { config.setChainEnd(); } return transitiveOutEdges; } else { return new ArrayList&lt;&gt;(); } } 遍历transitiveOutEdges，并将每一条StreamEdge边作为参数传入connect( )函数中: private void connect(Integer headOfChain, StreamEdge edge) { //将当前edge记录物理边界顺序集合中 physicalEdgesInOrder.add(edge); //获取当前edge的下游节点ID Integer downStreamvertexID = edge.getTargetId(); //获取上下游JobVertex节点 JobVertex headVertex = jobVertices.get(headOfChain); JobVertex downStreamVertex = jobVertices.get(downStreamvertexID); //获取下游JobVertex的配置 StreamConfig downStreamConfig = new StreamConfig(downStreamVertex.getConfiguration()); //下游JobVertex的输入计数器加1 downStreamConfig.setNumberOfInputs(downStreamConfig.getNumberOfInputs() + 1); StreamPartitioner&lt;?&gt; partitioner = edge.getPartitioner(); //根据shuffle模式不同创建不同的ResultPartitionType ResultPartitionType resultPartitionType; switch (edge.getShuffleMode()) { case PIPELINED://有限的或无限的 resultPartitionType = ResultPartitionType.PIPELINED_BOUNDED; break; case BATCH://仅在生成完整结果后向下游发送数据 resultPartitionType = ResultPartitionType.BLOCKING; break; case UNDEFINED://blockingConnectionsBetweenChains 为true BLOCKING flase 为PIPELINED_BOUNDED resultPartitionType = streamGraph.isBlockingConnectionsBetweenChains() ? ResultPartitionType.BLOCKING : ResultPartitionType.PIPELINED_BOUNDED; break; default: throw new UnsupportedOperationException(&quot;Data exchange mode &quot; + edge.getShuffleMode() + &quot; is not supported yet.&quot;); } //根据ForwardPartitioner和RescalePartitioner两种分区方式建立DistributionPattern.POINTWISE类型的JobEdge JobEdge jobEdge; if (partitioner instanceof ForwardPartitioner || partitioner instanceof RescalePartitioner) { jobEdge = downStreamVertex.connectNewDataSetAsInput( headVertex, DistributionPattern.POINTWISE, resultPartitionType); } else {//其他分区方式则是DistributionPattern.ALL_TO_ALL类型 jobEdge = downStreamVertex.connectNewDataSetAsInput( headVertex, DistributionPattern.ALL_TO_ALL, resultPartitionType); } // set strategy name so that web interface can show it. 设置策略名称方便web接口显示 jobEdge.setShipStrategyName(partitioner.toString()); if (LOG.isDebugEnabled()) { LOG.debug(&quot;CONNECTED: {} - {} -&gt; {}&quot;, partitioner.getClass().getSimpleName(), headOfChain, downStreamvertexID); } } public JobEdge connectNewDataSetAsInput( JobVertex input, DistributionPattern distPattern, ResultPartitionType partitionType) { //JobVertex和JobEdge之间通过创建IntermediateDataSet来连接 IntermediateDataSet dataSet = input.createAndAddResultDataSet(partitionType); JobEdge edge = new JobEdge(dataSet, this, distPattern); this.inputs.add(edge); dataSet.addConsumer(edge); return edge; } 最后附上一副 execute方法最后执行。miniCluster.executeJobBlocking(jobGraph);中间一些列Akka 的RPC通讯省略不表，对并发编程有兴趣可以研究下Akka 和Actor ExecutionGraph： public JobExecutionResult executeJobBlocking(JobGraph job) throws JobExecutionException, InterruptedException { checkNotNull(job, &quot;job is null&quot;); final CompletableFuture&lt;JobSubmissionResult&gt; submissionFuture = submitJob(job); final CompletableFuture&lt;JobResult&gt; jobResultFuture = submissionFuture.thenCompose( (JobSubmissionResult ignored) -&gt; requestJobResult(job.getJobID())); final JobResult jobResult; try { jobResult = jobResultFuture.get(); } catch (ExecutionException e) { throw new JobExecutionException(job.getJobID(), &quot;Could not retrieve JobResult.&quot;, ExceptionUtils.stripExecutionException(e)); } try { return jobResult.toJobExecutionResult(Thread.currentThread().getContextClassLoader()); } catch (IOException | ClassNotFoundException e) { throw new JobExecutionException(job.getJobID(), e); } } public CompletableFuture&lt;JobSubmissionResult&gt; submitJob(JobGraph jobGraph) { final CompletableFuture&lt;DispatcherGateway&gt; dispatcherGatewayFuture = getDispatcherGatewayFuture(); // we have to allow queued scheduling in Flip-6 mode because we need to request slots // from the ResourceManager jobGraph.setAllowQueuedScheduling(true); final CompletableFuture&lt;InetSocketAddress&gt; blobServerAddressFuture = createBlobServerAddress(dispatcherGatewayFuture); final CompletableFuture&lt;Void&gt; jarUploadFuture = uploadAndSetJobFiles(blobServerAddressFuture, jobGraph); final CompletableFuture&lt;Acknowledge&gt; acknowledgeCompletableFuture = jarUploadFuture .thenCombine( dispatcherGatewayFuture, (Void ack, DispatcherGateway dispatcherGateway) -&gt; dispatcherGateway.submitJob(jobGraph, rpcTimeout)) .thenCompose(Function.identity()); return acknowledgeCompletableFuture.thenApply( (Acknowledge ignored) -&gt; new JobSubmissionResult(jobGraph.getJobID())); }]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink源码2]]></title>
    <url>%2F2019%2F08%2F03%2Fflink%E6%BA%90%E7%A0%812%2F</url>
    <content type="text"><![CDATA[根据上篇文章内容扩展一下chaining demo 首先写一个streaming的 wordcount public class StreamingChainingDemo { @Data @AllArgsConstructor @NoArgsConstructor public static class KeyCount{ private String key; private int count; } public static void main(String[] args) throws Exception { // set up the streaming execution environment Configuration configuration= new Configuration(){ { setInteger(&quot;rest.port&quot;,9191); setBoolean(&quot;local.start-webserver&quot;,true); } }; final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration); env.setParallelism(2).setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); DataStreamSource&lt;String&gt; dataStreamSource = env.socketTextStream(&quot;localhost&quot;, 9999); dataStreamSource.flatMap((String line, Collector&lt;KeyCount&gt; out) -&gt; { Stream.of(line.split(&quot;\\s+&quot;)).forEach(value -&gt; out.collect(new KeyCount(value,1))); } ).returns(Types.POJO(KeyCount.class)) .keyBy(new KeySelector&lt;KeyCount, Object&gt;() { @Override public Object getKey(KeyCount value) throws Exception { return value.getKey(); } }).timeWindow(Time.seconds(10)).sum(&quot;count&quot;).print(); // execute program env.execute(&quot;Flink StreamingChainingDemo&quot;); } } 运行如上代码后我们看WEB UI可以看到keyBy操作和sink是chaining在一起的。如果我们在print()的后面加上.disableChaining() 可以看到keyBy和sink是forward的并非chaining在一起 此时我们在returns(Types.POJO(KeyCount.class))后面增加.filter(word-&gt; !””.equals(word))大家猜flat和filter会chaining在一起吗？ yeah，你答对了吗？flat和filter会chaining在一起。 这时 我引入并行度的概念：在.filter(word-&gt; !””.equals(word))后面机上.setParallelism(3) 就会发现flat和filter是Reblance的关系 StreamOperator 源码解析这个接口继承 CheckpointListener, KeyContext, Disposable, Serializable 提供了如下方法： 生命周期相关： open close dispose prepareSnapshotPreBarrier 容错与状态 snapshotState initializeState 与StreamRecord相关 setKeyContextElement1 setKeyContextElement2 chain相关 getChainingStrategy setChainingStrategy 监控相关 getMetricGroup getOperatorID AbstractStreamOperator，OneInputStreamOperator与TwoInputStreamOperator接口继承自StreamOperator OneInputStreamOperator有3个方法 processElement processWatermark processLatencyMarker TwoInputStreamOperator有6个方法 processElement1 processElement2 processWatermark1 processWatermark2 processLatencyMarker1 processLatencyMarker2 AbstractStreamOperator 重要的变量：后面会将具体的用法用处决定是否在生成JobGraph时对算子进行Chaining优化： protected ChainingStrategy chainingStrategy = ChainingStrategy.HEAD; 3个与状态相关的变量 private transient AbstractKeyedStateBackend&lt;?&gt; keyedStateBackend; private transient DefaultKeyedStateStore keyedStateStore; private transient OperatorStateBackend operatorStateBackend; 监控相关的变量 protected transient OperatorMetricGroup metrics; protected transient LatencyStats latencyStats; 方法作用和父类大同小异此处略 AbstractStreamOperator的子类抽象类AbstractUdfStreamOperator这个抽象类同时实现了OutputTypeConfigurable接口并重写了setOutputType方法设置了输出类型 最后我们来看OneInputStreamOperator这个类的实现类： StreamFilter，StreamMap与StreamFlatMap算子在实现的processElement分别调用传入的FilterFunction，MapFunction， FlatMapFunction的udf将element传到下游。其中StreamFlatMap用到了TimestampedCollector，它是output的一层封装，将timestamp加入到StreamRecord中发送到下游。 StreamGroupedReduce与StreamGroupedFold算子相似的点是都涉及到了操作状态, 所以在覆盖open方法时通过创建一个状态的描述符以及调用AbstractStreamOperator实现的getPartitionedState方法获取了一个stateBackend的操作句柄。在processElement方法中借助这个句柄获取当前状态值，在用UDF将新的元素聚合进去并更新状态值，最后输出到下游。不同的是Fold的输出类型可能不一样（所以实现了OutputTypeConfigurable接口的setOutputType方法），并且有初始值。 ProcessOperator， LegacyKeyedProcessOperator（@Deprecated）ProcessFunction是比较灵活的UDF，允许用户通过在processElement的时候可以通过传入的ctx操作TimerService注册ProcessingTimeTimer和EventTimeTimer，并且通过实现方法onTimer就可以在Timer被触发的时候执行回调的逻辑。 StreamSink：SimpleContext，可以获取processingTime，watermark和element的时间戳。GenericWriteAheadSink提供了一个可以被实现为Exactly once的sink的抽象类AsyncWaitOperator提供了异步处理的能力，是一个比较特殊的算子，对元素的处理和备份恢复都比较特殊。element的输出通过一个Emitter对象来实现TimestampsAndPeriodicWatermarksOperator，TimestampsAndPunctuatedWatermarksOperator通过TimestampAssigner提取timestamp并生按照规则生成watermark 和TwoInputStreamOperator这个类的实现类CoStreamMap， CoStreamFlatMap基本与单流的逻辑没什么区别，只是针对两个流的Function做类似的处理。IntervalJoinOperator对双流的元素根据提供的ProcessJoinFunction做内连接，并且每个元素都有失效时间。在processElement方法中，每当一个流的元素到达，会将它加入对应流的buffer，并且遍历另一个流的buffer找到所有join的选项。最后再根据失效时间注册一个状态清理的Timer防止buffer无限增长。 CoBroadcastWithKeyedOperator和CoBroadcastWithNonKeyedOperator提供了对(Keyed)BroadcastProcessFunction的支持，和CoProcess有一些类似，只是Broadcast的Stream只有读权限，没有写权限。并且可以通过context直接获得BroadcastState CoProcessOperator和KeyedCoProcessOperator本质上与单流的处理也没有什么区别，但是提供了双流之间共享状态的可能。CoProcessOperator也被用来实现NonWindowJoin]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink源码1]]></title>
    <url>%2F2019%2F08%2F02%2Fflink%E6%BA%90%E7%A0%811%2F</url>
    <content type="text"><![CDATA[nc -lk 9000 bin/flink run examples/streaming/SocketWindowWordCount.jar –hostname localhost –port 9000 编译flinkgit clone https://github.com/apache/flink.gitcd flinkmvn clean package -DskipTests # this will take up to 10 minutes Flink源码阅读：从读取文件开始：例如env.readFileStream共有如下DataSource： fromElements fromElements fromCollection fromCollection fromCollection fromCollection fromParallelCollection fromParallelCollection fromParallelCollection readTextFile readTextFile readFile readFile readFile readFileStream readFile socketTextStream socketTextStream socketTextStream socketTextStream socketTextStream createInput createInput createInput createFileInput addSource addSource addSource addSource public DataStream&lt;String&gt; readFileStream(String filePath, long intervalMillis, FileMonitoringFunction.WatchType watchType) { DataStream&lt;Tuple3&lt;String, Long, Long&gt;&gt; source = addSource(new FileMonitoringFunction( filePath, intervalMillis, watchType), &quot;Read File Stream source&quot;); return source.flatMap(new FileReadFunction()); } 第三个参数分为： ONLY_NEW_FILES, // Only new files will be processed. 仅处理新增文件 REPROCESS_WITH_APPENDED, // When some files are appended, all contents of the files will be processed. 当文件内容增加了之后会重新处理整个文件。 PROCESS_ONLY_APPENDED // When some files are appended, only appended contents will be processed. 当文件内容增加了之后只处理新增加内容 FileMonitoringFunction 继承于SourceFunction接口SourceFunction有两个方法：run 业务逻辑方法cancel 取消数据源的数据产生FileMonitoringFunction实现了这两个方法 @Override public void run(SourceContext&lt;Tuple3&lt;String, Long, Long&gt;&gt; ctx) throws Exception { FileSystem fileSystem = FileSystem.get(new URI(path)); while (isRunning) { List&lt;String&gt; files = listNewFiles(fileSystem);//列出新增文件 for (String filePath : files) { if (watchType == WatchType.ONLY_NEW_FILES || watchType == WatchType.REPROCESS_WITH_APPENDED) { ctx.collect(new Tuple3&lt;String, Long, Long&gt;(filePath, 0L, -1L));//从头到尾收集数据 offsetOfFiles.put(filePath, -1L); } else if (watchType == WatchType.PROCESS_ONLY_APPENDED) { long offset = 0; long fileSize = fileSystem.getFileStatus(new Path(filePath)).getLen(); if (offsetOfFiles.containsKey(filePath)) { offset = offsetOfFiles.get(filePath); } //只收集新增部分数据，即从上次获取的offset到这次文件末尾filesize ctx.collect(new Tuple3&lt;String, Long, Long&gt;(filePath, offset, fileSize)); offsetOfFiles.put(filePath, fileSize); LOG.info(&quot;File processed: {}, {}, {}&quot;, filePath, offset, fileSize); } } Thread.sleep(interval); } } private List&lt;String&gt; listNewFiles(FileSystem fileSystem) throws IOException { List&lt;String&gt; files = new ArrayList&lt;String&gt;(); FileStatus[] statuses = fileSystem.listStatus(new Path(path));//列出给定路径中文件/目录的状态（如果路径为一个目录。) //FileStatus 有getLen，getBlockSize,getReplication,getModificationTime，getAccessTime，isDir,getPath方法 if (statuses == null) { LOG.warn(&quot;Path does not exist: {}&quot;, path); } else { for (FileStatus status : statuses) { Path filePath = status.getPath(); String fileName = filePath.getName(); long modificationTime = status.getModificationTime(); if (!isFiltered(fileName, modificationTime)) { //当WatchType 为ONLY_NEW_FILES并且modificationTimes这个map包含读取的文件时即新增文件 // 或文件修改时间大于modificationTime时为true files.add(filePath.toString()); modificationTimes.put(fileName, modificationTime); } } } return files; } 接下来为们看canal方法只做了一件事挺直running @Override public void cancel() { isRunning = false; } 对FileMonitoringFunction的实现清楚之后，回到StreamExecutionEnvironment中，看addSource方法。 public &lt;OUT&gt; DataStreamSource&lt;OUT&gt; addSource(SourceFunction&lt;OUT&gt; function, String sourceName) { return addSource(function, sourceName, null); } public &lt;OUT&gt; DataStreamSource&lt;OUT&gt; addSource(SourceFunction&lt;OUT&gt; function, String sourceName, TypeInformation&lt;OUT&gt; typeInfo) { if (function instanceof ResultTypeQueryable) { //如果传入的function实现了ResultTypeQueryable接口, 则直接通过接口获取 typeInfo = ((ResultTypeQueryable&lt;OUT&gt;) function).getProducedType(); } if (typeInfo == null) { try { typeInfo = TypeExtractor.createTypeInfo( SourceFunction.class, function.getClass(), 0, null, null);//这个方法有点长，实际是typeInfo为空通过反射机制来提取typeInfo } catch (final InvalidTypesException e) { //获取失败返回MissingTypeInfo实例，里面两个变量：functionName，typeException typeInfo = (TypeInformation&lt;OUT&gt;) new MissingTypeInfo(sourceName, e); } } //根据function是实现了ParallelSourceFunction来判断是否是一个并行数据源节点 boolean isParallel = function instanceof ParallelSourceFunction; //闭包清理, 可减少序列化内容, 以及防止序列化出错 clean(function); //初始化一个ChainingStrategy.HEAD节点 final StreamSource&lt;OUT, ?&gt; sourceOperator = new StreamSource&lt;&gt;(function); return new DataStreamSource&lt;&gt;(this, typeInfo, sourceOperator, isParallel, sourceName);//返回DataStreamSource } 由于FileMonitoringFunction继承的是SourceFunction不是 ParallelSourceFunction 故isParallel为flase，即并行度为1上面看到ChainingStrategy这个枚举类实际有三个属性：ALWAYS 表示尽可能的与前后operator chainingNEVER 表示不会chainingHEAD 表示只会chaining后面。具体后面详细讲解其作用 接下来看最后一个函数DataStreamSource public DataStreamSource(StreamExecutionEnvironment environment, TypeInformation&lt;T&gt; outTypeInfo, StreamSource&lt;T, ?&gt; operator, boolean isParallel, String sourceName) { super(environment, new SourceTransformation&lt;&gt;(sourceName, operator, outTypeInfo, environment.getParallelism())); this.isParallel = isParallel; if (!isParallel) { setParallelism(1); } } protected SingleOutputStreamOperator(StreamExecutionEnvironment environment, Transformation&lt;T&gt; transformation) { super(environment, transformation); } public DataStream(StreamExecutionEnvironment environment, Transformation&lt;T&gt; transformation) { this.environment = Preconditions.checkNotNull(environment, &quot;Execution Environment must not be null.&quot;); this.transformation = Preconditions.checkNotNull(transformation, &quot;Stream Transformation must not be null.&quot;); } 后面就是一系列transform 后面章节详细介绍]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-神经网络]]></title>
    <url>%2F2018%2F08%2F25%2FMachine%20learning%20Artificial%20Neural%20Network%2F</url>
    <content type="text"><![CDATA[什么是人工神经网络模型人工神经网络(Artificial Neural Network, ANN)没有一个严格的正式定义。它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。 一个计算模型，要被称为为神经网络，通常需要大量彼此连接的节点 （也称 ‘神经元’），并且具备两个特性：每个神经元，通过某种特定的输出函数 （也叫激励函数 activation function），计算处理来自其它相邻神经元的加权输入值神经元之间的信息传递的强度，用所谓加权值来定义，算法会不断自我学习，调整这个加权值总结：神经网络算法的核心就是：计算、连接、评估、纠错、学习 神经网络模型可以分为：前向网络网络中各个神经元接受前一级的输入，并输出到下一级，网络中没有反馈，可以用一个有向无环路图表示。这种网络实现信号从输入空间到输出空间的变换，它的信息处理能力来自于简单非线性函数的多次复合。网络结构简单，易于实现。反传网络是一种典型的前向网络。 反馈网络网络内神经元间有反馈，可以用一个无向的完备图表示。这种神经网络的信息处理是状态的变换，可以用动力学系统理论处理。系统的稳定性与联想记忆功能有密切关系。Hopfield网络、波耳兹曼机均属于这种类型。 激活函数用于处理复杂的非线性分类情况。比线性回归、logistic回归灵活。训练的时候注意过拟合。非线性激活函数Sigmoid𝑓(𝑥)=1/(1+exp⁡(−𝑥))特点：当x趋近负无穷时，y趋近于0；趋近于正无穷时，y趋近于1；x超出[-6,6]的范围后，函数值基本上没有变化，值非常接近0或者1该函数的值域范围限制在(0,1)之间，这样sigmoid函数就能与一个概率分布联系起来了。𝑓^′ (𝑥)=𝑓(𝑥)(1−𝑓(𝑥)) 双曲正切tanh⁡(𝑥)=(𝑒^𝑥−𝑒^(−𝑥))/(𝑒^𝑥+𝑒^(−𝑥) )特点：当x趋近负无穷时，y趋近于-1；趋近于正无穷时，y趋近于1；x超出[-3,3]的范围后，函数值基本上没有变化，值非常接近-1或者1该函数的值域范围限制在(-1,1)之间tanh^′ (𝑥)=1−tanh(x)^2 修正线性单元Rectifier Linear Units（ReLU）𝑓(𝑥)=max⁡(0,𝑥)特点：只有有一半隐含层是处于激活状态，其余都是输出为0不会出现梯度消失的问题（即在sigmoid接近饱和区时，导数趋于0，这种情况会造成信息丢失）只需比较、乘加运算，因此计算方便，计算速度快，加速了网络的训练ReLU比sigmoid更接近生物学的激活模型还有一些改进或的变体 Softplus𝑓(𝑥)=log⁡(1+𝑒^𝑥 )特点：x趋于负无穷时，softplus趋于0；x趋于正无穷时， softplus趋于x它是ReLU的平滑版它是sigmoid的原函数 损失函数用于回归中的均方损失：𝐸=1/2 (𝑦−𝑦 ̂ )^2用于分类中的交叉熵损失函数：𝐸=−∑▒〖𝑦𝑘 𝑙𝑜𝑔((𝑦𝑘 ) ̂ )” “ 〗k=1,2,…,m表示m种类别。在违约预测中m=2 基于 Anaconda 的安装安装tensorflow建立一个 conda 计算环境名字叫tensorflow: Python 2.7$ conda create -n tensorflow python=2.7 Python 3.4$ conda create -n tensorflow python=3.4 activate tensorflow 安装tensorflowconda install –channel https://conda.anaconda.org/conda-forge tensorflow import tensorflow as tf退出python3环境或当你不用 TensorFlow 的时候,关闭环境:(tensorflow)$ deactivate$ # Your prompt should change back windows下安装升级pippython -m pip install –upgrade pip安装tensorflowpip3 install –upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0-cp35-cp35m-win_amd64.whl]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 安装使用]]></title>
    <url>%2F2018%2F08%2F18%2FInstall%20TensorFlow%2F</url>
    <content type="text"><![CDATA[基于Anaconda的Tensorflow安装Anaconda根据官网选择基于不同的python版本安装：https://www.anaconda.com/download/#windows 博主选择Python 3.6 version，windows 64bit 安装完成后需要配置环境变量，根目录和Scripts目录加入到Path下面G:\ProgramData\Anaconda3;G:\ProgramData\Anaconda3\Scripts 1.检测anaconda环境是否安装成功：conda –version2.检测目前安装了哪些环境变量：conda info –envs 3.安装python版本（博主选择3.5）：conda create –name tensorflow python=3.5安装后是3.5.64.激活tensflow的环境：activate tensorflow5.检测tensflow的环境添加到了Anaconda里面：conda info –envs6.安装tensorflow gru版本pip install –ignore-installed –upgrade tensorflow-gpu 安装其他组件：pip install pandasconda install scikit-learnconda install matplotlib IDE想要使用tensorflow 需要制定tensorflow的python版本]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-随机森林]]></title>
    <url>%2F2018%2F08%2F16%2FMathematical%20%20Random%20Forest%2F</url>
    <content type="text"><![CDATA[sklearn.ensemble.RandomForestClassifier n_estimators : integer, optional (default=10) 森林里（决策）树的数目 criterion : string, optional (default=”gini”) 衡量分裂质量的性能（函数）。 受支持的标准是基尼不纯度的”gini”,和信息增益的”entropy”（熵）。注意：这个参数是特定树的 max_features : int, float, string or None, optional (default=”auto”) 选择最适属性时划分的特征不能超过此值: 如果是int，就要考虑每一次分割处的max_feature特征 如果是float，那么max_features就是一个百分比，那么（max_feature*n_features）特征整数值是在每个分割处考虑的。 如果是auto，那么max_features=sqrt(n_features)，即n_features的平方根值。 如果是log2，那么max_features=log2(n_features) 如果是None,那么max_features=n_features 注意：寻找分割点不会停止，直到找到最少一个有效的节点划分区，即使它需要有效检查超过max_features的特征。 max_depth : integer or None, optional (default=None)（决策）树的最大深度。如果值为None，那么会扩展节点，直到所有的叶子是纯净的，或者直到所有叶子包含少于min_sample_split的样本。 min_samples_split : int, float, optional (default=2) 根据属性划分节点时，每个划分最少的样本数。 如果为int，那么考虑min_samples_split作为最小的数字。 如果为float，那么min_samples_split是一个百分比，并且把ceil(min_samples_split*n_samples)是每一个分割最小的样本数量。 在版本0.18中更改：为百分比添加浮点值。 叶子节点最少的样本数。 如果为int，那么考虑min_samples_leaf作为最小的数字。 如果为float，那么min_samples_leaf为一个百分比，并且ceil(min_samples_leaf*n_samples)是每一个节点的最小样本数量。 在版本0.18中更改：为百分比添加浮点值。 min_weight_fraction_leaf : float, optional (default=0.) 一个叶子节点所需要的权重总和（所有的输入样本）的最小加权分数。当sample_weight没有提供时，样本具有相同的权重 max_leaf_nodes : int or None, optional (default=None) 叶子树的最大样本数。 以最优的方法使用max_leaf_nodes来生长树。最好的节点被定义为不纯度上的相对减少。如果为None,那么不限制叶子节点的数量。 min_impurity_split : float, 树早期生长的阈值。如果一个节点的不纯度超过阈值那么这个节点将会分裂，否则它还是一片叶子。 自0.19版以后不推荐使用：min_impurity_split已被弃用，取而代之的是0.19中的min_impurity_decrease。min_impurity_split将在0.21中被删除。 使用min_impurity_decrease min_impurity_decrease : float, optional (default=0.) 如果节点的分裂导致的不纯度的下降程度大于或者等于这个节点的值，那么这个节点将会被分裂。 不纯度加权减少方程式如下： N_t / N (impurity - N_t_R / N_t right_impurity- N_t_L / N_t * left_impurity) N是样本总的数量，N_t是当前节点处的样本数量，N_t_L是左孩子节点样本的数量,还有N_t_R是右孩子节点的样本数量。 N，N_t，N_t_R和N_t_L全部是指加权总和，如果sample_weight通过的话。 0.19版本新加的参数。 bootstrap : boolean, optional (default=True) 建立决策树时，是否使用有放回抽样。 oob_score : bool (default=False) 是否使用袋外样本来估计泛化精度。 n_jobs : integer, optional (default=1) 用于拟合和预测的并行运行的工作（作业）数量。如果值为-1，那么工作数量被设置为核的数量。 random_state : int, RandomState instance or None, optional (default=None) RandomStateIf int，random_state是随机数生成器使用的种子; 如果是RandomState实例，random_state就是随机数生成器; 如果为None，则随机数生成器是np.random使用的RandomState实例。 verbose : int, optional (default=0) 控制决策树建立过程的冗余度。 warm_start : bool, optional (default=False) 当被设置为True时，重新使用之前呼叫的解决方案，用来给全体拟合和添加更多的估计器，反之，仅仅只是为了拟合一个全新的森林。 class_weight : dict, list of dicts, “balanced”, “balanced_subsample” 或者None,（默认值为None）,与格式{class_label: weight}相关联的类的可选的权值。如果没有给值，所有的类到都应该有一个权值。对于多输出问题，一个字典序 列可以按照y的列的顺利被提供。 请注意，对于多输出（包括多标签），其权值应该被定义为它自己字典的每一列的每一个类。例如，对于四类多标签分类，权值应该如[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] 这样，而不是[{1:1}, {2:5}, {3:1}, {4:1}].这样。 “balanced”模式使用y的值来自动的调整权值，与输入数据中类别频率成反比，如：n_samples / (n_classes * np.bincount(y)) “balanced_subsample”模式和”balanced”相同，除了权值是基于每棵成长树有放回抽样计算的。 对于多输出，y的每列权值将相乘。 请注意，如果指定了sample_weight,这些权值将会和sample_weight相乘（通过拟合方法传递）。 Attributes: 属性 estimators_ : 决策树分类器的序列 拟合的子估计器的集合。 classes_ : 数组维度=[n_classes]的数组或者一个这样数组的序列。 类别标签（单一输出问题），或者类别标签的数组序列（多输出问题）。 nclasses : int or list 类别的数量（单输出问题），或者一个序列，包含每一个输出的类别数量（多输出问题） nfeatures : int 执行拟合时的特征数量。 noutputs : int 执行拟合时的输出数量。 featureimportances : array of shape = [n_features] 特征的重要性（值越高，特征越重要） oobscore : float使用袋外估计获得的训练数据集的得分。 oob_decisionfunction :维度=[n_samples,n_classes]的数组 在训练集上用袋外估计计算的决策函数。如果n_estimators很小的话，那么在有放回抽样中，一个数据点也不会被忽略是可能的。在这种情况下，oob_decisionfunction 可能包括NaN。 参数的默认值控制决策树的大小（例如，max_depth，，min_samples_leaf等等），导致完全的生长和在某些数据集上可能非常大的未修剪的树。为了降低内容消耗，决策树的复杂度和大小应该通过设置这些参数值来控制。这些特征总是在每个分割中随机排列。 因此，即使使用相同的训练数据，max_features = n_features和bootstrap = False，如果在搜索最佳分割期间所列举的若干分割的准则的改进是相同的，那么找到的最佳分割点可能会不同。 为了在拟合过程中获得一个确定的行为，random_state将不得不被修正。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-梯度下降]]></title>
    <url>%2F2018%2F04%2F27%2FMathematical%20regression%20gradient%20descent%2F</url>
    <content type="text"><![CDATA[梯度下降：批量梯度下降法（Batch Gradient Descent，简称BGD） 优点：全局最优解；易于并行实现； 缺点：当样本数目很多时，训练过程会很慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD） 优点：训练速度快；迭代次数少 缺点：准确度下降，并不是全局最优；不易于并行实现。小批量梯度下降算法（MBGD）如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。 凸凹函数：设f(x)在区间D上连续，如果对D上任意两点a、b恒有f（（a+b）/2）&lt;(f(a)+f(b))/2那么称f(x)在D上的图形是（向上）凹的（或凹弧）；如果恒有f（（a+b）/2）&gt;(f(a)+f(b))/2那么称f(x)在D上的图形是（向上）凸的（或凸弧） 梯度下降相关概念： 步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。 2.特征（feature）：指的是样本中输入部分，比如2个单特征的样本（x(0),y(0)）,（x(1),y(1)）（x(0),y(0)）,（x(1),y(1)）,则第一个样本特征为x(0)x(0)，第一个样本输出为y(0)y(0)。 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为hθ(x)hθ(x)。比如对于单个特征的m个样本（x(i),y(i)）(i=1,2,…m)（x(i),y(i)）(i=1,2,…m),可以采用拟合函数如下： hθ(x)=θ0+θ1xhθ(x)=θ0+θ1x。 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于m个样本（xi,yi）(i=1,2,…m)（xi,yi）(i=1,2,…m),采用线性回归，损失函数为： J(θ0,θ1)=∑i=1m(hθ(xi)−yi)2J(θ0,θ1)=∑i=1m(hθ(xi)−yi)2 其中xixi表示第i个样本特征，yiyi表示第i个样本对应的输出，hθ(xi)hθ(xi)为假设函数。 局部加权回归简单来说，这个过程其实是在先拟合出一条曲线，然后再用这个曲线去预测需要预测的点。(源自百度)为什么改进要用加权回归呢？ 很简单，因为非线性拟合出直线误差会很大，这里的局部加权类似于knn算法的权重，即距离中心点越近的权重越大，对拟合曲线的影响也就越大，所以也有了局部加权这一名词 参考文献：https://blog.csdn.net/Gentle_Guan/article/details/76586689?locationNum=8&amp;fps=1]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-逻辑回归模型特征处理]]></title>
    <url>%2F2018%2F04%2F08%2FMathematical%20Feature%20processing%20of%20logistic%20regression%20model%2F</url>
    <content type="text"><![CDATA[如果有异常值，使用极大-极小归一化或均值-标准差归一化，计算之前需要将极端值排除在外。例如：x’=x−min/ max−min计算max与min时需要用P1与P99来代替。新生成的值如果超过1用1表示，如果小于0 用0表示]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-回归算法实例]]></title>
    <url>%2F2018%2F03%2F27%2FMathematical%20regression%20%2F</url>
    <content type="text"><![CDATA[概念梳理：数学期望：在概率论和统计学中，数学期望(mean)（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和，是最基本的数学特征之一。它反映随机变量平均取值的大小 方差：（variance)是在概率论和统计方差衡量随机变量或一组数据时离散程度的度量。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的平均数。在许多实际问题中，研究方差即偏离程度有着重要意义。 概率密度函数：在数学中，连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。而随机变量的取值落在某个区域之内的概率则为概率密度函数在这个区域上的积分。当概率密度函数存在的时候，累积分布函数是概率密度函数的积分。概率密度函数一般以小写标记正态分布是重要的概率分布。它的概率密度函数是：随着参数μ和σ变化，概率分布也产生变化。期望：μ方差：σ^2中位数：μ众44o6fdeswq DFGI-数：μ偏度：0峰度：3\]正态分布：又称为常态分布，高斯分布。若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。 线性回归：线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w’x+e，e为误差服从均值为0的正态分布。 回归数据：http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption 属性信息： 1.date：格式dd/mm/yyyy日期2.time：格式HH时间：MM：SS3.global_active_power：家用全球分钟平均有功功率（千瓦）4.global_reactive_power: 家用全球分钟平均无功功率（千瓦）5.voltage：分钟平均电压（伏特）6.global_intensity：家用全球分钟平均电流强度（安培）7.sub_metering_1：能耗分项计量1号（中有功电能电能）。它与厨房相对应，主要包括洗碗机、烤箱和微波炉（热板不是电动的，而是燃气驱动的）。8.sub_metering_2：能耗分项计量2号（中有功电能电能）。它对应洗衣房，包括洗衣机、滚筒烘干机、冰箱和灯。9.sub_metering_3：能耗分项计量3号（中有功电能电能）。它相当于一个电热水器和一个空调。 1234567891011121314151617import pandas as pdimport numpy as npimport timeimport sklearnimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitpath='C:/Users/zhanghongming/Documents/data/100.txt'names = ['Date','Time','Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']df=pd.read_csv(path,sep=';')print(df.head()) Date Time Global_active_power Global_reactive_power Voltage \ 0 16/12/2006 17:24:00 4.216 0.418 234.841 16/12/2006 17:25:00 5.360 0.436 233.632 16/12/2006 17:26:00 5.374 0.498 233.293 16/12/2006 17:27:00 5.388 0.502 233.744 16/12/2006 17:28:00 3.666 0.528 235.68 Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_30 18.4 0.0 1.0 17.01 23.0 0.0 1.0 16.02 23.0 0.0 2.0 17.03 23.0 0.0 1.0 17.04 15.8 0.0 1.0 17.0 看所有的变量值123for i in df.columns: print(df[i].value_counts()) Name: Date, dtype: int6419:01:00 117:27:00 118:24:00 1 .. Name: Time, Length: 99, dtype: int644.230 22.912 24.218 26.072 15.412 1 .. Name: Global_active_power, Length: 96, dtype: int640.000 330.090 70.054 40.144 3 .. Name: Global_reactive_power, dtype: int64235.84 3234.20 2235.68 2233.74 2 .. Name: Voltage, Length: 90, dtype: int6412.4 713.8 515.8 5 ..Name: Global_intensity, dtype: int640.0 99Name: Sub_metering_1, dtype: int641.0 500.0 262.0 8 Name: Sub_metering_2, dtype: int6417.0 7716.0 1818.0 4 Name: Sub_metering_3, dtype: int64 12345678910111213141516171819#空值处理new_df= df.replace('?',np.nan)datas = new_df.dropna(how='any')#定义时间格式化def datae_format(dt): t = time.strptime(' '.join(dt),'%d/%m/%Y %H:%M:%S') return (t.tm_year,t.tm_mon,t.tm_mday,t.tm_hour,t.tm_min,t.tm_sec)##分析功率和时间的线性关系。将时间转换为连续的X = datas[names[0:2]]X = X.apply(lambda x :pd.Series(datae_format(x)),axis=1)Y = datas[names[2]]print(X.head(5))print(Y.head(5)) 0 1 2 3 4 5 0 2006 12 16 17 24 01 2006 12 16 17 25 02 2006 12 16 17 26 03 2006 12 16 17 27 04 2006 12 16 17 28 00 4.2161 5.3602 5.3743 5.3884 3.666Name: Global_active_power, dtype: float64 函数讲解sklearn.model_selection.train_test_split随机划分训练集和测试集一般形式：train_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取train data和testdata，形式为：X_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)参数解释：train_data：所要划分的样本特征集train_target：所要划分的样本结果test_size：样本占比，如果是整数的话就是样本的数量random_state：是随机数的种子。随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。 1234567891011121314151617X_train,X_test,Y_train,Y_test = train_test_split( X, Y, test_size=0.2, random_state=42)## 数据标准换行ss = StandardScaler()X_train = ss.fit_transform(X_train)X_test = ss.fit_transform(X_test)##训练数据lr = LinearRegression()lr.fit(X_train,Y_train)##预测Y值y_predict = lr.predict(X_test)print("准确率:",lr.score(X_test,Y_test)) 样本数据100条：准确率: 0.0226499044921样本数据1000条：0.103073016594 模型保存及加载：12345678from sklearn.externals import joblib## 模型保存：joblib.dump(ss,"data_ss.model")joblib.dump(lr,"data_lr.model")## 加载模型joblib,load("data_ss.model")joblib,load("data_lr.model") plot文档：https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot 123456789101112## 解决中文问题mpl.rcParams['font.sans-serif'] = [u'SimHei'];mpl.rcParams['axes.unicode_minus'] = Falset=np.arange(len(X_test))plt.figure(facecolor='w')plt.plot(t,Y_test,'r--',linewidth=2,label=u'真实值')plt.plot(t,y_predict,'g--',linewidth=2,label=u'预测值')plt.legend(loc ='lower right')plt.title(u'线性回归时间与电压的关系',fontsize=20 )plt.grid(b=True)plt.show() 100条数据： 1000条数据： linear多项式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.grid_search import GridSearchCVmodels = [Pipeline([('Poly',PolynomialFeatures()),('Linear',LinearRegression())])]model = models[0]##获取X，Y变量，并将时间变量转换为数值型连续的X = datas[names[0:2]]X = X.apply(lambda x :pd.Series(datae_format(x)),axis=1)Y = datas[names[4]]## 对数据集进行划分X_train,X_test,Y_train,Y_test = train_test_split( X, Y, test_size=0.2, random_state=0)## 数据标准化ss = StandardScaler()X_train = ss.fit_transform(X_train)X_test = ss.fit_transform(X_test)## 模型训练t=np.arange(len(X_test))N =5d_pool= np.arange(1,N,1)m=d_pool.sizeclrs = [] # 颜色for c in np.linspace(16711680, 255, m,dtype='int64'): clrs.append('#%06x' % c)line_width = 3plt.figure(figsize=(12,6),facecolor='w')for i,d in enumerate(d_pool): plt.subplot(N-1,1,i+1) plt.plot(t,Y_test,'r--',label=u'真实值',ms=10,zorder=N) model.set_params(Poly__degree=d) # 设置多项式的阶 model.fit(X_train,Y_train) lin = model.get_params('Linear')['Linear'] output =u'%d阶，系数为：'%d print( output,lin.coef_.ravel()) y_hat = model.predict(X_test) s = model.score(X_test,Y_test) z=N-1 if (d==2) else 0 label=u'%d阶,准确率=%.3f'%(d,s) plt.plot(t,y_hat,color=clrs[i],lw=line_width,alpha = 0.75,label=label,zorder=z) plt.legend(loc = 'upper left') plt.grid(True) plt.ylabel(u'%d阶结果'%d,fontsize=12)# 预测值和真实值画图比较plt.legend(loc = 'lower right')plt.suptitle(u'线性回归时间与电压之间多项式关系')plt.grid(b=True)plt.show() 1阶，系数为： [ 0.00000000e+00 5.55111512e-17 0.00000000e+00 0.00000000e+00 -4.22939297e-01 -4.34494704e-01 0.00000000e+00]2阶，系数为： [ 2.47983335e-17 1.11022302e-16 -2.22044605e-16 -1.11022302e-16 -5.05820937e-01 -3.46571423e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 -8.58357173e-01 -7.57689882e-01 0.00000000e+00 -1.60364055e-01 0.00000000e+00 0.00000000e+00]3阶，系数为： [ -1.69309011e-15 -2.99760217e-15 3.33066907e-16 5.55111512e-16 -4.41970713e-02 -3.57278153e-01 0.00000000e+00 2.22044605e-16 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.43644538e-01 4.86208530e-01 0.00000000e+00 3.26242425e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.48140740e-01 6.37890832e-01 0.00000000e+00 -7.45035081e-01 0.00000000e+00 0.00000000e+00 -5.02511111e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]4阶，系数为： [ 2.22002972e-013 -8.01497757e-013 5.37209166e-014 -4.53519167e-013 7.19933381e-003 2.05441337e-001 3.86510268e-013 -5.94066463e-013 6.66133815e-015 6.66133815e-016 -1.88737914e-015 6.27276009e-015 -5.30131494e-015 -1.01585407e-014 3.35287353e-014 8.21565038e-015 -2.55351296e-015 -2.22044605e-016 -6.31088724e-030 3.02922588e-028 1.00974196e-028 -5.04870979e-029 -4.89052469e-002 3.28220946e-001 0.00000000e+000 6.15440583e-003 0.00000000e+000 0.00000000e+000 1.26217745e-029 0.00000000e+000 -5.60519386e-044 -8.40779079e-045 -7.00649232e-045 -2.24207754e-044 -5.60519386e-045 2.80259693e-045 1.40129846e-045 0.00000000e+000 0.00000000e+000 4.97841222e-060 1.55575382e-061 -1.24460306e-060 -2.48920611e-060 0.00000000e+000 -3.11150764e-061 -1.16681536e-061 -3.11150764e-061 -4.66726146e-061 2.21085915e-075 -5.52714788e-076 -1.38178697e-076 2.76357394e-076 -1.38178697e-076 -3.45446742e-077 0.00000000e+000 1.34940134e-079 0.00000000e+000 1.91761463e-093 1.49813643e-095 -5.99254573e-095 4.49440930e-095 1.12360233e-095 0.00000000e+000 7.49068217e-096 -2.34083818e-097 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 -1.66326556e-111 4.15816391e-112 -2.07908195e-112 0.00000000e+000 -6.13741990e-002 -8.70197287e-001 8.39734513e-140 -1.48604949e+000 0.00000000e+000 0.00000000e+000 -4.67097255e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 -2.42848401e-001 -9.28403263e-001 0.00000000e+000 -8.91115491e-001 0.00000000e+000 0.00000000e+000 1.33924630e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 2.81909059e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn pca]]></title>
    <url>%2F2018%2F03%2F03%2Fscikit-learn%20pca%2F</url>
    <content type="text"><![CDATA[from sklearn import datasets digits = datasets.load_digits()x = digits.datay = digits.target from sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=666) print(x_train.shape) #(1347, 64)from sklearn.neighbors import KNeighborsClassifierimport timestart = time.clock()knn_clf = KNeighborsClassifier() knn_clf.fit(x_train,y_train)end = time.clock() print(end-start) #0.009107513739889835score = knn_clf.score(x_test,y_test) print(score) #0.986666666667 from sklearn.decomposition import PCA pca = PCA(n_components=2)pca.fit(x_train)X_train_reduction = pca.transform(x_train)X_test_reduction = pca.transform(x_test) start2 = time.clock()knn_clf = KNeighborsClassifier()knn_clf.fit(X_train_reduction,y_train)end2 = time.clock() print(end2-start2) #0.0019209365663966915score = knn_clf.score(X_test_reduction,y_test)print(score) #0.606666666667 print(pca.explainedvariance) pca = PCA(n_components=x_train.shape[1]) pca3 = PCA(0.95)pca3.fit(x_train)print(pca3.ncomponents) #28 start3 = time.clock()knn_clf3 = KNeighborsClassifier()X_train_reduction = pca3.transform(x_train)X_test_reduction = pca3.transform(x_test)knn_clf3.fit(X_train_reduction,y_train)end3 = time.clock() print(end3-start3) #0.006395458395239972score = knn_clf3.score(X_test_reduction,y_test)print(score) #0.98]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn KNN]]></title>
    <url>%2F2018%2F03%2F02%2Fscikit-learn%20KNN%2F</url>
    <content type="text"><![CDATA[地址：http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier 1234567891011from sklearn.neighbors import KNeighborsClassifierX = [[0], [1], [2], [3]]y = [0, 0, 1, 1]neigh = KNeighborsClassifier(n_neighbors=3)neigh.fit(X, y)print(neigh.predict([[1.1]]))# [0]print(neigh.predict_proba([[0.9]]))#[[ 0.66666667 0.33333333]] 手写数字练习： 12345678910111213141516171819from sklearn import datasetsdigits = datasets.load_digits()x= digits.datay= digits.targetprint(x.shape)#(1797, 64)from sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=666)from sklearn.neighbors import KNeighborsClassifierknn_clf = KNeighborsClassifier(n_neighbors=3)knn_clf.fit(x_train,y_train)score= knn_clf.score(x_test,y_test)print(score)#0.988888888889 查找最佳超参数123456789101112beat_score=0break_k =-1for k in range(1,11): knn_clf2 = KNeighborsClassifier(n_neighbors=k) knn_clf2.fit(x_train,y_train) score= knn_clf2.score(x_test,y_test) if(score &gt;beat_score): break_k =k beat_score = scoreprint("beat_score",beat_score)print("break_k",break_k) beat_score 0.991666666667break_k 4 123456789101112131415beat_score=0break_k =-1beat_method=''for method in ['uniform','distance']: for k in range(1,11): knn_clf2 = KNeighborsClassifier(n_neighbors=k,weights=method) knn_clf2.fit(x_train,y_train) score= knn_clf2.score(x_test,y_test) if(score &gt;beat_score): beat_method = method break_k =k beat_score = scoreprint("beat_score",beat_score)print("break_k",break_k) 网格搜素：123456789101112131415161718192021222324252627import timestart = time.clock()param_grid=[ &#123; 'weights':['uniform'], 'n_neighbors':[i for i in range(1,11)] &#125;, &#123; 'weights':['distance'], 'n_neighbors':[i for i in range(1,11)], 'p':[i for i in range(1,6)] &#125;]knn_clf = KNeighborsClassifier()from sklearn.model_selection import GridSearchCVgrid_search = GridSearchCV(knn_clf,param_grid)grid_search.fit(x_train,y_train)print("grid_search.best_estimator_:",grid_search.best_estimator_)print("grid_search.best_score_:",grid_search.best_score_)print("grid_search.best_params_:",grid_search.best_params_)knn_clf = grid_search.best_estimator_knn_clf_score = knn_clf.score(x_test,y_test)print(knn_clf_score)end = time.clock()print(end-start) grid_search.bestestimator: KNeighborsClassifier(algorithm=’auto’, leaf_size=30, metric=’minkowski’, metric_params=None, n_jobs=1, n_neighbors=3, p=3, weights=’distance’)grid_search.bestscore: 0.985386221294grid_search.bestparams: {‘n_neighbors’: 3, ‘p’: 3, ‘weights’: ‘distance’}0.983333333333296.18877317471 增加并行化处理：1grid_search = GridSearchCV(knn_clf,param_grid,n_jobs=2,verbose=2) 速度变为：156.67693082041933 最值归一化：适用于分布有明显边界的情况，受outlier影响较大均值方差归一化：适用于分布没有明显边界的情况，有可能存在极端数值]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn regression]]></title>
    <url>%2F2018%2F03%2F01%2Fscikit-learn%20regression%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122from sklearn import datasetsboston = datasets.load_boston()x= boston.datay= boston.targefrom sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=666)from sklearn import linear_modelclf = linear_model.LinearRegression()clf.fit(x_train,y_train)y_predict = clf.predict(x_test)from sklearn.metrics import mean_squared_errorfrom sklearn.metrics import mean_absolute_errorMSE = mean_squared_error(y_test,y_predict)print(MSE)MAE = mean_absolute_error(y_test,y_predict)print(MAE)]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-决策树算法实例]]></title>
    <url>%2F2018%2F02%2F18%2FMathematical%20decision%20tree%2F</url>
    <content type="text"><![CDATA[决策树：有监督学习方法是一种预测模型是在已知各种情况发生概率基础上，通过构建决策树来进行分析的一种方法 树形结构从跟节点开始，预测待分类项对应的特征属性，按照值选择输出分支，直到叶子节点，将叶子节点的存放类别作为树的结果 决策树分为两类：分类，回归前者用于分类标签值，后者用于预测连续值常用算法ID3，C4,5，CART 数据标准化：StandardScaler (基于特征矩阵的列，将属性值转换至服从正态分布)标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下常用与基于正态分布的算法，比如回归数据归一化MinMaxScaler （区间缩放，基于最大最小值，将数据转换到0,1区间上的）提升模型收敛速度，提升模型精度常见用于神经网络Normalizer （基于矩阵的行，将样本向量转换为单位向量）其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准常见用于文本分类和聚类、logistic回归中也会使用，有效防止过拟合 特征选择：从已有的特征中选择出影响目标值最大的特征属性 常用方法：{ 分类：F统计量、卡方系数，互信息mutual_info_classif{ 连续：皮尔逊相关系数 F统计量 互信息mutual_info_classif SelectKBest（卡方系数）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-结构模式]]></title>
    <url>%2F2018%2F02%2F11%2FDesign%20pattern%20structural%2F</url>
    <content type="text"><![CDATA[1.适配器效果及优缺点：对于类适配器： 用一个具体的Adapter类对Adaptee和Taget进行匹配。结果是当我们想要匹配一个类以及所有它的子类时，类Adapter将不能胜任工作。 使得Adapter可以override（重定义） Adaptee的部分行为，因为Adapter是Adaptee的一个子类。对于对象适配器： 允许一个Adapter与多个Adaptee，即Adaptee本身以及它的所有子类（如果有子类的话）同时工作。Adapter也可以一次给所有的Adaptee添加功能。 使得override（重定义）Adaptee的行为比较困难。如果一定要override Adaptee的方法，就只好先做一个Adaptee的子类以override Adaptee的方法，然后再把这个子类当作真正的Adaptee源进行适配。 2.桥接继承是一种强耦合的结果，父类变，子类就必须要变。可以使用组合/继承来解耦合。将抽象和他的实现分离 3.组合将对象组合成属性结构以表示‘部分-整体’的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性。 组合模式描述了如何使用递归的组合，使客户不用区分这些类 4.装配器5.外观6.享元模式7.代理模式Copy-on-writedai代理：即写即复制“快照”虚拟代理的一种，把复制拖延到只有客户端需要时，才真正执行保护代理：允许在访问对象时附加管理任务 1.什么是代理模式：例如我们找房子找中介2.为什么要使用代理：我们不需要自己找房子]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-行为模式]]></title>
    <url>%2F2018%2F02%2F10%2FDesign%20pattern%20behavior%2F</url>
    <content type="text"><![CDATA[责任链模式命令模式命令Command ——声明执行操作的接口。具体命令ConcreteCommand ——定义接收对象和动作之间的绑定关系。 ——通过引起接收者的相应动作来实现执行。客户Client ——产生一个ConcreteCommand对象，并设置接收者。引发者Invoker ——要求命令执行请求。接收者Receiver ——知道如何执行与请求相联系的操作。 迭代器模式模板方法模式准备一个抽象类，定义一个算法的大体框架将部分逻辑以具体方法以及具体构造子的形式实现剩余的逻辑通过声明一些抽象方法来描述这些抽象方法要求子类实现，不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现。子类不改变算法的结构而重定义算法 观察者模式同一应用对象不同展示形式，如一组数据映射为表格和柱状图。用户更改表格数据，柱状图要同步修改 关键对象：抽象主题Subject提供一个连接观察者对象和解除连接的接口。知道它的观察者。可有任意数目的观察者对象观察一个主题。可以增加和删除观察者对象，具体主题ConcreteSubject：通常用一个具体子类实现。负责实现对观察者引用的聚集的管理力注。将有关状态存入ConcreteObserver对象。在具体主题内部状态改变时向它的观察者发送通知。 抽象观察者Observer ：一般用一个抽象类或者一个接口实现，为所有的具体观察者定义一个更新接口更新接口包含的方法叫更新方法。具体观察者ConcreteObserver通常用一个具体子类实现，保存一个指向ConcreteSubject对象的引用。存储要与主题一致的状态。实现抽象观察者角色所要求的更新接口，以便使本身的状态与主题的状态相协调。 状态模式策略模式访问者模式解释器模式]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018学习计划]]></title>
    <url>%2F2018%2F02%2F07%2F2018%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[大数据及机器学习学习计划 编程基础：Pythonhttps://cn.udacity.com/course/programming-foundations-with-python--ud036 计算机科学导论 72小时https://cn.udacity.com/course/intro-to-computer-science--cs101 推论统计学 48小时https://cn.udacity.com/course/intro-to-inferential-statistics--ud201 描述统计学 48小时https://cn.udacity.com/course/intro-to-inferential-statistics--ud201 机器学习 240小时https://cn.udacity.com/course/machine-learning--ud262 统计学入门https://cn.udacity.com/course/intro-to-statistics--st101 基础线性代数https://cn.udacity.com/course/linear-algebra-refresher-course--ud953 机器学习https://cn.udacity.com/course/machine-learning-engineer-nanodegree--nd009 Apache Storm 进行实时分析 48小时https://cn.udacity.com/course/real-time-analytics-with-apache-storm--ud381 Bash脚本 40+小时]]></content>
      <categories>
        <category>学习计划</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-支持向量机]]></title>
    <url>%2F2018%2F02%2F05%2FMathematical%20Support%20Vector%20Machine%2F</url>
    <content type="text"><![CDATA[支持向量机（Support Vector Machine，SVM）的基本概念：点到超平面的距离在分类任务中，为了获取稳健的线性分类器，一个很自然的想法是，找出一条分割线使得两侧样本与该分割线的平均距离足够的远。在欧式空间中，定义一个点𝒙到直线（或者高维空间中的超平面）𝒘^𝑇 𝒙+𝑏=0的距离公式是： 𝑟(𝑥)= (|𝒘^𝑇 𝒙+𝑏|)/(||𝒘||)在分类问题中，如果这样的分割线或者分割平面能够准确地将样本分开，对于样本{𝒙𝑖,𝑦𝑖}∈𝐷, 𝑦𝑖=±1 而言，若𝑦𝑖=1，则有𝒘^𝑇 𝒙𝒊+𝑏≥1，反之若𝑦𝑖=-1，则有𝒘^𝑇 𝒙_𝒊+𝑏≤−1.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式目录]]></title>
    <url>%2F2018%2F01%2F27%2FDesign%20pattern%20catalog%2F</url>
    <content type="text"><![CDATA[创建性模式：1.类的创建模式——使用继承关系，把类的创建延迟到子类2.对象的创建模式——把对象的创建过程动态地委派给另一个对象 封装要创建的具体类（类的实例）的信息 隐藏这些类（类的实例）被创建和组合的过程 包含抽象工厂、建造者、工厂方式、原型、单例 结构性模式：考虑如何组合类和对象构成较大的结构。1.结构性类模式：使用继承来组合接口或实现2.结构性对象模式：对象合成实现新功能。包含：适配器、桥接、组合、装饰着、外观、轻量、代理 行为模式：主要解决算法和对象之间的责任分配问题。对象或类的模式它们之间的通信模式。包含：责任链、命令、解释器、迭代、中介者、备忘录、观察者、状态、策略、模板方法、观察者 工厂方法主要针对一个产品等级结构抽象工厂模式需要面对多个产品等级结构]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-工厂模式]]></title>
    <url>%2F2018%2F01%2F27%2FDesign%20pattern%20factory%2F</url>
    <content type="text"><![CDATA[创建几个套皮肤，所有的UI控件 如按钮，滚动条，窗口 都要创建出来。现在需要红色主题，黑色主题，和蓝色主题3套皮肤。 接口类：1234567891011121314public interface Button &#123; public void display();&#125;public interface ScrollBar &#123; public void display();&#125;public interface Window &#123; public void display();&#125;public interface SkinFactory &#123; public ScrollBar createScrollBar(); public Button createButton(); public Window createWindow();&#125; 红色皮肤工厂123456789101112131415161718192021222324252627282930public class RedSkinFactory implements SkinFactory &#123; public ScrollBar createScrollBar() &#123; return new RedScrollBar(); &#125; public Button createButton() &#123; return new RedButton(); &#125; public Window createWindow() &#123; return new RedWindow(); &#125;&#125;public class RedScrollBar implements ScrollBar &#123; public void display() &#123; System.out.println("创建红色滚动条。"); &#125;&#125;public class RedButton implements Button &#123; public void display() &#123; System.out.println("创建红色按钮"); &#125;&#125;public class RedWindow implements Window &#123; public void display() &#123; System.out.println("创建红色窗口。"); &#125;&#125; 实现类123456789101112131415161718public class SkinClient &#123; public static void main(String[] args) &#123; SkinFactory BlackSkinFactory = new BlackSkinFactory(); BlackSkinFactory.createButton().display(); BlackSkinFactory.createScrollBar().display(); BlackSkinFactory.createWindow().display(); SkinFactory RedSkinFactory = new RedSkinFactory(); RedSkinFactory.createButton().display(); RedSkinFactory.createScrollBar().display(); RedSkinFactory.createWindow().display(); SkinFactory BlueSkinFactory = new BlueSkinFactory(); BlueSkinFactory.createButton().display(); BlueSkinFactory.createScrollBar().display(); BlueSkinFactory.createWindow().display(); &#125;&#125; sh输出结果123456789创建黑色按钮创建黑色滚动条。创建黑色窗口。创建红色按钮创建红色滚动条。创建红色窗口。创建蓝色按钮创建蓝色滚动条。创建蓝色窗口。 其他颜色同上代码结构截图：结果截图：]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kNN实现手写数字识别]]></title>
    <url>%2F2018%2F01%2F25%2FkNN%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[需求利用一个手写数字“先验数据”集，使用knn算法来实现对手写数字的自动识别；先验数据（训练数据）集： 数据维度比较大，样本数比较多。 数据集包括数字0-9的手写体。 每个数字大约有200个样本。 每个样本保持在一个txt文件中。 手写体图像本身的大小是32x32的二值图，转换到txt文件保存后，内容也是32x32个数字，0或者1，如下： 首先准备测试文件:1934个训练数据946个测试数据 分析：1、手写体因为每个人，甚至每次写的字都不会完全精确一致，所以，识别手写体的关键是“相似度”2、既然是要求样本之间的相似度，那么，首先需要将样本进行抽象，将每个样本变成一系列特征数据（即特征向量）3、手写体在直观上就是一个个的图片，而图片是由上述图示中的像素点来描述的，样本的相似度其实就是像素的位置和颜色之间的组合的相似度4、因此，将图片的像素按照固定顺序读取到一个个的向量中，即可很好地表示手写体样本5、抽象出了样本向量，及相似度计算模型，即可应用KNN来实现 代码：1) 一个用来生成将每个样本的txt文件转换为对应的一个向量，2) 一个用来加载整个数据集，3) 一个实现kNN分类算法。4) 最后就是实现加载、测试的函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125########################################## kNN: k Nearest Neighbors# 参数: inX: vector to compare to existing dataset (1xN)# dataSet: size m data set of known vectors (NxM)# labels: data set labels (1xM vector)# k: number of neighbors to use for comparison # 输出: 多数类#########################################from numpy import *import operatorimport os# KNN分类核心方法def kNNClassify(newInput, dataSet, labels, k): numSamples = dataSet.shape[0] # shape[0]代表行数 ## step 1: 计算欧式距离 # tile(A, reps): 将A重复reps次来构造一个矩阵 # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) - dataSet # Subtract element-wise squaredDiff = diff ** 2 # squared for the subtract squaredDist = sum(squaredDiff, axis = 1) # sum is performed by row distance = squaredDist ** 0.5 ## step 2: 对距离排序 # argsort()返回排序后的索引 sortedDistIndices = argsort(distance) classCount = &#123;&#125; # 定义一个空的字典 for i in xrange(k): ## step 3: 选择k个最小距离 voteLabel = labels[sortedDistIndices[i]] ## step 4: 计算类别的出现次数 # when the key voteLabel is not in dictionary classCount, get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ## step 5: 返回出现次数最多的类别作为分类结果 maxCount = 0 for key, value in classCount.items(): if value &gt; maxCount: maxCount = value maxIndex = key return maxIndex # 将图片转换为向量def img2vector(filename): rows = 32 cols = 32 imgVector = zeros((1, rows * cols)) fileIn = open(filename) for row in xrange(rows): lineStr = fileIn.readline() for col in xrange(cols): imgVector[0, row * 32 + col] = int(lineStr[col]) return imgVector# 加载数据集def loadDataSet(): ## step 1: 读取训练数据集 print "---Getting training set..." dataSetDir = 'E:/Python/ml/knn/' trainingFileList = os.listdir(dataSetDir + 'trainingDigits') # 加载测试数据 numSamples = len(trainingFileList) train_x = zeros((numSamples, 1024)) train_y = [] for i in xrange(numSamples): filename = trainingFileList[i] # get train_x train_x[i, :] = img2vector(dataSetDir + 'trainingDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 train_y.append(label) ## step 2:读取测试数据集 print "---Getting testing set..." testingFileList = os.listdir(dataSetDir + 'testDigits') # load the testing set numSamples = len(testingFileList) test_x = zeros((numSamples, 1024)) test_y = [] for i in xrange(numSamples): filename = testingFileList[i] # get train_x test_x[i, :] = img2vector(dataSetDir + 'testDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 test_y.append(label) return train_x, train_y, test_x, test_y# 手写识别主流程def testHandWritingClass(): ## step 1: 加载数据 print "step 1: load data..." train_x, train_y, test_x, test_y = loadDataSet() ## step 2: 模型训练. print "step 2: training..." pass ## step 3: 测试 print "step 3: testing..." numTestSamples = test_x.shape[0] matchCount = 0 for i in xrange(numTestSamples): predict = kNNClassify(test_x[i], train_x, train_y, 3) if predict == test_y[i]: matchCount += 1 accuracy = float(matchCount) / numTestSamples ## step 4: 输出结果 print "step 4: show the result..." print 'The classify accuracy is: %.2f%%' % (accuracy * 100) 测试12import kNNkNN.testHandWritingClass() 执行结果：1234567tep 1: load data...---Getting training set...---Getting testing set...step 2: training...step 3: testing...step 4: show the result...The classify accuracy is: 98.84%]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之KNN算法推演]]></title>
    <url>%2F2018%2F01%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN%E7%AE%97%E6%B3%95%E6%8E%A8%E6%BC%94%2F</url>
    <content type="text"><![CDATA[从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。算法涉及3个主要因素：1) 训练数据集2) 距离或相似度的计算衡量3) k的大小绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。 KNN分类算法Python实战有以下先验数据，使用knn算法对未知类别数据分类 x轴 y轴 类型 1.0 0.9 A 1.0 1.0 A 0.0 0.1 B 0.1 0.2 B 未知类别数据 x轴 y轴 类型 1.2 1.0 ？ 0.1 0.3 ？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960########################################## kNN: k Nearest Neighbors# 输入: newInput: (1xN)的待分类向量# dataSet: (NxM)的训练数据集# labels: 训练数据集的类别标签向量# k: 近邻数 # 输出: 可能性最大的分类标签#########################################from numpy import *import operator#创建一个数据集，包含2个类别共4个样本def createDataSet(): # 生成一个矩阵，每行表示一个样本 group = array([[1.0, 0.9], [1.0, 1.0], [0.1, 0.2], [0.0, 0.1]]) # 4个样本分别所属的类别 labels = ['A', 'A', 'B', 'B'] return group, labels# KNN分类算法函数定义def kNNClassify(newInput, dataSet, labels, k): numSamples = dataSet.shape[0] # shape[0]表示行数 ## step 1: 计算距离 # tile(A, reps): 构造一个矩阵，通过A重复reps次得到(tile(A, （repsX，repsY）)此处在行的方向重复repsX次，在Y的方向重复repsY次) # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) - dataSet # 按元素求差值 squaredDiff = diff ** 2 #将差值平方 squaredDist = sum(squaredDiff, axis = 1) # 按行累加 distance = squaredDist ** 0.5 #将差值平方和求开方，即得距离 ## step 2: 对距离排序 ## 此处排序需要注意样本标签的顺序。排序后存的是角标号。后续直接从排序后的标签序列找下标是排序内容的数据 # argsort() 返回排序后的索引值 sortedDistIndices = argsort(distance) #或distance.argsort() classCount = &#123;&#125; # define a dictionary (can be append element) for i in xrange(k): ## step 3: 选择k个最近邻 voteLabel = labels[sortedDistIndices[i]] ## step 4: 计算k个最近邻中各类别出现的次数 # when the key voteLabel is not in dictionary classCount, get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ## step 5: 返回出现次数最多的类别标签 maxCount = 0 for key, value in classCount.items(): if value &gt; maxCount: maxCount = value maxIndex = key return maxIndex ##step 5 可以改成： # sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True) #return sortedClassCount[0][0] 然后调用算法进行测试：1234567891011121314import kNNfrom numpy import * #生成数据集和类别标签dataSet, labels = kNN.createDataSet()#定义一个未知类别的数据testX = array([1.2, 1.0])k = 3#调用分类函数对未知数据分类outputLabel = kNN.kNNClassify(testX, dataSet, labels, 3)print "Your input is:", testX, "and classified to class: ", outputLabeltestX = array([0.1, 0.3])outputLabel = kNN.kNNClassify(testX, dataSet, labels, 3)print "Your input is:", testX, "and classified to class: ", outputLabel 这时候会输出12Your input is: [ 1.2 1.0] and classified to class: AYour input is: [ 0.1 0.3] and classified to class: B]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-概念]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[监督学习：我们向系统输入我们所称的“已标记样本”已标记样本指我们给系统一些信息，让它用于理解我们向它输入的数据。例如我们创建一个系统来识别我的脸，我们将给他一些我的面部照片，一些别人的面部照片，我们会告诉系统 这些照片是David的，那些照片是别人的。随时间推移，它会利用这些信息提升它的理解程度。”监督“表示你有很多样本，你了解这些样本的正确答案 非监督学习比如在医疗保健方面，如果我们已经了解某种疾病，监督学习可用以帮助系统，识别哪些人有患该病的风险，而非监督学习实际上可以根据常见症状的模式帮助我们发现甚至我们还不知道的疾病。 聚类：K-MEANS 朴素贝叶斯：推导过程和用法：http://scikit-learn.org/stable/modules/naive_bayes.html 还有高斯朴素贝叶斯 python有一个库叫sklearn]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-预测泰坦尼克号乘客生还率]]></title>
    <url>%2F2018%2F01%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E8%BF%98%E7%8E%87%2F</url>
    <content type="text"><![CDATA[机器学习项目 0: 预测泰坦尼克号乘客生还率1912年，泰坦尼克号在第一次航行中就与冰山相撞沉没，导致了大部分乘客和船员身亡。在这个入门项目中，我们将探索部分泰坦尼克号旅客名单，来确定哪些特征可以最好地预测一个人是否会生还。为了完成这个项目，你将需要实现几个基于条件的预测并回答下面的问题。我们将根据代码的完成度和对问题的解答来对你提交的项目的进行评估。 开始当我们开始处理泰坦尼克号乘客数据时，会先导入我们需要的功能模块以及将数据加载到 pandas DataFrame。运行下面区域中的代码加载数据，并使用 .head() 函数显示前几项乘客数据。 1234567891011121314151617import numpy as npimport pandas as pd# RMS Titanic data visualization code # 数据可视化代码from titanic_visualizations import survival_statsfrom IPython.display import display%matplotlib inline# Load the dataset # 加载数据集in_file = 'titanic_data.csv'full_data = pd.read_csv(in_file)# Print the first few entries of the RMS Titanic data # 显示数据列表中的前几项乘客数据display(full_data.head()) 序号 PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th… female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 从泰坦尼克号的数据样本中，我们可以看到船上每位旅客的特征 Survived：是否存活（0代表否，1代表是） Pclass：社会阶级（1代表上层阶级，2代表中层阶级，3代表底层阶级） Name：船上乘客的名字 Sex：船上乘客的性别 Age:船上乘客的年龄（可能存在 NaN） SibSp：乘客在船上的兄弟姐妹和配偶的数量 Parch：乘客在船上的父母以及小孩的数量 Ticket：乘客船票的编号 Fare：乘客为船票支付的费用 Cabin：乘客所在船舱的编号（可能存在 NaN） Embarked：乘客上船的港口（C 代表从 Cherbourg 登船，Q 代表从 Queenstown 登船，S 代表从 Southampton 登船） 因为我们感兴趣的是每个乘客或船员是否在事故中活了下来。可以将 Survived 这一特征从这个数据集移除，并且用一个单独的变量 outcomes 来存储。它也做为我们要预测的目标。 运行该代码，从数据集中移除 Survived 这个特征，并将它存储在变量 outcomes 中。 123456789101112# Store the 'Survived' feature in a new variable and remove it from the dataset # 从数据集中移除 'Survived' 这个特征，并将它存储在一个新的变量中。outcomes = full_data['Survived']data = full_data.drop('Survived', axis = 1)# Show the new dataset with 'Survived' removed# 显示已移除 'Survived' 特征的数据集display(data.head())display(outcomes[1])display(data.loc[1]) 序号 PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 Cumings, Mrs. John Bradley (Florence Briggs Th… female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1 PassengerId 2 Pclass 1 Name Cumings, Mrs. John Bradley (Florence Briggs Th... Sex female Age 38 SibSp 1 Parch 0 Ticket PC 17599 Fare 71.2833 Cabin C85 Embarked C Name: 1, dtype: object 这个例子展示了如何将泰坦尼克号的 Survived 数据从 DataFrame 移除。注意到 data（乘客数据）和 outcomes （是否存活）现在已经匹配好。这意味着对于任何乘客的 data.loc[i] 都有对应的存活的结果 outcome[i]。 为了验证我们预测的结果，我们需要一个标准来给我们的预测打分。因为我们最感兴趣的是我们预测的准确率，既正确预测乘客存活的比例。运行下面的代码来创建我们的 accuracy_score 函数以对前五名乘客的预测来做测试。 思考题：从第六个乘客算起，如果我们预测他们全部都存活，你觉得我们预测的准确率是多少？ 123456789101112131415161718192021222324def accuracy_score(truth, pred): """ Returns accuracy score for input truth and predictions. """ # Ensure that the number of predictions matches number of outcomes # 确保预测的数量与结果的数量一致 if len(truth) == len(pred): # print "kaish1"# print truth# print "kaish2"# print pred# print "end" # Calculate and return the accuracy as a percent # 计算预测准确率（百分比） return "Predictions have an accuracy of &#123;:.2f&#125;%.".format((truth == pred).mean()*100) else: return "Number of predictions does not match number of outcomes!" # Test the 'accuracy_score' function# 测试 'accuracy_score' 函数# print np.ones(5, dtype = int)# print outcomes[:5]predictions = pd.Series(np.ones(5, dtype = int))# print accuracy_score(outcomes[:5], predictions) 提示：如果你保存 iPython Notebook，代码运行的输出也将被保存。但是，一旦你重新打开项目，你的工作区将会被重置。请确保每次都从上次离开的地方运行代码来重新生成变量和函数。 预测如果我们要预测泰坦尼克号上的乘客是否存活，但是我们又对他们一无所知，那么最好的预测就是船上的人无一幸免。这是因为，我们可以假定当船沉没的时候大多数乘客都遇难了。下面的 predictions_0 函数就预测船上的乘客全部遇难。 1234567891011121314151617def predictions_0(data): """ Model with no features. Always predicts a passenger did not survive. """ predictions = [] for _, passenger in data.iterrows(): # Predict the survival of 'passenger' # 预测 'passenger' 的生还率 predictions.append(0) # Return our predictions # 返回预测结果 return pd.Series(predictions)# Make the predictions# 进行预测predictions = predictions_0(data) 问题1对比真实的泰坦尼克号的数据，如果我们做一个所有乘客都没有存活的预测，你认为这个预测的准确率能达到多少？ 提示：运行下面的代码来查看预测的准确率。 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 61.62%. 回答: 请用上面出现的预测结果来替换掉这里的文字 61.62% 我们可以使用 survival_stats 函数来看看 Sex 这一特征对乘客的存活率有多大影响。这个函数定义在名为 titanic_visualizations.py 的 Python 脚本文件中，我们的项目提供了这个文件。传递给函数的前两个参数分别是泰坦尼克号的乘客数据和乘客的 生还结果。第三个参数表明我们会依据哪个特征来绘制图形。 运行下面的代码绘制出依据乘客性别计算存活率的柱形图。 1survival_stats(data, outcomes, 'Sex') 观察泰坦尼克号上乘客存活的数据统计，我们可以发现大部分男性乘客在船沉没的时候都遇难了。相反的，大部分女性乘客都在事故中生还。让我们在先前推断的基础上继续创建：如果乘客是男性，那么我们就预测他们遇难；如果乘客是女性，那么我们预测他们在事故中活了下来。 将下面的代码补充完整，让函数可以进行正确预测。 提示：您可以用访问 dictionary（字典）的方法来访问船上乘客的每个特征对应的值。例如， passenger[&#39;Sex&#39;] 返回乘客的性别。 123456789101112131415161718192021222324252627def predictions_1(data): """ Model with one feature: - Predict a passenger survived if they are female. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # 移除下方的 'pass' 声明 # and write your prediction conditions here # 输入你自己的预测条件 # print passenger['Sex'] if(passenger['Sex'] == "male") : predictions.append(0) else : predictions.append(1) # Return our predictions # 返回预测结果 return pd.Series(predictions)# Make the predictions# 进行预测predictions = predictions_1(data)# print predictions 问题2当我们预测船上女性乘客全部存活，而剩下的人全部遇难，那么我们预测的准确率会达到多少？ 提示：运行下面的代码来查看我们预测的准确率。 12#print predictionsprint accuracy_score(outcomes, predictions) Predictions have an accuracy of 78.68%. 回答: 用上面出现的预测结果来替换掉这里的文字 78.68%. 仅仅使用乘客性别（Sex）这一特征，我们预测的准确性就有了明显的提高。现在再看一下使用额外的特征能否更进一步提升我们的预测准确度。例如，综合考虑所有在泰坦尼克号上的男性乘客：我们是否找到这些乘客中的一个子集，他们的存活概率较高。让我们再次使用 survival_stats 函数来看看每位男性乘客的年龄（Age）。这一次，我们将使用第四个参数来限定柱形图中只有男性乘客。 运行下面这段代码，把男性基于年龄的生存结果绘制出来。 1survival_stats(data, outcomes, 'Age', ["Sex == 'male'"]) 仔细观察泰坦尼克号存活的数据统计，在船沉没的时候，大部分小于10岁的男孩都活着，而大多数10岁以上的男性都随着船的沉没而遇难。让我们继续在先前预测的基础上构建：如果乘客是女性，那么我们就预测她们全部存活；如果乘客是男性并且小于10岁，我们也会预测他们全部存活；所有其它我们就预测他们都没有幸存。 将下面缺失的代码补充完整，让我们的函数可以实现预测。提示: 您可以用之前 predictions_1 的代码作为开始来修改代码，实现新的预测函数。 12345678910111213141516171819202122232425262728293031def predictions_2(data): """ Model with two features: - Predict a passenger survived if they are female. - Predict a passenger survived if they are male and younger than 10. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # 移除下方的 'pass' 声明 # and write your prediction conditions here # 输入你自己的预测条件 if(passenger['Sex'] == "male") : if(passenger['Age'] &lt; 10) : predictions.append(1) else : predictions.append(0) else : predictions.append(1) # Return our predictions # 返回预测结果 return pd.Series(predictions)# Make the predictions# 进行预测predictions = predictions_2(data)# print predictions 问题3当预测所有女性以及小于10岁的男性都存活的时候，预测的准确率会达到多少？ 提示：运行下面的代码来查看预测的准确率。 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 79.35%. 回答: 用上面出现的预测结果来替换掉这里的文字 79.35% 添加年龄（Age）特征与性别（Sex）的结合比单独使用性别（Sex）也提高了不少准确度。现在该你来做预测了：找到一系列的特征和条件来对数据进行划分，使得预测结果提高到80%以上。这可能需要多个特性和多个层次的条件语句才会成功。你可以在不同的条件下多次使用相同的特征。Pclass，Sex，Age，SibSp 和 Parch 是建议尝试使用的特征。 使用 survival_stats 函数来观测泰坦尼克号上乘客存活的数据统计。提示: 要使用多个过滤条件，把每一个条件放在一个列表里作为最后一个参数传递进去。例如: [&quot;Sex == &#39;male&#39;&quot;, &quot;Age &lt; 18&quot;] 1survival_stats(data, outcomes, 'Embarked', ["Sex == 'female'","Pclass == 3"]) 当查看和研究了图形化的泰坦尼克号上乘客的数据统计后，请补全下面这段代码中缺失的部分，使得函数可以返回你的预测。在到达最终的预测模型前请确保记录你尝试过的各种特征和条件。提示: 您可以用之前 predictions_2 的代码作为开始来修改代码，实现新的预测函数。 123456789101112131415161718192021222324252627282930313233343536373839def predictions_3(data): """ Model with multiple features. Makes a prediction with an accuracy of at least 80%. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # and write your prediction conditions here if(passenger['Sex'] == "male") : if(passenger['Age'] &lt; 18) : if(passenger['Pclass'] &lt; 3 ) : predictions.append(1) else : predictions.append(0) else : predictions.append(0) # print passenger['Pclass']# if(passenger['Pclass'] &gt; 1 ) :# predictions.append(0)# else :# predictions.append(1) else : if(passenger['Pclass'] == 3 ) : if(passenger['Embarked'] == 'S' ) : predictions.append(0) else : predictions.append(1) else : predictions.append(1) # Return our predictions return pd.Series(predictions)# Make the predictionspredictions = predictions_3(data) 结论请描述你实现80%准确度的预测模型所经历的步骤。您观察过哪些特征？某些特性是否比其他特征更有帮助？你用了什么条件来预测生还结果？你最终的预测的准确率是多少？提示:运行下面的代码来查看你的预测准确度。 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 82.38%. 回答: 用上面问题的答案来替换掉这里的文字 结论经过了数次对数据的探索和分类，你创建了一个预测泰坦尼克号乘客存活率的有用的算法。在这个项目中你手动地实现了一个简单的机器学习模型——决策树（decision tree）。决策树每次按照一个特征把数据分割成越来越小的群组（被称为 nodes）。每次数据的一个子集被分出来，如果分割结果的子集中的数据比之前更同质（包含近似的标签），我们的预测也就更加准确。电脑来帮助我们做这件事会比手动做更彻底，更精确。这个链接提供了另一个使用决策树做机器学习入门的例子。 决策树是许多监督学习算法中的一种。在监督学习中，我们关心的是使用数据的特征并根据数据的结果标签进行预测或建模。也就是说，每一组数据都有一个真正的结果值，不论是像泰坦尼克号生存数据集一样的标签，或者是连续的房价预测。 问题5想象一个真实世界中应用监督学习的场景，你期望预测的结果是什么？举出两个在这个场景中能够帮助你进行预测的数据集中的特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python numpy]]></title>
    <url>%2F2018%2F01%2F10%2Fpython%20numpy%2F</url>
    <content type="text"><![CDATA[jupyter notebook:1.arange生成数组np.arange(10)array([0,1,2,3,4,5,6,7,8,9]) 2.等差数列是指从第二项起，每一项与它的前一项的差等于同一个常数的一种数列，常用A、P表示等差数列 linspacenp.linspace(1,10,5)array([1. , 3.25 ,5.5,5.75,10]) np.linspace(1,10,5,endpoint=False) 相当于生成6个数只显示前五个array([1. , 2.8 ,4.6,6.5,8.2]) 3.等比数列是指从第二项起，每一项与它的前一项的比值等于同一个常数的一种数列等比数列logspacenp.logspace(1,10,5) 4.shapearr = np.array([ [1,2,3], [2,3,4] ]) arr.shape(2, 3) 5.zeroshelp(np.zeros)zeros(shape, dtype=float, order=’C’) Examples12345678910111213141516171819&gt;&gt;&gt; np.zeros(5)array([ 0., 0., 0., 0., 0.])&gt;&gt;&gt; np.zeros((5,), dtype=np.int)array([0, 0, 0, 0, 0])&gt;&gt;&gt; np.zeros((2, 1))array([[ 0.], [ 0.]])&gt;&gt;&gt; s = (2,2)&gt;&gt;&gt; np.zeros(s)array([[ 0., 0.], [ 0., 0.]])&gt;&gt;&gt; np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtypearray([(0, 0), (0, 0)], dtype=[('x', '&lt;i4'), ('y', '&lt;i4')]) 6.ones类似zeros,只不过填充的是1 7.empty类似zeros,只不过填充的是随机值 8.reshape把一维数组转置为多维数组reshape 不会改变原来的ndarray，但是得到新的ndarray是原数组的视图对于ndarray的一些方法操作，首先要区分是否会改变原来变量，以此来判断是视图还是副本]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python—pandas]]></title>
    <url>%2F2018%2F01%2F08%2Fpython%E2%80%94pandas%2F</url>
    <content type="text"><![CDATA[from pandas import Series,DataFrameimport pandas as pd Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。 索引可重复1234567from pandas import Series,DataFrameimport pandas as pdimport numpy as nparr = np.array([1,3,5,np.NaN,10])serises0=Series(arr)serises0 0 1.01 3.02 5.03 NaN4 10.0dtype: float641serises0.dtype dtype(‘float64’) 1serises0.index RangeIndex(start=0, stop=5, step=1) 1serises0.values array([ 1., 3., 5., nan, 10.]) 12serises1=Series(data=[91,92,93],dtype=np.float64,index=[u'数学',u'语文',u'外语'])serises1 数学 91.0语文 92.0外语 93.0dtype: float64 12dict0=&#123;u'数学':91.0,u'语文':92,u'外语':93&#125;dict0 {u’\u5916\u8bed’: 93, u’\u6570\u5b66’: 91.0, u’\u8bed\u6587’: 92} 12serises2=Series(dict0)serises2 外语 93.0数学 91.0语文 92.0dtype: float64 1serises2[0] 93.0 1serises2[u'外语'] 93.0 1serises2['外语':'语文'] 外语 93.0数学 91.0语文 92.0dtype: float64 Series运算，自动对齐索引123serises2=Series(data=[11,12,13],dtype=np.float64,index=['p1','p2','p3'])serises3=Series(data=[22,23,24,25],dtype=np.float64,index=['p2','p3','p4','p5'])serises2 +serises3 p1 NaNp2 34.0p3 36.0p4 NaNp5 NaNdtype: float64 123serises1.name='name'serises1.index.name='考试科目'serises1 考试科目数学 91.0语文 92.0外语 93.0Name: name, dtype: float64 DataFrame ： 表格形式的数据结构，包含一组有序的列，每列可以是不同的值类型，DataFrame既有行索引又有列索引，可以看做是由Series组成的字典通过二维数组创建12df01=DataFrame([['Tom','John'],[88,90]])df01 0 1 0 Tom John1 88 90 通过字典创建123data=&#123;'Tom':[88,55],'John':[90,22]&#125;df02=DataFrame(data)df02 John Tom0 90 881 22 55 DataFrame 可以增加数据1df02.ix['2']=np.NaN John Tom 0 90.0 88.01 22.0 55.02 NaN NaN 数据删除12df02=df02.dropna()df02 John Tom 0 90.0 88.01 22.0 55.0 123456arr1=np.random.randint(5,10,(4,4))df1=pd.DataFrame(arr1)df1df1.ix[:2,1]=np.NANdf1.ix[:1,2]=np.NANdf1 0 1 2 3 0 8 NaN NaN 71 5 NaN NaN 92 5 NaN 5.0 53 9 8.0 7.0 5 loc ilociloc 对于下标进行操作loc 对于索引值进行操作]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 机器学习入门(二)]]></title>
    <url>%2F2017%2F10%2F19%2Fspark%20ml2%2F</url>
    <content type="text"><![CDATA[Spark MLlib 核心组件Ø DataFrame 包含不同的列，可存储文本、 图片、 标签、 特征向量、 预测值等Ø Estimator 用于从训练数据算出机器学习模型，输入DataFrame输出ModelØ Transformer 输入DataFrame输出DataFrame• Model即为一种Transformer，用于从测试数据生成预测结果• 特征转换Transformer转换一个或多个特征，并将转换后的特征列追加到输出DataFrame中Ø Pipeline• 将多个Transformer和Estimator链接起来形成一个机器学习workflow• Pipeline也是一种Estimator，生成一个PipelineModelØ Parameter 所有的Transformer和Estimator共享该通用的API来指定参数Ø PipelineModel 保证对测试数据使用与训练数据完全相同的数据转换 训练模型： Tokenizer and HashingTF 是TransformersLogisticRegression 是Estimator 预测结果Pipeline 是 Estimatorfit()方法会产生一个PipelineModeltransform()方法Pipelines and PipelineModels帮助确保训练数据和测试数据经过相同特征的处理步骤。 SparseMatrix介绍推荐博主：http://blog.csdn.net/sinat_29508201/article/details/54089771]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 机器学习入门(一)]]></title>
    <url>%2F2017%2F10%2F18%2Fspark%20ml1%2F</url>
    <content type="text"><![CDATA[目前基于RDD的MLlib已经进入维护莫斯。大概在spark2.3基于RDD的MLlib API将要被废弃。未来是基于DataFrame的API 1.基本统计计算两组数据之间的相关性 皮尔森相关系数（Pearson correlation coefficient）也称皮尔森积矩相关系数(Pearson product-moment correlation coefficient) ，是一种线性相关系数。皮尔森相关系数是用来反映两个变量线性相关程度的统计量。相关系数用r表示，其中n为样本量，分别为两个变量的观测值和均值。r描述的是两个变量间线性相关强弱的程度。r的绝对值越大表明相关性越强 按照高中数学水平来理解, 它很简单, 可以看做将两组数据首先做Z分数处理之后, 然后两组数据的乘积和除以样本数 Z分数一般代表正态分布中, 数据偏离中心点的距离.等于变量减掉平均数再除以标准差.(就是高考的标准分类似的处理) 标准差则等于变量减掉平均数的平方和,再除以样本数,最后再开方. 所以, 根据这个最朴素的理解,我们可以将公式依次精简为: spearman相关系数：是衡量分级定序变量之间的相关程度的统计量，对不服从正态分布的资料、原始资料等级资料、一侧开口资料、总体分布类型未知的资料不符合使用积矩相关系数来描述关联性。此时可采用秩相关（rank correlation），也称等级相关，来描述两个变量之间的关联程度与方向。 1234567891011121314151617181920212223242526272829303132import org.apache.spark.ml.linalg.&#123;Matrix, Vectors&#125;import org.apache.spark.ml.stat.Correlationimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.Rowobject ml_1 &#123;def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder .master("local") .getOrCreate() import spark.implicits._ // $example on$ val data = Seq( Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))), Vectors.dense(4.0, 5.0, 0.0, 3.0), Vectors.dense(6.0, 7.0, 0.0, 8.0), Vectors.sparse(4, Seq((0, 9.0), (3, 1.0))) ) val df = data.map(Tuple1.apply).toDF("features") val Row(coeff1: Matrix) = Correlation.corr(df, "features").head println("Pearson correlation matrix:\n" + coeff1.toString) val Row(coeff2: Matrix) = Correlation.corr(df, "features", "spearman").head println("Spearman correlation matrix:\n" + coeff2.toString) // $example off$ spark.stop() &#125;&#125; 1.0为第一列和第一列计算0.055641488407465814为第一列和第二列计算0.4004714203168137 为第三列和第四列计算 计算结果:12345678910Pearson correlation matrix:1.0 0.055641488407465814 NaN 0.4004714203168137 0.055641488407465814 1.0 NaN 0.9135958615342522 NaN NaN 1.0 NaN 0.4004714203168137 0.9135958615342522 NaN 1.0 Spearman correlation matrix:1.0 0.10540925533894532 NaN 0.40000000000000174 0.10540925533894532 1.0 NaN 0.9486832980505141 NaN NaN 1.0 NaN 0.40000000000000174 0.9486832980505141 NaN 1.0 卡方检验卡方检验是用途非常广的一种假设检验方法，它在分类资料统计推断中的应用，包括：两个率或两个构成比比较的卡方检验；多个率或多个构成比比较的卡方检验以及分类资料的相关分析等。卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。 1234567891011121314151617181920212223242526272829303132333435import org.apache.spark.ml.linalg.Vectorimport org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.ml.stat.ChiSquareTestimport org.apache.spark.sql.SparkSession/** * Created by Administrator on 2017/10/18. */object ChiSquareTest &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder .master("local") .getOrCreate() import spark.implicits._ val data = Seq( (0.0, Vectors.dense(0.5, 10.0)), (0.0, Vectors.dense(1.5, 20.0)), (1.0, Vectors.dense(1.5, 30.0)), (0.0, Vectors.dense(3.5, 30.0)), (0.0, Vectors.dense(3.5, 40.0)), (1.0, Vectors.dense(3.5, 40.0)) ) val df = data.toDF("label", "features") val chi = ChiSquareTest.test(df, "features", "label").head println("pValues = " + chi.getAs[Vector](0)) println("degreesOfFreedom = " + chi.getSeq[Int](1).mkString("[", ",", "]")) println("statistics = " + chi.getAs[Vector](2)) spark.stop() &#125;&#125; 结果：123pValues = [0.6872892787909721,0.6822703303362126]degreesOfFreedom = [2,3]statistics = [0.75,1.5]]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark shuffle 调优]]></title>
    <url>%2F2017%2F08%2F30%2Fspark%20shuffle%20%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[shuffle实现的具体过程1.Spark系统在运行含shuffle过程的应用时，Executor进程除了运行task，还要负责写shuffle 数据，给其他Executor提供shuffle数据。 当Executor进程任务过重，导致GC而不能为其 他Executor提供shuffle数据时，会影响任务运行。 这里实际上是利用External Shuffle Service 来提升性能，External shuffle Service是长期存在于NodeManager进程中的一个辅助服务。 通过该服务 来抓取shuffle数据，减少了Executor的压力，在Executor GC的时候也不会影响其他 Executor的任务运行。 启用方法： 一. 在NodeManager中启动External shuffle Service。a. 在“yarn-site.xml”中添加如下配置项： &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;spark_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt; &lt;/property&gt; 配置参数描述 yarn.nodemanager.aux-services ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle计算性能。 yarn.nodemanager.auxservices.spark_shuffle.class ：NodeManager中辅助服务对应的类。 spark.shuffle.service.port ：Shuffle服务监听数据获取请求的端口。可选配置，默认值为“7337”。 b. 添加依赖的jar包 拷贝“${SPARK_HOME}/lib/spark-1.3.0-yarn-shuffle.jar”到“${HADOOP_HOME}/share/hadoop/yarn/lib/”目录下。 c. 重启NodeManager进程，也就启动了External shuffle Service。 二. Spark应用使用External shuffle Service。在“spark-defaults.conf”中必须添加如下配置项： spark.shuffle.service.enabled true spark.shuffle.service.port 7337 说明 1.如果1.如果“yarn.nodemanager.aux-services”配置项已存在，则在value中添加 “spark_shuffle”，且用逗号和其他值分开。 2.“spark.shuffle.service.port”的值需要和上面“yarn-site.xml”中的值一样。 配置参数描述 spark.shuffle.service.enabled ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle 计算性能。默认为false，表示不启用该功能。 spark.shuffle.service.port ：Shuffle服务监听数据获取请求的端口。可选配置，默认值 为“7337”。 Hash Shuffle不足map task会根据reduce的数量（partition） 生成相应的bucket 写shuffle blockFile 如果map 和reduce数量过多，会写很多blockFile，造成问题1：超过操作系统所能打开最大文件数，问题2：大量随机写随机读 解决方案： 1.shuffle 参数：spark.shuffle.consolidateFiles 默认为false如果设置为”true”，在shuffle期间，合并的中间文件将会被创建。创建更少的文件可以提供文件系统的shuffle的效 率。这些shuffle都伴随着大量递归任务。当用ext4和dfs文件系统时，推荐设置为”true”。在ext3中，因为文件系统的限制，这个选项可 能机器（大于8核）降低效率 2.sort shuffle每个map只写到一个文件，和上面的写到reduce个数个文件不同 不同算子影响shuffle表现形式不同reduceByKey(func, numPartitions=None)也就是，reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。 groupByKey(numPartitions=None)也就是，groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 机器学习入门(二)]]></title>
    <url>%2F2017%2F08%2F29%2Fspark%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[部署优化：磁盘：挂载磁盘时使用noatime和nodiratime选项减少写的开销 linux每个文件都会保留3个时间戳用stat 文件名 来查看Acess：文件访问时间Modfiy：内容修改时间Change：文件名修改时间 参数含义：磁盘下的所有文件不更新访问时间内存：JVM 内存不建议每个executor 超过200G CPU每台机器的Vcore数不建议小于8 JOB调度：Fail Schedule 可最大程度保证各个Job都有机会获取资源 数据序列化：Kyro serialization序列化速度更快，结果更紧凑为了更好的性能，需提前注册被序列化的类，否则会存在大量的空间浪费通过spark.serializer指定 减少内存消耗：尽量使用基本数据类型和数组，避免使用java集合类尽量减少包含大量小对象的嵌套结构Key尽量使用数值或枚举类型而不是字符串RAM小于32GB时，使用-XX:+UseCompressedOops使用4字节（而非8字节）的指针 调整并行度：调整Map侧并行度对于kafka direct stream 可通过调整Topic的Patition个数调整Spark Map侧并行度对于spark.textFile，通过参数调整 调整Reduce侧并行度通过spark.default.parallelism设置shuffle时默认并行度]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源爬虫介绍]]></title>
    <url>%2F2017%2F08%2F05%2F%E5%BC%80%E6%BA%90%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[https://github.com/Chyroc/WechatSogou 微信公众号爬虫。基于搜狗微信搜索的微信公众号爬虫接口，可以扩展成基于搜狗搜索的爬虫，返回结果是列表，每一项均是公众号具体信息字典。 https://github.com/lanbing510/DouBanSpider 豆瓣读书爬虫。可以爬下豆瓣读书标签下的所有图书，按评分排名依次存储，存储到Excel中，可方便大家筛选搜罗，比如筛选评价人数&gt;1000的高分书籍；可依据不同的主题存储到Excel不同的Sheet ，采用User Agent伪装为浏览器进行爬取，并加入随机延时来更好的模仿浏览器行为，避免爬虫被封。 https://github.com/LiuRoy/zhihu_spider知乎爬虫。此项目的功能是爬取知乎用户信息以及人际拓扑关系，爬虫框架使用scrapy，数据存储使用mongo https://github.com/airingursb/bilibili-userBilibili用户爬虫。总数据数：20119918，抓取字段：用户id，昵称，性别，头像，等级，经验值，粉丝数，生日，地址，注册时间，签名，等级与经验值等。抓取之后生成B站用户数据报告。 https://github.com/LiuXingMing/SinaSpider新浪微博爬虫。主要爬取新浪微博用户的个人信息、微博信息、粉丝和关注。代码获取新浪微博Cookie进行登录，可通过多账号登录来防止新浪的反扒。主要使用 scrapy 爬虫框架 https://github.com/gnemoug/distribute_crawler小说下载分布式爬虫。使用scrapy,Redis, MongoDB,graphite实现的一个分布式网络爬虫,底层存储mongodb集群,分布式使用redis实现,爬虫状态显示使用graphite实现，主要针对一个小说站点。 https://github.com/yanzhou/CnkiSpider中国知网爬虫。设置检索条件后，执行src/CnkiSpider.py抓取数据，抓取数据存储在/data目录下，每个数据文件的第一行为字段名称。 https://github.com/lanbing510/LianJiaSpider链家网爬虫。爬取北京地区链家历年二手房成交记录。涵盖链家爬虫一文的全部代码，包括链家模拟登录代码。 https://github.com/taizilongxu/scrapy_jingdong京东爬虫。基于scrapy的京东网站爬虫，保存格式为csv。 https://github.com/caspartse/QQ-Groups-SpiderQQ 群爬虫。批量抓取 QQ 群信息，包括群名称、群号、群人数、群主、群简介等内容，最终生成 XLS(X) / CSV 结果文件。 https://github.com/hanc00l/wooyun_public乌云爬虫。 乌云公开漏洞、知识库爬虫和搜索。全部公开漏洞的列表和每个漏洞的文本内容存在mongodb中，大概约2G内容；如果整站爬全部文本和图片作为离线查询，大概需要10G空间、2小时（10M电信带宽）；爬取全部知识库，总共约500M空间。漏洞搜索使用了Flask作为web server，bootstrap作为前端。 https://github.com/fankcoder/findtrip机票爬虫（去哪儿和携程网）。Findtrip是一个基于Scrapy的机票爬虫，目前整合了国内两大机票网站（去哪儿 + 携程）。 https://github.com/leyle/163spider基于requests、MySQLdb、torndb的网易客户端内容爬虫 https://github.com/fanpei91/doubanspiders豆瓣电影、书籍、小组、相册、东西等爬虫集 https://github.com/LiuXingMing/QQSpiderQQ空间爬虫，包括日志、说说、个人信息等，一天可抓取 400 万条数据。 https://github.com/Shu-Ji/baidu-music-spider百度mp3全站爬虫，使用redis支持断点续传。 https://github.com/pakoo/tbcrawler淘宝和天猫的爬虫,可以根据搜索关键词,物品id来抓去页面的信息，数据存储在mongodb。 https://github.com/benitoro/stockholm一个股票数据（沪深）爬虫和选股策略测试框架。根据选定的日期范围抓取所有沪深两市股票的行情数据。支持使用表达式定义选股策略。支持多线程处理。保存数据到JSON文件、CSV文件。 https://github.com/k1995/BaiduyunSpider百度云盘爬虫]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH 安装spark2.2]]></title>
    <url>%2F2017%2F08%2F05%2FCDH%20%E5%AE%89%E8%A3%85spark2.2%2F</url>
    <content type="text"><![CDATA[官方链接https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html 1.下载Spark2 CSDhttps://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging 1.1.1 CSD笔者下载2.2版本http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar1.1.2 Parcelhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/ 1.1.2.1 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcelhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel 1.1.2.2 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel.sha1然后将SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1改名为SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha 1.1.2.3 manifest.jsonhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/manifest.json 停止服务/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server stop/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent stop 将csd文件放到/soft/bigdata/clouderamanager/cloudera/csd 将parcel文件放到/soft/bigdata/clouderamanager/cloudera/parcel-repo 修改权限chgrp cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jarchown cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jar 开启服务/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start 主机-&gt; Parcel就能看到spark2了 分配，激活。然后就可以添加服务了 如果添加服务不成功需要把jar文件放到/opt/cloudera/csd]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 2.2.0源码编译]]></title>
    <url>%2F2017%2F08%2F02%2Fspark%202.2.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[官网下载spark源码https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz 然后在idea中导入spark源码项目(idea maven配置正确)，然后对spark项目build。Build成功后在进行编译。Build过程中遇到问题： 找不到org.apache.spark.streaming.flume.sink.SparkFlumeProtocol找不到org.apache.spark.sql.catalyst.parser.SqlBaseParser 设置maven的参数，否则一直出现outofMemory在apache-maven-3.3.9-bin\bin下面的mvn.cmd文件里的：@REM set MAVEN_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000下面添加set MAVEN_OPTS= -Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m 在git bash 里编译进入spark源码目录mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Ambari安装部署Spark集群]]></title>
    <url>%2F2017%2F06%2F30%2F%E4%BD%BF%E7%94%A8Ambari%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Spark%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[下载进入官网，选择产品里面的下载页面或者直接登录https://hortonworks.com/downloads/选择HDP® 2.6: Ready for the enterprise下面的Automated Install Guide 因为博主是CENTOS 7在这个页面直接选择：RHEL/CentOS/Oracle Linux 7 wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo -O /etc/yum.repos.d/ambari.repoyum repolist yum install ambari-server 进入​Setup Optionsambari-server setup –j /usr/java/default提示参数只能是一个，看来jdk要后设置 ambari-server setup各种回车，jdk的时候选择安装1.8的如果输入自定义的jdk要注意权限问题 启动：ambari-server start 查看：[root@storm01 storm]# lsof -i:8080COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 7648 storm 1438u IPv6 124470 0t0 TCP *:webcache (LISTEN) 关闭防火墙systemctl disable firewalld.servicesystemctl stop firewalld.service 在浏览器输入http://storm01:8080即可进入UI用户密码都为admin 在每个机器上安装agentyum install -y ambari-agent 修改配置文件 vi /etc/ambari-agent/conf/ambari-agent.ini hostname=localhost更改为 hostname=storm01 启动agent service ambari-agent start 设置hive用户密码都为hive 需要PostgreSQL支持远程连接find / -name pg_hba.conf vi /var/lib/pgsql/data/pg_hba.conf将local all postgres peer改成local all postgres trusthost all postgres 127.0.0.1/32 ident改成host all postgres 127.0.0.1/32 trustlocal all ambari,mapred md5改成local all ambari,mapred,hive trusthost all ambari,mapred 0.0.0.0/0 md5改成host all ambari,mapred,hive 0.0.0.0/0 md5host all ambari,mapred ::/0 md5改成host all ambari,mapred,hive ::/0 md5 重启数据库service postgresql restart 检查lsof -i:5432 去仓库下载postgresql http://mvnrepository.com/ 下载 PostgreSQL JDBC Driver JDBC 4.2的9.2-1002-jdbc4 或者直接输入：wget http://central.maven.org/maven2/org/postgresql/postgresql/9.2-1002-jdbc4/postgresql-9.2-1002-jdbc4.jar ambari-server setup –jdbc-db=postgres –jdbc-driver=/root/postgresql-9.2-1002-jdbc4.jar 创建用户psql -U postgres -d postgrescreate user hive;alter user hive password ‘hive’;create database hive;grant all on database hive to hive;alter database hive owner to hive; 此时test ConnectionConnection 就可以测试成功 设置Ambari Metrics和Smart Sense用户密码都为admin 如果发现失败，需要设置/etc/hosts文件192.168.247.180 spark01192.168.247.181 spark02192.168.247.182 spark03 安装完成]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phoenix创建二级索引]]></title>
    <url>%2F2017%2F05%2F25%2Fphoenix%E5%88%9B%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引分为两种：Global IndexingLocal Indexing Global Indexing ：使用于读多，写少的场景.select 查出来的数据必须是索引字段才能使用到索引 Local Indexing ：hbase bulk load数据上传到hdfs：/phoenix_data/fanpu.csv HADOOP_CLASSPATH=$(hbase classpath) hadoop jar phoenix-4.8.0-cdh5.8.0-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool-libjars /soft/phoenix/phoenix-4.8.0-cdh5.8.0/lib/commons-csv-1.0.jar,/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib/joda-time-2.7.jar –table fanpu –input /phoenix_data/fanpu.csv 我如果使用Global Indexing：创建索引：create index fanpu_index on fanpu (“family”.”id_no”,”family”.”name”,”family”.”mobile”); select from fanpu where “family”.”id_no”=’xxxx’ and “family”.”name”=’马文学’ and “family”.”mobile”=’xxxx’; 是不能使用索引的*× select “family”.”id_no”,”family”.”name” from fanpu where “family”.”id_no”=’xxxx’ and “family”.”name”=’马文学’ and “family”.”mobile”=’xxxx’; 才可以√ 12345678910111213141516171819202122230: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_RECEIPT_NO&quot;);76,392 rows affected (6.256 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER on &quot;LENDER-LEN_TRANSFER&quot; (&quot;family&quot;.&quot;LEN_LOAN_SELL_CONFIRM_ID&quot;);99,937 rows affected (6.256 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER_DTL on &quot;LENDER-LEN_TRANSFER_DTL&quot; (&quot;family&quot;.&quot;LENDERINFOID&quot;,&quot;family&quot;.&quot;LOAN_TRANSFER_MODEL_ID&quot;);153,029 rows affected (6.239 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL2 on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_RECEIPT_NO&quot;);76,392 rows affected (6.231 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL3 on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_ID&quot;);76,392 rows affected (6.234 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LOAN_MAIN on &quot;LENDER-LEN_LOAN_MAIN&quot; (&quot;family&quot;.&quot;ASSUME_DEBTOR_ID&quot;);20,865 rows affected (6.233 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LOAN_MAIN2 on &quot;LENDER-LEN_LOAN_MAIN&quot; (&quot;family&quot;.&quot;LOAN_CONTRACT_ID&quot;);20,865 rows affected (6.231 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_INFO on &quot;LENDER-LEN_LENDER_INFO&quot; (&quot;family&quot;.&quot;CUSTOMER_ID&quot;);17,053 rows affected (6.235 seconds) 0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_SELL_CONFIRM on “LENDER-LEN_SELL_CONFIRM” (“ID”);9,417 rows affected (6.245 seconds) 0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER3 on “LENDER-LEN_TRANSFER” (“family”.”SELLER_LOAN_DTL_ID”);99,937 rows affected (6.251 seconds)]]></content>
      <categories>
        <category>phoenix</category>
      </categories>
      <tags>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH oozie]]></title>
    <url>%2F2017%2F05%2F18%2FCDH%20oozie%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装完oozie，打开UI:http://master1:11000/oozie/ 显示Oozie web console is disabled.解决方案;原因是oozie的/var/lib/oozie目录里缺少EXT的包 点击Documentation链接里quickstart给出了解决方案Download ExtJS library (it must be version 2.2) 下载地址http://dev.sencha.com/deploy/ext-2.2.zip 如果下载不下来可以试试CSDNhttp://download.csdn.net/download/start_baby/6280675或者：http://archive.cloudera.com/gplextras/misc/ext-2.2.zip unzip解压然后刷新页面成功进入oozie的web界面 编写job.properties（nameNode要当时active的）123456nameNode=hdfs://master2:8020jobTracker=master2:8032queueName=defaultexamplesRoot=user/oozie/my-apps/shelloozie.wf.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/workflow.xmlEXEC=emp-join-demp.sh 编写workflow.xml1234567891011121314151617181920212223&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt; &lt;start to=&quot;shell-node&quot;/&gt; &lt;action name=&quot;shell-node&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt; &lt;file&gt;$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;!--Copy the executable to compute node&apos;s current working directory --&gt; &lt;/shell&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; 编写emp-join-demp.sh12#!/bin/shjava -cp PhoenixAPI-1.0-SNAPSHOT.jar com.zwjf.Month [hdfs@master1 data]$ hadoop fs -mkdir -p /user/oozie/my-apps/shell[hdfs@master1 data]$ hadoop fs -put workflow.xml /user/oozie/my-apps/shell[hdfs@master1 data]$ hadoop fs -put emp-join-demp.sh /user/oozie/my-apps/shell 上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作hadoop fs -put PhoenixAPI-1.0-SNAPSHOT.jar /user/oozie/my-apps/shell [hdfs@master1 data]$ /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie job -oozie http://master1:11000/oozie -config /soft/data/job.properties -run 提交出错：去历史服务器查看问题http://master2:19888/jobhistory 报错：Launcher ERROR, reason: Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1] 是因为分发的时候找不到用户的jar包，在workflow.xml的 ${nameNode}/${examplesRoot}/${EXEC}#${EXEC}添加你的jar包并上传hdfs就可以## 例如：${nameNode}/${examplesRoot}/PhoenixAPI-1.0-SNAPSHOT.jar#PhoenixAPI-1.0-SNAPSHOT.jar 可参考如下文章http://blog.sina.com.cn/s/blog_e699b42b0102xh3o.html 增加定时任务job.properties增加123oozie.coord.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/coordinator.xmlstart=2017-05-18T16:30Zend=2019-07-30T16:00Z oozie.wf.application.path=${nameNode}/${examplesRoot}/workflow.xml改成workflowAppUri=${nameNode}/${examplesRoot}/workflow.xml 新建coordinator.xml12345678910111213141516171819202122232425&lt;coordinator-app name=&quot;coordinator&quot; frequency=&quot;$&#123;coord:minutes(10)&#125;&quot; start=&quot;$&#123;start&#125;&quot; end=&quot;$&#123;end&#125;&quot; timezone=&quot;Asia/Shanghai&quot; xmlns=&quot;uri:oozie:coordinator:0.2&quot;&gt; &lt;action&gt; &lt;workflow&gt; &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;jobTracker&lt;/name&gt; &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;EXEC&lt;/name&gt; &lt;value&gt;$&#123;EXEC&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nameNode&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;queueName&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/workflow&gt; &lt;/action&gt;&lt;/coordinator-app&gt; 上传hadoop fs -put coordinator.xml /user/oozie/my-apps/shell 停止任务：/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie job -oozie http://master1:11000/oozie -kill 0000005-170518154227460-oozie-oozi-C 注：设置的时间不能小于当前时间，否则会把之前没执行的都执行]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka源码阅读2]]></title>
    <url>%2F2017%2F05%2F08%2Fkafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB2%2F</url>
    <content type="text"><![CDATA[导入IDEA即可看kafka源码：启动之前需要安装zookeeper地址：http://apache.fayea.com/zookeeper/http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz 解压后再当前目录增加dataLogDir和data目录复制一份配置文件改名为zoo.cfg修改配置文件：zoo.cfg修改并增加dataDir=D:\tool\zookeeper-3.4.6\datadataLogDir=D:\tool\zookeeper-3.4.6\dataLogDir 启动zkServer.cmd 启动ZkCli.cmd kafka启动。在配置文件修改Program arguments：config/server.properties 然后修改server.properties里面的参数即可。 启动前： 启动后： 源码阅读（一）从启动入口分析：Kafka.scala 12345678910111213141516171819202122232425262728293031323334def main(args: Array[String]): Unit = &#123; try &#123; /*从配置文件读取kafka服务器启动参数--将传入的参数转换成Properties 文件，如果参数为空将提示： * USAGE: java [options] KafkaServer server.properties [--override property=value]* * Option Description * ------ ----------- * --override &lt;String&gt; Optional property that should override values set in * server.properties file * * 判断参数是否大于1，将后面的参数放到Properties里 * */ val serverProps = getPropsFromArgs(args) //创建KafkaServerStartable对象 val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps) // attach shutdown handler to catch control-c Runtime.getRuntime().addShutdownHook(new Thread() &#123; override def run() = &#123; kafkaServerStartable.shutdown &#125; &#125;) kafkaServerStartable.startup kafkaServerStartable.awaitShutdown &#125; catch &#123; case e: Throwable =&gt; fatal(e) System.exit(1) &#125; System.exit(0) &#125; 这上面有个小知识点：_* 告诉编译器你希望将某个参数当作参数序列处理123456def echo(args: String*) = for (arg &lt;- args) println(arg)def main(args: Array[String]) = &#123; var args = Array("config/server.properties","canshu1","canshu2") echo(args.slice(1, args.length): _*)&#125; 输出是canshu1canshu2 kafkaServerStartable封装了KafkaServer 1.具体的启动类在：KafkaServerStartable的startup方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124def startup() &#123; try &#123; info("starting") if(isShuttingDown.get) throw new IllegalStateException("Kafka server is still shutting down, cannot re-start!") if(startupComplete.get) return val canStartup = isStartingUp.compareAndSet(false, true) if (canStartup) &#123; brokerState.newState(Starting) /* start scheduler */ kafkaScheduler.startup() /* setup zookeeper */ zkUtils = initZk() /* Get or create cluster_id */ _clusterId = getOrGenerateClusterId(zkUtils) info(s"Cluster ID = $clusterId") /* generate brokerId */ config.brokerId = getBrokerId this.logIdent = "[Kafka Server " + config.brokerId + "], " /* create and configure metrics */ val reporters = config.getConfiguredInstances(KafkaConfig.MetricReporterClassesProp, classOf[MetricsReporter], Map[String, AnyRef](KafkaConfig.BrokerIdProp -&gt; (config.brokerId.toString)).asJava) reporters.add(new JmxReporter(jmxPrefix)) val metricConfig = KafkaServer.metricConfig(config) metrics = new Metrics(metricConfig, reporters, time, true) quotaManagers = QuotaFactory.instantiate(config, metrics, time) notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala) /* start log manager */ logManager = createLogManager(zkUtils.zkClient, brokerState) logManager.startup() metadataCache = new MetadataCache(config.brokerId) credentialProvider = new CredentialProvider(config.saslEnabledMechanisms) socketServer = new SocketServer(config, metrics, time, credentialProvider) socketServer.startup() /* start replica manager */ replicaManager = new ReplicaManager(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower) replicaManager.startup() /* start kafka controller */ kafkaController = new KafkaController(config, zkUtils, brokerState, time, metrics, threadNamePrefix) kafkaController.startup() adminManager = new AdminManager(config, metrics, metadataCache, zkUtils) /* start group coordinator */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM) groupCoordinator.startup() /* Get the authorizer and initialize it if one is specified.*/ authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt; val authZ = CoreUtils.createObject[Authorizer](authorizerClassName) authZ.configure(config.originals()) authZ &#125; /* start processing requests */ apis = new KafkaApis(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator, kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers, clusterId, time) requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, time, config.numIoThreads) Mx4jLoader.maybeLoad() /* start dynamic config manager */ dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers), ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers), ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider), ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers)) // Create the config manager. start listening to notifications dynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers) dynamicConfigManager.startup() /* tell everyone we are alive */ val listeners = config.advertisedListeners.map &#123; endpoint =&gt; if (endpoint.port == 0) endpoint.copy(port = socketServer.boundPort(endpoint.listenerName)) else endpoint &#125; kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack, config.interBrokerProtocolVersion) kafkaHealthcheck.startup() // Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it checkpointBrokerId(config.brokerId) /* register broker metrics */ registerStats() brokerState.newState(RunningAsBroker) shutdownLatch = new CountDownLatch(1) startupComplete.set(true) isStartingUp.set(false) AppInfoParser.registerAppInfo(jmxPrefix, config.brokerId.toString) info("started") &#125; &#125; catch &#123; case e: Throwable =&gt; fatal("Fatal error during KafkaServer startup. Prepare to shutdown", e) isStartingUp.set(false) shutdown() throw e &#125; &#125; Starting 继承BrokerStates，BrokerStates是一个sealed trait1234567sealed trait BrokerStates &#123; def state: Byte &#125;case object NotRunning extends BrokerStates &#123; val state: Byte = 0 &#125;case object Starting extends BrokerStates &#123; val state: Byte = 1 &#125;case object RecoveringFromUncleanShutdown extends BrokerStates &#123; val state: Byte = 2 &#125;case object RunningAsBroker extends BrokerStates &#123; val state: Byte = 3 &#125;case object PendingControlledShutdown extends BrokerStates &#123; val state: Byte = 6 &#125;case object BrokerShuttingDown extends BrokerStates &#123; val state: Byte = 7 &#125; trait定义为sealed 有两层含义1.其修饰的trait class只能在当前文件里面被继承2.用sealed修饰这样做的目的是告诉scala编译器在检查模式匹配的时候，让scala知道这些case的所有情况，scala就能够在编译的时候进行检查，看你写的代码是否有没有漏掉什么没case到，减少编程的错误。 2.start scheduler:1kafkaScheduler.startup() 12345678910111213141516171819202122class KafkaScheduler(val threads: Int, val threadNamePrefix: String = "kafka-scheduler-", daemon: Boolean = true) extends Scheduler with Logging &#123; private var executor: ScheduledThreadPoolExecutor = null private val schedulerThreadId = new AtomicInteger(0) override def startup() &#123; debug("Initializing task scheduler.") this synchronized &#123; if(isStarted) throw new IllegalStateException("This scheduler has already been started!") executor = new ScheduledThreadPoolExecutor(threads) executor.setContinueExistingPeriodicTasksAfterShutdownPolicy(false) executor.setExecuteExistingDelayedTasksAfterShutdownPolicy(false) executor.setThreadFactory(new ThreadFactory() &#123; def newThread(runnable: Runnable): Thread = Utils.newThread(threadNamePrefix + schedulerThreadId.getAndIncrement(), runnable, daemon) &#125;) &#125; &#125; ...&#125; 1.此处使用了同步锁，如果已经启动，直接抛个IllegalStateException异常，由外面通用异常Throwable捕获。2.根据配置文件的background.threads 创建一个ScheduledThreadPoolExecutor(threads)【java.util.concurrent包下的】 3.setup zookeeper1zkUtils = initZk() 123456789101112131415161718192021222324252627282930313233private def initZk(): ZkUtils = &#123; info(s"Connecting to zookeeper on $&#123;config.zkConnect&#125;") val chrootIndex = config.zkConnect.indexOf("/") val chrootOption = &#123; if (chrootIndex &gt; 0) Some(config.zkConnect.substring(chrootIndex)) else None &#125; val secureAclsEnabled = config.zkEnableSecureAcls val isZkSecurityEnabled = JaasUtils.isZkSecurityEnabled() if (secureAclsEnabled &amp;&amp; !isZkSecurityEnabled) throw new java.lang.SecurityException(s"$&#123;KafkaConfig.ZkEnableSecureAclsProp&#125; is true, but the verification of the JAAS login file failed.") chrootOption.foreach &#123; chroot =&gt; val zkConnForChrootCreation = config.zkConnect.substring(0, chrootIndex) val zkClientForChrootCreation = ZkUtils(zkConnForChrootCreation, sessionTimeout = config.zkSessionTimeoutMs, connectionTimeout = config.zkConnectionTimeoutMs, secureAclsEnabled) zkClientForChrootCreation.makeSurePersistentPathExists(chroot) info(s"Created zookeeper path $chroot") zkClientForChrootCreation.zkClient.close() &#125; val zkUtils = ZkUtils(config.zkConnect, sessionTimeout = config.zkSessionTimeoutMs, connectionTimeout = config.zkConnectionTimeoutMs, secureAclsEnabled) zkUtils.setupCommonPaths() zkUtils &#125; 1.创建连接123456789val persistentZkPaths = Seq(ConsumersPath, BrokerIdsPath, BrokerTopicsPath, ConfigChangesPath, getEntityConfigRootPath(ConfigType.Topic), getEntityConfigRootPath(ConfigType.Client), DeleteTopicsPath, BrokerSequenceIdPath, IsrChangeNotificationPath) 2.设置通用路径/consumers/brokers/ids/brokers/topics/config/changes/config/topics/config/clients/admin/delete_topics/brokers/seqid/isr_change_notification ISR：Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas） set，这个set里的所有replica都跟上了leader，只有ISR里的成员才有被选为leader的可能 4.Get or create cluster_id12_clusterId = getOrGenerateClusterId(zkUtils)info(s"Cluster ID = $clusterId") 1234567891011121314151617181920212223242526 def getOrGenerateClusterId(zkUtils: ZkUtils): String = &#123; zkUtils.getClusterId.getOrElse(zkUtils.createOrGetClusterId(CoreUtils.generateUuidAsBase64)) &#125; def createOrGetClusterId(proposedClusterId: String): String = &#123; try &#123; createPersistentPath(ClusterIdPath, ClusterId.toJson(proposedClusterId)) proposedClusterId &#125; catch &#123; case _: ZkNodeExistsException =&gt; getClusterId.getOrElse(throw new KafkaException("Failed to get cluster id from Zookeeper. This can only happen if /cluster/id is deleted from Zookeeper.")) &#125; &#125;/** * Create an persistent node with the given path and data. Create parents if necessary. */ def createPersistentPath(path: String, data: String = "", acls: java.util.List[ACL] = UseDefaultAcls): Unit = &#123; val acl = if (acls eq UseDefaultAcls) ZkUtils.defaultAcls(isSecure, path) else acls try &#123; ZkPath.createPersistent(zkClient, path, data, acl) &#125; catch &#123; case _: ZkNoNodeException =&gt; createParentPath(path) ZkPath.createPersistent(zkClient, path, data, acl) &#125; &#125; 此处会创建一个persistent节点/cluster/id 如果节点已经存在，则刨除异常，上次获取异常，然后去节点下获取_clusterId，如果不存在，将创建的proposedClusterId返回 此处需要注意zookeeper的节点类型分为：持久节点（PERSISTENT）持久顺序节点（PERSISTENT_SEQUENTIAL）临时节点（EPHEMERAL）临时顺序节点（EPHEMERAL_SEQUENTIAL）顺序节点即创建有序的节点，节点名加上一个数字后缀。临时节点和客户端绑定，会话失效（非连接断开）则自动清楚 临时顺序节点可用来实现分布式锁1.客户端调用create()方法创建名为“locknode/guid-lock-”的节点，需要注意的是，这里节点的创建类型需要设置为EPHEMERAL_SEQUENTIAL。2.客户端调用getChildren(“locknode”)方法来获取所有已经创建的子节点，注意，这里不注册任何Watcher。3.客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点序号最小，那么就认为这个客户端获得了锁。4.如果在步骤3中发现自己并非所有子节点中最小的，说明自己还没有获取到锁。此时客户端需要找到比自己小的那个节点，然后对其调用exist()方法，同时注册事件监听。5.之后当这个被关注的节点被移除了，客户端会收到相应的通知。这个时候客户端需要再次调用getChildren(“locknode”)方法来获取所有已经创建的子节点，确保自己确实是最小的节点了，然后进入步骤3。 5.generate brokerId12config.brokerId = getBrokerIdthis.logIdent = "[Kafka Server " + config.brokerId + "], " 略 6.create and configure metrics内部状态的监控模块12345678val reporters = config.getConfiguredInstances(KafkaConfig.MetricReporterClassesProp, classOf[MetricsReporter], Map[String, AnyRef](KafkaConfig.BrokerIdProp -&gt; (config.brokerId.toString)).asJava)reporters.add(new JmxReporter(jmxPrefix))val metricConfig = KafkaServer.metricConfig(config)metrics = new Metrics(metricConfig, reporters, time, true)quotaManagers = QuotaFactory.instantiate(config, metrics, time)notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala) 从配置参数metric.reporters 获取监控类 此处有个小概念;集合允许使用asScala和asJava方法来做scala和java之间的转换 metricConfig里封装了metrics.num.samples（ 用于维护metrics的样本数）metrics.recording.levelmetrics.sample.window.ms（metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口推出后，我们会擦除并重写最老的窗口） 小概念：4种操作符的区别和联系:: 该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。用法为 x::list,其中x为加入到头部的元素，无论x是列表与否，它都只将成为新生成列表的第一个元素，也就是说新生成的列表长度为list的长度＋1(BTW ， x::list等价于list.::(x)):+和+: 两者的区别在于:+方法用于在尾部追加元素，+:方法用于在头部追加元素，和::很类似，但是::可以用于pattern match ，而+:则不行. 关于+:和:+,只要记住冒号永远靠近集合类型就OK了。++ 该方法用于连接两个集合，list1++list2::: 该方法只能用于连接两个List类型的集合 7.start log manager12345678logManager = createLogManager(zkUtils.zkClient, brokerState)logManager.startup()metadataCache = new MetadataCache(config.brokerId)credentialProvider = new CredentialProvider(config.saslEnabledMechanisms)socketServer = new SocketServer(config, metrics, time, credentialProvider)socketServer.startup() 根据一系列配置参数，启动LogManager。详情见kafka.log.CleanerConfig和kafka.log.LogManager startup创建了4个线程，分别负责创建日志，写日志，检索日志，清理日志1234567891011121314151617181920212223242526272829def startup() &#123; /* Schedule the cleanup task to delete old logs */ if(scheduler != null) &#123; info("Starting log cleanup with a period of %d ms.".format(retentionCheckMs)) scheduler.schedule("kafka-log-retention", cleanupLogs, delay = InitialTaskDelayMs, period = retentionCheckMs, TimeUnit.MILLISECONDS) info("Starting log flusher with a default period of %d ms.".format(flushCheckMs)) scheduler.schedule("kafka-log-flusher", flushDirtyLogs, delay = InitialTaskDelayMs, period = flushCheckMs, TimeUnit.MILLISECONDS) scheduler.schedule("kafka-recovery-point-checkpoint", checkpointRecoveryPointOffsets, delay = InitialTaskDelayMs, period = flushCheckpointMs, TimeUnit.MILLISECONDS) scheduler.schedule("kafka-delete-logs", deleteLogs, delay = InitialTaskDelayMs, period = defaultConfig.fileDeleteDelayMs, TimeUnit.MILLISECONDS) &#125; if(cleanerConfig.enableCleaner) cleaner.startup() &#125; 8.start replica manager123replicaManager = new ReplicaManager(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower)replicaManager.startup() 启动isr-expiration线程启动isr-change-propagation线程在/controller下建了一个监听 此处有个小技巧12345678def inLock[T](lock: Lock)(fun: =&gt; T): T = &#123; lock.lock() try &#123; fun &#125; finally &#123; lock.unlock() &#125; &#125; : =&gt;注意:后面要有空格，此处标明调用的时候才执行，否则在用inlock的函数时候fun已经在锁外面执行了. 参考：http://www.jianshu.com/p/f53e0b54a44a 9.start kafka controller1234kafkaController = new KafkaController(config, zkUtils, brokerState, time, metrics, threadNamePrefix)kafkaController.startup()adminManager = new AdminManager(config, metrics, metadataCache, zkUtils) 123456789def startup() = &#123; inLock(controllerContext.controllerLock) &#123; info("Controller starting up") registerSessionExpirationListener() isRunning = true controllerElector.startup info("Controller startup complete") &#125;&#125; 分支1.registerSessionExpirationListener-&gt;SessionExpirationListener-&gt;handleNewSession 当会话超时，重新连接上的时候，调用之前注册在ZookeeperLeaderElector的onControllerResignation函数controllerElector.elect 重新选举 分支2.ZookeeperLeaderElector-&gt;（onControllerFailover，onControllerResignation）-&gt;LeaderChangeListenercontrollerElector就是ZookeeperLeaderElector 是kafka的选举机制ZookeeperLeaderElector：通过zk创建Ephemeral Node（临时节点）的方式来进行选举，即如果存在并发情况下向zk的同一个路径创建node的话，有且只有1个客户端会创建成功，其它客户端创建失败，但是当创建成功的客户端和zk的链接断开之后，这个node也会消失，其它的客户端从而继续竞争 123456def startup &#123; inLock(controllerContext.controllerLock) &#123; controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener) elect &#125; &#125; 1.监听electionPath（/controller）2.elect选举 1234val ControllerPath = "/controller"val electString = Json.encode(Map("version" -&gt; 1, "brokerid" -&gt; brokerId, "timestamp" -&gt; timestamp))val zkCheckedEphemeral = new ZKCheckedEphemeral(electionPath,lectString, controllerContext.zkUtils.zkConnection.getZookeeper, JaasUtils.isZkSecurityEnabled())leaderId = getControllerID 此处会在/controller 下面写一个类似如下的内容：{“version”:1,”brokerid”:102,”timestamp”:”1495880001272”}通过getControllerID获取当前的leaderId然后通过amILeader看自己是否是leader ZookeeperLeaderElecto12345678910111213141516171819202122232425262728293031323334353637383940class LeaderChangeListener extends IZkDataListener with Logging &#123; /** * Called when the leader information stored in zookeeper has changed. Record the new leader in memory * @throws Exception On any error. */ @throws[Exception] def handleDataChange(dataPath: String, data: Object) &#123; val shouldResign = inLock(controllerContext.controllerLock) &#123; val amILeaderBeforeDataChange = amILeader leaderId = KafkaController.parseControllerId(data.toString) info("New leader is %d".format(leaderId)) // The old leader needs to resign leadership if it is no longer the leader amILeaderBeforeDataChange &amp;&amp; !amILeader &#125; if (shouldResign) onResigningAsLeader() &#125; /** * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader * @throws Exception * On any error. */ @throws[Exception] def handleDataDeleted(dataPath: String) &#123; val shouldResign = inLock(controllerContext.controllerLock) &#123; debug("%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader" .format(brokerId, dataPath)) amILeader &#125; if (shouldResign) onResigningAsLeader() inLock(controllerContext.controllerLock) &#123; elect &#125; &#125; &#125; 如果节点下线会调用handleDataDeleted。看自己是否是leader，如果是需要先退休onResigningAsLeader。然后选举123456789101112131415161718192021222324try &#123; val zkCheckedEphemeral = new ZKCheckedEphemeral(electionPath, electString, controllerContext.zkUtils.zkConnection.getZookeeper, JaasUtils.isZkSecurityEnabled()) zkCheckedEphemeral.create() info(brokerId + " successfully elected as leader") leaderId = brokerId onBecomingLeader() &#125; catch &#123; case _: ZkNodeExistsException =&gt; // If someone else has written the path, then leaderId = getControllerID if (leaderId != -1) debug("Broker %d was elected as leader instead of broker %d".format(leaderId, brokerId)) else warn("A leader has been elected but just resigned, this will result in another round of election") case e2: Throwable =&gt; error("Error while electing or becoming leader on broker %d".format(brokerId), e2) resign() &#125; amILeader 创建临时节点 onControllerFailover:12345678910111213141516171819202122232425262728293031323334353637383940def onControllerFailover() &#123; if(isRunning) &#123; info("Broker %d starting become controller state transition".format(config.brokerId)) readControllerEpochFromZookeeper() incrementControllerEpoch(zkUtils.zkClient) // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks registerReassignedPartitionsListener() registerIsrChangeNotificationListener() registerPreferredReplicaElectionListener() partitionStateMachine.registerListeners() replicaStateMachine.registerListeners() initializeControllerContext() // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and // partitionStateMachine.startup(). sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq) replicaStateMachine.startup() partitionStateMachine.startup() // register the partition change listeners for all existing topics on failover controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic)) info("Broker %d is ready to serve as the new controller with epoch %d".format(config.brokerId, epoch)) maybeTriggerPartitionReassignment() maybeTriggerPreferredReplicaElection() if (config.autoLeaderRebalanceEnable) &#123; info("starting the partition rebalance scheduler") autoRebalanceScheduler.startup() autoRebalanceScheduler.schedule("partition-rebalance-thread", checkAndTriggerPartitionRebalance, 5, config.leaderImbalanceCheckIntervalSeconds.toLong, TimeUnit.SECONDS) &#125; deleteTopicManager.start() &#125; else info("Controller has been shut down, aborting startup/failover") &#125; 在/admin/reassign_partitions目录注册PartitionsReassignedListener监听函数在/isr_change_notification目录注册IsrChangeNotificationListener监听函数在/admin/preferred_replica_election目录注册PreferredReplicaElectionListener监听函数在/brokers/topics目录注册TopicChangeListener监听函数在/admin/delete_topics目录注册DeleteTopicsListener监听函数在/brokers/ids目录注册BrokerChangeListener监听函数 监听是调用zk的zkUtils.zkClient.subscribeChildChanges函数，参数是路径和监听函数监听函数实现IZkChildListener接口实现handleChildChange方法 初始化ControllerContext上下文,里面包含存活的broker，所有主题，分区副本，分区的leader和已经下线的broker。更新leader和isr缓存。启动ControllerChannelManager初始化所有的replica状态初始化所有的partition状态如果auto.leader.rebalance.enable 为true会启动Rebalance调度最后删除主题 通过replicaStateMachine初始化所有的replica状态replicaStateMachine的handleStateChanges12345678910111213def handleStateChanges(replicas: Set[PartitionAndReplica], targetState: ReplicaState, callbacks: Callbacks = (new CallbackBuilder).build) &#123; if(replicas.nonEmpty) &#123; info("Invoking state change to %s for replicas %s".format(targetState, replicas.mkString(","))) try &#123; brokerRequestBatch.newBatch() replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks)) brokerRequestBatch.sendRequestsToBrokers(controller.epoch) &#125;catch &#123; case e: Throwable =&gt; error("Error while moving some replicas to %s state".format(targetState), e) &#125; &#125; &#125; 通过partitionStateMachine初始化所有的partition状态1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private def handleStateChange(topic: String, partition: Int, targetState: PartitionState, leaderSelector: PartitionLeaderSelector, callbacks: Callbacks) &#123; val topicAndPartition = TopicAndPartition(topic, partition) if (!hasStarted.get) throw new StateChangeFailedException(("Controller %d epoch %d initiated state change for partition %s to %s failed because " + "the partition state machine has not started") .format(controllerId, controller.epoch, topicAndPartition, targetState)) val currState = partitionState.getOrElseUpdate(topicAndPartition, NonExistentPartition) try &#123; targetState match &#123; case NewPartition =&gt; // pre: partition did not exist before this assertValidPreviousStates(topicAndPartition, List(NonExistentPartition), NewPartition) partitionState.put(topicAndPartition, NewPartition) val assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(",") stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, assignedReplicas)) // post: partition has been assigned replicas case OnlinePartition =&gt; assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OnlinePartition) partitionState(topicAndPartition) match &#123; case NewPartition =&gt; // initialize leader and isr path for new partition initializeLeaderAndIsrForPartition(topicAndPartition) case OfflinePartition =&gt; electLeaderForPartition(topic, partition, leaderSelector) case OnlinePartition =&gt; // invoked when the leader needs to be re-elected electLeaderForPartition(topic, partition, leaderSelector) case _ =&gt; // should never come here since illegal previous states are checked above &#125; partitionState.put(topicAndPartition, OnlinePartition) val leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader stateChangeLogger.trace("Controller %d epoch %d changed partition %s from %s to %s with leader %d" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader)) // post: partition has a leader case OfflinePartition =&gt; // pre: partition should be in New or Online state assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OfflinePartition) // should be called when the leader for a partition is no longer alive stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState)) partitionState.put(topicAndPartition, OfflinePartition) // post: partition has no alive leader case NonExistentPartition =&gt; // pre: partition should be in Offline state assertValidPreviousStates(topicAndPartition, List(OfflinePartition), NonExistentPartition) stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState)) partitionState.put(topicAndPartition, NonExistentPartition) // post: partition state is deleted from all brokers and zookeeper &#125; &#125; catch &#123; case t: Throwable =&gt; stateChangeLogger.error("Controller %d epoch %d initiated state change for partition %s from %s to %s failed" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState), t) &#125; &#125; PartitionStateMachine实现了topic的分区状态切换功能，Partition存在的状态如下：NewPartition 分区之前不存在，创建后被分配了replicas，但是还没有leader/isrOnlinePartition partition在replicas中选举某个成为leader之后OfflinePartition partition的replicas中的leader下线之后，没有重新选举新的leader之前 或 partition创建之后直接被下线NonExistentPartition partition重来没有被创建 或 partition创建之后被删除 scala小知识：mkString1234567891011121314151617scala &gt; val a = Array("apple", "banana", "cherry")a: Array[String] = Array(apple, banana, cherry)scala &gt; a.mkString(",")res2: String = apple,banana,cherryscala&gt; a.mkString("[", ", ", "]")res3: String = [apple, banana, cherry]如果是数组需要先展开数组scala&gt; val b = Array(Array("a", "b"), Array("c", "d"))b: Array[Array[String]] = Array(Array(a, b), Array(c, d))错误的scala&gt; b.mkString(",")res4: String = [Ljava.lang.String;@64a9fca7,[Ljava.lang.String;@22f756c5正确的scala&gt; b.flatten.mkString(",")res5: String = a,b,c,d OnlinePartition ：检查前置状态是否为NewPartition, OnlinePartition, OfflinePartition中的一种，1.如果是NewPartition：1234567891011121314151617181920212223242526272829303132333435363738394041424344private def initializeLeaderAndIsrForPartition(topicAndPartition: TopicAndPartition) &#123; val replicaAssignment = controllerContext.partitionReplicaAssignment(topicAndPartition) val liveAssignedReplicas = replicaAssignment.filter(r =&gt; controllerContext.liveBrokerIds.contains(r)) liveAssignedReplicas.size match &#123; case 0 =&gt; val failMsg = ("encountered error during state change of partition %s from New to Online, assigned replicas are [%s], " + "live brokers are [%s]. No assigned replica is alive.") .format(topicAndPartition, replicaAssignment.mkString(","), controllerContext.liveBrokerIds) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) case _ =&gt; debug("Live assigned replicas for partition %s are: [%s]".format(topicAndPartition, liveAssignedReplicas)) // make the first replica in the list of assigned replicas, the leader //根据partitionReplicaAssignment中信息选择第一个live的replica为leader,其余为isr val leader = liveAssignedReplicas.head val leaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(new LeaderAndIsr(leader, liveAssignedReplicas.toList), controller.epoch) debug("Initializing leader and isr for partition %s to %s".format(topicAndPartition, leaderIsrAndControllerEpoch)) try &#123; //将leader和isr持久化到zookeeper zkUtils.createPersistentPath( getTopicPartitionLeaderAndIsrPath(topicAndPartition.topic, topicAndPartition.partition), zkUtils.leaderAndIsrZkData(leaderIsrAndControllerEpoch.leaderAndIsr, controller.epoch)) // NOTE: the above write can fail only if the current controller lost its zk session and the new controller // took over and initialized this partition. This can happen if the current controller went into a long // GC pause //更新controllerContext中的partitionLeadershipInfo controllerContext.partitionLeadershipInfo.put(topicAndPartition, leaderIsrAndControllerEpoch) //封装发送给这些replica所在的broker的LeaderAndIsrRequest请求，交由ControllerBrokerRequestBatch(brokerRequestBatch)处理 brokerRequestBatch.addLeaderAndIsrRequestForBrokers(liveAssignedReplicas, topicAndPartition.topic, topicAndPartition.partition, leaderIsrAndControllerEpoch, replicaAssignment) &#125; catch &#123; case _: ZkNodeExistsException =&gt; // read the controller epoch val leaderIsrAndEpoch = ReplicationUtils.getLeaderIsrAndEpochForPartition(zkUtils, topicAndPartition.topic, topicAndPartition.partition).get val failMsg = ("encountered error while changing partition %s's state from New to Online since LeaderAndIsr path already " + "exists with value %s and controller epoch %d") .format(topicAndPartition, leaderIsrAndEpoch.leaderAndIsr.toString(), leaderIsrAndEpoch.controllerEpoch) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) &#125; &#125; &#125; 2.如果是OfflinePartition，OnlinePartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071def electLeaderForPartition(topic: String, partition: Int, leaderSelector: PartitionLeaderSelector) &#123; val topicAndPartition = TopicAndPartition(topic, partition) // handle leader election for the partitions whose leader is no longer alive stateChangeLogger.trace("Controller %d epoch %d started leader election for partition %s" .format(controllerId, controller.epoch, topicAndPartition)) try &#123; var zookeeperPathUpdateSucceeded: Boolean = false var newLeaderAndIsr: LeaderAndIsr = null var replicasForThisPartition: Seq[Int] = Seq.empty[Int] while(!zookeeperPathUpdateSucceeded) &#123; val currentLeaderIsrAndEpoch = getLeaderIsrAndEpochOrThrowException(topic, partition) val currentLeaderAndIsr = currentLeaderIsrAndEpoch.leaderAndIsr val controllerEpoch = currentLeaderIsrAndEpoch.controllerEpoch if (controllerEpoch &gt; controller.epoch) &#123; val failMsg = ("aborted leader election for partition [%s,%d] since the LeaderAndIsr path was " + "already written by another controller. This probably means that the current controller %d went through " + "a soft failure and another controller was elected with epoch %d.") .format(topic, partition, controllerId, controllerEpoch) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) &#125; // elect new leader or throw exception val (leaderAndIsr, replicas) = leaderSelector.selectLeader(topicAndPartition, currentLeaderAndIsr) val (updateSucceeded, newVersion) = ReplicationUtils.updateLeaderAndIsr(zkUtils, topic, partition, leaderAndIsr, controller.epoch, currentLeaderAndIsr.zkVersion) //根据不同的leaderSelector选举新的leader，这里一般调用的是OfflinePartitionLeaderSelector newLeaderAndIsr = leaderAndIsr newLeaderAndIsr.zkVersion = newVersion //将leader和isr持久化到zookeeper zookeeperPathUpdateSucceeded = updateSucceeded replicasForThisPartition = replicas &#125; val newLeaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(newLeaderAndIsr, controller.epoch) // update the leader cache //更新controllerContext中的partitionLeadershipInfo controllerContext.partitionLeadershipInfo.put(TopicAndPartition(topic, partition), newLeaderIsrAndControllerEpoch) stateChangeLogger.trace("Controller %d epoch %d elected leader %d for Offline partition %s" .format(controllerId, controller.epoch, newLeaderAndIsr.leader, topicAndPartition)) val replicas = controllerContext.partitionReplicaAssignment(TopicAndPartition(topic, partition)) // store new leader and isr info in cache //封装发送给这些replica所在的broker的LeaderAndIsrRequest请求，交由ControllerBrokerRequestBatch(brokerRequestBatch)处理 brokerRequestBatch.addLeaderAndIsrRequestForBrokers(replicasForThisPartition, topic, partition, newLeaderIsrAndControllerEpoch, replicas) &#125; catch &#123; case _: LeaderElectionNotNeededException =&gt; // swallow case nroe: NoReplicaOnlineException =&gt; throw nroe case sce: Throwable =&gt; val failMsg = "encountered error while electing leader for partition %s due to: %s.".format(topicAndPartition, sce.getMessage) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg, sce) &#125; debug("After leader election, leader cache is updated to %s".format(controllerContext.partitionLeadershipInfo.map(l =&gt; (l._1, l._2)))) &#125;···在brokers/topics/***(具体的topic名字)/目录下注册PartitionModificationsListener-&gt;AddPartitionsListener监听通过处理之前启动留下的partition重分配的情况处理之前启动留下的replica重新选举的情况向其它KafkaServer发送集群topic的元数据信息已进行数据的同步更新根据配置是否开启自动均衡开始删除topic### 10.start group coordinator```scala// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issuegroupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM)groupCoordinator.startup() 11.Get the authorizer and initialize it if one is specified.12345authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt; val authZ = CoreUtils.createObject[Authorizer](authorizerClassName) authZ.configure(config.originals()) authZ&#125; 12.start processing requests1234567apis = new KafkaApis(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator, kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers, clusterId, time)requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, time, config.numIoThreads)Mx4jLoader.maybeLoad()&#125; 13.start dynamic config manager123456 dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers),ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers),ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider),ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers))// Create the config manager. start listening to notificationsdynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers)dynamicConfigManager.startup()&#125; 14.tell everyone we are alive12345678910111213val listeners = config.advertisedListeners.map &#123; endpoint =&gt; if (endpoint.port == 0) endpoint.copy(port = socketServer.boundPort(endpoint.listenerName)) else endpoint&#125;kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack, config.interBrokerProtocolVersion)kafkaHealthcheck.startup()// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint itcheckpointBrokerId(config.brokerId)&#125; kafkaHealthcheck.startup()12345678910111213141516171819202122232425def startup() &#123; zkUtils.zkClient.subscribeStateChanges(sessionExpireListener) register() &#125; /** * Register this broker as "alive" in zookeeper */ def register() &#123; val jmxPort = System.getProperty("com.sun.management.jmxremote.port", "-1").toInt val updatedEndpoints = advertisedEndpoints.map(endpoint =&gt; if (endpoint.host == null || endpoint.host.trim.isEmpty) endpoint.copy(host = InetAddress.getLocalHost.getCanonicalHostName) else endpoint ) // the default host and port are here for compatibility with older clients that only support PLAINTEXT // we choose the first plaintext port, if there is one // or we register an empty endpoint, which means that older clients will not be able to connect val plaintextEndpoint = updatedEndpoints.find(_.securityProtocol == SecurityProtocol.PLAINTEXT).getOrElse( new EndPoint(null, -1, null, null)) zkUtils.registerBrokerInZk(brokerId, plaintextEndpoint.host, plaintextEndpoint.port, updatedEndpoints, jmxPort, rack, interBrokerProtocolVersion) &#125; 注册新的brokerid 15.register broker metrics.12 registerStats()&#125;]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka源码阅读]]></title>
    <url>%2F2017%2F05%2F02%2Fkafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[下载kafka的源码包：http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz 安装gradle 1.官网下载https://gradle.org/releases选择当时最新版本：https://downloads.gradle.org/distributions/gradle-3.5-all.zip 2.解压解压到D:\tool\gradle-3.5 3.配置环境变量GRADLE_HOMED:\tool\gradle-3.5 Path追加;%GRADLE_HOME%\BIN;4.测试：gradle -v 安装参考:http://blog.csdn.net/lizhitao/article/details/26875463 编译：（此处博主用某云主机编译的，很快）gradle idea （编译完把/root/.gradle/caches/modules-2下下载的文件放到我们的环境中C:\Users\Administrator.gradle\caches\modules-2 再windows上编译 ） 用IDEA工具打开 目录介绍： 模块名 含义 admin 管理员模块，操作和管理topic，paritions相关，包含create/delete topic,扩展patitions api 主要负责交互数据的组装，客户端与服务端交互数据编解码 client Producer读取kafka broker元数据信息，topic和partitions，以及leader cluster 包含几个实体类，Broker,Cluster,Partition,Replica,解释他们之间关系： Cluster由多个broker组成，一个Broker包含多个partition，一个topic的所有partitions分布在不同broker的中，一个Replica包含多个Partition。 common 异常类和错误验证 consumer 负责所有客户端消费者数据和逻辑处理 controller 负责中央控制器选举，partition的leader选举，副本分配，副本重新分配，partition和replica扩容。 coordinator partition分配机制 javaapi 提供java的producer和consumer接口api log Kafka文件存储模块，负责读写所有kafka的topic消息数据。 message 封装多个消息组成一个“消息集”或压缩消息集。 metrics 内部状态的监控模块 network 网络事件处理模块，负责处理和接收客户端连接 producer producer实现模块，包括同步和异步发送消息。 security 安全 serializer 序列化或反序列化当前消息 server tools 工具模块， 包含a.导出对应consumer的offset值.b.导出LogSegments信息，当前topic的log写的位置信息.c.导出zk上所有consumer的offset值.d.修改注册在zk的consumer的offset值.f.producer和consumer的使用例子. utils Json工具类，Zkutils工具类，Utils创建线程工具类，KafkaScheduler公共调度器类，公共日志类等等。]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH oozie]]></title>
    <url>%2F2017%2F04%2F18%2FCDH%20oozie%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[注：本次oozie是安装到自定义的集群上，因为其他节点都是用CDH手动包安装的，不是CDH自动一键式安装方式 下载CDH的oozie ，已经编译好，无需编译本次安装选择了下面网址的CDH 5.7.0 版本CDHhttps://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball_57.html#concept_0vc_ddn_yk Linux机器上执行如下命令下载：wget https://archive.cloudera.com/cdh5/cdh/5/oozie-4.1.0-cdh5.7.0.tar.gz 解压后将如下内容添加到core-site.xml12345678910vi $HADOOP_HOME/etc/hadoop/core-site.xml &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;hadoop000,hadoop001,hadoop002&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 注意proxyuser后面那个root是使用oozie的用户，hadoop000,hadoop001,hadoop002是集群的主机 将oozie添加到环境变量12export OOIZE_HOME=/home/app/oozie-4.1.0-cdh5.7.0export PATH=$PATH:$OOIZE_HOME/bin:$PATH 添加以下内容到oozie配置文件，$OOIZE_HOME/conf/oozie-site.xml12345678910111213141516171819202122232425262728293031323334&lt;property&gt; &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt; &lt;value&gt;*=/root/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop&lt;/value&gt; &lt;description&gt; Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of the Hadoop service (JobTracker, HDFS). The wildcard &apos;*&apos; configuration is used when there is no exact match for an authority. The HADOOP_CONF_DIR contains the relevant Hadoop *-site.xml files. If the path is relative is looked within the Oozie configuration directory; though the path can be absolute (i.e. to point to Hadoop client conf/ directories in the local filesystem. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.driver&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.url&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/oozie?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.username&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.password&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.processing.timezone&lt;/name&gt; &lt;value&gt;GMT+0800&lt;/value&gt;&lt;/property&gt; 在解压oozie二进制发行包的目录中，解压hadooplibs发行包，也就是oozie-hadooplibs-4.1.0-cdh5.7.0.tar.gz[hadoop@h71 oozie-4.1.0-cdh5.5.2]$ tar -zxvf oozie-hadooplibs-4.1.0-cdh5.5.2.tar.gz 在oozie的解压目录下创建libext目录。并将hadooplibs下的jar包拷贝到这个目录里，需要注意的是hadooplibs目录下有个文件夹hadooplib-2.6.0-cdh5.7.0.oozie-4.1.0-cdh5.7.0 hadooplib-2.6.0-mr1-cdh5.7.0.oozie-4.1.0-cdh5.7.0；后者对应于mapreduce1，所以我们拷贝第一个文件夹下的jar包即可。上传ext文件到hadooplibs上传mysql的驱动文件到hadooplibs具体上传什么版本的ext可以看oozie-setup.sh脚本里指定的ext-后面的版本号 将hadooplibs里面的文件拷贝到libext 1234567891011121314151617[root@hadoop000 libext]# ls /home/app/oozie-4.1.0-cdh5.7.0/libextactivation-1.1.jar commons-digester-1.8.jar hadoop-aws-2.6.0-cdh5.7.0.jar jackson-annotations-2.2.3.jar netty-all-4.0.23.Final.jarapacheds-i18n-2.0.0-M15.jar commons-httpclient-3.1.jar hadoop-client-2.6.0-cdh5.7.0.jar jackson-core-2.2.3.jar paranamer-2.3.jarapacheds-kerberos-codec-2.0.0-M15.jar commons-io-2.4.jar hadoop-common-2.6.0-cdh5.7.0.jar jackson-core-asl-1.8.8.jar protobuf-java-2.5.0.jarapi-asn1-api-1.0.0-M20.jar commons-lang-2.4.jar hadoop-hdfs-2.6.0-cdh5.7.0.jar jackson-databind-2.2.3.jar servlet-api-2.5.jarapi-util-1.0.0-M20.jar commons-logging-1.1.jar hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar jackson-jaxrs-1.8.8.jar slf4j-api-1.7.5.jaravro-1.7.6-cdh5.7.0.jar commons-math3-3.1.1.jar hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar jackson-mapper-asl-1.8.8.jar slf4j-log4j12-1.7.5.jaraws-java-sdk-core-1.10.6.jar commons-net-3.1.jar hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar jackson-xc-1.8.8.jar snappy-java-1.0.4.1.jaraws-java-sdk-kms-1.10.6.jar curator-client-2.7.1.jar hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar jaxb-api-2.2.2.jar stax-api-1.0-2.jaraws-java-sdk-s3-1.10.6.jar curator-framework-2.7.1.jar hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar jersey-client-1.9.jar xercesImpl-2.10.0.jarcommons-beanutils-1.7.0.jar curator-recipes-2.7.1.jar hadoop-yarn-api-2.6.0-cdh5.7.0.jar jersey-core-1.9.jar xml-apis-1.4.01.jarcommons-beanutils-core-1.8.0.jar ext-2.2 hadoop-yarn-client-2.6.0-cdh5.7.0.jar jetty-util-6.1.26.cloudera.2.jar xmlenc-0.52.jarcommons-cli-1.2.jar ext-2.2.zip hadoop-yarn-common-2.6.0-cdh5.7.0.jar jsr305-3.0.0.jar xz-1.0.jarcommons-codec-1.4.jar gson-2.2.4.jar hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar leveldbjni-all-1.8.jar zookeeper-3.4.5-cdh5.7.0.jarcommons-collections-3.2.2.jar guava-11.0.2.jar htrace-core4-4.0.1-incubating.jar log4j-1.2.17.jarcommons-compress-1.4.1.jar hadoop-annotations-2.6.0-cdh5.7.0.jar httpclient-4.2.5.jar mysql-connector-java-5.1.33-bin.jarcommons-configuration-1.6.jar hadoop-auth-2.6.0-cdh5.7.0.jar httpcore-4.2.5.jar netty-3.6.2.Final.jar 使用命令创建数据库bin/oozie-setup.sh db create -run oozie.sql 编译war包 bin/oozie-setup.sh prepare-war 在$oozie_home下，有2个sharelib压缩包，分别是 oozie-sharelib-4.1.0-cdh5.7.0.tar.gz 和oozie-sharelib-4.1.0-cdh5.7.0-yarn.tar.gz 很明显，我们必须拷贝第二个带yarn的压缩包（前边的是1.0版本的，不带yarn的） bin/oozie-setup.sh sharelib create -fs hdfs://hadoop000:9000 -locallib oozie-sharelib-4.1.0-cdh5.7.0-yarn.tar.gz 重启hadoop 启动oozie bin/oozied.sh start启动后，访问网址http://hadoop000:11000/oozie/ 测试OOZIE 解压tar -zxvf oozie-examples.tar.gz进入cd examples/apps 以map-reduce目录为例子1234567891011121314151617181920212223242526[root@hadoop000 map-reduce]# vi job.properties ## Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#nameNode=hdfs://hadoop000:9000jobTracker=hadoop000:8032queueName=defaultexamplesRoot=examplesoozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/apps/map-reduce/workflow.xmloutputDir=map-reduce 修改workflow.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@hadoop000 map-reduce]# vi workflow.xml &lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.2&quot; name=&quot;map-reduce-wf&quot;&gt; &lt;start to=&quot;mr-node&quot;/&gt; &lt;action name=&quot;mr-node&quot;&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&quot;/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/ 上传文件hadoop fs -put examples/ /user/root 启动 [root@hadoop000 oozie-4.1.0-cdh5.7.0]# bin/oozie job -oozie http://hadoop000:11000/oozie -config examples/apps/map-reduce/job.properties -runjob: 0000000-180830181747338-oozie-root-W 杀掉jobbin/oozie job -oozie http://hadoop000:11000/oozie -kill 0000000-180830181747338-oozie-root-W 如果启动后发现时间不对，需要修改时间vi /home/app/oozie-4.1.0-cdh5.7.0/src/webapp/src/main/webapp/oozie-console.js function getTimeZone() {Ext.state.Manager.setProvider(new Ext.state.CookieProvider());return Ext.state.Manager.get(“TimezoneId”,”GMT+0800”);} Oozie重启步骤： 1.进入oozie目录 2.将Oozie停止运行./bin/oozie-stop.sh 3.重新进行打包war包 ./bin/oozie-setup.sh prepare-war 4.重新打开Oozie：./bin/oozie-start.sh 另外在修改web显示的时间 oozie添加告警配置oozie-site.xml文件 oozie.email.smtp.host smtp.163.com oozie.email.from.address test@163.com oozie.email.smtp.auth true oozie.email.smtp.username test@163.com oozie.email.smtp.password 123456 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.2&quot; name=&quot;map-reduce-wf&quot;&gt; &lt;start to=&quot;mr-node&quot;/&gt; &lt;action name=&quot;mr-node&quot;&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&quot;/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to=&quot;goemail&quot; /&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt;&lt;action name=&quot;goemail&quot;&gt; &lt;email xmlns=&quot;uri:oozie:email-action:0.1&quot;&gt; &lt;to&gt;5lovezhm@163.com&lt;/to&gt; &lt;subject&gt;Email notifications for $&#123;wf:id()&#125;&lt;/subject&gt; &lt;body&gt;The wf $&#123;wf:id()&#125; successfully completed.&lt;/body&gt; &lt;/email&gt; &lt;ok to =&quot;end&quot;/&gt; &lt;error to =&quot;fail&quot;/&gt; &lt;/action&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; 如果报错： JA006: Call From hadoop000/xxx.xx.xx.xx to hadoop000:10020 failed on connection exception: java.net.ConnectException: ????; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused 启动history$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH phoenix安装]]></title>
    <url>%2F2017%2F04%2F18%2FCDH%20phoenix%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[首先下载jdk 略安装maven 下载yum源wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo 安装maven：yum -y install apache-maven 编译phoenix找到最新版本的phoenixhttps://github.com/chiastic-security/phoenix-for-cloudera/tree/4.8-HBase-1.2-cdh5.8 下载（博主下载到/soft下）并编译mvn clean package -DskipTests -Dcdh.flume.version=1.6.0 打开路径：/soft/phoenix-for-cloudera-4.8-HBase-1.2-cdh5.8/phoenix-assembly/target 找到phoenix-4.8.0-cdh5.8.0.tar.gz将phoenix-4.8.0-cdh5.8.0中的phoenix-4.8.0-cdh5.8.0-server.jar拷贝到每一个RegionServer下/opt/cloudera/parcels/CDH/lib/hbase/lib 启动：./sqlline.py slave1:2181/hbase 如果报错：Error: org.apache.hadoop.hbase.DoNotRetryIOException: Class org.apache.phoenix.coprocessor.MetaDataEndpointImpl cannot be loaded Set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks at org.apache.hadoop.hbase.master.HMaster.warnOrThrowExceptionForFailure(HMaster.java:1741) at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1602) at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1531) at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:469) at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55682) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:185) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:165) (state=08000,code=101) 需要点击CDH-&gt;hbase -&gt;配置-&gt;高级hbase-site.xml 的 HBase 服务高级配置代码段（安全阀）添加hbase.table.sanity.checksfalse 重启即可如果想要安装phonenix先下载http://squirrel-sql.sourceforge.net/#installation 安装（安装的时候勾选imort-data和mysql）注：下载后直接安装jar包即可，不要解压缩由于是CDH 我拷贝了如下jar防盗lib下面phoenix-core-4.8.0-cdh5.8.0.jarphoenix-4.8.0-cdh5.8.0-client.jarphoenix-pherf-4.8.0-cdh5.8.0-minimal.jar 链接参考;http://www.cnblogs.com/raphael5200/p/5260198.html有图 phoenix API http://phoenix.apache.org/language/functions.html http://phoenix.apache.org/language/index.html]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH-spark]]></title>
    <url>%2F2017%2F03%2F29%2FCDH-spark%2F</url>
    <content type="text"><![CDATA[添加服务选择spark on yarn安装的时候会选择history server，博主选择的是slave1，后续可以通过这个地址看spark的UI 博主安装的是：scala version 2.10.5（后续再UI上看到的）spark version 1.6.0（执行spark-shell时看到的） 安装完后测试： su hdfsspark-submit –class org.apache.spark.examples.SparkPi –executor-memory 1G –total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100 查看UI：http://slave1:18088/ 编写代码测试离线功能： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.zwjf.spark&lt;/groupId&gt; &lt;artifactId&gt;SpakrZwujf&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.10.5&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-make:transitive&lt;/arg&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;cn.zwjf.uidPhone&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 1234567891011121314151617181920212223package cn.zwjfimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/29. */object uidPhone &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName("uid_phone_etl") val sc = new SparkContext(conf) sc.textFile(args(0)).map(line =&gt; &#123; val arr = line.split(",") if(arr.length &gt; 1 &amp;&amp; !arr(1).equals("null"))&#123; arr(0)+","+arr(1) &#125;else&#123; "null" &#125; &#125;).filter(!_.equals("null")).saveAsTextFile(args(1)) sc.stop() &#125;&#125; 打包提交到服务器 spark-submit \–class cn.zwjf.uidPhone –executor-memory 2G –total-executor-cores 4 \/var/lib/hadoop-hdfs/data/spark/SpakrZwujf-1.0-SNAPSHOT.jar \/bigdata/data/hdfs/Baidu/uid_phone/* \/bigdata/data/hdfs/Baidu/uid_phone3 /bigdata/data/hdfs/Baidu/uid_phone/* /bigdata/data/hdfs/Baidu/uid_phone2 本地调试方法：、http://www.jianshu.com/p/c801761ce088]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[strom 监控]]></title>
    <url>%2F2017%2F03%2F26%2Fstrom-%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[看当前GC的情况jstat -gcutil 端口号 1000 看wait的是否过高top -p 端口号 把某个端口当前的堆栈信息dump到某个文件jstack 端口号 &gt; 文件 看磁盘iostat -x 1 如果发现分配不均匀，例如FiledGrouping userid=1特别多，导致下游nolt或execute搞死了，重新启动也很快就死掉。可以对userid 做hash或md5或者组成更多条件，让下游Bolt分布更均匀 complete latency(这里主要针对spout)是一个tuple从发出, 到经过bolt处理完成, 最终调用spout的ack这个完整的过程所花的时间.complete latency和吞吐量可以认为是互斥的, 二者不可兼得, 要提高吞吐量, 必然会增加complete latency, 反之亦然. 所以我们需要根据具体的场景, 在二者之间找到一个平衡.complete latency受到两个因素的影响:bolt的处理时间spout的parallelism数量 官网对并行度等的设置方式：storm.apache.org/releases/1.0.3/Understanding-the-parallelism-of-a-Storm-topology.html 如果executors总是挂就需要关注]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[strom-Metrics]]></title>
    <url>%2F2017%2F03%2F25%2Fstrom-Metrics%2F</url>
    <content type="text"><![CDATA[Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。使用 Metrics 只需要实现一个接口方法：getValueAndReset，在方法中可以查找汇总值、并将该值复位为初始值。例如，在 MeanReducer 中就实现了通过运行总数除以对应的运行计数的方式来求取均值，然后将两个值都重新设置为 0。 Storm 提供了以下几种 metric 类型： AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。CombinedMetric – 可以对 metric 进行关联更新的通用接口。CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。MultiCountMetric – 返回包含一组 CountMetric 的 HashMapReducedMetricMeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap 自定义Metric：代码注册：conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);或者修改配置文件topology.metrics.consumer.register: class: “org.apache.storm.metric.LoggingMetricsConsumer”parallelism.hint: 1 class: “org.apache.storm.metric.HttpForwardingMetricsConsumer”parallelism.hint: 1argument: “http://example.com:8080/metrics/my-topology/“ 构建自己的Metric定义不可被序列号类型transientprivate transient CountMetric countMetric; 重写prepare@Overridepublic void prepare(Map conf, TopologyContext context, OutputCollector collector) { // other intialization here. countMetric = new CountMetric(); context.registerMetric(“execute_count”, countMetric, 60);} bolt的execute public void execute(Tuple input) { countMetric.incr(); // handle tuple here.} builtin_metrics.clj 为内部的 metrics 设置了数据结构，以及其他框架组件可以用于更新的虚拟方法。metrics 本身是在回调代码中实现计算的 – 请参考 clj/b/s/daemon/daemon/executor.clj 中的 ack-spout-msg 的例子。 LoggingMetricsConsumer,统计指标值将输出到metric.log日志文件中。]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bash脚本实战]]></title>
    <url>%2F2017%2F03%2F19%2Fbash%E8%84%9A%E6%9C%AC%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[练习题目写一个脚本getinterface.sh，脚本可以接受参数(i,I,a)，完成以下任务： (1)使用以下形式：getinterface.sh [-i interface|-I IP|-a] (2)当用户使用-i选项时，显示其指定网卡的IP地址； (3)当用户使用-I选项时，显示其后面的IP地址所属的网络接口；（如 192.168.199.183：eth0） (4)当用户单独使用-a选项时，显示所有网络接口及其IP地址（lo除外） 分析参数1为param1 参数2位param2（1）利用CAT &lt;&lt; EOF * EOF打印信息12345678cat &lt;&lt; EOF getinterface.sh [-i interface|-I IP|-a] -i interface) show ip of the interface -I IP) show interface of the IP and IP with :; -a) list all interfaces and their IPs except lo; *) quit=================================================================EOF （2）校验接口是否存在（参数2）1234ifconfig $interface &gt;/dev/null $? -ne 1 #如果存在打印网卡的IPifconfig $parma2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' （3）先打印所有接口1netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125; 并存在一个变量中 1list=`netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125;'` 遍历集合，查看和输入IP相同的打印IP和接口12345678for inter in $list; do ip_temp=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' if [ $IP == $ip_temp ];then echo "$IP : $inter" exit 0 fi done （4）遍历集合，过滤接口为lo的，打印IP和接口12345678for inter in $list; do ip_temp=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' if [ $inter != "lo" ];then echo "$IP : $inter" exit 0 fi done 最终的脚本如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344cat &lt;&lt; EOF getinterface.sh [-i interface|-I IP|-a] -i interface) show ip of the interface -I IP) show interface of the IP and IP with :; -a) list all interfaces and their IPs except lo; *) quit=================================================================EOFread -p "please choice " param1 param2list=`netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125;'`if [[ "$param1" == '-i' ]]; then ifconfig $interface &gt;/dev/null flag=$? if [ $flag -ne 1 ];then ip=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;'` echo "$param2 $ip" else echo "$parma2 is not exist" fielif [[ "$param1" == '-I' ]]; thenfor lt in $list; do ip =$(ifconfig $lt | awk -F" " '/inet.*netmask/&#123;print $2&#125;') #ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;') if [ $param2 == $ip ];then echo "$lt : $ip" fi doneelif [[ "$param1" == '-a' ]]; then for lt in $list; do ip=$(ifconfig $lt | awk -F" " '/inet.*netmask/&#123;print $2&#125;') #ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;') if [ $lt != "lo" ];then echo "$lt : $ip" exit 0 fi doneelse echo "quit" exit 0fi 注意如上内容根据你个人机器显示情况而定，如果是显示 inet addr:127.0.0.1 Mask:255.0.0.0 就要把正则改成 ip=$(ifconfig $lt | awk -F” “ ‘/inet.*Mask/{print $2}’ | awk -F: ‘{print $2}’)]]></content>
      <categories>
        <category>bash</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH hadoop实战]]></title>
    <url>%2F2017%2F03%2F14%2FCDH%20hadoop%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[解决问题：因为数据手机号一项包含为NULL的数据，需要清洗。 创建工程，添加如下依赖123456789101112131415161718192021222324&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写mapper类1234567891011121314151617181920212223242526package map;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * Created by Administrator on 2017/3/14. */public class UidPhoneMapper extends Mapper&lt;Text, Text, Text, Text&gt; &#123; @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; String arr[] = key.toString().split(","); if (arr.length != 2) &#123; return; &#125; if(arr[1] == null)&#123; return; &#125; context.write(new Text(arr[0]), new Text(arr[1])); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package main;import map.UidPhoneMapper;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.GenericOptionsParser;import java.io.IOException;import org.apache.hadoop.fs.Path;/** * Created by Administrator on 2017/3/14. */public class UidPhoneDropNull &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); GenericOptionsParser parser = new GenericOptionsParser(conf, args); String[] otherArgs = parser.getRemainingArgs(); if (args.length != 2) &#123; System.err.println("Usage: NewlyJoin &lt;inpath&gt; &lt;output&gt;"); System.exit(2); &#125; Job job = new Job(conf, "UidPhoneDropNull"); // 设置运行的job job.setJarByClass(UidPhoneDropNull.class); // 设置Map相关内容 job.setMapperClass(UidPhoneMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setInputFormatClass(KeyValueTextInputFormat.class); //设置文件输入格式 job.setNumReduceTasks(0); //设置Reduce个数为0 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); // 设置输入和输出的目录 FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); // 执行，直到结束就退出 System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 运行上面的程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[hdfs@slave3 jar]$ hadoop jar bigdataMR.jar main.UidPhoneDropNull /bigdata/data/hdfs/BD/UP/* /bigdata/data/hdfs/BD/UP2 17/03/14 17:37:54 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:803217/03/14 17:37:55 INFO input.FileInputFormat: Total input paths to process : 317/03/14 17:37:55 INFO mapreduce.JobSubmitter: number of splits:317/03/14 17:37:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_004417/03/14 17:37:55 INFO impl.YarnClientImpl: Submitted application application_1488970234114_004417/03/14 17:37:55 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0044/17/03/14 17:37:55 INFO mapreduce.Job: Running job: job_1488970234114_004417/03/14 17:38:00 INFO mapreduce.Job: Job job_1488970234114_0044 running in uber mode : false17/03/14 17:38:00 INFO mapreduce.Job: map 0% reduce 0%17/03/14 17:38:04 INFO mapreduce.Job: map 33% reduce 0%17/03/14 17:38:05 INFO mapreduce.Job: map 100% reduce 0%17/03/14 17:38:05 INFO mapreduce.Job: Job job_1488970234114_0044 completed successfully17/03/14 17:38:05 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=378748 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=13514456 HDFS: Number of bytes written=10824901 HDFS: Number of read operations=15 HDFS: Number of large read operations=0 HDFS: Number of write operations=6 Job Counters Launched map tasks=3 Data-local map tasks=3 Total time spent by all maps in occupied slots (ms)=8240 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=8240 Total vcore-seconds taken by all map tasks=8240 Total megabyte-seconds taken by all map tasks=8437760 Map-Reduce Framework Map input records=661590 Map output records=485478 Input split bytes=393 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=175 CPU time spent (ms)=3730 Physical memory (bytes) snapshot=712597504 Virtual memory (bytes) snapshot=8316772352 Total committed heap usage (bytes)=814743552 File Input Format Counters Bytes Read=13514063 File Output Format Counters Bytes Written=10824901[hdfs@slave3 jar]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP2Found 4 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7609438 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00000-rw-r--r-- 3 hdfs supergroup 3215441 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00001-rw-r--r-- 3 hdfs supergroup 22 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00002 最后将数据导入到hivehive&gt; create EXTERNAL table IF NOT EXISTS UP2 (uid STRING,phone STRING) row format delimited fields terminated by ‘,’ location ‘/bigdata/data/hdfs/BD/UP2/‘;OKTime taken: 0.026 secondshive&gt; select count(1) from UP2;Query ID = hdfs_20170314174141_1b60d560-2036-44e6-ab65-19f17efc5b1bTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=In order to limit the maximum number of reducers: set hive.exec.reducers.max=In order to set a constant number of reducers: set mapreduce.job.reduces=Starting Job = job_1488970234114_0045, Tracking URL = http://master2:8088/proxy/application_1488970234114_0045/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0045Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 17:41:33,200 Stage-1 map = 0%, reduce = 0%2017-03-14 17:41:38,371 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.54 sec2017-03-14 17:41:43,486 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.38 secMapReduce Total cumulative CPU time: 4 seconds 380 msecEnded Job = job_1488970234114_0045MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 4.38 sec HDFS Read: 10831709 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 380 msecOK485478Time taken: 16.267 seconds, Fetched: 1 row(s) 比清洗前数据661590少了176112条]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH hive 实战]]></title>
    <url>%2F2017%2F03%2F14%2FCDH%20hive%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[创建数据库123456hive&gt; create database BD;OKTime taken: 0.161 secondshive&gt; use BD;OKTime taken: 0.013 seconds 创建表12345678hive&gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by ',' ;OKTime taken: 0.211 secondshive&gt; load data inpath '/bigdata/data/hdfs/shuju/SO/part-m-00000' into table UP;Loading data to table BD.UPTable BD.UP stats: [numFiles=1, totalSize=898611236]OKTime taken: 0.317 seconds 查询表数据1234567891011121314151617181920212223242526hive&gt; select count(1) from UP;Query ID = hdfs_20170314111111_d29152a3-56b3-42f9-b17b-1ddaf7451117Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0025, Tracking URL = http://master2:8088/proxy/application_1488970234114_0025/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0025Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 12017-03-14 11:11:49,267 Stage-1 map = 0%, reduce = 0%2017-03-14 11:11:55,519 Stage-1 map = 25%, reduce = 0%, Cumulative CPU 3.31 sec2017-03-14 11:11:56,542 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 16.57 sec2017-03-14 11:12:01,655 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 18.41 secMapReduce Total cumulative CPU time: 18 seconds 410 msecEnded Job = job_1488970234114_0025MapReduce Jobs Launched: Stage-Stage-1: Map: 4 Reduce: 1 Cumulative CPU: 18.41 sec HDFS Read: 899021344 HDFS Write: 8 SUCCESSTotal MapReduce CPU Time Spent: 18 seconds 410 msecOK5674200Time taken: 19.429 seconds, Fetched: 1 row(s) 删除表：123hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.088 seconds 查看hdfs上的数据，发现数据被删除123[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/SOFound 1 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-13 19:16 /bigdata/data/hdfs/shuju/SO/_SUCCESS 测试2：首先查询，发现biaoming数据存在1234[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/biaomingFound 2 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS-rw-r--r-- 3 hdfs supergroup 13514063 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/part-m-00000 创建外部表：123hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' ;OKTime taken: 0.033 seconds 加载数据12345hive&gt; load data inpath '/bigdata/data/hdfs/shuju/biaoming/part-m-00000' into table UP;Loading data to table BD.UPTable BD.UP stats: [numFiles=1, totalSize=13514063]OKTime taken: 0.201 seconds 查询数据：12345678910111213141516171819202122232425hive&gt; select count(1) from UP;Query ID = hdfs_20170314115959_6004ef81-913b-4d38-99dc-1199cc6e73f2Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0027, Tracking URL = http://master2:8088/proxy/application_1488970234114_0027/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0027Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 11:59:09,331 Stage-1 map = 0%, reduce = 0%2017-03-14 11:59:14,463 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.06 sec2017-03-14 11:59:19,587 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.9 secMapReduce Total cumulative CPU time: 4 seconds 900 msecEnded Job = job_1488970234114_0027MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 4.9 sec HDFS Read: 13520683 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 900 msecOK661590Time taken: 17.707 seconds, Fetched: 1 row(s) 删除数据123hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.037 seconds 再查看元数据，发现依然被删除了123[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/biaomingFound 1 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS 测试3：重新上传数据：1sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver --connect 'jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=BD' --table=UP --target-dir /bigdata/data/hdfs/BD/UP --split-by uid -m 3 查看数据：1234567[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPFound 5 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000-rw-r--r-- 3 hdfs supergroup 2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001-rw-r--r-- 3 hdfs supergroup 3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002-rw-r--r-- 3 hdfs supergroup 22 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00003 创建外部表数据：123456789101112131415161718192021222324252627282930hive&gt; select count(1) from UP;FAILED: SemanticException [Error 10001]: Line 1:21 Table not found 'UP'hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' location '/bigdata/data/hdfs/BD/UP/';OKTime taken: 0.03 secondshive&gt; select count(1) from UP;Query ID = hdfs_20170314141111_5be2c701-ad6a-4717-8801-513f38d64928Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0031, Tracking URL = http://master2:8088/proxy/application_1488970234114_0031/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0031Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 14:11:48,608 Stage-1 map = 0%, reduce = 0%2017-03-14 14:11:54,743 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.26 sec2017-03-14 14:11:58,833 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 5.08 secMapReduce Total cumulative CPU time: 5 seconds 80 msecEnded Job = job_1488970234114_0031MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 5.08 sec HDFS Read: 13520952 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 5 seconds 80 msecOK661590Time taken: 16.253 seconds, Fetched: 1 row(s) 此时再删除表后数据还存在。123456789hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.037 seconds[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPFound 5 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000-rw-r--r-- 3 hdfs supergroup 2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001-rw-r--r-- 3 hdfs supergroup 3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002 创建普通表：123hive&gt; create table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' location '/bigdata/data/hdfs/BD/UP/';OKTime taken: 0.029 seconds 删除表后数据不存在12345hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.046 seconds[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPls: `/bigdata/data/hdfs/BD/UP': No such file or directory 总结用load方式无论外部表还是内部表数据都会删除，用location方式，外部表不会删除数据，内部表会删除数据]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH sqoop1 实战-带条件的导入]]></title>
    <url>%2F2017%2F03%2F13%2FCDH%20sqoop1%20%E5%AE%9E%E6%88%98-%E5%B8%A6%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[导入[hdfs@slave3 lib]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query “SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1” –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id –hive-import –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.017/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‘$CONDITIONS’ in WHERE clause. at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332) at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858) at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) 修改为：sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’ –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id –hive-import –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming 在执行，发现报错说我database不存在，并在目录下生成了一个metastore_db，原来是执行sqoop的机器不对，换了一台机器执行没有问题，很奇怪明明是分布式的为什么会出现这个问题呢 最终的具体日志： [hdfs@master1 root]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’ –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id –hive-import –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming –hive-database shujuWarning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.17/03/15 14:32:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.017/03/15 14:32:53 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/15 14:32:53 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/15 14:32:53 INFO tool.CodeGenTool: Beginning code generation17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/15 14:33:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.jar17/03/15 14:33:00 INFO mapreduce.ImportJobBase: Beginning query import.17/03/15 14:33:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/15 14:33:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/15 14:33:01 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:803217/03/15 14:33:02 INFO db.DBInputFormat: Using read commited transaction isolation17/03/15 14:33:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM (SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 1) ) AS t117/03/15 14:33:03 INFO db.IntegerSplitter: Split size: 7508318; Num splits: 4 from: 20079 to: 3005335417/03/15 14:33:03 INFO mapreduce.JobSubmitter: number of splits:417/03/15 14:33:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_005717/03/15 14:33:03 INFO impl.YarnClientImpl: Submitted application application_1488970234114_005717/03/15 14:33:03 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0057/17/03/15 14:33:03 INFO mapreduce.Job: Running job: job_1488970234114_005717/03/15 14:33:08 INFO mapreduce.Job: Job job_1488970234114_0057 running in uber mode : false17/03/15 14:33:08 INFO mapreduce.Job: map 0% reduce 0%17/03/15 14:33:15 INFO mapreduce.Job: map 75% reduce 0%17/03/15 14:33:16 INFO mapreduce.Job: map 100% reduce 0%17/03/15 14:33:16 INFO mapreduce.Job: Job job_1488970234114_0057 completed successfully17/03/15 14:33:16 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=608160 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=428 HDFS: Number of bytes written=17645206 HDFS: Number of read operations=16 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 Job Counters Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=17968 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=17968 Total vcore-seconds taken by all map tasks=17968 Total megabyte-seconds taken by all map tasks=18399232 Map-Reduce Framework Map input records=283659 Map output records=283659 Input split bytes=428 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=268 CPU time spent (ms)=12960 Physical memory (bytes) snapshot=1151381504 Virtual memory (bytes) snapshot=11157348352 Total committed heap usage (bytes)=1045954560 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=1764520617/03/15 14:33:16 INFO mapreduce.ImportJobBase: Transferred 16.8278 MB in 15.7965 seconds (1.0653 MB/sec)17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Retrieved 283659 records.17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:33:16 WARN hive.TableDefWriter: Column update_time had to be cast to a less precise type in Hive17/03/15 14:33:16 INFO hive.HiveImport: Loading uploaded data into Hive Logging initialized using configuration in jar:file:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.propertiesOKTime taken: 1.874 secondsLoading data to table shuju.card_recordTable shuju.card_record stats: [numFiles=4, totalSize=17645206]OKTime taken: 0.367 seconds]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH sqoop1 实战]]></title>
    <url>%2F2017%2F03%2F13%2FCDH%20sqoop1%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[下载sqlserver的jdbc驱动包https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599解压将sqljdbc4.jar放在：/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/lib 下载SQL Server-Hadoop Connector：sqoop-sqlserver-1.0.tar.gz http://www.microsoft.com/en-us/download/details.aspx?id=27584 导入数据：[root@slave3 sqoop]# ./bin/sqoop import –connect ‘jdbc: server://10.105.32.246 username=sa password=123456 database=databaseName –table=tableName –target-dir /bigdata/data/hdfsWarning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/bin/../../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.+======================================================================+| Error: JAVA_HOME is not set |+———————————————————————-+| Please download the latest Sun JDK from the Sun Java web site || &gt; http://www.oracle.com/technetwork/java/javase/downloads || || HBase requires Java 1.7 or later. |+======================================================================+Error: JAVA_HOME is not set and could not be found. 报错解决方法：修改bin下的configure-sqoop 注释以下代码：123456789101112131415161718192021222324252627282930313233343536373839#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hive-hcatalog# elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hcatalog# else# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog# if [ ! -d $&#123;HCAT_HOME&#125; ]; then# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog# fi# fi#fi#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/accumulo&quot; ]; then# ACCUMULO_HOME=/usr/lib/accumulo# else# ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo# fi#fi#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hive-hcatalog# elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hcatalog# else# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog# if [ ! -d $&#123;HCAT_HOME&#125; ]; then# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog# fi# fi#fi#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/accumulo&quot; ]; then# ACCUMULO_HOME=/usr/lib/accumulo# else# ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo# fi#fi 在执行如果提示JAVA_HOME不存在，手动执行一下export JAVA_HOME=/usr/java/jdk1.8.0_121 再执行提示：ERROR tool.BaseSqoopTool: Error parsing arguments for import: ./bin/sqoop import –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’ –table=tableName –target-dir /bigdata/data/hdfs –split-by order_id –fields-terminated-by ‘\t’ –m 3 需要下载sqoop-sqlserver下载地址已经失效，在CSDN上找到：https://passport.csdn.net/account/login?from=http://download.csdn.net/download/nma_123456/9405343 解压进入目录执行：[root@slave3 sqoop-sqlserver-1.0]# export SQOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop[root@slave3 sqoop-sqlserver-1.0]# ./install.sh 再次执行：报如下错误Exception in thread “main” java.lang.NoClassDefFoundError: org/json/JSONObject at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:43) at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:767) at org.apache.sqoop.mapreduce.JobBase.putSqoopOptionsToConfiguration(JobBase.java:388) at org.apache.sqoop.mapreduce.JobBase.createJob(JobBase.java:374) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:256) at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236)Caused by: java.lang.ClassNotFoundException: org.json.JSONObject at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) 分析原因缺少org/json/JSONObject，上网下载一个json.jar地址：http://download.csdn.net/download/haixia_12/8462933 扔进去重新执行就OK了 最终命令：sqoop import –driver com.microsoft.sqlserver.jdbc.SQLServerDriver –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’ –table=tableName –target-dir /bigdata/data/hdfs/cards –split-by order_id -m 3 查看sqlserver上有什么数据： sqoop list-tables –driver com.microsoft.sqlserver.jdbc.SQLServerDriver –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’ sqlserver如果连接失败，要看是否开启了远程访问和TCP/UDP端口映射 报错17/03/13 17:20:01 ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.17/03/13 17:20:01 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf 增加：export HADOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoopexport HIVE_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive export HIVE_CONF_DIR=/etc/hive/confexport HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib/*]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH 安装]]></title>
    <url>%2F2017%2F03%2F08%2FCDH%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[运行环境： 主机IP 主机名 内存 1.1.1.147 po-master1 16G 1.1.1.127 po-master2 8G 1.1.1.118 po-slave1 8G 1.1.1.92 po-slave2 8G 1.1.1.230 po-slave3 8G 配置主机名(分别在五台机器上执行)vi /etc/sysconfig/networkhostname +主机名例如： hostname po-master1 配置映射关系(把以下五条命令在五台机器上执行)echo “1.1.1.147 po-master1”&gt;&gt;/etc/hostsecho “1.1.1.127 po-master2”&gt;&gt;/etc/hostsecho “1.1.1.118 po-slave1”&gt;&gt;/etc/hostsecho “1.1.1.92 po-slave2”&gt;&gt;/etc/hostsecho “1.1.1.230 po-slave3”&gt;&gt;/etc/hosts 安装JDK（在po-master1上执行）1.下载JDK安装包：jdk-8u102-linux-x64.tar.gz注：作者放到/soft/java具体位置可自行安排 安装： 123cd /soft/javamkdir jdk1.8.0_121rpm -ivh jdk-7u76-linux-x64.rpm --prefix=/soft/java 创建连接 1ln -s -f jdk1.8.0_121/ jdk 开放端口（五台机器上都需要配置）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEP/sbin/iptables -A INPUT -s 0.0.0.0/0 -p tcp --dport 22 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 22 -j ACCEPT/sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 1004 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50075 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 1006 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50070 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50470 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50495 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8485 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8480 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8032 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8030 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8031 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8033 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8088 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8040 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8042 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8041 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 10020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 13562 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 19888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60030 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8080 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8085 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9095 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9083 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 10000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 16000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 4181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8019 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 11000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 11001 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 14000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 14001 -j ACCEPT /etc/rc.d/init.d/iptables save 关闭端口详解-参考CDH官网https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_ports_cdh5.html 测试（可忽略）1/etc/init.d/iptables status 五台机器配置互相免秘钥登陆1.创建ssh文件如果已经创建不要覆盖cat ~/.ssh/id_rsa.pub &gt;&gt;~/.ssh/authorized_keys分别把五台机器的公钥加载到authorized_keys 2.vi /etc/ssh/sshd_config打开如下内容HostKey /etc/ssh/ssh_host_rsa_keyRSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys 3.重启/etc/init.d/sshd restart 4.测试sshssh po-master1ssh po-master2ssh po-slave1ssh po-slave2ssh po-slave3 向其他机器分发jdkscp -rp /soft/java/ root@po-master2:/soft/javascp -rp /soft/java/ root@po-salve1:/soft/javascp -rp /soft/java/ root@po-salve2:/soft/javascp -rp /soft/java/ root@po-salve3:/soft/java 配置环境变量(分别在五台机器上执行)执行如下命令：1234echo "export JAVA_HOME=/soft/java/jdk" &gt;&gt; /etc/profileecho "export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/" &gt;&gt; /etc/profileecho "export CLASSPATH=.:$JAVA_HOME/lib" &gt;&gt; /etc/profile. /etc/profile 测试（可忽略）1234[root@po-master1 java]# java -versionjava version "1.8.0_121"Java(TM) SE Runtime Environment (build 1.8.0_121-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) 配置NTP服务器和客户端（因为使用阿里云此处省略）配置mysql1.上传mysql文件（博主放到/soft/mysql目录下）2.解压cd /soft/mysqltar -zxvf mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz -C /usr/local3.将目录重命名cd /usr/localmv mysql-5.7.17-linux-glibc2.5-x86_64/ mysql4.创建data目录 mkdir /usr/local/mysql/data 5.安装插件（网上有人说不安装提示libiao错误，博主用阿里云libaio已经是最新版本，所以不用安装，也不知道没安装有什么坏处） yum install libaio 6.安装mysqlcd /usr/local/mysql/bin./mysql_install_db –user=root –basedir=/usr/local/mysql –datadir=//usr/local/mysql/data 1.官网下载yum源https://dev.mysql.com/downloads/repo/yum/2.安装yum源yum localinstall mysql57-community-release-el6-9.noarch.rpm3.安装mysqlyum install mysql-community-server 4.创建组和用户groupadd mysqluseradd mysql -g mysql 5.修改配置文件开启二进制日志vi /etc/my.cnf （在[mysqld]下面添加如下内容）server-id=1log-bin=/home/mysql/log/logbin.log 6.开启服务service mysqld start 7.查看mysql默认的密码grep ‘temporary password’ /var/log/mysqld.log 8.根据密码进入mysqlmysql -u root -pALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘MyNewPass4!’; 例如：ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘password’;Query OK, 0 rows affected (0.01 sec) 注：MySQL’s validate_password plugin is installed by default. This will require that passwords contain at least one upper case letter, one lower case letter, one digit, and one special character, and that the total password length is at least 8 characters. 9.授权（给其他四台机器授权）grant all privileges on oozie. to ‘oozie’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on oozie. to ‘oozie’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on oozie. to ‘oozie’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on oozie. to ‘oozie’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on oozie.* to ‘oozie’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; GRANT all privileges on . to ‘root’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‘root’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‘root’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‘root’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO ‘root’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; GRANT ALL PRIVILEGES ON . TO ‘root’@’182.48.105.23’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; grant all privileges on hive. to ‘hive’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on hive. to ‘hive’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on hive. to ‘hive’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on hive. to ‘hive’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;grant all privileges on hive.* to ‘hive’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; flush privileges; 关于新版本的账户说明：https://dev.mysql.com/doc/refman/5.7/en/adding-users.html 10.创建数据库create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; ##安装cloudera manager1.下载地址：http://archive-primary.cloudera.com/cm5/cm/5/（博主下载的是cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz 放在/soft/bigdata/clouderamanager下）cd /soft/bigdata/clouderamanagertar -xvf cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz 测试：（可选）cat /etc/passwd 创建用户（所有节点）useradd –system –home=/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server –no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm 测试（可选）[root@master1 cloudera-scm-server]# cat /etc/passwd….cloudera-scm:x:498:498:Cloudera SCM User:/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server:/bin/false 修改主机名和端口号cd /soft/bigdata/clouderamanager/cm-5.10.0/etc/cloudera-scm-agentvi config.ini Hostname of the CM server.server_host=po-master1 Port that the CM server is listening on.server_port=7182 下载驱动包下载mysql-connector-java-*.jar（博主下载的mysql-connector-java-5.1.7-bin.jar）放到/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib 目录下 为Cloudera Manager 5建立数据库：/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/schema/scm_prepare_database.sh mysql scm -hlocalhost -uroot -ppassword –scm-host localhost scm password scm格式是:scm_prepare_database.sh 数据库类型 数据库 服务器 用户名 密码 –scm-host Cloudera_Manager_Server所在的机器，后面那三个不知道代表什么，直接照抄官网的了。 开启Cloudera Manager 5 Server端： 注意scm见面是两个-，因为博客关系不能全部显示 向其他机器分发CDHscp -rp /soft/bigdata/clouderamanager root@po-master2:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave1:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave2:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave3:/soft/bigdata 准备Parcels，用以安装CDH5 （博主放在:/soft/bigdata/clouderamanager/cloudera/parcel-repo，路径必须包含cloudera/parcel-repo）官方地址：http://archive.cloudera.com/cdh5/parcels博主选择的http://archive.cloudera.com/cdh5/parcels/latest/ 需要下载以下两个文件• CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel• manifest.json 打开 manifest.json找到CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel的hash值里的内容“hash”: “52f95da433f203a05c2fd33eb0f144e6a5c9d558”echo ‘52f95da433f203a05c2fd33eb0f144e6a5c9d558’ &gt;&gt; CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha 测试（可选）[root@master1 parcel-repo]# lltotal 1466572-rw-r–r– 1 root root 1501694035 Mar 6 14:24 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel-rw-r–r– 1 root root 41 Mar 20 15:26 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha-rw-r–r– 1 root root 64807 Mar 17 17:07 manifest.json 启动/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start（主节点启动）/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start（所有节点上启动） 测试netstat -an | grep 7182netstat -an | grep 7180 登陆http://po-master1:7180默认用户密码都是admin 点击继续选择免费的点击继续 勾选机器 点击更多选项修改parcel路径/soft/bigdata/clouderamanager/cloudera/parcel-repo插入图5 需要重启所有节点的服务/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server restart（主节点启动）/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent restart（所有节点上启动） 选择如下内容点击继续 等待安装… 安装完成，点击继续 安装过程有个小提示：已启用透明大页面压缩，可能会导致重大性能问题。请运行“echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag”以禁用此设置，然后将同一命令添加到 /etc/rc.local 等初始脚本中，以便在系统重启时予以设置。以下主机将受到影响： 选择自定义服务，选择自己需要的服务 等待安装 安装过程中会遇到错误： 是缺少jdbc驱动把文件考入到lib下即可 配置NameNode HA进入HDFS界面，点击“启用High Availability”输入NameService名称，这里设置为：nameservice1，点击继续按钮。配置JourNode的路径，(博主修改为/opt/dfs/jn) 错误整理;Fatal error during KafkaServer startup. Prepare to shutdownkafka.common.InconsistentBrokerIdException: Configured broker.id 52 doesn’t match stored broker.id 102 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs). at kafka.server.KafkaServer.getBrokerId(KafkaServer.scala:648) at kafka.server.KafkaServer.startup(KafkaServer.scala:187) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:37) at kafka.Kafka$.main(Kafka.scala:67) at com.cloudera.kafka.wrap.Kafka$.main(Kafka.scala:76) at com.cloudera.kafka.wrap.Kafka.main(Kafka.scala) 进入到/var/local/kafka/data目录查看meta.propertie里面的kakfa 的broker id是什么 [main]: Metastore Thrift Server threw an exception…javax.jdo.JDOFatalInternalException: Error creating transactional connection factory at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)NestedThrowablesStackTrace:java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) at org.datanucleus.store.AbstractStoreManager.(AbstractStoreManager.java:240) at org.datanucleus.store.rdbms.RDBMSStoreManager.(RDBMSStoreManager.java:286) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the “BONECP” plugin to create a ConnectionPool gave an error : The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.(ConnectionFactoryImpl.java:85) … 54 moreCaused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58) at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) … 56 more 把驱动程序放在/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib SERVER[po-master1] E0103: Could not load service classes, Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’ at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)Caused by: org.apache.openjpa.persistence.PersistenceException: Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’ at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:102) at org.apache.openjpa.jdbc.conf.JDBCConfigurationImpl.getDBDictionaryInstance(JDBCConfigurationImpl.java:603) at org.apache.openjpa.jdbc.meta.MappingRepository.endConfiguration(MappingRepository.java:1518) at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:531) at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:456) at org.apache.openjpa.lib.conf.PluginValue.instantiate(PluginValue.java:120) at org.apache.openjpa.conf.MetaDataRepositoryValue.instantiate(MetaDataRepositoryValue.java:68) at org.apache.openjpa.lib.conf.ObjectValue.instantiate(ObjectValue.java:83) at org.apache.openjpa.conf.OpenJPAConfigurationImpl.newMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:967) at org.apache.openjpa.conf.OpenJPAConfigurationImpl.getMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:958) at org.apache.openjpa.kernel.AbstractBrokerFactory.makeReadOnly(AbstractBrokerFactory.java:644) at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:203) at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:156) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:227) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:154) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:60) at org.apache.oozie.service.JPAService.getEntityManager(JPAService.java:514) at org.apache.oozie.service.JPAService.init(JPAService.java:215) at org.apache.oozie.service.Services.setServiceInternal(Services.java:386) at org.apache.oozie.service.Services.setService(Services.java:372) at org.apache.oozie.service.Services.loadServices(Services.java:305) … 26 moreCaused by: org.apache.commons.dbcp.SQLNestedException: Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’ at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1429) at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1371) at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044) at org.apache.openjpa.lib.jdbc.DelegatingDataSource.getConnection(DelegatingDataSource.java:110) at org.apache.openjpa.lib.jdbc.DecoratingDataSource.getConnection(DecoratingDataSource.java:87) at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:91) … 46 moreCaused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680) at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526) at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1420) … 51 more 把mysql-connector-java.jar，mysql-connector-java-5.1.39.jar驱动程序放在：/var/lib/oozie [main]: Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidatesRequired table missing : “DBS“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables”org.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : “DBS“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables” at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:485) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3380) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:185) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.(MetaStoreDirectSql.java:136) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:340) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136) SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table &apos;oozie.VALIDATE_CONN&apos; doesn&apos;t exist) org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist) at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414) 报这个错误需要修改hive的配置。搜索autoCreateSchema 改为true SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist)org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist) at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414) 点击界面上的Oozie 点击操作，创建Oozie数据库表 最后导入环境变量就可以测试了export ZK_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/export HBASE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hbase/export HADOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/export HIVE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/export SQOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/export OOZIE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/oozie/export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$OOZIE_HOME/bin:$ZK_HOME/bin:$HBASE_HOME/bin 最后测试阶段，可忽略，本文完。测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）：1234[root@po-slave1 data]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: leader 1234[root@po-slave2 data]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower 1234[root@po-slave3 dfs]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower 12345678910111213141516171819202122232425262728293031323334[root@po-slave3 dfs]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower[root@po-slave3 dfs]# zkCli.sh Connecting to localhost:21812017-03-21 16:05:56,829 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.10.0--1, built on 01/20/2017 20:10 GMT2017-03-21 16:05:56,832 [myid:] - INFO [main:Environment@100] - Client environment:host.name=po-slave32017-03-21 16:05:56,832 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_1212017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/soft/java/jdk/jre2017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/classes:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../src/java/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf:.:/soft/java/jdk/lib2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd642017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.version=2.6.32-642.13.1.el6.x86_642017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.name=root2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/root2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/opt/dfs2017-03-21 16:05:56,836 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@506c589eWelcome to ZooKeeper!2017-03-21 16:05:56,873 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@975] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2017-03-21 16:05:56,935 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@852] - Socket connection established, initiating session, client: /127.0.0.1:42694, server: localhost/127.0.0.1:21812017-03-21 16:05:56,941 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15aeb7f0edb054c, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0] ls /[controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, config, hbase][zk: localhost:2181(CONNECTED) 1] 测试hdfs1234567891011121314[root@po-master1 ~]# hadoop dfs -ls /DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 3 itemsdrwxr-xr-x - hbase hbase 0 2017-03-21 10:30 /hbasedrwxrwxrwt - hdfs supergroup 0 2017-03-20 17:06 /tmpdrwxr-xr-x - hdfs supergroup 0 2017-03-20 17:06 /user[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -mkdir /data2[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -put hdfs-audit.log /data2/hdfs-audit.log[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -ls /data2Found 1 items-rw-r--r-- 3 hdfs supergroup 2908825 2017-03-21 17:28 /data2/hdfs-audit.log 测试网页 测试hadoop页面http://po-master1:50030/jobtracker.jsp 测试hive1234567891011121314151617[root@po-master1 ~]# hiveJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future releaseJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive&gt; show databases;OKdefaultTime taken: 1.836 seconds, Fetched: 1 row(s)hive&gt; create database test;OKTime taken: 0.06 secondshive&gt; drop database test;OKTime taken: 0.184 seconds 测试hbase12345678910111213141516[root@po-master1 ~]# hbase shellJava HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/21 16:20:23 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.availableHBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.Type "exit&lt;RETURN&gt;" to leave the HBase ShellVersion 1.2.0-cdh5.10.0, rUnknown, Fri Jan 20 12:13:18 PST 2017hbase(main):001:0&gt; listTABLE 0 row(s) in 0.2020 seconds=&gt; []hbase(main):002:0&gt; create 't1','id','name'0 row(s) in 2.3540 seconds=&gt; Hbase::Table - t1hbase(main):003:0&gt; listTABLE t1 1 row(s) in 0.0100 seconds=&gt; ["t1"]hbase(main):004:0&gt; 卸载安装：1umount /soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-agent/process 问题汇总：如果启动CDH后无法点击HDFS 的WEB UI，查看端口又是被监听是因为需要修改配置：![此处输入图片的描述][19] 同理job 的web UI也需要修改![此处输入图片的描述][20]]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH Kudu安装及介绍]]></title>
    <url>%2F2017%2F03%2F01%2FCDH%20Kudu%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[安装Kudu的要求1.操作系统和版本支持Cloudera。2.通过Cloudera Manager管理Kudu，要求Cloudera Manager5.4.3或更改的版本。CDH 5.4或更高版本的要求。推荐Cloudera Manager5.4.7，因为它增加了Kudu采集的指标支持。3.如果固态存储是可用的，在这种高性能的媒体存储Kudu WALs可以显著改善时的Kudu配置高耐用性。 通过Cloudera Manager安装Kudu想要通过Cloudera Manager安装Kudu，首先下载Kudu的定制服务描述符（CSD）文件http://archive.cloudera.com/kudu/csd/KUDU-5.10.0.jar?_ga=1.154711740.507225699.1488251539并上传到/opt/cloudera/csd/用以下操作系统命令重启Cloudera管理服务器。 1$ sudo service cloudera-scm-server restart 从http://archive.cloudera.com/kudu/parcels/网站找到相应的Kudu parcel， 并添加到 Parcel Settings &gt; 远程 Parcel 存储库 URL 接下来可以通过以下两种方式安装Using Parcels1.Hosts &gt; Parcels &gt; Kudu &gt; Download2.Locations &gt; Distribute &gt; Activate3.重启集群4.Actions &gt; Add a Service. &gt; Kudu (Beta) &gt; Continue5.选择一个主机作为master，一些主机作为tablet 服务角色。一个主机即可以是master又可以是tablet，但是对于大集群来说会造成性能问题。Kudu的master不是一个资源密集型的，被其他相似的处理例如：HDFS的namenode，YARN的ResourceManager收集。选择完主机点击Continue6.在masters 和 tablet配置Kudu的存储位置和预写日志（WAL）文件，Cloudera Manager将会创建文件夹。6.1 你可以使用相同的目录存储数据和WALs6.2 你不可以将WALs 目录设置为数据的子目录6.3 如果你的主机及是master又是tablet，配置不同的master和tablet服务，例如 /data/kudu/master and /data/kudu/tserver. 6.4 如果你选择的文件系统不支持打洞技术;服务启动失败6.4.1 退出配置向导，点击Cloudera Manager 接口上的Cloudera 图标6.4.2 到 Kudu (Beta) 服务6.4.3 Configuration &gt; Kudu (Beta) Service Advanced Configuration Snippet (Safety Valve) &gt; gflagfile5.4.4 添加如下内容并保存改变1--block_manager=file Note: The file block manager does not perform well at scale and should only be used for small-scale development and testing. 7.如果你不需要退出，点击Continue。Kudu的master和tablet服务已经启动。否则点击 Kudu (Beta) &gt; Actions &gt; Start.8.使用其中一种方法验证服务：8.1 通过ps命令验证一个或全部kudu-master或kudu-tserver程序是否运行8.2 通过打开Web浏览器中的URL访问master或者tablet。master的URL： http://:8051/tablet的URL： http://:8050/9.重启监控服务并检查Kudu的图表，到Cloudera Manager服务点击Service Monitor &gt; Actions &gt; Restart10.管理角色。Kudu (Beta) 服务 使用 Actions 来停止，启动，重启或者其他管理服务 Using PackagesKudu 仓库 和 Package 链接 Operating System Repository Package Individual Packages RHEL RHEL 6 RHEL 6 Ubuntu Trusty Trusty 1.1 下载Kudu的yum源文件到：RHEL（/etc/yum.repos.d/） 或者 Ubuntu（/etc/apt/sources.list.d/）1.2 如果你需要C++客户端开发库或Kudu的SDK，RHEL kudu-client-devel 包 或 Ubuntu的 libkuduclient0 and libkuduclient-dev 包1.3 如果你使用Cloudera Manager，不安装 kudu-master 和 kudu-tserver包，Cloudera Manager使用Kudu 提供操作系统启动脚本。 2.群集上安装Kudu服务。去你想安装Kudu所在的集群。单击 Actions &gt; Add a Service &gt; Kudu &gt; Continue。 3.选择一个主机作为master，一些主机作为tablet 服务角色。一个主机即可以是master又可以是tablet，但是对于大集群来说会造成性能问题。Kudu的master不是一个资源密集型的，被其他相似的处理例如：HDFS的namenode，YARN的ResourceManager收集。选择完主机点击Continue4.在masters 和 tablet配置Kudu的存储位置和预写日志（WAL）文件，Cloudera Manager将会创建文件夹。4.1 你可以使用相同的目录存储数据和WALs4.2 你不可以将WALs 目录设置为数据的子目录5.3 如果你的主机及是master又是tablet，配置不同的master和tablet服务，例如 /data/kudu/master and /data/kudu/tserver. 5 如果你选择的文件系统不支持打洞技术;服务启动失败点击 Continue &gt; Kudu masters 和 tablet 启动。否则到Kudu的服务上点击 Actions &gt; Start. 6.使用其中一种方法验证服务：6.1 通过ps命令验证一个或全部kudu-master或kudu-tserver程序是否运行6.2 通过打开Web浏览器中的URL访问master或者tablet。master的URL： http://:8051/tablet的URL： http://:8050/ 7.重启监控服务并检查Kudu的图表，到Cloudera Manager服务点击Service Monitor &gt; Actions &gt; Restart8.管理角色。Kudu 服务 使用 Actions 来停止，启动，重启或者其他管理服务 此乃官方推荐，方法1，博主亲试没有成功，安装完不显示服务 本文参考页面：https://www.cloudera.com/documentation/enterprise/latest/topics/kudu_install_cm.html]]></content>
      <categories>
        <category>cloudera</category>
      </categories>
      <tags>
        <tag>cloudera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canal分析binlog]]></title>
    <url>%2F2017%2F02%2F23%2Fcanal%E5%88%86%E6%9E%90binlog%2F</url>
    <content type="text"><![CDATA[环境介绍linux服务器：centos7.1mysql : 5.7.10canal : 1.0.23 一.centos7下安装mysql1.下载mysql的repo源1wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 2.安装mysql-community-release-el7-5.noarch.rpm包1rpm -ivh mysql-community-release-el7-5.noarch.rpm 3.安装mysql1yum install mysql-server 4.设置用户12345CREATE USER &apos;canal&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;localhost&apos; WITH GRANT OPTION;CREATE USER &apos;canal&apos;@&apos;%&apos; IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos; WITH GRANT OPTION;flush privileges; 注：canal的原理是模拟自己为mysql slave，所以这里一定需要做为mysql slave的相关权限.CREATE USER canal IDENTIFIED BY ‘canal’;GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON . TO ‘canal’@’%’;– GRANT ALL PRIVILEGES ON . TO ‘canal’@’%’ ;FLUSH PRIVILEGES; 5.修改配置文件1234[mysqld]log-bin=mysql-bin #添加这一行就okbinlog-format=ROW #选择row模式server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复 二.安装canal1.下载canal1wget https://github.com/alibaba/canal/releases/download/v1.0.23/canal.deployer-1.0.23.tar.gz 2.解压缩12mkdir /root/canaltar zxvf canal.deployer-1.0.23.tar.gz -C /root/canal 3.修改配置文件(如果是访问本机，并且用户密码都为canal则不需要修改配置文件)1vi /root/canal/conf/example/instance.properties 4.启动1sh /root/canal/bin/startup.sh 5.查看日志1234567[root@zhm1 ~]# cat /root/canal/logs/canal/canal.logOpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release.2017-02-22 17:40:08.901 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.2017-02-22 17:40:09.069 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.118.128:11111]2017-02-22 17:40:09.758 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 具体instance的日志：1[root@zhm1 ~]# cat /root/canal/logs/example/example.log 6.关闭sh /root/canal/bin/stop.sh 三.写客户端代码1.在maven的setting.xml加入阿里的镜像123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;&lt;/mirror&gt; 2.创建初始项目1mvn archetype:generate -DgroupId=com.alibaba.otter -DartifactId=canal.sample 3.pom文件增加12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.0.12&lt;/version&gt;&lt;/dependency&gt; 4. ClientSample代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package com.alibaba.otter;/** * Created by Administrator on 2017/2/23. */import java.net.InetSocketAddress;import java.util.List;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.common.utils.AddressUtils;import com.alibaba.otter.canal.protocol.Message;import com.alibaba.otter.canal.protocol.CanalEntry.Column;import com.alibaba.otter.canal.protocol.CanalEntry.Entry;import com.alibaba.otter.canal.protocol.CanalEntry.EntryType;import com.alibaba.otter.canal.protocol.CanalEntry.EventType;import com.alibaba.otter.canal.protocol.CanalEntry.RowChange;import com.alibaba.otter.canal.protocol.CanalEntry.RowData;public class ClientSample &#123; public static void main(String args[]) &#123; // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector( new InetSocketAddress("192.168.118.128",//AddressUtils.getHostIp(), 11111), "example", "", ""); int batchSize = 1000; int emptyCount = 0; try &#123; connector.connect(); connector.subscribe(".*\\..*"); connector.rollback(); int totalEmptyCount = 120; while (emptyCount &lt; totalEmptyCount) &#123; Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据 long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) &#123; emptyCount++; System.out.println("empty count : " + emptyCount); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; &#125; else &#123; emptyCount = 0; // System.out.printf("message[batchId=%s,size=%s] \n", batchId, size); printEntry(message.getEntries()); &#125; connector.ack(batchId); // 提交确认 // connector.rollback(batchId); // 处理失败, 回滚数据 &#125; System.out.println("empty too many times, exit"); &#125; finally &#123; connector.disconnect(); &#125; &#125; private static void printEntry(List&lt;Entry&gt; entrys) &#123; for (Entry entry : entrys) &#123; if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) &#123; continue; &#125; RowChange rowChage = null; try &#123; rowChage = RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException("ERROR ## parser of eromanga-event has an error, data:" + entry.toString(),e); &#125; EventType eventType = rowChage.getEventType(); System.out.println(String.format("================&gt; binlog[%s:%s] , name[%s,%s] , eventType : %s", entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); for (RowData rowData : rowChage.getRowDatasList()) &#123; if (eventType == EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); &#125; else if (eventType == EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); &#125; else &#123; System.out.println("-------&gt; before"); printColumn(rowData.getBeforeColumnsList()); System.out.println("-------&gt; after"); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;Column&gt; columns) &#123; for (Column column : columns) &#123; System.out.println(column.getName() + " : " + column.getValue() + " update=" + column.getUpdated()); &#125; &#125;&#125; 5. mysql下执行操作1234567891011mysql&gt; use test;Database changedmysql&gt; CREATE TABLE `xdual` ( -&gt; `ID` int(11) NOT NULL AUTO_INCREMENT, -&gt; `X` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, -&gt; PRIMARY KEY (`ID`) -&gt; ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ;Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into xdual(id,x) values(4,now());Query OK, 1 row affected (0.06 sec) 可以从控制台中看到：1234567empty count : 1empty count : 2empty count : 3empty count : 4================&gt; binlog[mysql-bin.001946:313661577] , name[test,xdual] , eventType : INSERTID : 4 update=trueX : 2017-02-23 14:20:00 update=true 四.canal集群搭建1.安装zookeeper略 2.修改配置文件123456vi /root/canal/conf/canal.propertiescanal.zkServers=1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181canal.instance.global.spring.xml = classpath:spring/default-instance.xmlvi /root/canal/conf/example/instance.propertiescanal.instance.mysql.slaveId = 1234 另外一个机器改成1235与1234不同即可 分别在两台机器上启动，发现只有一台机器logs下面有example目录，并且显示启动成功 3.进入到zkClient查看状态获取正在运行的canal server1get /otter/canal/destinations/example/running 获取正在连接的canal client1get /otter/canal/destinations/example/1001/running 获取当前最后一次消费车成功的binlog1get /otter/canal/destinations/example/1001/cursor 4.客户端代码修改如下：1CanalConnector connector = CanalConnectors.newClusterConnector("1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181", "example", "", ""); 代码启动 停止一个canal 全部停止canal 查看当前canal消费到哪个position[zk: localhost:2181(CONNECTED) 15] get /otter/canal/destinations/example/1001/cursor{“@type”:”com.alibaba.otter.canal.protocol.position.LogPosition”,”identity”:{“slaveId”:-1,”sourceAddress”:{“address”:”po-master1”,”port”:3306}},”postion”:{“included”:false,”journalName”:”logbin.000004”,”position”:6897322,”serverId”:1,”timestamp”:1492065268000}} 查看mysql的模式show global variables like ‘%binlog_format%’;查看binlog位置show master status 错误整理：启动发现错误：[root@slave1 canal]# cat logs/borrow/borrow.log2017-06-12 03:57:25.931 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]2017-06-12 03:57:25.938 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [borrow/instance.properties]2017-06-12 03:57:25.946 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration, e.g. on the BeanWrapper/BeanFactory!2017-06-12 03:57:25.951 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-borrow2017-06-12 03:57:25.953 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to test_ydxsb..*2017-06-12 03:57:25.953 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful….2017-06-12 03:57:25.971 [destination = borrow , address = /10.105.0.191:3306 , EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::14969673010002017-06-12 03:57:26.047 [destination = borrow , address = /10.105.0.191:3306 , EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didn’t find the corresponding binlog files from mysql-bin.000001 to mysql-bin.0000012017-06-12 03:57:26.050 [destination = borrow , address = /10.105.0.191:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.105.0.191:3306 has an error, retrying. caused bycom.alibaba.otter.canal.parse.exception.CanalParseException: can’t find start position for borrow2017-06-12 03:57:26.052 [destination = borrow , address = /10.105.0.191:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:borrow[com.alibaba.otter.canal.parse.exception.CanalParseException: can’t find start position for borrow 需要删掉borrow目录下的meta.dat 如果提示：找不到日志的位置在mysql下执行(可能是设置完格式，格式不对)set global binlog_checksum=’NONE’]]></content>
      <categories>
        <category>canal</category>
      </categories>
      <tags>
        <tag>canal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm1.0.3集群部署]]></title>
    <url>%2F2017%2F02%2F15%2Fstorm1.0.3%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[昨日已经发布storm1.0.3版本，来玩一玩 环境：zookeeper3.4.9节点：storm01，storm02，storm03storm1.0.3 nimbus节点storm01，supervisor节点storm02，storm03 1234567[storm@localhost app]$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz[storm@localhost app]$ tar zxvf zookeeper-3.4.9.tar.gz[storm@localhost app]$ ln -s -f zookeeper-3.4.9 zookeeper[storm@localhost app]$ mkdir zookeeper/data[storm@localhost app]$ cd zookeeper/conf/[storm@localhost conf]$ cp zoo_sample.cfg zoo.cfg[storm@localhost conf]$ vi zoo.cfg 修改dataDir1234dataDir=/home/storm/app/zookeeper/dataserver.1=storm01:2888:3888server.2=storm02:2888:3888server.3=storm03:2888:3888 在storm01下执行12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 1 &gt; myid 在storm02下执行12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 2 &gt; myid 在storm03下执行12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 3 &gt; myid 部署storm123[storm@localhost app]$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/storm/apache-storm-1.0.3/apache-storm-1.0.3.tar.gz[storm@localhost app]$ tar zxvf apache-storm-1.0.3.tar.gz[storm@localhost app]$ ln -s -f apache-storm-1.0.3 storm 修改配置文件，jdk安装此处略12345678[storm@localhost app]$ su[root@localhost app]# echo &quot;export JAVA_HOME=/home/storm/app/jdk&quot;&gt;&gt; /etc/profile[root@localhost app]# echo &quot;export ZOOKEEPER_HOME=/home/storm/app/zookeeper&quot;&gt;&gt; /etc/profile[root@localhost app]# echo &quot;export STORM_HOME=/home/storm/app/storm&quot;&gt;&gt; /etc/profile[root@localhost app]# source /etc/profile[root@localhost app]# echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;ZOOKEEPER_HOME&#125;/bin:$&#123;STORM_HOME&#125;/bin:$PATH&quot;&gt;&gt; /etc/profile[root@localhost app]# source /etc/profile[root@localhost app]# su storm 修改stom配置文件12[storm@localhost app]$ cd storm/conf[storm@localhost conf]$ vi storm.yaml 123456789101112storm.zookeeper.servers: - &quot;storm01&quot; - &quot;storm02&quot; - &quot;storm03&quot;storm.local.dir: &quot;/home/storm/app/storm/data&quot;storm.zookeeper.root: &quot;/storm&quot; nimbus.seeds: [&quot;storm01&quot;]supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 启动zookeeper，在三个节点分别执行如下命令1[storm@storm01 zookeeper]$ zkServer.sh start 1[storm@storm02 zookeeper]$ zkServer.sh start 1[storm@storm03 zookeeper]$ zkServer.sh start 启动storm集群在storm01节点上启动nimbus，ui，logviewer ：123[storm@storm01 zookeeper]$ nohup storm nimbus &amp;[storm@storm01 zookeeper]$ nohup storm ui &amp;[storm@storm01 zookeeper]$ nohup storm logviewer &amp; 在stomr02，storm03节点上启动supervisor12[storm@storm02 zookeeper]$ nohup storm supervisor &amp;[storm@storm02 zookeeper]$ nohup storm logviewer &amp; 12[storm@storm03 zookeeper]$ nohup storm supervisor &amp;[storm@storm03 zookeeper]$ nohup storm logviewer &amp;]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm1.0.2安装]]></title>
    <url>%2F2017%2F01%2F28%2Fstorm1.0.2%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装配置zookeeper略 下载：wget http://mirrors.cnnic.cn/apache/storm/apache-storm-1.0.2/apache-storm-1.0.2.tar.gz 解压，并创建符号链接修改profile配置文件修改配置文件：vi storm.yaml 添加如下内容storm.zookeeper.servers: - &quot;www.hadoop01.com&quot; storm.zookeeper.root: “/storm_1.0.2”nimbus.seeds: [“www.hadoop01.com”]supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 启动启动zookeeper启动nimbus./bin/storm nimbus &amp;启动ui ./bin/storm ui &amp; 登录UI：http://www.hadoop01.com:8080/index.html启动supervisor./bin/storm supervisor &amp;启动logviewer./bin/storm logviewer &amp; 执行democd /home/hadoop/apps/storm/storm/examples/storm-starter storm jar storm-starter-topologies-1.0.2.jar org.apache.storm.starter.WordCountTopology first-topology 查看UI点击spout:]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python春节祝福语自动回复]]></title>
    <url>%2F2017%2F01%2F27%2FPython%E6%98%A5%E8%8A%82%E7%A5%9D%E7%A6%8F%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%9B%9E%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[首先安装两个库pip install itchat pillow 1234567891011121314151617环境win7，pyython3编写py文件输入以下内容import itchat, time, refrom itchat.content import *@itchat.msg_register([TEXT])def text_reply(msg): match = re.search('年', msg['Text']).span() if match: itchat.send(('鸡年大吉'), msg['FromUserName'])@itchat.msg_register([PICTURE, RECORDING, VIDEO, SHARING])def other_reply(msg): itchat.send(('鸡年大吉'), msg['FromUserName'])itchat.auto_login(enableCmdQR=True,hotReload=True)itchat.run() 运行后，扫描生成的二维码即可 登录成功有以下提示]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python机器学习准备]]></title>
    <url>%2F2017%2F01%2F27%2FPython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[下载NumPyhttp://download.csdn.net/download/z1137730824/8384347（numpy 64位）http://download.csdn.net/detail/u010156024/9302649（numpy 32位） 下载matplotlibhttps://pypi.python.org/pypi/matplotlib/ 博主选择matplotlib-2.0.0-cp27-cp27m-win_amd64.whl (md5)下载完把matplotlib-2.0.0-cp27-cp27m-win_amd64.whl改成matplotlib-2.0.0-cp27-cp27m-win_amd64.zip解压到Python目录下的Lib文件夹下的site-packages目录 安装dateutil直接用pip install python-dateutil或者去网上下载https://pypi.python.org/pypi/python-dateutil 安装pyparsingpip install pyparsing或者去网上下载https://pypi.python.org/pypi/pyparsing/2.0.2http://pyparsing.wikispaces.com/Download+and+Installation 最终安装如下安装包：numpy, setuptools, python-dateutil, pytz, pyparsing, and cycler，functools32]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm基础知识]]></title>
    <url>%2F2017%2F01%2F27%2Fstorm%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Storm基本概念Storm是一个分布式计算框架，主要由Clojure编程语言编写。最初是由Nathan Marz及其团队创建于BackType，该项目在被Twitter取得后开源。它使用用户创建的“管（spouts）”和“螺栓（bolts）”来定义信息源和操作来允许批量、分布式处理流式数据。最初的版本发布于2011年9月17日。Storm应用被设计成为一个拓扑结构，其接口创建一个转换“流”。它提供与MapReduce作业类似的功能，当遇到异常时该拓扑结构理论上将不确定地运行，直到它被手动终止 主要版本 版本 发布日期 0.9.0.1 2013年12月8日 0.9.0 2013年12月8日 0.8.0 2012年8月2日 0.7.0 2012年2月28日 0.6.0 2011年12月15日 0.5.0 2011年9月19日 编程模型Topology一个实时计算应用程序逻辑被封装在Topology对象中， 类似Hadoop中的job， Topology会一直运行直到你显式杀死它 ###DataSource外部数据源 Spout接受外部数据源的组件，将外部数据源转化成Storm内部的数据，以Tuple为基本的传输单元下发给Bolt Bolt接受Spout发送的数据，或上游的bolt的发送的数据。根据业务逻辑进行处理。发送给下一个Bolt或者是存储到某种介质上。介质可以是Redis可以是mysql，或者其他。 TupleStorm内部中数据传输的基本单元，里面封装了一个List对象，用来保存数据。 StreamGrouping:数据分组策略7种：shuffleGrouping(Random函数),NonGrouping(Random函数,目前和shuffleGrouping一样),FieldGrouping(Hash取模),Local or ShuffleGrouping （本地或随机，优先本地）,Fields grouping（根据Tuple中的某一个Filed或者多个Filed的是值来划分。 比如Stream根据Field为user-id来grouping， 相同user-id值的Tuple会被分发到相同的Task中）,Global grouping（整个Stream会选择一个Task作为分发的目的地， 通常是最新的那个id的Task）Direct grouping（产生数据的Spout/Bolt自己明确决定这个Tuple被Bolt的那些Task所消费） Storm优点健壮性当Worker失效或机器出现故障时， 自动分配新的Worker替换失效Worker 准确性采用Acker机制，保证数据不丢失采用事务机制，保证数据准确性 storm架构 Storm的主线主要包括4条：nimbus, supervisor, worker和task。 对于storm0.9.6的配置文件nimbus配置参数还是nimbus.hosts 现在新版本1.0.2已经修改为nimbus.seeds,已经可以支持HA此处之一配置nimbus.seeds: XX。此处nimbus.seeds:后面要有一个空格，否则启动报错storm启动nimbus的时候jps会有一个进程config_value然后变成nimbus。其他启动同上]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python网络爬虫]]></title>
    <url>%2F2017%2F01%2F26%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[python官方提供的网页下载器是urllib2第三方有更强大的下载器是requests 在python2.x里我们可以使用urllib2.urlopen(“http://www.baidu.com&quot;)打开网页但是在python3.x里 urllib2 需要改成urllib.request 在python2.x里可以使用urllib.urlencode：例如：values = {“username”:”Python爬虫”,”password”:”123456789”}data = urllib.urlencode(values)但是在python3.x里 urllib2 需要改成urllib.requestdata = urllib.parse.urlencode(values)而且还需要把data格式转换data = data.encode(‘utf-8’)或data = data.encode(encoding=’UTF8’)否则会提示TypeError: POST data should be bytes or an iterable of bytes. It cannot be of type str.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Kylin]]></title>
    <url>%2F2017%2F01%2F18%2FApache%20Kylin%2F</url>
    <content type="text"><![CDATA[kylinApache Kylin™ is an open source Distributed Analytics Engine designed to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets, original contributed from eBay Inc. Apache Kylin™ lets you query massive data set at sub-second latency in 3 steps. Identify a Star Schema on Hadoop. Build Cube from the identified tables. Query with ANSI-SQL and get results in sub-second, via ODBC, JDBC or RESTful API.]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Beam]]></title>
    <url>%2F2017%2F01%2F16%2FApache%20Beam%2F</url>
    <content type="text"><![CDATA[Apache BeamApache Beam provides an advanced unified programming model, allowing you to implement batch and streaming data processing jobs that can run on any execution engine. Apache Beam提供了一个先进的统一编程模型，可以实现批量和流数据处理工作，可以运行在任何执行引擎。Apache Beam is: UNIFIED - Use a single programming model for both batch and streaming use cases. PORTABLE - Execute pipelines on multiple execution environments, including Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow. EXTENSIBLE - Write and share new SDKs, IO connectors, and transformation libraries. 统一的 - 对批处理和流媒体用例使用单一编程模型。 轻便的(可移植) - 便携式管道上执行多个执行环境，包括Apache的先端，ApacheFlink，Apache Spark，和谷歌云数据流。 可扩展的 - 写和分享新的软件开发工具包，IO连接器，和动态库。 Get Started To use Beam for your data processing tasks, start by reading the Beam Overview and performing the steps in the Quickstart. Then dive into the Documentation section for in-depth concepts and reference materials for the Beam model, SDKs, and runners.使用Beam为你的数据处理任务，开始通过阅读Beam概述和示例执行以下步骤。然后潜入文档部分进行深入的概念和Beam模型，SDKs，和执行。 Contribute Beam is an Apache Software Foundation project, available under the Apache v2 license. Beam is an open source community and contributions are greatly appreciated! If you’d like to contribute, please see the Contribute section.Beam 是Apache软件基金会的项目，Apache v2许可下可用。Beam是一个开源社区和贡献非常感谢！如果你想贡献，请参阅投稿部分。 Apache Beam Overview Apache Beam is an open source, unified programming model that you can use to create a data processing pipeline. You start by building a program that defines the pipeline using one of the open source Beam SDKs. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow.Apache的Beam是一个开源的、统一的编程模型，可以用来创建一个数据处理管道。你开始建立一个程序，定义了管道使用一个开源的Beam的SDK。管道是通过一Beam的支持分布式处理后端执行，包括Apache的先端，Apache Flink，Apache的Spark，和谷歌云数据流。 Beam is particularly useful for Embarrassingly Parallel data processing tasks, in which the problem can be decomposed into many smaller bundles of data that can be processed independently and in parallel. You can also use Beam for Extract, Transform, and Load (ETL) tasks and pure data integration. These tasks are useful for moving data between different storage media and data sources, transforming data into a more desirable format, or loading data onto a new system.Beam的高度并行的数据处理任务是特别有用的，其中的问题可以被分解成许多较小的许多数据，可以独立和并行处理。你也可以使用Beam变换，提取，和加载（ETL）任务和纯数据集成。这些任务用于在不同的存储介质和数据源之间移动数据，将数据转换成更理想的格式，或者将数据加载到新系统上。 Apache Beam SDKs The Beam SDKs provide a unified programming model that can represent and transform data sets of any size, whether the input is a finite data set from a batch data source, or an infinite data set from a streaming data source. The Beam SDKs use the same classes to represent both bounded and unbounded data, and the same transforms to operate on that data. You use the Beam SDK of your choice to build a program that defines your data processing pipeline.Beam的SDK提供了一个统一的编程模型，可以表示和变换任意大小的数据集，输入是否是一个有限的数据集从一个批处理的数据源，或无限的数据集从一个流的数据源。Beam的SDK使用同一类的代表有界和无界的数据，和相同的变换，对这些数据的操作。你用你选择的Beam SDK构建的程序定义数据处理管道。 Beam currently supports the following language-specific SDKs:Language SDK StatusJava Active DevelopmentPython Coming SoonOther TBD Apache Beam Pipeline Runners The Beam Pipeline Runners translate the data processing pipeline you define with your Beam program into the API compatible with the distributed processing back-end of your choice. When you run your Beam program, you’ll need to specify the appropriate runner for the back-end where you want to execute your pipeline.Beam流管道翻译您定义的数据处理管道与您的Beam程序到API与您选择的分布式处理后端兼容。当你运行你的Beam束计划时，你需要指定你想要执行你的管道的后端的合适的执行.。 Beam currently supports Runners that work with the following distributed processing back-ends:Beam目前支持运行与下列分布式处理后端Runner StatusApache Apex In DevelopmentApache Flink In DevelopmentApache Spark In DevelopmentGoogle Cloud Dataflow In Development Quickstart 来一段hello World吧环境准备：JDK1.7+。Maven maven命令：12345678910$ mvn archetype:generate \ -DarchetypeRepository=https://repository.apache.org/content/groups/snapshots \ -DarchetypeGroupId=org.apache.beam \ -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples \ -DarchetypeVersion=LATEST \ -DgroupId=org.example \ -DartifactId=word-count-beam \ -Dversion="0.1" \ -Dpackage=org.apache.beam.examples \ -DinteractiveMode=false 查看文件12345[root@zhm1 Beam]# lsword-count-beam[root@zhm1 Beam]# cd word-count-beam/[root@zhm1 word-count-beam]# ls src/main/java/org/apache/beam/examples/common DebuggingWordCount.java MinimalWordCount.java WindowedWordCount.java WordCount.java 运行：12mvn compile exec:java -Dexec.mainClass=org.apache.beam.examples.WordCount \&gt; -Dexec.args="--inputFile=pom.xml --output=counts" -Pdirect-runner 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150[root@zhm1 word-count-beam]# ll -lh counts*-rw-r--r--. 1 root root 618 1月 19 14:02 counts-00000-of-00005-rw-r--r--. 1 root root 596 1月 19 14:02 counts-00001-of-00005-rw-r--r--. 1 root root 585 1月 19 14:02 counts-00002-of-00005-rw-r--r--. 1 root root 581 1月 19 14:02 counts-00003-of-00005-rw-r--r--. 1 root root 593 1月 19 14:02 counts-00004-of-00005[root@zhm1 word-count-beam]# cat counts-00000-of-00005work: 1IS: 1versions: 1direct: 3specified: 1incubating: 1more: 1snapshots: 4submission: 1... Minimal WordCount demonstrates the basic principles involved in building a pipeline. WordCount introduces some of the more common best practices in creating re-usable and maintainable pipelines. Debugging WordCount introduces logging and debugging practices. Windowed WordCount demonstrates how you can use Beam’s programming model to handle both bounded and unbounded datasets.```java从Minimal WordCount分析代码：/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * "License"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.beam.examples;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Count;import org.apache.beam.sdk.transforms.DoFn;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.ParDo;import org.apache.beam.sdk.transforms.SimpleFunction;import org.apache.beam.sdk.values.KV;/** * An example that counts words in Shakespeare. * * &lt;p&gt;This class, &#123;@link MinimalWordCount&#125;, is the first in a series of four successively more * detailed 'word count' examples. Here, for simplicity, we don't show any error-checking or * argument processing, and focus on construction of the pipeline, which chains together the * application of core transforms. * * &lt;p&gt;Next, see the &#123;@link WordCount&#125; pipeline, then the &#123;@link DebuggingWordCount&#125;, and finally the * &#123;@link WindowedWordCount&#125; pipeline, for more detailed examples that introduce additional * concepts. * * &lt;p&gt;Concepts: * * &lt;pre&gt; * 1. Reading data from text files * 2. Specifying 'inline' transforms * 3. Counting items in a PCollection * 4. Writing data to text files * &lt;/pre&gt; * * &lt;p&gt;No arguments are required to run this pipeline. It will be executed with the DirectRunner. You * can see the results in the output files in your current working directory, with names like * "wordcounts-00001-of-00005. When running on a distributed service, you would use an appropriate * file service. */public class MinimalWordCount &#123; public static void main(String[] args) &#123; // Create a PipelineOptions object. This object lets us set various execution // options for our pipeline, such as the runner you wish to use. This example // will run with the DirectRunner by default, based on the class path configured // in its dependencies. PipelineOptions options = PipelineOptionsFactory.create(); //默认是options.setRunner(DirectRunner.class); // Create the Pipeline object with the options we defined above. Pipeline p = Pipeline.create(options); // Apply the pipeline's transforms. // Concept #1: Apply a root transform to the pipeline; in this case, TextIO.Read to read a set // of input text files. TextIO.Read returns a PCollection where each element is one line from // the input text (a set of Shakespeare's texts). // This example reads a public data set consisting of the complete works of Shakespeare. p.apply(TextIO.Read.from("gs://apache-beam-samples/shakespeare/*")) // 读取本地文件，构建第一个PTransfor // Concept #2: Apply a ParDo transform to our PCollection of text lines. This ParDo invokes a // DoFn (defined in-line) on each element that tokenizes the text line into individual words. // The ParDo returns a PCollection&lt;String&gt;, where each element is an individual word in // Shakespeare's collected texts. .apply("ExtractWords", ParDo.of(new DoFn&lt;String, String&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; for (String word : c.element().split("[^a-zA-Z']+")) &#123; if (!word.isEmpty()) &#123; c.output(word); &#125; &#125; &#125; &#125;)) // Concept #3: Apply the Count transform to our PCollection of individual words. The Count // transform returns a new PCollection of key/value pairs, where each key represents a unique // word in the text. The associated value is the occurrence count for that word. .apply(Count.&lt;String&gt;perElement()) // Apply a MapElements transform that formats our PCollection of word counts into a printable // string, suitable for writing to an output file. .apply("FormatResults", MapElements.via(new SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Long&gt; input) &#123; return input.getKey() + ": " + input.getValue(); &#125; &#125;)) // Concept #4: Apply a write transform, TextIO.Write, at the end of the pipeline. // TextIO.Write writes the contents of a PCollection (in this case, our PCollection of // formatted strings) to a series of text files. // // By default, it will write to a set of files with names like wordcount-00001-of-00005 .apply(TextIO.Write.to("wordcounts")); // Run the pipeline. p.run().waitUntilFinish(); &#125;&#125; 创建管道1PipelineOptions options = PipelineOptionsFactory.create(); 下一步是使用我们刚才构建的选项创建一个Pipeline对象。Pipeline对象构建要执行的变换图，与特定流水线相关联。1Pipeline p = Pipeline.create(options); 应用管道变换 Minimal的WordCount流水线包含几个变换，以将数据读入流水线，操纵或以其他方式transform数据，并写出结果。每个transform表示管道中的操作。 每个transform采用某种输入（数据或其他），并产生一些输出数据。输入和输出数据是由SDK类表示PCollection。PCollection是一个特殊的类，由Beam SDK提供，您可以使用它来表示几乎任何大小的数据集，包括无限数据集。 Minimal WordCount流水线包含五个transform 1.一个文本文件Read transform应用于流水线对象本身，并产生PCollection作为输出。输出PCollection中的每个元素表示输入文件中的一行文本。此示例恰好使用存储在可公开访问的Google Cloud Storage存储桶（“gs：//”）中的输入数据。1p.apply(TextIO.Read.from("gs://apache-beam-samples/shakespeare/*")) 2.一个ParDo transform它调用DoFn了tokenizes文本行成单个的单词每个元素（在线作为一个匿名类中定义）。对于此transform的输入是PCollection由先前生成的文本的行TextIO.Read变换。的ParDo变换输出一个新的PCollection，其中每个元素表示的文本的单词。12345678910.apply("ExtractWords", ParDo.of(new DoFn&lt;String, String&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; for (String word : c.element().split("[^a-zA-Z']+")) &#123; if (!word.isEmpty()) &#123; c.output(word); &#125; &#125; &#125; &#125;)) 3.该SDK提供的Count transform是一个通用的transform，它接受一个PCollection任何类型的，和返回PCollection键/值对。每个键表示输入集合中的唯一元素，每个值表示键在输入集合中出现的次数。 这条管线，用于将输入Count的是PCollection由以前产生个别单词ParDo，并输出为一个PCollection，其中每个键代表一个唯一字中的文本和相关值是出现计数每个键/值对。1.apply(Count.&lt;String&gt;perElement()) 4.下一个transform将唯一字和出现计数的每个键/值对格式化为适于写入输出文件的可打印字符串。 MapElements是一个更高层次的复合转换，它封装了一个简单的ParDo; 为输入中的每个元件PCollection，MapElements适用于产生恰好一个输出元件的功能。在这个例子中，MapElements调用SimpleFunction（在线作为匿名类中定义），该确实的格式。作为输入，MapElements取PCollection所产生的键/值对的Count，并产生一个新PCollection的可打印字符串。123456.apply("FormatResults", MapElements.via(new SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Long&gt; input) &#123; return input.getKey() + ": " + input.getValue(); &#125; &#125;)) 5.一个文本文件Write。此transform需要的最终PCollection格式字符串作为输入和每个元素写入到输出文本文件。输入中的每个元素PCollection表示在所产生的输出文件的文本的一个行。1.apply(TextIO.Write.to("wordcounts")); 注意，该Write变换产生类型的琐碎结果值PDone，在这种情况下被忽略 运行管道通过调用运行管线run的方法，它会将您的管道由您在创建您的管道中指定的管道运行程序执行。1p.run().waitUntilFinish(); 注意，该run方法是异步的。对于阻止执行，而不是，运行管线追加waitUntilFinish方法。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Beam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法推演]]></title>
    <url>%2F2017%2F01%2F10%2F%E7%AE%97%E6%B3%95%E6%8E%A8%E6%BC%94%2F</url>
    <content type="text"><![CDATA[条件概率事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P（A|B），读作“在B条件下A的概率”。 设A，B 是两个事件，且A不是不可能事件，则称为在事件A发生的条件下，事件B发生的条件概率。一般地，，且它满足以下三条件：（1）非负性；（2）规范性；（3）可列可加性。 如上图求：红球来自A的概率P（A|红） = P（A红）/P（红） 全概率公式全概率公式为概率论中的重要公式，它将对一复杂事件A的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题。 公式表示若事件A1，A2，…，An构成一个完备事件组且都有正概率，则对任意一个事件B都有公式成立。 如上图求：红球的概率P（红） = P（A红）+P（B红） 贝叶斯定理贝叶斯定理是关于随机事件A和B的条件概率（或边缘概率）的一则定理。其中P(A|B)是在B发生的情况下A发生的可能性。 欧几里得定理二维空间的公式0ρ = sqrt( (x1-x2)^2+(y1-y2)^2 ) |x| = √( x2 + y2 )三维空间的公式0ρ = √( (x1-x2)^2+(y1-y2)^2+(z1-z2)^2 ) |x| = √( x2 + y2 + z2 )n维空间的公式n维欧氏空间是一个点集,它的每个点 X 或向量 x 可以表示为 (x1，x2，…，x[n]) ，其中 xi 是实数，称为 X 的第i个坐标。两个点 A = (a1，a2，…，a[n]) 和 B = (b1，b2，…，b[n]) 之间的距离 ρ(A，B) 定义为下面的公式：ρ(A，B) =√ [ ∑( a[i] - b[i] )^2 ] (i = 1，2，…，n)向量 x = (x1，x2，…，x[n]) 的自然长度 |x| 定义为下面的公式：|x| = √( x1^2 + x2^2 + … + x[n]^2 ) 线性方程线性方程也称一次方程。指未知数都是一次的方程。其一般的形式是ax+by+…+cz+d=0。线性方程的本质是等式两边乘以任何相同的非零数，方程的本质都不受影响。ax + by = c机器学习里面用逼近论来算]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 初探]]></title>
    <url>%2F2017%2F01%2F10%2Fpython%20%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[初探Python是一种解释型语言，不用编译Python是交互式语言Python是面向对象的语言 从简单的文字处理到爬虫游戏 python有2个分支，一个是2.x 一个是3.x语法会不同例如print 1 在3.x版本是一个函数要写成print(1) 和java不同的是有以下两个类型定义元祖类型() 元组是可读的定义字典类型{} 初始化a={2,3} type以下a的类型为set 引用模块import 包名.类名from 包名.类名 import *from 包名.类名 import 方法名 as 别名 如果想别的模块不能引用自己模块的内容可以增加if name==’main‘ 每个目录下都有init.py文件，这个是初始化模块 python开启多线程，可以使用内置关键字threading私有变量前面加__实例不能访问私有变量 python使用引用计数法追踪内存并回收 继承child(parent)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法思维]]></title>
    <url>%2F2017%2F01%2F04%2F%E7%AE%97%E6%B3%95%E6%80%9D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[火柴棍游戏 移动两根火柴得到的最大值是多少？ 一般人的第一反应是： 如果我们想到了位数越多，数字越大，就会想到： 此时如果想到了平方计算 那么你需要的是继续发散四位，想到科学计数法 但是此时并不是最大的 只有当你想到了以下的数据，那么恭喜你成功了 思路决定出路]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot solr]]></title>
    <url>%2F2016%2F12%2F28%2Fspring%20boot%20solr%2F</url>
    <content type="text"><![CDATA[pom引入solr的jar包1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-solr&lt;/artifactId&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker版Kafka集群]]></title>
    <url>%2F2016%2F12%2F19%2Fdocker%E7%8E%A9kafka%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[入手centos7首先打开centos 官网下载https://www.centos.org/download/ 选择DVD ISO 找到地址并下载http://101.96.8.151/isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso 安装参考：http://blog.csdn.net/alex_my/article/details/38142229 安装docker安装详情可参考官网https://docs.docker.com/engine/installation/linux/centos/博主使用另外一种简单方式安装安装docker1yum install docker 启动服务（CENTOS7之前的版本）12service docker startchkconfig docker on Centos之后的：12systemctl start docker.servicesystemctl enable docker.service 123456789101112131415161718docker versionClient: Version: 1.10.3 API version: 1.22 Package version: docker-common-1.10.3-59.el7.centos.x86_64 Go version: go1.6.3 Git commit: 3999ccb-unsupported Built: Thu Dec 15 17:24:43 2016 OS/Arch: linux/amd64Server: Version: 1.10.3 API version: 1.22 Package version: docker-common-1.10.3-59.el7.centos.x86_64 Go version: go1.6.3 Git commit: 3999ccb-unsupported Built: Thu Dec 15 17:24:43 2016 OS/Arch: linux/amd64 安装docker-compose博主当时有1.7.1的版本在安装完之后启动报错Cannot open self /usr/bin/docker-compose or archive /usr/bin/docker-compose.pkg 后改用早一点的版本1.7.0测试没有问题下载和安装地址可参考：https://github.com/docker/compose/releases?after=docs-v1.7.1-2016-05-3112345678curl -L https://github.com/docker/compose/releases/download/1.7.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composedocker-compose versiondocker-compose version 1.7.0, build 0d7bf73docker-py version: 1.8.0CPython version: 2.7.9OpenSSL version: OpenSSL 1.0.1e 11 Feb 2013 编写docker文件，此处参考Jason大神的，尊重原创，转载请标注kafka.Dockerfile12345678910111213141516171819202122232425262728293031323334353637383940414243444546FROM centos:6.6RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\ mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoRUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pipRUN mkdir /opt/java &amp;&amp;\ wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/javaENV KAFKA_VERSION "0.8.2.2"RUN mkdir /opt/kafka &amp;&amp;\ wget http://apache.fayea.com/kafka/$KAFKA_VERSION/kafka_2.11-$KAFKA_VERSION.tgz -P /opt/kafkaRUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\ JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\ sed -i "/^PATH/i export JAVA_HOME=$JAVA_HOME" /root/.bash_profile &amp;&amp;\ sed -i "s%^PATH.*$%&amp;:$JAVA_HOME/bin%g" /root/.bash_profile &amp;&amp;\ source /root/.bash_profileRUN tar zxvf /opt/kafka/kafka*.tgz -C /opt/kafka &amp;&amp;\ sed -i 's/num.partitions.*$/num.partitions=3/g' /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.propertiesRUN echo "source /root/.bash_profile" &gt; /opt/kafka/start.sh &amp;&amp;\ echo "cd /opt/kafka/kafka_2.11-"$KAFKA_VERSION &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ #echo "sed -i 's%zookeeper.connect=.*$%zookeeper.connect=zookeeper:2181%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""ZOOKEEPER_CONNECT"" ] &amp;&amp; sed -i 's%.*zookeeper.connect=.*$%zookeeper.connect='$""ZOOKEEPER_CONNECT'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""BROKER_ID"" ] &amp;&amp; sed -i 's%broker.id=.*$%broker.id='$""BROKER_ID'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""BROKER_PORT"" ] &amp;&amp; sed -i 's%port=.*$%port='$""BROKER_PORT'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "sed -i 's%#advertised.host.name=.*$%advertised.host.name='$""(hostname -i)'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""ADVERTISED_HOST_NAME"" ] &amp;&amp; sed -i 's%.*advertised.host.name=.*$%advertised.host.name='$""ADVERTISED_HOST_NAME'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "sed -i 's%#host.name=.*$%host.name='$""(hostname -i)'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""HOST_NAME"" ] &amp;&amp; sed -i 's%.*host.name=.*$%host.name='$""HOST_NAME'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "delete.topic.enable=true" &gt;&gt; /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.properties &amp;&amp;\ echo "bin/kafka-server-start.sh config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ chmod a+x /opt/kafka/start.shRUN yum install -y ncEXPOSE 9092WORKDIR /opt/kafka/kafka_2.11-$KAFKA_VERSIONENTRYPOINT ["sh", "/opt/kafka/start.sh"] zookeeper.Dockerfile1234567891011121314151617181920212223242526272829303132333435363738FROM centos:6.6RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\ mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoRUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pipRUN mkdir /opt/java &amp;&amp;\ wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/javaRUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\ JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\ sed -i &quot;/^PATH/i export JAVA_HOME=$JAVA_HOME&quot; /root/.bash_profile &amp;&amp;\ sed -i &quot;s%^PATH.*$%&amp;:$JAVA_HOME/bin%g&quot; /root/.bash_profile &amp;&amp;\ source /root/.bash_profileENV ZOOKEEPER_VERSION &quot;3.4.6&quot;RUN mkdir /opt/zookeeper &amp;&amp;\ wget http://mirror.olnevhost.net/pub/apache/zookeeper/zookeeper-$ZOOKEEPER_VERSION/zookeeper-$ZOOKEEPER_VERSION.tar.gz -P /opt/zookeeperRUN tar zxvf /opt/zookeeper/zookeeper*.tar.gz -C /opt/zookeeperRUN echo &quot;source /root/.bash_profile&quot; &gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;cp /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo_sample.cfg /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_PORT&quot;&quot; ] &amp;&amp; sed -i &apos;s%.*clientPort=.*$%clientPort=&apos;$&quot;&quot;ZOOKEEPER_PORT&apos;&quot;&quot;%g&apos; /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_ID&quot;&quot; ] &amp;&amp; mkdir -p /tmp/zookeeper &amp;&amp; echo $&quot;&quot;ZOOKEEPER_ID &gt; /tmp/zookeeper/myid&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[[ ! -z $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot; ]] &amp;&amp; for server in $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot;; do echo $&quot;&quot;server&quot;&quot; &gt;&gt; /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg; done&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;/opt/zookeeper/zookeeper-$&quot;ZOOKEEPER_VERSION&quot;/bin/zkServer.sh start-foreground&quot; &gt;&gt; /opt/zookeeper/start.shRUN yum install -y ncEXPOSE 2181WORKDIR /opt/zookeeper/zookeeper-$ZOOKEEPER_VERSIONENTRYPOINT [&quot;sh&quot;, &quot;/opt/zookeeper/start.sh&quot;] docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139version: &apos;2.0&apos;services: zookeeper0: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper0 hostname: zookeeper0 ports: - &quot;2181:2181&quot; - &quot;2888:2888&quot; - &quot;3888:3888&quot; expose: - 2181 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_ID: 0 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 zookeeper1: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper1 hostname: zookeeper1 ports: - &quot;2182:2182&quot; - &quot;28881:28881&quot; - &quot;38881:38881&quot; expose: - 2182 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2182 ZOOKEEPER_ID: 1 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882# depends_on:# - zookeeper0 zookeeper2: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper2 hostname: zookeeper2 ports: - &quot;2183:2183&quot; - &quot;28882:28882&quot; - &quot;38882:38882&quot; expose: - 2183 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2183 ZOOKEEPER_ID: 2 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882# depends_on:# - zookeeper1 kafka0: build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka0 hostname: kafka0 ports: - &quot;9092:9092&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 0 BROKER_PORT: 9092 ADVERTISED_HOST_NAME: kafka0 HOST_NAME: kafka0 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9092# links:# - zookeeper kafka1: build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka1 hostname: kafka1 ports: - &quot;9093:9093&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 1 BROKER_PORT: 9093 ADVERTISED_HOST_NAME: kafka1 HOST_NAME: kafka1 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9093# links:# - zookeeper kafka2: build: . build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka2 hostname: kafka2 ports: - &quot;9094:9094&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 2 BROKER_PORT: 9094 ADVERTISED_HOST_NAME: kafka2 HOST_NAME: kafka2 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9094# links:# - zookeeper 启动docker-compose1[root@zhm1 docker-file]# docker-compose up -d 如果曾经有启动的需要先停止移除在启动1[root@zhm1 docker-file]# docker-compose stop;docker-compose rm -f 12345678[root@zhm1 docker-file]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES891585b4767a jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 9092/tcp, 0.0.0.0:9093-&gt;9093/tcp kafka181f90ccb917a jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 9092/tcp, 0.0.0.0:9094-&gt;9094/tcp kafka22edb39ed0e97 jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 0.0.0.0:9092-&gt;9092/tcp kafka0b21ac8fb0f72 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 2181/tcp, 2888/tcp, 0.0.0.0:2183-&gt;2183/tcp, 0.0.0.0:28882-&gt;28882/tcp, 3888/tcp, 0.0.0.0:38882-&gt;38882/tcp zookeeper248f5ada24ff7 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2888-&gt;2888/tcp, 0.0.0.0:3888-&gt;3888/tcp zookeeper0d247a8ac6a30 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 2181/tcp, 2888/tcp, 0.0.0.0:2182-&gt;2182/tcp, 0.0.0.0:28881-&gt;28881/tcp, 3888/tcp, 0.0.0.0:38881-&gt;38881/tcp zookeeper1 此时可以进入容器内查看：123docker exec -it zookeeper0 bashsource /root/.bash_profilebin/zkCli.sh -server zookeeper0:2181 1234567[zk: zookeeper0:2182(CONNECTED) 0] ls /[zookeeper, kafka][zk: zookeeper0:2182(CONNECTED) 1] ls /kafka[admin, consumers, controller, controller_epoch, brokers, config][zk: zookeeper0:2182(CONNECTED) 2] ls /kafka/brokers/ids[0, 1, 2][zk: zookeeper0:2182(CONNECTED) 3] 此处注意，因为docker-compose.yml文件里配置的kafka引用zookeeper根目录下的kafka目录，故在写zookeeper的时候要增加/kafka123456789[root@zhm1 docker-file]# docker exec -it kafka0 bash[root@kafka0 kafka_2.11-0.8.2.2]# source /root/.bash_profile[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --create --partitions 3 --replication-factor 1Created topic "topic1".[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --describe Topic:topic1 PartitionCount:3 ReplicationFactor:1 Configs: Topic: topic1 Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: topic1 Partition: 1 Leader: 2 Replicas: 2 Isr: 2 Topic: topic1 Partition: 2 Leader: 0 Replicas: 0 Isr: 0 kafka术语TopicTopic,是KAFKA对消息分类的依据;一条消息,必须有一个与之对应的Topic;比如现在又两个Topic,分别是TopicA和TopicB,Producer向TopicA发送一个消息messageA,然后向TopicB发送一个消息messaeB;那么,订阅TopicA的Consumer就会收到消息messageA,订阅TopicB的Consumer就会收到消息messaeB;(每个Consumer可以同时订阅多个Topic,也即是说,同时订阅TopicA和TopicB的Consumer可以收到messageA和messaeB)。同一个Group id的consumers在同一个Topic的同一条消息只能被一个consumer消费，实现了点对点模式，不同Group id的Consumers在同一个Topic上的同一条消息可以同时消费到，则实现了发布订阅模式。通过Consumer的Group id实现了JMS的消息模式 Partition每一个Topic可以有多个Partition,这样做是为了提高KAFKA系统的并发能力，每个Partition中按照消息发送的顺序保存着Producer发来的消息,每个消息用ID标识,代表这个消息在改Partition中的偏移量,这样,知道了ID,就可以方便的定位一个消息了;每个新提交过来的消息,被追加到Partition的尾部;如果一个Partition被写满了,就不再追加;(注意,KAFKA不保证不同Partition之间的消息有序保存) LeaderPartition中负责消息读写的节点;Leader是从Partition的节点中随机选取的。每个Partition都会在集中的其中一台服务器存在Leader。一个Topic如果有多个Partition，则会有多个Leader。 ReplicationFactor一个Partition中复制数据的所有节点,包括已经挂了的;数量不会超过集群中broker的数量 isrReplicationFactor的子集,存活的且和Leader保持同步的节点;leade会维护一个与其基本保持同步的Replica列表，该列表成为ISR（in-sync Replica）.如果一个Follower比leader落后太多，或者超过一段时间未发起数据复制请求（kafka的数据复制是follower的pull形式），则leader将其从ISR中移除 balabala..那么多，来个实例玩玩吧 打开另外一个终端，进入kafka1启动个消费者，此处要注意消费者路径也要指定kafka1[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginning 1[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-console-producer.sh --broker-list kafka0:9092 --topic topic3 输入hello world 另外一个终端显示12[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginninghello world 如果设置topic的时候设置log.cleanup.policy为compact，则消费的消息会被压缩]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[用git上传代码到码云]]></title>
    <url>%2F2016%2F12%2F13%2F%E7%A0%81%E4%BA%91%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[用git上传代码到码云 下载git客户端Window 下的安装从 http://git-scm.com/download 上下载window版的客户端，然后一直下一步下一步安装git即可，请注意，如果你不熟悉每个选项的意思，请保持默认的选项 git config –global user.name Jenickgit config –global user.email 258409707@qq.com 以下内容Wie转载：1.先在远程库创建项目，然后复制远程库里面的项目的地址； 2.打开命令行（电脑上用windows+R键），创建一个文件夹用于从远程库里克 隆所要更改的文件：命令为：mkdir + “目录名”；3.用cd + “目录名”跳转到所创建的目录名下，用命令git clone +远程库 文件地址；此时刷新所创建的目录，会发现新出现了一个文件夹，那就是 从远程库里面复制的文件目录（里面包含有后缀为.git的文件） 4.用cd 命令跳转到新的文件夹里，把所需要提交的文件复制到这个文件夹 里； 5.用git status命令能看到所改变了的文件目录列表，用git diff命令能看 到具体改变了的哪一行代码； 6.用命令git branch + “分支名”创建本地分支，用命令git branch可查看 目前所在分支，用 git checkout “你所创建的分支名”；跳转到你所创建的分 支； 7.用git add+(空格)+ “.”将所有的修改追加到文件中去； 8.用git commit -m“更改的文件备注”命令将你的文件提交到本地库； 9.用git checkout master 切换回到主分支； 10.用git pull origin master 命令将主分支从远程仓库里面拉过来； 11.用git merge+”你所创建的分支名”合并分支； 12.用git push origin master 命令将合并分支提交到远程库； 13.刷新一下远程库，就能发现所要提交的文件了 文／玄薛烨（简书作者）原文链接：http://www.jianshu.com/p/edf037f921c7著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka stream 实战3]]></title>
    <url>%2F2016%2F12%2F07%2Fkafka%20stream%20%E5%AE%9E%E6%88%983%2F</url>
    <content type="text"><![CDATA[场景： 每5秒输出过去1小时18岁到35岁用户所购买的商品中，每种品类销售额排名前十的订单汇总信息。 使用数据内的时间(Event Time)作为timestamp 每5秒输出一次 每次计算到输出为止过去1小时的数据 支持订单详情和用户详情的更新和增加 输出字段包含时间窗口（起始时间，结束时间），品类（category），商品名（item_name），销量（quantity），单价（price），总销售额，该商品在该品类内的销售额排名 随机创建数据的类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.io.*;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;import java.util.Random;/** * Created by Administrator on 2016/12/30. */public class createFile &#123; //c创建订单文件 public static void main(String[] args) throws IOException, ParseException &#123; FileOutputStream outSTr = null; FileOutputStream out = null; BufferedOutputStream Buff=null; outSTr = new FileOutputStream(new File("C:/add2.csv")); Buff=new BufferedOutputStream(outSTr); String[] a1 = &#123;"Jack", "Lily", "Mike", "Lucy", "LiLei", "HanMeimei"&#125;; Random rand = new Random(); String[] a2 = &#123;"iphone", "ipad", "iwatch", "ipod"&#125;; Calendar calendar = Calendar.getInstance(); String str="2016-11-11 00:00:00"; SimpleDateFormat sdf= new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); Date date =sdf.parse(str); calendar.setTime(date); System.out.println (calendar.getTime ()); SimpleDateFormat sdf2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); String dateStr = ""; int count = 0; while(! (count == 60*60*24))&#123;//86400 System.out.println(count); count++; System.out.println(dateStr); int num = rand.nextInt(a1.length); // System.out.println(a1[num]); int num2 = rand.nextInt(a2.length); // System.out.println(a2[num2]); calendar.add (Calendar.SECOND, 1); sdf2.format(calendar.getTime()); dateStr = sdf2.format(calendar.getTime()); // System.out.println (dateStr); Buff.write((a1[num]+","+a2[num2]+", "+dateStr+","+rand.nextInt(5)+"\r\n").getBytes()); Buff.flush(); &#125; &#125;&#125; 其他文件在实践1里有 编写DSL文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426import java.io.IOException;import java.util.ArrayList;import java.util.Collection;import java.util.Collections;import java.util.Comparator;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import org.apache.kafka.streams.kstream.TimeWindows;import org.apache.kafka.streams.kstream.Windowed;import com.jasongj.kafka.stream.model.Item;import com.jasongj.kafka.stream.model.Order;import com.jasongj.kafka.stream.model.User;import com.jasongj.kafka.stream.serdes.SerdesFactory;import com.jasongj.kafka.stream.timeextractor.OrderTimestampExtractor;/** * Created by root on 17-1-5. */public class topN &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items", "items-state-store"); KStream&lt;Windowed&lt;String&gt;, GroupInfo&gt; kStream = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; (orderUser.userAddress != null &amp;&amp; (orderUser.getAge() &gt;= 18 &amp;&amp; orderUser.getAge() &lt;= 35 ))) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;String, GroupInfo&gt;pair((orderUserItem.itemType), new GroupInfo(orderUserItem.itemType, new ArrayList&lt;GroupItemInfo&gt;()&#123; private static final long serialVersionUID = 1L; &#123; add(new GroupItemInfo(orderUserItem.transactionDate, orderUserItem.itemName, orderUserItem.quantity, orderUserItem.itemPrice, (orderUserItem.quantity * orderUserItem.itemPrice))); &#125;&#125;) )) .groupByKey(Serdes.String(), SerdesFactory.serdFrom(GroupInfo.class)) .reduce((GroupInfo v1, GroupInfo v2) -&gt; &#123; GroupInfo v3 = new GroupInfo(v1.getItemType()); List&lt;GroupItemInfo&gt; newItemlist = new ArrayList&lt;GroupItemInfo&gt;(); newItemlist.addAll(v1.getItemList()); newItemlist.addAll(v2.getItemList()); v3.setItemList(newItemlist); return v3; &#125; , TimeWindows.of(1000 * 60 * 60).advanceBy(1000 * 5) , "gender-amount-state-store").toStream(); kStream.map((Windowed&lt;String&gt; window, GroupInfo groupInfo) -&gt; &#123; return new KeyValue&lt;String, String&gt;(window.key(), groupInfo.printTop10(window.window().start(), window.window().end())); &#125;).to(Serdes.String(), Serdes.String(), "gender-amount"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class GroupInfo&#123; private String itemType; private List&lt;GroupItemInfo&gt; itemList; public GroupInfo()&#123; &#125; public GroupInfo(String itemType)&#123; this.itemType = itemType; &#125; public GroupInfo(String itemType, List&lt;GroupItemInfo&gt; itemList)&#123; this.itemType = itemType; this.itemList = itemList; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public List&lt;GroupItemInfo&gt; getItemList() &#123; return itemList; &#125; public void setItemList(List&lt;GroupItemInfo&gt; itemList) &#123; this.itemList = itemList; &#125; /** * 根据金额汇总倒序 * @param allItems * @return */ private List&lt;GroupItemInfo&gt; sortBySumDesc(Collection&lt;GroupItemInfo&gt; allItems)&#123; List&lt;GroupItemInfo&gt; result = new ArrayList&lt;GroupItemInfo&gt;(); result.addAll(allItems); Collections.sort(result, new Comparator&lt;GroupItemInfo&gt;()&#123; @Override public int compare(GroupItemInfo o1, GroupItemInfo o2) &#123; if(o1.getSum() == o2.getSum())&#123; return 0; &#125;else if(o1.getSum() &gt; o2.getSum())&#123; return -1; &#125;else&#123; return 1; &#125; &#125; &#125;); return result; &#125; /** * 找回前10名 * @param startDate * @param endDate * @return */ public String printTop10(long startDate, long endDate)&#123; double allAmount = 0.0; Map&lt;String, GroupItemInfo&gt; groupMap = new HashMap&lt;String, GroupItemInfo&gt;(); for(GroupItemInfo item : itemList)&#123; String key = item.getItemName(); allAmount += item.getSum(); if(groupMap.containsKey(key))&#123; GroupItemInfo oldItem = groupMap.get(key); oldItem.setCount(oldItem.getCount() + item.getCount()); oldItem.setSum(oldItem.getSum() + item.getSum()); &#125;else&#123; groupMap.put(key, item); &#125; &#125; List&lt;GroupItemInfo&gt; sortedResult = sortBySumDesc(groupMap.values()); StringBuffer sb = new StringBuffer(); for(int i = 1; i &lt;= 10 ; i++)&#123; if(sortedResult.size() &gt;= i &amp;&amp; sortedResult.get(i-1) != null)&#123; GroupItemInfo oneItem = sortedResult.get(i-1); sb.append(startDate).append(",").append(endDate).append(",").append(itemType).append(",").append(oneItem.getItemName()).append(",") .append(oneItem.getCount()).append(",").append(oneItem.getPrice()).append(",").append(oneItem.getSum()).append(",").append(allAmount) .append(",").append(i).append("\n"); &#125;else&#123; break; &#125; &#125; return sb.toString(); &#125; &#125; private static class GroupItemInfo&#123; private long transactionDate; private String itemName; private int count; private double price; private double sum; public GroupItemInfo()&#123; &#125; public GroupItemInfo(long transactionDate, String itemName, int count, double price, double sum) &#123; this.transactionDate = transactionDate; this.itemName = itemName; this.count = count; this.price = price; this.sum = sum; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; public double getSum() &#123; return sum; &#125; public void setSum(double sum) &#123; this.sum = sum; &#125; &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if(order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if(user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if(orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if(item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka stream 实战2]]></title>
    <url>%2F2016%2F12%2F06%2Fkafka%20stream%20%E5%AE%9E%E6%88%982%2F</url>
    <content type="text"><![CDATA[场景： 在上一个示例（算出用户与商品同地址的订单中，男女分别总共花了多少钱）的基础上，算出不同地区（用户地址），不同性别的订单数及商品总数和总金额。输出结果schema如下地区（用户地区，如SH），性别，订单总数，商品总数，总金额 示例输出 SH, male, 3, 4, 188888.88 BJ, femail, 5, 8, 288888.88 推演过程： 实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741package com.jasongj.kafka.stream;import java.io.IOException;import java.util.Properties;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import com.jasongj.kafka.stream.model.Item;import com.jasongj.kafka.stream.model.Order;import com.jasongj.kafka.stream.model.User;import com.jasongj.kafka.stream.serdes.SerdesFactory;import com.jasongj.kafka.stream.timeextractor.OrderTimestampExtractor;/** * Created by root on 17-1-5. */public class Di9KeZuoYe &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items", "items-state-store"); KTable&lt;AddrSex, OrderMoney&gt; kTable = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != null) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .filter((String item, OrderUserItem orderUserItem) -&gt; StringUtils.compare(orderUserItem.userAddress, orderUserItem.itemAddress) == 0) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;AddrSex, OrderMoney&gt;pair(new AddrSex(orderUserItem.userAddress, orderUserItem.gender), OrderMoney.fromItem(orderUserItem.quantity, orderUserItem.itemPrice))) .groupByKey(SerdesFactory.serdFrom(AddrSex.class), SerdesFactory.serdFrom(OrderMoney.class)) .reduce((OrderMoney v1, OrderMoney v2) -&gt; new OrderMoney(v1.orderNum + v2.orderNum, v1.itemNum + v2.itemNum, v1.TotalMoney + v2.TotalMoney), "gender-amount-state-store"); kTable .toStream() .map((AddrSex addrSex, OrderMoney orderMoney) -&gt; new KeyValue&lt;String, String&gt;(addrSex.toString(), orderMoney.toString())) .to("gender-amount"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if (order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if (user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if (orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if (item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125; public static class AddrSex &#123; private String addr; private String gender; public AddrSex() &#123; &#125; public AddrSex(String addr, String gender) &#123; this.addr = addr; this.gender = gender; &#125; public String getAddr() &#123; return addr; &#125; public void setAddr(String addr) &#123; this.addr = addr; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; AddrSex addrSex = (AddrSex) o; if (!addr.equals(addrSex.addr)) return false; return gender.equals(addrSex.gender); &#125; @Override public int hashCode() &#123; int result = addr.hashCode(); result = 31 * result + gender.hashCode(); return result; &#125; @Override public String toString() &#123; return addr + " " + gender; &#125; &#125; public static class OrderMoney &#123; private int orderNum; private int itemNum; private Double TotalMoney; public OrderMoney() &#123; &#125; public OrderMoney(int orderNum, int itemNum, Double totalMoney) &#123; this.orderNum = orderNum; this.itemNum = itemNum; TotalMoney = totalMoney; &#125; public int getOrderNum() &#123; return orderNum; &#125; public void setOrderNum(int orderNum) &#123; this.orderNum = orderNum; &#125; public int getItemNum() &#123; return itemNum; &#125; public void setItemNum(int itemNum) &#123; this.itemNum = itemNum; &#125; public Double getTotalMoney() &#123; return TotalMoney; &#125; public void setTotalMoney(Double totalMoney) &#123; TotalMoney = totalMoney; &#125; public static OrderMoney fromItem(int quantity, Double itemPrice) &#123; OrderMoney orderMoney = new OrderMoney(); orderMoney.setOrderNum(1); orderMoney.setItemNum(quantity); orderMoney.setTotalMoney((double) quantity * itemPrice); return orderMoney; &#125; @Override public String toString() &#123; return orderNum + " " + itemNum + " " + TotalMoney; &#125; &#125;&#125; 输出结果]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka stream 实战]]></title>
    <url>%2F2016%2F12%2F05%2Fkafka%20stream%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[场景：目前有三个表：产品表，用户表，订单表。求性别平均消费金额 数据：产品表1234iphone, BJ, phone, 5388.88ipad, SH, pad, 4888.88iwatch, SZ, watch, 2668.88ipod, GZ, pod, 1888.88 用户表1234Jack, BJ, male, 23Lily, SH, female, 21Mike, SZ, male, 22Lucy, GZ, female, 20 订单表12345678910111213141516171819Jack, iphone, 2016-11-11 00:00:01, 3Jack, ipad, 2016-11-11 00:00:02, 4Jack, iwatch, 2016-11-11 00:00:03, 5Jack, ipod, 2016-11-11 00:00:04, 4Lily, ipad, 2016-11-11 00:00:06, 3Lily, iwatch, 2016-11-11 00:00:07, 4Lily, iphone, 2016-11-11 00:00:08, 2Lily, ipod, 2016-11-11 00:00:09, 3Mike, ipad, 2016-11-11 00:00:11, 2Mike, iwatch, 2016-11-11 00:00:12, 3Mike, iphone, 2016-11-11 00:00:13, 4Mike, ipod, 2016-11-11 00:00:14, 3Lucy, ipod, 2016-11-11 00:00:16, 3Lucy, ipad, 2016-11-11 00:00:17, 4Lucy, iwatch, 2016-11-11 00:00:18, 3Lucy, iphone, 2016-11-11 00:00:19, 5 创建打印类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.Arrays;import java.util.Properties;import java.util.concurrent.atomic.AtomicLong;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.DoubleDeserializer;import org.apache.kafka.common.serialization.StringDeserializer;public class DemoConsumerManualCommit &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; "jenick.com:9092", "gender-amount2", "group4", "consumer2" &#125;; if (args == null || args.length != 4) &#123; System.err.println( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"); System.exit(1); &#125; String bootstrap = args[0]; String topic = args[1]; String groupid = args[2]; String clientid = args[3]; Properties props = new Properties(); props.put("bootstrap.servers", bootstrap); props.put("group.id", groupid); props.put("enable.auto.commit", "false"); props.put("key.deserializer", StringDeserializer.class.getName()); props.put("value.deserializer", DoubleDeserializer.class.getName()); props.put("max.poll.interval.ms", "300000"); props.put("max.poll.records", "500"); props.put("auto.offset.reset", "earliest"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(topic)); AtomicLong atomicLong = new AtomicLong(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); records.forEach(record -&gt; &#123; System.out.printf("client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); if (atomicLong.get() % 10 == 0) &#123;// consumer.commitSync(); &#125; &#125;); &#125; &#125;&#125; TimestampExtractor12345678910111213141516171819202122232425262728293031323334import java.time.LocalDateTime;import java.time.ZoneOffset;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.streams.processor.TimestampExtractor;import com.fasterxml.jackson.databind.JsonNode;import com.kafka.stream.model.Item;import com.kafka.stream.model.Order;import com.kafka.stream.model.User;public class OrderTimestampExtractor implements TimestampExtractor &#123; @Override public long extract(ConsumerRecord&lt;Object, Object&gt; record) &#123; Object value = record.value(); if (record.value() instanceof Order) &#123; Order order = (Order) value; return order.getTransactionDate(); &#125; if (value instanceof JsonNode) &#123; return ((JsonNode) record.value()).get("transactionDate").longValue(); &#125; if (value instanceof Item) &#123; return LocalDateTime.of(2015, 12,11,1,0,10).toEpochSecond(ZoneOffset.UTC) * 1000; &#125; if (value instanceof User) &#123; return LocalDateTime.of(2015, 12,11,0,0,10).toEpochSecond(ZoneOffset.UTC) * 1000; &#125; return LocalDateTime.of(2015, 11,10,0,0,10).toEpochSecond(ZoneOffset.UTC) * 1000;// throw new IllegalArgumentException("OrderTimestampExtractor cannot recognize the record value " + record.value()); &#125;&#125; 创建stream处理类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271import java.io.IOException;import java.util.Properties;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import com.kafka.stream.model.Item;import com.kafka.stream.model.Order;import com.kafka.stream.model.User;import com.kafka.stream.serdes.SerdesFactory;import com.kafka.stream.timeextractor.OrderTimestampExtractor;public class PurchaseAnalysis &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items2", "items-state-store");// itemTable.toStream().foreach((String itemName, Item item) -&gt; System.out.printf("Item info %s-%s-%s-%s\n", item.getItemName(), item.getAddress(), item.getType(), item.getPrice())); KTable&lt;String, Double&gt; kTable = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != null) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item2") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .filter((String item, OrderUserItem orderUserItem) -&gt; StringUtils.compare(orderUserItem.userAddress, orderUserItem.itemAddress) == 0)// .foreach((String itemName, OrderUserItem orderUserItem) -&gt; System.out.printf("%s-%s-%s-%s\n", itemName, orderUserItem.itemAddress, orderUserItem.userName, orderUserItem.userAddress)) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;String, Double&gt;pair(orderUserItem.gender, (Double)(orderUserItem.quantity * orderUserItem.itemPrice))) .groupByKey(Serdes.String(), Serdes.Double()) .reduce((Double v1, Double v2) -&gt; v1 + v2, "gender-amount-state-store");// kTable.foreach((str, dou) -&gt; System.out.printf("%s-%s\n", str, dou)); kTable .toStream() .map((String gender, Double total) -&gt; new KeyValue&lt;String, String&gt;(gender, String.valueOf(total))) .to("gender-amount2"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if(order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if(user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if(orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if(item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125;&#125; Hash分区123456789101112131415161718192021222324252627282930313233343536import java.util.List;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import org.apache.kafka.common.PartitionInfo;public class HashPartitioner implements Partitioner &#123; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes != null) &#123; int hashCode = 0; if (key instanceof Integer || key instanceof Long) &#123; hashCode = (int) key; &#125; else &#123; hashCode = key.hashCode(); &#125; hashCode = hashCode &amp; 0x7fffffff; return hashCode % numPartitions; &#125; else &#123; return 0; &#125; &#125; @Override public void close() &#123; &#125;&#125; 序列化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.io.IOException;import java.util.Map;import org.apache.kafka.common.errors.SerializationException;import org.apache.kafka.common.serialization.Deserializer;import com.fasterxml.jackson.databind.ObjectMapper;public class GenericDeserializer&lt;T&gt; implements Deserializer&lt;T&gt; &#123; private Class&lt;T&gt; type; private ObjectMapper objectMapper = new ObjectMapper(); public GenericDeserializer() &#123;&#125; public GenericDeserializer(Class&lt;T&gt; type) &#123; this.type = type; &#125; @SuppressWarnings("unchecked") @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; if(type != null) &#123; return; &#125; String typeProp = isKey ? "key.deserializer.type" : "value.deserializer.type"; String typeName = (String)configs.get(typeProp); try &#123; type = (Class&lt;T&gt;)Class.forName(typeName); &#125; catch (Exception ex) &#123; throw new SerializationException("Failed to initialize GenericDeserializer for " + typeName, ex); &#125; &#125; @Override public T deserialize(String topic, byte[] data) &#123; if (data == null) &#123; return null; &#125; try &#123; return this.objectMapper.readValue(data, type); &#125; catch (IOException ex) &#123; throw new SerializationException(ex); &#125; &#125; @Override public void close() &#123; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.io.IOException;import java.util.Map;import org.apache.kafka.common.errors.SerializationException;import org.apache.kafka.common.serialization.Serde;import org.apache.kafka.common.serialization.Serializer;import com.fasterxml.jackson.databind.ObjectMapper;import com.kafka.stream.model.User;public class GenericSerializer&lt;T&gt; implements Serializer&lt;T&gt; &#123; private Class&lt;T&gt; type; private ObjectMapper objectMapper = new ObjectMapper(); public GenericSerializer() &#123;&#125; public GenericSerializer(Class&lt;T&gt; type) &#123; this.type = type; &#125; @SuppressWarnings("unchecked") @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; if(type != null) &#123; return; &#125; String typeProp = isKey ? "key.serializer.type" : "value.serializer.type"; String typeName = (String)configs.get(typeProp); try &#123; type = (Class&lt;T&gt;)Class.forName(typeName); &#125; catch (Exception ex) &#123; throw new SerializationException("Failed to initialize GenericSerializer for " + typeName, ex); &#125; &#125; @Override public byte[] serialize(String topic, T object) &#123; if (object == null) &#123; return null; &#125; try &#123; return this.objectMapper.writerFor(type).writeValueAsBytes(object); &#125; catch (IOException ex) &#123; throw new SerializationException(ex); &#125; &#125; @Override public void close() &#123; &#125;&#125; 1234567891011121314151617import org.apache.kafka.common.serialization.Serde;import org.apache.kafka.common.serialization.Serdes;public class SerdesFactory &#123; /** * @param &lt;T&gt; The class should have a constructor without any * arguments and have setter and getter for every member variable * @param pojoClass POJO class. * @return Instance of &#123;@link Serde&#125; */ public static &lt;T&gt; Serde&lt;T&gt; serdFrom(Class&lt;T&gt; pojoClass) &#123; return Serdes.serdeFrom(new GenericSerializer&lt;T&gt;(pojoClass), new GenericDeserializer&lt;T&gt;(pojoClass)); &#125;&#125; bean对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Item &#123; private String itemName; private String address; private String type; private double price; public Item() &#123;&#125; public Item(String itemName, String address, String type, double price) &#123; this.itemName = itemName; this.address = address; this.type = type; this.price = price; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getType() &#123; return type; &#125; public void setType(String type) &#123; this.type = type; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Order &#123; private String userName; private String itemName; private long transactionDate; private int quantity; public Order() &#123;&#125; public Order(String userName, String itemName, long transactionDate, int quantity) &#123; this.userName = userName; this.itemName = itemName; this.transactionDate = transactionDate; this.quantity = quantity; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class User &#123; private String name; private String address; private String gender; private int age; public User() &#123;&#125; public User(String name, String address, String gender, int age) &#123; this.name = name; this.address = address; this.gender = gender; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 用户生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.IOException;import java.nio.charset.Charset;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.User;import com.kafka.stream.serdes.GenericSerializer;public class UserProducer &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", User.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, User&gt; producer = new KafkaProducer&lt;String, User&gt;(props); List&lt;User&gt; users = readUser(); users.forEach((User user) -&gt; producer.send(new ProducerRecord&lt;String, User&gt;("users", user.getName(), user))); producer.close(); &#125; public static List&lt;User&gt; readUser() throws IOException &#123; List&lt;String&gt; lines = IOUtils.readLines(OrderProducer.class.getResourceAsStream("/users.csv"), Charset.forName("UTF-8")); List&lt;User&gt; users = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new User(values[0], values[1], values[2], Integer.parseInt(values[3]))) .collect(Collectors.toList()); return users; &#125;&#125; 产品生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.IOException;import java.nio.charset.Charset;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.Item;import com.kafka.stream.serdes.GenericSerializer;public class ItemProducer &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", Item.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, Item&gt; producer = new KafkaProducer&lt;String, Item&gt;(props); List&lt;Item&gt; items = readItem(); items.forEach((Item item) -&gt; producer.send(new ProducerRecord&lt;String, Item&gt;("items2", item.getItemName(), item))); producer.close(); &#125; public static List&lt;Item&gt; readItem() throws IOException &#123; List&lt;String&gt; lines = IOUtils.readLines(OrderProducer.class.getResourceAsStream("/items.csv"), Charset.forName("UTF-8")); List&lt;Item&gt; items = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new Item(values[0], values[1], values[2], Double.parseDouble(values[3]))) .collect(Collectors.toList()); return items; &#125;&#125; 订单生产者123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.io.IOException;import java.io.InputStream;import java.nio.charset.Charset;import java.time.LocalDateTime;import java.time.ZoneOffset;import java.time.format.DateTimeFormatter;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.Order;import com.kafka.stream.serdes.GenericSerializer;public class OrderProducer &#123; private static DateTimeFormatter dataTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", Order.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, Order&gt; producer = new KafkaProducer&lt;String, Order&gt;(props); List&lt;Order&gt; orders = readOrder(); orders.forEach((Order order) -&gt; producer.send(new ProducerRecord&lt;String, Order&gt;("orders", order.getUserName(), order))); producer.close(); &#125; public static List&lt;Order&gt; readOrder() throws IOException &#123; InputStream inputStream = OrderProducer.class.getResourceAsStream("/orders.csv"); List&lt;String&gt; lines = IOUtils.readLines(inputStream, Charset.forName("UTF-8")); List&lt;Order&gt; orders = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new Order(values[0], values[1], LocalDateTime.parse(values[2], dataTimeFormatter).toEpochSecond(ZoneOffset.UTC) * 1000, Integer.parseInt(values[3]))) .collect(Collectors.toList()); return orders; &#125;&#125; 需要创建topic如下：12345/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic orders/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic items/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic users/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic gender-amount/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic orderuser-repartition-by-item 首先启动DemoConsumerManualCommit，然后启动PurchaseAnalysis先把table的数据创建进去UserProducer、ItemProducer最后启动stream的OrderProducer 运行结果： 12client : consumer2 , topic: gender-amount2 , partition: 0 , offset = 0, key = male, value = 7.489721245848924E-67client : consumer2 , topic: gender-amount2 , partition: 2 , offset = 0, key = female, value = 6.008913963681483E-67]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[codis ha]]></title>
    <url>%2F2016%2F12%2F03%2Fcodis%20ha%2F</url>
    <content type="text"><![CDATA[启动多个实例：cd /usr/local/codis/src/github.com/CodisLabs/codis[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf –protected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf –protected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6381.conf –protected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6382.conf –protected-mode no 启动zookeeper[root@zhm1 codis]# zkServer.sh start 启动dashboard[root@zhm1 codis]# zkServer.sh startnohup bin/codis-dashboard –ncpu=2 –config=dashboard.conf –log=dashboard.log –log-level=WARN &amp; 启动codis-proxy[root@zhm1 codis]# nohup bin/codis-proxy –ncpu=2 –config=proxy.conf –log =proxy.log –log-level=WARN &amp; 启动codis-fe[root@zhm1 codis]# ./bin/codis-fe –ncpu=2 –log=fe.log –log-level=WARN –dashboard-list=conf/codis.json –listen=192.168.110.129:18090 &amp; 将实例添加到group 分配slot 启动codis-hanohup ./bin/codis-ha –log=ha.log –log-level=WARN –dashboard=127.0.0.1:18080 &amp; [root@zhm1 codis]# ps -ef | grep codisroot 3096 2884 0 09:51 pts/0 00:00:32 bin/codis-dashboard –ncpu=2 –config=dashboard.conf –log=dashboard.log –log-level=WARNroot 3114 2884 0 09:52 pts/0 00:00:04 bin/codis-proxy –ncpu=2 –config=proxy.conf –log =proxy.log –log-level=WARNroot 3149 2884 0 09:53 pts/0 00:00:02 ./bin/codis-fe –ncpu=2 –log=fe.log –log-level=WARN –dashboard-list=conf/codis.json –listen=192.168.110.129:18090root 3175 1 0 09:54 ? 00:00:08 ./bin/codis-server :6379root 3179 1 0 09:54 ? 00:00:08 ./bin/codis-server :6380root 3888 1 0 10:46 ? 00:00:02 ./bin/codis-server :6382root 3892 1 0 10:46 ? 00:00:02 ./bin/codis-server :6381root 4680 2884 0 11:12 pts/0 00:00:00 ./bin/codis-ha –log=ha.log –log-level=WARN –dashboard=127.0.0.1:18080root 4712 2884 0 11:15 pts/0 00:00:00 grep –color=auto codis kill6379 的端口kill -9 3175查看6380 的日志3179:M 23 Dec 11:19:07.739 * MASTER MODE enabled (user request from ‘id=7 addr=192.168.110.129:51438 fd=8 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof’)看到6380成为主 再启动6379 并没有成为6380的主，需要手动点击web上绿色的工具扳手怀疑可能是我的配置文件没有写slave of的原因 再6380输入内容[root@zhm1 codis]# ./bin/redis-cli -p 6380127.0.0.1:6380&gt; set aaa 123OK127.0.0.1:6380&gt; 启动6379看[root@zhm1 codis]# ./bin/redis-cli -p 6379127.0.0.1:6379&gt; get aaa“123” 总结codis和redis集群的区别 a. Redis Cluster 的集群信息存储在每个集群节点上，而Codis 集群的信息存储在一个独立的存储系统（Zookeeper）里。b. 外部访问集群：对Redis Cluster，直接通过以集群模式启动的redis 客户端 “redis-cli -c”来访问。对 Codis 集群通过独立的 codis-proxy 节点来访问。c. Redis Cluster 有 16384 个slot 可以分配，而 Codis 集群只有1024 个slot 可以分配。d. 监控和操作：Codis 集群有图形化的 Code FE 管理工具（可以完成Codis Proxy，Codis Group、Codis Server 的添加和删除，分配 Slot、提升Slave 为Master 等操作），而 Redis Cluster 好像没有这类工具。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka Stream]]></title>
    <url>%2F2016%2F12%2F02%2FKafkaStream%2F</url>
    <content type="text"><![CDATA[以下内容摘自官网，熟悉的同学可跳过 Use Kafka Streams to process dataKafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the WordCountDemo example code (converted to use Java 8 lambda expressions for easy reading).Kafka Streams是Kafka中用于客户端的库，主要用于获取实时流处理以及分析Kafka brokers中存储的数据。这个例子将会展示如何使用这个库来运行一个流式处理应用。这里有一个WordCountDemo的主要代码（转换成Java8 lambda表达式更易读）： 123456789KTable wordCounts = textLines // Split each text line, by whitespace, into words. .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split("\\W+"))) // Ensure the words are available as record keys for the next aggregate operation. .map((key, value) -&gt; new KeyValue&lt;&gt;(value, value)) // Count the occurrences of each word (record key) and store the results into a table named "Counts". .countByKey("Counts") It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on an infinite, unbounded stream of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data. 它实现了WordCount算法，计算了输入文本中的词频。然而，并不像其他的WordCount的例子，都是计算固定大小的数据，这个WordCount demo应用稍微有点不同，它是基于不会终止的数据流计算的。和计算固定数据的模型比较形似的是，它也会不停的更新词频计算结果。然而，由于它是基于永不停止的数据流，所以会周期性的输出当前的计算结果，他会不停的处理更多的数据，因为它也不知道何时它处理过“所有”的输入数据。 We will now prepare input data to a Kafka topic, which will subsequently be processed by a Kafka Streams application.现在我们将输入数据导入Kafka topic，这些数据将会被Kafka Streams应用处理 1&gt; echo -e "all streams lead to kafka\nhello kafka streams\njoin kafka summit" &gt; file-input.txt Or on Windows:123&gt; echo all streams lead to kafka&gt; file-input.txt&gt; echo hello kafka streams&gt;&gt; file-input.txt&gt; echo|set /p=join kafka summit&gt;&gt; file-input.txt Next, we send this input data to the input topic named streams-file-input using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):接着，我们使用终端producer来将这些输入数据发送到名为streams-file-input的topic（在实践中，流数据会持续不断的流入kafka，当应用将会启动并运行时）：12345&gt; bin/kafka-topics.sh --create \ --zookeeper localhost:2181 \ --replication-factor 1 \ --partitions 1 \ --topic streams-file-input 1&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input &lt; file-input.txt We can now run the WordCount demo application to process the input data:我们可以运行WordCount demo应用来处理输入数据1&gt; bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo There won’t be any STDOUT output except log entries as the results are continuously written back into another topic named streams-wordcount-output in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.不会有任何的stdout输出除了日志条目，结果会持续不断的写回kafka中另一个名为streams-wordcount-output的topic。这个demo将会运行数秒，不会像典型的流处理应用，自动终止。 We can now inspect the output of the WordCount demo application by reading from its output topic:我们现在通过阅读主题的数来来检查WordCount demo应用程序的输出： 12345678&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \ --topic streams-wordcount-output \ --from-beginning \ --formatter kafka.tools.DefaultMessageFormatter \ --property print.key=true \ --property print.value=true \ --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \ --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer with the following output data being printed to the console:终端会打印出以下数据：12345678all 1lead 1to 1hello 1streams 2join 1kafka 3summit 1 Here, the first column is the Kafka message key, and the second column is the message value, both in in java.lang.String format. Note that the output is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word, aka record key such as “kafka”. For multiple records with the same key, each later record is an update of the previous one.第一列是Kafka消息的key，第二列是消息value，两者都是java.lang.String格式。注意，输出实际上应该是持续的更新数据流，数据流中的每一个记录（例如，上面输出的每一行）都是一个单独词汇的数量，或者是记录了key的数量，例如上面的“kafka”。对于多条记录的key一致这种情况，每一条后面的记录都是对前一条记录的更新。 Now you can write more input messages to the streams-file-input topic and observe additional messages added to streams-wordcount-output topic, reflecting updated word counts (e.g., using the console producer and the console consumer, as described above).现在你可以写入更多的消息到streams-file-input这个topic，可以观察到更多的消息会发送到streams-wordcount-output这个topic，反映了更新之后的词汇数量。 You can stop the console consumer via Ctrl-C.你可以使用Ctrl+C结束控制台的消费者 Stream ProcessingMany users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might normalize or deduplicate this content and published the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza.很多用户将kafka用作多级数据处理之间的消息管道：原始数据存放于Kafka不同的topics中，然后经过聚合、增强、或者其他的转换之后，导入Kafka新的topics中，以供后面的消费。例如，对于新闻推荐的处理流程来所：首先从RSS信息流中获取文章内容，然后导入名为“articles”的topic; 其次，后面的处理可能是对这些内容进行规范化或者精简操作，然后将经过上述处理的内容导入新的topic;最后的处理可能是试图将这些内容推荐给用户。这样的处理流程实际展现了实时流在独立的topics之间流动的流程图。从0.10.0.0开始，Apache Kafka推出了一款称为Kafka Streams的流式处理库，优点是轻量级同时性能很好，它可以完成上面所描述的多级处理。除了Kafka streams之外，还有一些开源流式处理工具可以选用，包括Apache Storm和Samza。 OverviewKafka Streams is a client library for processing and analyzing data stored in Kafka and either write the resulting data back to Kafka or send the final output to an external system. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management of application state. Kafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka’s parallelism model.Kafka Streams是一个客户端程序库用于处理和分析存储在Kafka中的数据，并将得到的数据写入Kafka或发送最终输出到外部系统。它建立在如适当区分事件的时间和加工时间，窗口函数的支持，和简单而高效的应用程序状态管理。Kafka Streams有一个低门槛进入：你可以快速编写和运行一个小规模的概念证明在一台机器上，你只需要运行在多台机器上的应用程序的额外的实例扩展到高容量的生产工作负载。Kafka Streams 透明地处理相同的应用程序通过利用Kafka 的并行模型的多个实例的负载平衡。 Some highlights of Kafka Streams:Kafka Streams的一些亮点 Designed as a simple and lightweight client library, which can beeasily embedded in any Java application and integrated with anyexisting packaging, deployment and operational tools that users havefor their streaming applications. 作为一个简单而轻量级的客户端库，它可以方便的嵌入任何java应用和集成任何现有的包，部署和运营工具，用户有对于他们的流应用。 Has no external dependencies on systems other than Apache Kafkaitself as the internal messaging layer; notably, it uses Kafka’spartitioning model to horizontally scale processing while maintainingstrong ordering guarantees. 对其他比Apache Kafka 没有外部依赖它本身作为内部消息层，特别是，它使用Kafka的 分割模型在保持同时进行水平缩放处理的分区模型强排序保证。 Supports fault-tolerant local state, which enables very fast andefficient stateful operations like joins and windowed aggregations. 支持容错的本地状态，使非常快速和高效的状态操作的加入和窗口聚集。 Employs one-record-at-a-time processing to achieve low processinglatency, and supports event-time based windowing operations. 采用同一时刻只有一条记录处理实现低的处理延迟，并支持基于时间事件的窗口操作。 Offers necessary stream processing primitives, along with ahigh-level Streams DSL and a low-level Processor API. 提供必要的流处理基元，以及 high-level Streams DSL和 low-level Processor API。 Developer Guide 开发者指南There is a quickstart example that provides how to run a stream processing program coded in the Kafka Streams library. This section focuses on how to write, configure, and execute a Kafka Streams application. 有一个快速入门示例提供了如何运行一个流处理程序在卡夫卡流的库代码。本节重点介绍如何编写、配置和执行卡夫卡流应用程序。 Core Concepts 核心概念We first summarize the key concepts of Kafka Streams.我们首先总结了Kafka Streams的关键概念。 Stream Processing Topology 流处理Topology A stream is the most important abstraction provided by Kafka Streams:it represents an unbounded, continuously updating data set. A streamis an ordered, replayable, and fault-tolerant sequence of immutabledata records, where a data record is defined as a key-value pair. 流是Kafka Streams提供的最重要的抽象：它表示一个无界的，不断更新的数据集。一个流是一个有序的、可重复的，和不变的容错序列数据记录，其中一个数据记录被定义为一个键值对。 A stream processing application written in Kafka Streams defines itscomputational logic through one or more processor topologies, where aprocessor topology is a graph of stream processors (nodes) that areconnected by streams (edges).在Kafka Streams中写的流处理应用程序定义了它的计算逻辑通过一个或多个处理器的topologies，其中处理器的topology是一个流处理器（节点）的图形由流连接（边缘）。 A stream processor is a node in the processor topology; it representsa processing step to transform data in streams by receiving one inputrecord at a time from its upstream processors in the topology,applying its operation to it, and may subsequently producing one ormore output records to its downstream processors. 流处理器是处理器topology中的一个节点；它表示通过接收一个输入来变换流中的数据的处理步骤在topology中的上游处理器上记录的时间，应用它的操作，并可能随后产生一个或向下游处理器的更多输出记录。 Kafka Streams offers two ways to define the stream processing topology: the Kafka Streams DSL provides the most common data transformation operations such as map and filter; the lower-level Processor API allows developers define and connect custom processors as well as to interact with state stores.Kafka Streams 提供了两种方式来定义流处理topology：Kafka Streams DSL提供了最常用的数据转换操作，如map和filter；lower-level Processor API允许开发者定义和连接定制处理器以及存储交互的状态。 Time 时间A critical aspect in stream processing is the notion of time, and how it is modeled and integrated. For example, some operations such as windowing are defined based on time boundaries.流处理中的一个关键方面是时间的概念，以及它是如何建模和集成。例如，一些操作如窗口是基于时间边界的定义。 Common notions of time in streams are:流中的时间的共同概念是： Event time - The point in time when an event or data record occurred,i.e. was originally created “at the source”. 事件时间 - 当发生事件或数据记录时的时间点，即最初创建的“在源头上”。 Processing time - The point in time when the event or data recordhappens to be processed by the stream processing application, i.e.when the record is being consumed. The processing time may bemilliseconds, hours, or days etc. later than the original event time. 处理时间 - 事件或数据记录的时间点碰巧被流处理应用程序处理，即当记录被消耗。处理时间可能是比原始事件时间晚的毫秒数、小时或数天等。 Ingestion time - The point in time when an event or data record isstored in a topic partition by a Kafka broker. The difference toevent time is that this ingestion timestamp is generated when therecord is appended to the target topic by the Kafka broker, not whenthe record is created “at the source”. The difference to processingtime is that processing time is when the stream processingapplication processes the record. For example, if a record is neverprocessed, there is no notion of processing time for it, but it stillhas an ingestion time. 摄取时间 - 当一个事件或数据记录的时间点存储在Kafka broker的主题分区中。事件时间不同的是，这种摄取时间戳时产生的记录追加到目标主题由Kafka broker ，而不是当记录是在“源”创建的。处理差异时间是处理时间的时候是流处理的应用程序处理记录。例如，如果一个记录是从来没有处理，没有处理时间的概念，但它仍然有一个摄取时间 The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka’s configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps.事件的时间和摄取时间之间的选择实际上是通过Kafka的配置（不是Kafka Streams）：从Kafka0.10.x起，时间戳被自动嵌入到Kafka的消息中。根据Kafka的配置这些时间戳表示事件时间或摄取时间。各自的Kafka配置设置可以在broker级别或每个主题上指定。在Kafka Streams的默认时间戳提取器将检索这些嵌入时间戳as-is。因此，您的应用程序的有效时间语义依赖于有效的Kafka配置这些嵌入时间戳。 Kafka Streams assigns a timestamp to every data record via the TimestampExtractor interface. Concrete implementations of this interface may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce different notions of time depending on their business needs. For example, per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins.Kafka Streams分配一个时间戳的每一个数据记录通过TimestampExtractor接口。这个接口的具体实现可以检索或计算基于数据记录如嵌入时间戳字段提供事件时间语义内容的时间戳，或使用任何其他的方法，如加工时返回当前时钟时间，从而产生语义流处理应用程序的处理时间。因此，开发人员可以执行不同的时间概念，这取决于他们的业务需求。例如，每个记录时间戳的描述关于流时间的进展（虽然记录可能会超出流）和促使时间依赖操作例如 joins。 Finally, whenever a Kafka Streams application writes records to Kafka, then it will also assign timestamps to these new records. The way the timestamps are assigned depends on the context:最后，当Kafka记录写入到Kafka Streams应用，那么它也会对新纪录指定时间戳。时间戳是分配方式取决于context： When new output records are generated via processing some inputrecord, for example, context.forward() triggered in the process()function call, output record timestamps are inherited from inputrecord timestamps directly. 当通过处理一些输入而产生新的输出记录时记录，例如，context.forward()引发的process()函数调用，输出记录的时间戳是继承自输入直接记录时间戳。 When new output records are generated via periodic functions such aspunctuate(), the output record timestamp is defined as the currentinternal time (obtained through context.timestamp()) of the streamtask. 当新的输出记录通过诸如punctuate()周期函数生成的输出记录时间戳定义为当前的内部时间（context.timestamp()获得）的流任务。 For aggregations, the timestamp of a resulting aggregate updaterecord will be that of the latest arrived input record that triggeredthe update. 对于一个聚合，形成的聚合更新记录的时间戳将最新到达的输入记录触发更新。 States 状态Some stream processing applications don’t require state, which means the processing of a message is independent from the processing of all other messages. However, being able to maintain state opens up many possibilities for sophisticated stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.一些流处理应用程序不需要状态，这意味着消息的处理是独立于所有其他消息的处理。然而，能够保持状态为复杂的流处理应用程序打开了许多可能性：您可以加入输入流，或组和汇总数据记录。许多这样的状态的操作，由Kafka Streams DSL提供。 Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data. This is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a persistent key-value store, an in-memory hashmap, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.Kafka Streams提供了所谓的状态存储，它可以用于流处理应用程序来存储和查询数据。当实施状态操作时这是一个重要的能力。在Kafka Streams的每一项任务的一个或多个状态存储将可以通过API来存储和查询处理所需的数据访问。这些状态存储可以是一个持续的键值存储，内存中的HashMap，或另一个方便的数据结构。Kafka Streams提供了容错和本地状态存储的自动恢复。 Kafka Streams allows direct read-only queries of the state stores by methods, threads, processes or applications external to the stream processing application that created the state stores. This is provided through a feature called Interactive Queries. All stores are named and Interactive Queries exposes only the read operations of the underlying implementation.Kafka Streams允许通过方法，线程，进程或应用程序的外部的流处理应用程序创建的状态存储的状态存储的直接只读查询。这是通过一个被称为交互式查询的功能。所有的存储都被命名和交互查询只公开底层实现的读操作。 As we have mentioned above, the computational logic of a Kafka Streams application is defined as a processor topology. Currently Kafka Streams provides two sets of APIs to define the processor topology, which will be described in the subsequent sections.正如我们上文所提到的，Kafka Streams应用程序的计算逻辑被定义为一个处理topology。目前，Kafka Streams提供了两组的接口来定义处理的topology，这将在随后的章节中描述。 Low-Level Processor APIProcessorDevelopers can define their customized processing logic by implementing the Processor interface, which provides process and punctuate methods. The process method is performed on each of the received record; and the punctuate method is performed periodically based on elapsed time. In addition, the processor can maintain the current ProcessorContext instance variable initialized in the init method, and use the context to schedule the punctuation period (context().schedule), to forward the modified / new key-value pair to downstream processors (context().forward), to commit the current processing progress (context().commit), etc.开发者可以通过处理器接口定义自己的定制的处理逻辑，它提供了方法和标点的方法。process方法是对每个接收的记录执行；和标点法是基于时间进行定期。此外，该处理器可以维持目前的ProcessorContext实例变量在init方法初始化，并使用context安排标点符号周期（context().schedule），提出修改/新的键值对下游处理器（context().forward），把当前的处理进度（context().commit），等。 1234567891011121314151617181920212223242526272829303132333435363738public class MyProcessor extends Processor &#123; private ProcessorContext context; private KeyValueStore kvStore; @Override @SuppressWarnings("unchecked") public void init(ProcessorContext context) &#123; this.context = context; this.context.schedule(1000); this.kvStore = (KeyValueStore) context.getStateStore("Counts"); &#125; @Override public void process(String dummy, String line) &#123; String[] words = line.toLowerCase().split(" "); for (String word : words) &#123; Integer oldValue = this.kvStore.get(word); if (oldValue == null) &#123; this.kvStore.put(word, 1); &#125; else &#123; this.kvStore.put(word, oldValue + 1); &#125; &#125; &#125; @Override public void punctuate(long timestamp) &#123; KeyValueIterator iter = this.kvStore.all(); while (iter.hasNext()) &#123; KeyValue entry = iter.next(); context.forward(entry.key, entry.value.toString()); &#125; iter.close(); context.commit(); &#125; @Override public void close() &#123; this.kvStore.close(); &#125; &#125;; In the above implementation, the following actions are performed:在上面的实现中，执行以下操作： In the init method, schedule the punctuation every 1 second andretrieve the local state store by its name “Counts”. 在init方法，schedule每1秒和标点符号检索本地状态存储由它的名称“计数”。 In the process method, upon each received record, split the valuestring into words, and update their counts into the state store (wewill talk about this feature later in the section). 在处理方法中，在每个接收到的记录中，将值字符串分割成单词，并更新他们的计数到状态存储区（我们将在本节中讨论这个功能）。 In the punctuate method, iterate the local state store and send theaggregated counts to the downstream processor, and commit the currentstream state. 在标点法，迭代局部状态存储和发送汇总计数到下游的处理器，并承诺目前流状态。 Processor TopologyWith the customized processors defined in the Processor API, developers can use the TopologyBuilder to build a processor topology by connecting these processors together:与定制的处理器在Processor API，开发者可以使用TopologyBuilder来连接这些处理器一起建立一个topology：12345678TopologyBuilder builder = new TopologyBuilder();builder.addSource("SOURCE", "src-topic") .addProcessor("PROCESS1", MyProcessor1::new /* the ProcessorSupplier that can generate MyProcessor1 */, "SOURCE") .addProcessor("PROCESS2", MyProcessor2::new /* the ProcessorSupplier that can generate MyProcessor2 */, "PROCESS1") .addProcessor("PROCESS3", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") .addSink("SINK1", "sink-topic1", "PROCESS1") .addSink("SINK2", "sink-topic2", "PROCESS2") .addSink("SINK3", "sink-topic3", "PROCESS3"); There are several steps in the above code to build the topology, and here is a quick walk through:在上面的代码中有几个步骤来构建topology，这里是一个快速的步行通过： First of all a source node named “SOURCE” is added to the topologyusing the addSource method, with one Kafka topic “src-topic” fed toit. 首先，源节点命名为“源”添加到topology使用addSource方法，使用“src-topic”这个Kafka 主题。 Three processor nodes are then added using the addProcessor method;here the first processor is a child of the “SOURCE” node, but is theparent of the other two processors. 三个processor节点，然后使用addProcessor方法添加；这里第一个processor是一个“源”节点的子节点，但是其他两处processors Finally three sink nodes are added to complete the topology using theaddSink method, each piping from a different parent processor nodeand writing to a separate topic. 最后，添加三个汇聚节点来完成使用的topology addSink方法，每个管道从不同的父节点的processor和写到一个单独的主题。 Local State Store本地状态存储Note that the Processor API is not limited to only accessing the current records as they arrive, but can also maintain local state stores that keep recently arrived records to use in stateful processing operations such as aggregation or windowed joins. To take advantage of this local states, developers can use the TopologyBuilder.addStateStore method when building the processor topology to create the local state and associate it with the processor nodes that needs to access it; or they can connect a created local state store with the existing processor nodes through TopologyBuilder.connectProcessorAndStateStores.注意Processor API不仅限于访问当前记录，还可以维持本地状态村粗，使记录使用状态的处理操作，如聚集或窗口的加入。利用局部状态，开发人员可以使用TopologyBuilder.addStateStore方法当搭建processor topology创建本地状态，它与processor节点需要访问它的联想；或者他们可以连接创建的局部状态存储与现有的处理器节点通过TopologyBuilder.connectProcessorAndStateStores.123456789101112TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", "src-topic") .addProcessor("PROCESS1", MyProcessor1::new, "SOURCE") // create the in-memory state store "COUNTS" associated with processor "PROCESS1" .addStateStore(Stores.create("COUNTS").withStringKeys().withStringValues().inMemory().build(), "PROCESS1") .addProcessor("PROCESS2", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") .addProcessor("PROCESS3", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") // connect the state store "COUNTS" with processor "PROCESS2" .connectProcessorAndStateStores("PROCESS2", "COUNTS"); .addSink("SINK1", "sink-topic1", "PROCESS1") .addSink("SINK2", "sink-topic2", "PROCESS2") .addSink("SINK3", "sink-topic3", "PROCESS3"); 例子：搭建kafka_2.11-0.10.1.0 集群：设置server.properties1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#broker的全局唯一编号，不能重复broker.id=0#用来监听链接的端口，producer或consumer将在此端口建立连接port=9092#处理网络请求的线程数量num.network.threads=3#用来处理磁盘IO的线程数量num.io.threads=8#发送套接字的缓冲区大小socket.send.buffer.bytes=102400#接受套接字的缓冲区大小socket.receive.buffer.bytes=102400#请求套接字的缓冲区大小socket.request.max.bytes=104857600#kafka运行日志存放的路径log.dirs=/home/hadoop/apps/kafka_2.11-0.10.1.0/logs/kafka#topic在当前broker上的分片个数num.partitions=2#用来恢复和清理data下数据的线程数量num.recovery.threads.per.data.dir=1#segment文件保留的最长时间，超时将被删除log.retention.hours=168#滚动生成新的segment文件的最大时间log.roll.hours=168#日志文件中每个segment的大小，默认为1Glog.segment.bytes=1073741824#周期性检查文件大小的时间log.retention.check.interval.ms=300000#日志清理是否打开log.cleaner.enable=true#broker需要使用zookeeper保存meta数据zookeeper.connect=www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka0.10.1.0#zookeeper链接超时时间zookeeper.connection.timeout.ms=6000#partion buffer中，消息的条数达到阈值，将触发flush到磁盘log.flush.interval.messages=10000#消息buffer的时间，达到阈值，将触发flush到磁盘log.flush.interval.ms=3000#删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除delete.topic.enable=true#此处的host.name为本机IP(重要),如果不改,则客户端会抛出:Producer connection to localhost:9092 unsuccessful 错误!host.name=www.hadoop01.com 分发到其他机器12scp -rp kafka_2.11-0.10.1.0/ hadoop@www.hadoop02.com:/home/hadoop/apps/scp -rp kafka_2.11-0.10.1.0/ hadoop@www.hadoop03.com:/home/hadoop/apps/ 修改server.properties 以下两个参数12broker.id=0host.name=www.hadoop01.com zookeeper设置，启动略。 创建主题 12345678910[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --replication-factor 1 --partitions 1 --topic wordsCreated topic "words".[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --replication-factor 1 --partitions 1 --topic countsCreated topic "counts".[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --describe --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --topic words Topic:words PartitionCount:1 ReplicationFactor:1 Configs: Topic: words Partition: 0 Leader: 2 Replicas: 2 Isr: 2[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --describe --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --topic counts Topic:counts PartitionCount:1 ReplicationFactor:1 Configs: Topic: counts Partition: 0 Leader: 2 Replicas: 2 Isr: 2 依次启动： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.Arrays;import java.util.Properties;import java.util.concurrent.atomic.AtomicLong;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.DoubleDeserializer;import org.apache.kafka.common.serialization.IntegerDeserializer;import org.apache.kafka.common.serialization.StringDeserializer;public class DemoConsumerManualCommit &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; "www.hadoop01.com:9092", "gender-amount", "group4", "consumer2" &#125;; if (args == null || args.length != 4) &#123; System.err.println( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"); System.exit(1); &#125; String bootstrap = args[0]; String topic = args[1]; String groupid = args[2]; String clientid = args[3]; Properties props = new Properties(); props.put("bootstrap.servers", bootstrap); props.put("group.id", groupid); props.put("enable.auto.commit", "false"); props.put("key.deserializer", StringDeserializer.class.getName()); //props.put("value.deserializer", DoubleDeserializer.class.getName()); props.put("value.deserializer", IntegerDeserializer.class.getName()); props.put("max.poll.interval.ms", "300000"); props.put("max.poll.records", "500"); props.put("auto.offset.reset", "earliest"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(topic)); AtomicLong atomicLong = new AtomicLong(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); records.forEach(record -&gt; &#123; System.out.printf("client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); if (atomicLong.get() % 10 == 0) &#123;// consumer.commitSync(); &#125; &#125;); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839import java.io.IOException;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.IntegerSerializer;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.kafka.common.serialization.StringSerializer;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.TopologyBuilder;import org.apache.kafka.streams.state.Stores;public class WordCountTopology &#123; public static void main(String[] args) throws IOException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount-processor"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "www.hadoop01.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "www.hadoop01.com:2181/kafka0.10.1.0"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", new StringDeserializer(), new StringDeserializer(), "words") .addProcessor("WordCountProcessor", WordCountProcessor::new, "SOURCE") .addStateStore(Stores.create("Counts").withStringKeys().withIntegerValues().inMemory().build(), "WordCountProcessor")// .connectProcessorAndStateStores("WordCountProcessor", "Counts") .addSink("SINK", "count", new StringSerializer(), new IntegerSerializer(), "WordCountProcessor"); KafkaStreams stream = new KafkaStreams(builder, props); stream.start(); System.in.read(); stream.close(); stream.cleanUp(); &#125;&#125; 启动producer12kafka-console-producer.sh --broker-list www.hadoop01.com:9092 --topic wordshello apache hello kafka 报错：123456Exception in thread "StreamThread-1" org.apache.kafka.streams.errors.StreamsException: Extracted timestamp value is negative, which is not allowed. at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:111) at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117) at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:144) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:415) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:242) 为时间戳的原因kafka 18May的时候何如了时间戳的东东后来更改了主题：12bin/kafka-topics.sh --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --create --topic word --partitions 1 --replication-factor 1 --config message.timestamp.type=LogAppendTimebin/kafka-topics.sh --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --create --topic count --partitions 1 --replication-factor 1 --config message.timestamp.type=LogAppendTime 先启动DemoConsumerManualCommit再启动WordCountTopology 命令行输入12kafka-console-producer.sh --broker-list www.hadoop01.com:9092 --topic word_testhello apache kafka hello apache spark hello storm 最后控制台输出 应该是12345client : consumer2 , topic: count , partition: 0 , offset = 0, key = apache, value = 2client : consumer2 , topic: count , partition: 0 , offset = 1, key = hello, value = 3client : consumer2 , topic: count , partition: 0 , offset = 2, key = kafka, value = 1client : consumer2 , topic: count , partition: 0 , offset = 3, key = spark, value = 1client : consumer2 , topic: count , partition: 0 , offset = 4, key = storm, value = 1 查看所有主题1bin/kafka-topics.sh --list --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 发现多了一个streams-wordcount-processor-Counts-changelog In the next section we present another way to build the processor topology: the Kafka Streams DSL. High-Level Streams DSLTo build a processor topology using the Streams DSL, developers can apply the KStreamBuilder class, which is extended from the TopologyBuilder. A simple example is included with the source code for Kafka in the streams/examples package. The rest of this section will walk through some code to demonstrate the key steps in creating a topology using the Streams DSL, but we recommend developers to read the full example source codes for details.使用DSL Streams创建topology，开发人员可以应用KStreamBuilder类，这是从TopologyBuilder延伸。一个简单的例子是包含了streams/examples package的源代码。本节的其余部分将通过一些代码来演示使用流DSL创建topology的关键步骤，但我们建议开发者阅读完整的示例源代码的细节。 KStream and KTable The DSL uses two main abstractions. A KStream is an abstraction of a record stream, where each data record represents a self-contained datum in the unbounded data set. A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered a create). To illustrate the difference between KStreams and KTables, let’s imagine the following two data records are being sent to the stream: (“alice”, 1) –&gt; (“alice”, 3). If these records a KStream and the stream processing application were to sum the values it would return 4. If these records were a KTable, the return would be 3, since the last record would be considered as an update. DSL使用的两种主要的抽象。一个KStream是记录流的一个抽象的概念，其中每个数据记录代表一个独立的数据的数据集。一个KTable是一个变更的流的一个抽象的概念，其中每个数据记录的更新。更精确地说，数据记录中的值被认为是同一个记录键的最后一个值的更新，如果有（如果一个相应的key不存在，则该更新将被视为一个创建）。说明kstreams和KTables之间的差异，让我们想象一下以下两个数据记录被发送到流：(“alice”, 1)——&gt;(“alice”, 3)。如果这些记录KStream和流处理应用进行总结的值将返回4。如果这些记录是一个KTable，返回的返回的将是3，因为过去的记录将被视为一种更新。 Create Source Streams from Kafka创建Kafka Streams Either a record stream (defined as KStream) or a changelog stream (defined as KTable) can be created as a source stream from one or more Kafka topics (for KTable you can only create the source stream from a single topic).一个记录流（定义为KStream）或更新流（定义为KTable）可以创建从一个或多个Kafka主题的源流（为KTable你只能从一个单一的主题创建源流）。 123KStreamBuilder builder = new KStreamBuilder();KStream source1 = builder.stream("topic1", "topic2");KTable source2 = builder.table("topic3", "stateStoreName"); Windowing a stream 窗口流A stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for join and aggregation operations, etc. Kafka Streams currently defines the following types of windows: 流处理器可能需要将数据记录划分成时间桶，即通过时间窗口的流。这通常是需要的连接和聚合操作等。Kafka流目前定义了以下类型的窗口： Hopping time windows are windows based on time intervals. They modelfixed-sized, (possibly) overlapping windows. A hopping window isdefined by two properties: the window’s size and its advance interval(aka “hop”). The advance interval specifies by how much a windowmoves forward relative to the previous one. For example, you canconfigure a hopping window with a size 5 minutes and an advanceinterval of 1 minute. Since hopping windows can overlap a data recordmay belong to more than one such windows.跳跃的时间（跳时?）窗口是基于时间间隔的窗口。他们模型固定大小的，（可能）重叠的窗口。跳跃窗口是由两个属性定义的：窗口的大小和它的提前间隔（又名”hop”）。提前间隔由一个窗口相对于前一个窗口移动的多少来指定。例如，您可以配置一个具有大小5分钟和一个提前间隔1分钟的跳跃窗口。由于跳跃窗口可以重叠数据记录，可能属于多个这样的窗口。 Tumbling time windows are a special case of hopping time windows and,like the latter, are windows based on time intervals. They modelfixed-size, non-overlapping, gap-less windows. A tumbling window isdefined by a single property: the window’s size. A tumbling window isa hopping window whose window size is equal to its advance interval.Since tumbling windows never overlap, a data record will belong toone and only one window.翻滚时间窗口是一个特殊的情况下的跳跃时间窗口，和后者一样，是基于时间间隔的窗口。他们模型固定的大小，不重叠，无缝隙的窗口。一个翻滚窗口是由一个单一属性定义的：窗口的大小。一个翻滚窗口是一个跳跃的窗口，它的窗口大小等于它的预先间隔。由于翻滚的窗口永远不会重叠，数据记录将属于一个并且只有一个窗口。 Sliding windows model a fixed-size window that slides continuouslyover the time axis; here, two data records are said to be included inthe same window if the difference of their timestamps is within thewindow size. Thus, sliding windows are not aligned to the epoch, buton the data record timestamps. In Kafka Streams, sliding windows areused only for join operations, and can be specified through theJoinWindows class.滑动窗口模型一个固定大小的窗口，幻灯片不断在时间轴上；在这里，两个数据记录，说是如果他们的时间差异是在窗口的大小，包括在同一个窗口。因此，滑动窗口不一致的时代，但在数据记录的时间戳。Kafka流，滑动窗口只用于连接操作，并可以通过JoinWindows类指定。 JoinsA join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely. In Kafka Streams, you may perform the following join operations:一个连接操作基于它们的数据记录的键合并两个流，并产生一个新的流。在记录流的加入通常需要执行在视窗基础否则记录必须保持履行加入的数量可以无限增长。在Kafka Streams中，您可以执行以下连接操作： KStream-to-KStream Joins are always windowed joins, since otherwisethe memory and state required to compute the join would growinfinitely in size. Here, a newly received record from one of thestreams is joined with the other stream’s records within thespecified window interval to produce one result for each matchingpair based on user-provided ValueJoiner. A new KStream instancerepresenting the result stream of the join is returned from thisoperator.KStream-to-KStream Joins总是窗口连接，否则内存和计算所需的join会无限增长的大小。在这里，新接收的记录从一条数据流与其他流的记录在指定的窗口间隔为对每一个匹配的基于用户提供的ValueJoiner产生的一个结果。一个新的KStream实例表示的加入导致流从这个操作符返回。 KTable-to-KTable Joins are join operations designed to be consistentwith the ones in relational databases. Here, both changelog streamsare materialized into local state stores first. When a new record isreceived from one of the streams, it is joined with the otherstream’s materialized state stores to produce one result for eachmatching pair based on user-provided ValueJoiner. A new KTableinstance representing the result stream of the join, which is also achangelog stream of the represented table, is returned from thisoperator.KTable-to-KTable Joins 操作设计与关系数据库中的一致行动。在这里，无论是修改流物化在当地商店。当一个新的记录从一个流的接收，它与其他流的物化状态存储为对每一个匹配的基于用户提供的ValueJoiner产生的一个结果。一个新的KTable实例代表了连接流，这也是一个代表表更新流，是从这个操作符返回。 KStream-to-KTable Joins allow you to perform table lookups against achangelog stream (KTable) upon receiving a new record from anotherrecord stream (KStream). An example use case would be to enrich astream of user activities (KStream) with the latest user profileinformation (KTable). Only records received from the record streamwill trigger the join and produce results via ValueJoiner, not viceversa (i.e., records received from the changelog stream will be usedonly to update the materialized state store). A new KStream instancerepresenting the result stream of the join is returned from thisoperator.KStream-to-KTable Joins允许你执行表查找和修改流（ktable）在从另一个记录流接收一个新的记录（KStream）。一个例子使用案例将丰富用户活动流（KStream）最新的用户配置文件信息（KTable）。只记录收到的记录流会触发连接并通过ValueJoiner产生结果，反之亦然（即记录收到更新流只会被用来更新物化状态存储）。一个新的KStream实例表示的加入导致流从这个操作符返回。 Depending on the operands the following join operations are supported: inner joins, outer joins and left joins. Their semantics are similar to the corresponding operators in relational databases. aTransform a stream There is a list of transformation operations provided for KStream and KTable respectively. Each of these operations may generate either one or more KStream and KTable objects and can be translated into one or more connected processors into the underlying processor topology. All these transformation methods can be chained together to compose a complex processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types. Among these transformations, filter, map, mapValues, etc, are stateless transformation operations and can be applied to both KStream and KTable, where users can usually pass a customized function to these functions as a parameter, such as Predicate for filter, KeyValueMapper for map, etc:12// written in Java 8+, using lambda expressionsKStream mapped = source1.mapValue(record -&gt; record.get("category")); Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor; Stateful transformations, on the other hand, require accessing an associated state for processing and producing outputs. For example, in join and aggregate operations, a windowing state is usually used to store all the received records within the defined window boundary so far. The operators can then access these accumulated records in the store and compute based on them.1234567891011// written in Java 8+, using lambda expressionsKTable, Long&gt; counts = source1.groupByKey().aggregate( () -&gt; 0L, // initial value (aggKey, value, aggregate) -&gt; aggregate + 1L, // aggregating value TimeWindows.of("counts", 5000L).advanceBy(1000L), // intervals in milliseconds Serdes.Long() // serde for aggregated value);KStream joined = source1.leftJoin(source2, (record1, record2) -&gt; record1.get("user") + "-" + record2.get("region");); Write streams back to Kafka At the end of the processing, users can choose to (continuously) write the final resulted streams back to a Kafka topic through KStream.to and KTable.to.1joined.to("topic4"); If your application needs to continue reading and processing the records after they have been materialized to a topic via to above, one option is to construct a new stream that reads from the output topic; Kafka Streams provides a convenience method called through:12345// equivalent to//// joined.to("topic4");// materialized = builder.stream("topic4");KStream materialized = joined.through("topic4");]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis 集群]]></title>
    <url>%2F2016%2F12%2F02%2Fredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[同时在每台机器配置如下内容：创建一个空的集群文件 vi nodes-6379.conf :wq 修改配置文件，开启集群模式 vi redis.conf cluster-enabled yes cluster-config-file nodes-6379.conf 启动： ./src/redis-server redis.conf 然后输入查看命令1234567891011[hadoop@www.hadoop01.com ~]$ps -ef | grep redishadoop 2657 2506 0 21:59 pts/0 00:00:00 ./src/redis-server *:6379 [cluster]hadoop 2695 2670 0 22:00 pts/1 00:00:00 grep redis [hadoop@www.hadoop02.com ~]$ps -ef | grep redishadoop 2991 2662 0 10:12 pts/0 00:00:01 ./src/redis-server *:6379 [cluster]hadoop 3039 2999 0 10:21 pts/1 00:00:00 grep redis [hadoop@www.hadoop03.com ~]$ps -ef | grep redishadoop 2482 2417 0 13:41 pts/0 00:00:01 ./src/redis-server *:6379 [cluster]hadoop 2530 2490 0 13:51 pts/1 00:00:00 grep redis 每个后面都有一个cluster此时并没有把节点添加到一个集群，用命令查看nodes为112345678910111213[hadoop@www.hadoop01.com redis-3.0.7]$./src/redis-cli127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:5cluster_slots_ok:5cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:1cluster_size:1cluster_current_epoch:0cluster_my_epoch:0cluster_stats_messages_sent:0cluster_stats_messages_received:0 这时需要添加集群节点：123456789101112131415161718127.0.0.1:6379&gt; Cluster meet www.hadoop02.com 6379(error) ERR Invalid node address specified: www.hadoop02.com:6379127.0.0.1:6379&gt; Cluster meet 192.168.247.152 6379OK127.0.0.1:6379&gt; Cluster meet 192.168.247.154 6379OK127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:5cluster_slots_ok:5cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:27cluster_stats_messages_received:26 可以看到节点数为3-&gt;cluster_known_nodes:3，但是集群状态是失败-&gt;cluster_state:fail 分配槽位一共16384个槽位cluster addslots 0 1 2 … 写个脚本：1234567vi fenpeicaowei.sh#bashfor i in &#123;10000..16384&#125;; do ./src/redis-cli cluster addslots $i; donechmod a+x fenpeicaowei.sh./fenpeicaowei.sh 查看有哪些节点(因为之前配置了主从，读者忽略slave节点，另外从节点分配槽位是不会被同步的)12345./src/redis-cli -c127.0.0.1:6379&gt; cluster nodes276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481166569926 1 connectedda0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481166571948 1 connected64a2d824459edc01c7b6b205021d01da96513121 192.168.247.152:6379 myself,master - 0 0 1 connected 1584 2171 5649 9842 10000-16383 查看槽位分配到哪些节点1234567891011121314151617181920212223242526272829303132333435363738394041127.0.0.1:6379&gt; cluster slots1) 1) (integer) 1584 2) (integer) 1584 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63792) 1) (integer) 2171 2) (integer) 2171 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63793) 1) (integer) 5649 2) (integer) 5649 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63794) 1) (integer) 9842 2) (integer) 9842 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63795) 1) (integer) 10000 2) (integer) 16383 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 6379 123456789101112127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:6388cluster_slots_ok:6388cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:3002cluster_stats_messages_received:2999 全部分配完成，就可以看到123456789101112127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:792cluster_stats_messages_received:798 查看某个key对应的槽位12127.0.0.1:6379&gt; cluster keyslot 3(integer) 1584 重新分片redis提供一个工具：redis-trib.rb 但是要用这个工具需要安装ruby环境 下载repo文件wget http://mirrors.163.com/.help/CentOS6-Base-163.repo 备份并替换系统的repo文件cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo.bakmv CentOS6-Base-163.repo CentOS-Base.repo 执行yum源更新命令123456789yum clean allyum makecacheyum updateyum install -y ruby* --skip-broken yum install rubygemswget https://rubygems.global.ssl.fastly.net/gems/redis-3.2.2.gemgem install redisgem install -l ./redis-3.2.2.gem 用redis-trib.rb创建集群./src/redis-trib.rb create –replicas 1 192.168.1.150:6379 192.168.1.152:6379 192.168.1.154:6379可能会报错，是版本太低的原因解决方案：ftp://ftp.ruby-lang.org/pub/ruby/ 下载最新的ruby安装包 ruby-2.3.3.tar.gzgem update –system 升级到最新版本2.5.1gem install redis 安装redis模块 重新执行123[hadoop@www.hadoop02.com redis-3.0.7]$./src/redis-trib.rb create --replicas 1 192.168.247.150:6379 192.168.247.152:6379 192.168.247.154:6379&gt;&gt;&gt; Creating cluster[ERR] Node 192.168.247.150:6379 is not empty. Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0. 因为刚才已经分配了节点 现在做重新分区查看分区前：1234127.0.0.1:6379&gt; cluster nodes276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481242407249 1 connected64a2d824459edc01c7b6b205021d01da96513121 192.168.247.152:6379 master - 0 1481242405165 1 connected 0-16383da0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 myself,slave 64a2d824459edc01c7b6b205021d01da96513121 0 0 0 connected 分区：12345678910111213141516171819202122232425262728293031323334[hadoop@www.hadoop02.com redis-3.0.7]$./src/redis-trib.rb reshard 127.0.0.1:6379&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6379)M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379 slots:0-16383 (16384 slots) master 2 additional replica(s)S: 276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slots: (0 slots) slave replicates 64a2d824459edc01c7b6b205021d01da96513121S: da0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 slots: (0 slots) slave replicates 64a2d824459edc01c7b6b205021d01da96513121[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 5000What is the receiving node ID? 276a3675a706d4626a25c54892c368ca6c37002c*** The specified node is not known or not a master, please retry.What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da9651312*** The specified node is not known or not a master, please retry.What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da96513121Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs.Source node #1:allReady to move 5000 slots. Source nodes: Destination node: M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379 slots:0-16383 (16384 slots) master 2 additional replica(s) Resharding plan:Do you want to proceed with the proposed reshard plan (yes/no)? yes rehash只能在主节点进行，故247.152分配给了247.152，没有变化]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[codis 集群]]></title>
    <url>%2F2016%2F12%2F02%2Fcodis%20%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[安装go语言http://www.golangtc.com/download 选择1.7.3下载1tar -zxvf go1.7.3.linux-amd64.tar.gz -C /usr/local/ 下载jdk和zookeeper并解压 配置zookeeper 创建文件夹 zkData和zkLog123456cp zoo_sample.cfg zoo.cfgvi zoo.cfg dataDir=/root/zookeeper/zookeeper-3.4.6/zkDatadataLogDir=/usr/bigdata/zookeeper-3.4.6/zkLogserver.1=master:2888:3888 进入zkData1echo 1 &gt; myid – 配置环境变量12345export JAVA_HOME=/root/java/jdk1.8.0_111export GOROOT=/usr/local/goexport GOPATH=/usr/local/codisexport ZOOKEEPER_HOME=/root/zookeeper/zookeeper-3.4.6export PATH=$PATH:$JAVA_HOME/bin:$GOROOT/bin:$GOPATH/bin:$ZOOKEEPER_HOME/bin 下载codishttps://github.com/CodisLabs/codis/releases选择与go版本一样的codis版本 123456mkdir -p $GOPATH/src/github.com/CodisLabstar -xzf codis-3.1.0.tar.gz -C /usr/local/codis/src/github.com/CodisLabs/cd /usr/local/codis/src/github.com/CodisLabs/mv codis-3.1.0/ codiscd codis/make 1make gotest 执行成功后进入bin 目录1cd /usr/local/codis/src/github.com/CodisLabs/codis/bin 123mkdir /etc/codiscd /usr/local/codis/src/github.com/CodisLabs/codis/externcp redis-2.8.21/redis.conf /etc/codis/redis6379.conf 创建redis 配置文件中所指定的目录123456789101112131415161718192021222324[root@zhm1 codis]# cd /opt[root@zhm1 opt]# mkdir -p codisapp/run[root@zhm1 opt]# mkdir -p codisapp/logs[root@zhm1 opt]# mkdir -p codisapp/data/6379 codisapp/data/6380[root@zhm1 opt]# vi /etc/codis/redis6379.confdaemonize yespidfile /opt/codisapp/run/redis6379.pidport 6379logfile "/opt/codisapp/logs/redis6379.log"dbfilename dump.rdbdir /opt/codisapp/data/6379cp /etc/codis/redis6379.conf /etc/codis/redis6380.confvi /etc/codis/redis6380.confdaemonize yespidfile /opt/codisapp/run/redis6380.pidport 6380logfile "/opt/codisapp/logs/redis6380.log"dbfilename dump.rdbdir /opt/codisapp/data/6380 启动redis123[root@zhm1 opt]# cd /usr/local/codis/src/github.com/CodisLabs/codis/[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf --protected-mode no [root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf --protected-mode no 此处启动增加–protected-mode no 参数可以防止后面添加server的时候提示1Cause": "DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside. 启动dashboard自定义dashboard 配置文件1./bin/codis-dashboard --default-config | tee dashboard.conf 修改IP1234567891011121314151617181920[root@zhm1 codis]# vi dashboard.conf ################################################### ## Codis-Dashboard ## #################################################### Set Coordinator, only accept "zookeeper" &amp; "etcd".coordinator_name = "zookeeper"coordinator_addr = "192.168.110.129:2181"# Set Codis Product Name/Auth.product_name = "codis-demo"product_auth = ""# Set bind address for admin(rpc), tcp only.admin_addr = "192.168.110.129:18080"# Set quorum value for sentinel, default is 2.sentinel_quorum = 2 启动zookeeper1234[root@zhm1 codis]# zkServer.sh startJMX enabled by defaultUsing config: /root/zookeeper/zookeeper-3.4.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 启动dashboard1[root@zhm1 codis]# nohup bin/codis-dashboard --ncpu=2 --config=dashboard.conf --log=dashboard.log --log-level=WARN &amp; 查看日志输出，连接zookeeper 成功1234567891011121314151617181920[root@zhm1 codis]# cat dashboard.log.2016-12-212016/12/21 11:18:34 main.go:77: [WARN] set ncpu = 22016/12/21 11:18:34 topom.go:110: [WARN] create new topom:&#123; "token": "6047b5ce9ef2826c1eb40692023b9bf4", "start_time": "2016-12-21 11:18:34.481106135 +0800 CST", "admin_addr": "192.168.110.129:18080", "product_name": "codis-demo", "pid": 66994, "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis", "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux"&#125;2016/12/21 11:18:35 main.go:124: [WARN] create topom with configcoordinator_name = "zookeeper"coordinator_addr = "192.168.110.129:2181"admin_addr = "192.168.110.129:18080"product_name = "codis-demo"product_auth = ""sentinel_quorum = 22016/12/21 11:18:36 topom.go:381: [WARN] admin start service on 192.168.110.129:18080 启动codis-proxy编译codis-proxy 配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[root@zhm1 codis]# ./bin/codis-proxy --default-config | tee proxy.conf################################################### ## Codis-Proxy ## #################################################### Set Codis Product Name/Auth.product_name = "codis-demo"product_auth = ""# Set bind address for admin(rpc), tcp only.admin_addr = "0.0.0.0:11080"# Set bind address for proxy, proto_type can be "tcp", "tcp4", "tcp6", "unix" or "unixpacket".proto_type = "tcp4"proxy_addr = "0.0.0.0:19000"# Set jodis address &amp; session timeout, only accept "zookeeper" &amp; "etcd".jodis_name = ""jodis_addr = ""jodis_timeout = "20s"jodis_compatible = false# Set datacenter of proxy.proxy_datacenter = ""# Set max number of alive sessions.proxy_max_clients = 1000# Set max offheap memory size. (0 to disable)proxy_max_offheap_size = "1024mb"# Set heap placeholder to reduce GC frequency.proxy_heap_placeholder = "256mb"# Proxy will ping backend redis in a predefined interval. (0 to disable)backend_ping_period = "5s"# Set backend recv buffer size &amp; timeout.backend_recv_bufsize = "128kb"backend_recv_timeout = "30s"# Set backend send buffer &amp; timeout.backend_send_bufsize = "128kb"backend_send_timeout = "30s"# Set backend pipeline buffer size.backend_max_pipeline = 1024# Set backend never read replica groups, default is falsebackend_primary_only = false# Set backend parallel connections per serverbackend_primary_parallel = 1backend_replica_parallel = 1# Set backend tcp keepalive period. (0 to disable)backend_keepalive_period = "75s"# If there is no request from client for a long time, the connection will be closed. (0 to disable)# Set session recv buffer size &amp; timeout.session_recv_bufsize = "128kb"session_recv_timeout = "30m"# Set session send buffer size &amp; timeout.session_send_bufsize = "64kb"session_send_timeout = "30s"# Make sure this is higher than the max number of requests for each pipeline request, or your client may be blocked.# Set session pipeline buffer size.session_max_pipeline = 512# Set session tcp keepalive period. (0 to disable)session_keepalive_period = "75s"# Set metrics server (such as http://localhost:28000), proxy will report json formatted metrics to specified server in a predefined period.metrics_report_server = ""metrics_report_period = "1s"# Set influxdb server (such as http://localhost:8086), proxy will report metrics to influxdb.metrics_report_influxdb_server = ""metrics_report_influxdb_period = "1s"metrics_report_influxdb_username = ""metrics_report_influxdb_password = ""metrics_report_influxdb_database = "" product_name 集群名称，参考dashboard 参数说明jodis_addr Jodis 注册zookeeper 地址此处把IP修改一下jodis_addr = “192.168.110.129:2181” 启动codis-proxy 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859nohup bin/codis-proxy --ncpu=2 --config=proxy.conf --log =proxy.log --log-level=WARN &amp;[root@zhm1 codis]# cat =proxy.log.2016-12-212016/12/21 11:25:01 main.go:100: [WARN] set ncpu = 2, max-ncpu = 02016/12/21 11:25:01 proxy.go:89: [WARN] [0xc42010e840] create new proxy:&#123; "token": "673cc48d563a3614c13a3a294067f0b9", "start_time": "2016-12-21 11:25:01.70765896 +0800 CST", "admin_addr": "192.168.110.129:11080", "proto_type": "tcp4", "proxy_addr": "192.168.110.129:19000", "product_name": "codis-demo", "pid": 67097, "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis", "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux", "hostname": "zhm1.cn", "datacenter": ""&#125;2016/12/21 11:25:01 proxy.go:353: [WARN] [0xc42010e840] admin start service on [::]:110802016/12/21 11:25:01 main.go:159: [WARN] create proxy with configproto_type = "tcp4"proxy_addr = "0.0.0.0:19000"admin_addr = "0.0.0.0:11080"jodis_name = ""jodis_addr = "192.168.110.129:2181"jodis_timeout = "20s"jodis_compatible = falseproduct_name = "codis-demo"product_auth = ""proxy_datacenter = ""proxy_max_clients = 1000proxy_max_offheap_size = "1gb"proxy_heap_placeholder = "256mb"backend_ping_period = "5s"backend_recv_bufsize = "128kb"backend_recv_timeout = "30s"backend_send_bufsize = "128kb"backend_send_timeout = "30s"backend_max_pipeline = 1024backend_primary_only = falsebackend_primary_parallel = 1backend_replica_parallel = 1backend_keepalive_period = "75s"session_recv_bufsize = "128kb"session_recv_timeout = "30m"session_send_bufsize = "64kb"session_send_timeout = "30s"session_max_pipeline = 512session_keepalive_period = "75s"metrics_report_server = ""metrics_report_period = "1s"metrics_report_influxdb_server = ""metrics_report_influxdb_period = "1s"metrics_report_influxdb_username = ""metrics_report_influxdb_password = ""metrics_report_influxdb_database = ""2016/12/21 11:25:01 main.go:180: [WARN] [0xc42010e840] proxy waiting online ...2016/12/21 11:25:01 proxy.go:377: [WARN] [0xc42010e840] proxy start service on 0.0.0.0:190002016/12/21 11:25:02 main.go:180: [WARN] [0xc42010e840] proxy waiting online ... codis-proxy 启动后，处于waiting 状态，监听proxy_addr 地址，但是不会accept 连接，添加到集群并完成集群状态的同步，才能改变状态为online。 添加codis-proxy 到集群添加的方法有以下两种：通过codis-fe 添加，通过Add Proxy 按钮，将admin_addr 加入到集群中；通过codis-admin 命令行工具添加：./bin/codis-admin –dashboard=192.168.110.129:18080 –create-proxy -x 192.168.110.129:11080 查看zookeeper 存储的数据1234567891011zkCli.sh[zk: localhost:2181(CONNECTED) 0] ls /[codis3, zookeeper, kafka][zk: localhost:2181(CONNECTED) 1] ls /codis3[codis-demo][zk: localhost:2181(CONNECTED) 2] ls /codis3/codis-demo[proxy, topom][zk: localhost:2181(CONNECTED) 3] ls /codis3/codis-demo/proxy[proxy-673cc48d563a3614c13a3a294067f0b9][zk: localhost:2181(CONNECTED) 4] ls /codis3/codis-demo/topom[] 配置启动Cdis FE 集群管理界面创建conf 目录，放下面生成的codis.json 文件12345678910111213mkdir conf./bin/codis-admin --dashboard-list --zookeeper=192.168.110.129 | tee conf/codis.json2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - zkclient setup new connection to 192.168.110.1292016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Connected to 192.168.110.129:21812016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Authenticated: id=25085718467510274, timeout=40000[ &#123; "name": "codis-demo", "dashboard": "192.168.110.129:18080" &#125;]2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Recv loop terminated: err=EOF2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Send loop terminated: err=&lt;nil&gt; 启动codis-fe1234567./bin/codis-fe --ncpu=2 --log=fe.log --log-level=WARN --dashboard-list=conf/codis.json --listen=192.168.110.129:18090 &amp;[3] 67271[root@zhm1 codis]# cat fe.log.2016-12-21 2016/12/21 11:33:34 main.go:102: [WARN] set ncpu = 22016/12/21 11:33:34 main.go:105: [WARN] set listen = 192.168.110.129:180902016/12/21 11:33:34 main.go:117: [WARN] set assets = /usr/local/codis/src/github.com/CodisLabs/codis/bin/assets2016/12/21 11:33:34 main.go:132: [WARN] set --dashboard-list = conf/codis.json 至此codis 的图形界面已经能够显示。打开浏览器访问http://192.168.110.129:18090/，通过管理界面操作codis 如果是机器外访问需要关闭防火墙centos7的命令是1234[root@zhm1 codis]# systemctl stop firewalld.service[root@zhm1 codis]# systemctl disable firewalld.serviceRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service. 创建组和实例操作步骤：在New Group 后面输入1（表示增加编号为1 的组），点击New Group 完成组的创建；然后，Add Server，后面设置ip 和端口，并指向组编号1，点击Add Server 完成redis 实例的创建； 添加第2 个组，以及第2 个组的实例，操作步骤同上。 对slot 进行分组输入slot范围和组 另外分享一个github上的好文docker玩codis集群https://github.com/ruo91/docker-codis]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hbase region处理]]></title>
    <url>%2F2016%2F12%2F01%2FRegionServer%2F</url>
    <content type="text"><![CDATA[减少RegionServer节点退役：在制定节点上运行，停止的节点上的RegionServer1./bin/hbase-daemon.sh stop regionserver 如果在关闭时Hbase存在正在运行的负载均衡，那么Maser针对Region的恢复操作和负载均衡之前可能会产生竞争，尤其是在生产环境中，碰到的几率更大，所以建议现金用负载均衡 12./hbase shell./hbase(main):001:0&gt;blance_switch false 滚动重启1./bin/graceful_stop slave2 如果同时退役多个节点，为了防止正在退役的节点把region move到即将退役的节点上可以利用zookeeper，在Hbase_root/training znode中创建要退役的RegionServer的entry滚动重启三种方式：使用rolling-restart.sh脚本手工进行滚动重启编写自定义的滚动重启脚本 备Master为避免Master故障，添加备用Master在所有节点的conf目录下创建文件backup-masters在里面添加备份主机名 启动：1./bin/hbase-daemon.sh start master 如果发现端口冲突，需要修改hbase-site.xml1234&lt;property&gt;&lt;name&gt;hbase.regionserver.port&lt;/name&gt;&lt;value&gt;16012&lt;/value&gt;&lt;/property&gt; 查看进程占用端口号：1netstat -an -p | grep -i PID 增加节点在conf/regionservers增加节点，直接start会跳过已经启动的regionserver Hbase冷备需要停止hbasedistcp该命令位于hdoop的tools包中hadoop distcp /hbase /hbasebackup hbase-site.xml里面有hbase.rootdir参数，可以修改为备份后的路径来恢复数据 操作流程：123456789101112131415161718192021222324./bin/hbase shellcreate 'testbackup' 'columnfamily'put 'testbackup','row1','columnfamily:column1','value1'scan 'testbackup'./bin/stop-hbase.shhadoop distcp /hbae /hbasebackup./bin/start-hbase.sh./bin/hbase shelldisable 'testbackup'drop 'testbackup'list./bin/stop-hbase.shhdfs dfs -mv /hbase /hbase_tmphadoop distcp -overwrite /hbasebackupd /hbase./bin/start-hbase.sh./bin/hbase shelllistscan 'testbackup' Hbase热备不需要停止hbase方式一：CopyTble方式二：Export方式三：集群复制 方式二12345678910111213./bin/hbase org.apache.hadoop.hbase.mapreduce.Export testbackup /tmp/test./bin/hbase org.apache.hadoop.hbase.mapreduce.Export testbackup hdfs://www.hadoop01.com/test./bin/hbase shelldisbale testbackupdrop testbackupcreate 'testbackup' 'columnfamily'scan testbackup.Import testbackup /tmp/test./bin/hbase shellscan testbackup 方式一：12345create 'mubiao' 'columnfamily'./bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=mubiao testbackup./hbase shellscan 'mubiao']]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker_install]]></title>
    <url>%2F2016%2F11%2F27%2Fdocker-install%2F</url>
    <content type="text"><![CDATA[1.增加yum源在命令行输入：12345678cat &gt;/etc/yum.repos.d/docker.repo &lt;&lt;-EOF[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/6enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF 通过yum加载docker1yum install docker-engine 2.禁用selinux#####2.1.查看selinux状态 [root@localhost ~]# cat /etc/selinux/config12345678910111213141516171819202122232425# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=enforcing# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted``` 2.2.修改该配置文件中将enforcing替换为disabled [root@localhost ~]# cat /etc/selinux/config```js# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted 2.3.Docker使用非root用户将当前用户加入docker组sudo gpasswd -a ${USER} docker 2.4.然后重启restart 3.启动docker123456789101112131415161718/etc/init.d/docker start[hadoop@www.hadoop02.com ~]$ps -ef | grep docker root 2398 1 0 19:26 ? 00:00:00 /usr/bin/docker -dhadoop 2817 2761 0 19:27 pts/0 00:00:00 grep docker[hadoop@www.hadoop02.com ~]$docker versionClient version: 1.7.1Client API version: 1.19Go version (client): go1.4.2Git commit (client): 786b29dOS/Arch (client): linux/amd64Get http:///var/run/docker.sock/v1.19/version: dial unix /var/run/docker.sock: permission denied. Are you trying to connect to a TLS-enabled daemon without TLS?[hadoop@www.hadoop02.com ~]$docker -d FATA[0000] Error starting daemon: open /var/run/docker.pid: permission denied 添加hadoop用户到docker组12[hadoop@www.hadoop02.com run]$sudo gpasswd -a hadoop dockerAdding user hadoop to group docker 查看docker信息12345678910111213141516171819202122232425262728[hadoop@www.hadoop02.com ~]$docker infoContainers: 0Images: 0Storage Driver: devicemapper Pool Name: docker-253:0-798129-pool Pool Blocksize: 65.54 kB Backing Filesystem: extfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 305.7 MB Data Space Total: 107.4 GB Data Space Available: 3.967 GB Metadata Space Used: 729.1 kB Metadata Space Total: 2.147 GB Metadata Space Available: 2.147 GB Udev Sync Supported: true Deferred Removal Enabled: false Data loop file: /var/lib/docker/devicemapper/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata Library Version: 1.02.95-RHEL6 (2015-04-15)Execution Driver: native-0.2Logging Driver: json-fileKernel Version: 2.6.32-573.el6.x86_64Operating System: &lt;unknown&gt;CPUs: 1Total Memory: 1.82 GiBName: www.hadoop02.comID: XL2K:2DEZ:GF27:JL4P:AHWJ:HEGF:QNQ7:HVT3:3SLS:75GJ:LRQM:E7OJ 可以用ip addr看docker0网桥分配了一个私有网段12345docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN link/ether 26:75:34:2e:06:26 brd ff:ff:ff:ff:ff:ff inet 172.17.42.1/16 scope global docker0 inet6 fe80::2475:34ff:fe2e:626/64 scope link valid_lft forever preferred_lft forever 上传docker文件build文件，注意最后有个点docker build -t hadoop/zookeeper:3.4.6 -f zookeeper.Dockerfile .发现报错，原因是没有安装REPL 12345wget http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpmrpm -ivh epel-release-6-8.noarch.rpmyum repolistyum makecache 不执行此句也可 执行docker search centos可以看到信息123456789101112131415161718192021222324252627docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 2757 [OK] jdeathe/centos-ssh CentOS-6 6.8 x86_64 / CentOS-7 7.2.1511 x8... 42 [OK]jdeathe/centos-ssh-apache-php CentOS-6 6.8 x86_64 / Apache / PHP / PHP M... 21 [OK]nimmis/java-centos This is docker images of CentOS 7 with dif... 16 [OK]gluster/gluster-centos Official GlusterFS Image [ CentOS7 + Glus... 12 [OK]million12/centos-supervisor Base CentOS-7 with supervisord launcher, h... 12 [OK]torusware/speedus-centos Always updated official CentOS docker imag... 8 [OK]nickistre/centos-lamp LAMP on centos setup 7 [OK]kinogmt/centos-ssh CentOS with SSH 6 [OK]nathonfowlie/centos-jre Latest CentOS image with the JRE pre-insta... 4 [OK]centos/mariadb55-centos7 3 [OK]consol/sakuli-centos-xfce Sakuli JavaScript based end-2-end testing ... 2 [OK]timhughes/centos Centos with systemd installed and running 1 [OK]blacklabelops/centos CentOS Base Image! Built and Updates Daily! 1 [OK]darksheer/centos Base Centos Image -- Updated hourly 1 [OK]harisekhon/centos-scala Scala + CentOS (OpenJDK tags 2.10-jre7 - 2... 1 [OK]harisekhon/centos-java Java on CentOS (OpenJDK, tags jre/jdk7-8) 1 [OK]repositoryjp/centos Docker Image for CentOS. 0 [OK]labengine/centos Centos image base 0 [OK]smartentry/centos centos with smartentry 0 [OK]januswel/centos yum update-ed CentOS image 0 [OK]vcatechnology/centos A CentOS Image which is updated daily 0 [OK]grayzone/centos auto build for centos. 0 [OK]ustclug/centos USTC centos 0 [OK]kz8s/centos Official CentOS plus epel-release 0 [OK] 重新执行docker build 12345678[hadoop@www.hadoop02.com ~]$docker run -itd --name zookeeper -h zookeeper -p 2181:2181 jason/zookeeper:3.4.6 bashb023a24424772462cc8d63e79dc3c3fe9e06dd0825ad617c65006e1ef331656d[hadoop@www.hadoop02.com ~]$docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb023a2442477 jason/zookeeper:3.4.6 "sh /opt/zookeeper/s 11 seconds ago Up 8 seconds 0.0.0.0:2181-&gt;2181/tcp zookeeper docker build -t hadoop/kafka:0.8.2.2 -f zookeeper.Dockerfile .]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka_high_level_api测试]]></title>
    <url>%2F2016%2F11%2F25%2Fkafka_high_level_api%2F</url>
    <content type="text"><![CDATA[1.查看官网APIKafka API Automatic Offset Committing1234567891011121314Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("foo", "bar")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; Manual Offset Control123456789101112131415161718192021Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "false"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("foo", "bar")); final int minBatchSize = 200; List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = new ArrayList&lt;&gt;(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; buffer.add(record); &#125; if (buffer.size() &gt;= minBatchSize) &#123; insertIntoDb(buffer); consumer.commitSync(); buffer.clear(); &#125; &#125; 另外auto.offset.reset这个参数可以设置为smallest和largestlargest表示接受接收最大的offset(即最新消息),smallest表示最小offset,即从topic的开始位置消费所有消息. Name Description Type Default Valid Values Importance auto.offset.reset What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted) earliest: automatically reset the offset to the earliest offset latest: automatically reset the offset to the latest offset. none: throw exception to the consumer if no previous offset is found for the consumer’s group anything else: throw exception to the consumer. string latest [latest, earliest, none] medium kafka high level API提交offset首先判断config.offsetsStorage == “zookeeper”如果是zk，通过foreach提交所消费的所有的topic的所有的partition的offset。提交的方式是commitOffsetToZooKeeper-&gt;updatePersistentPath如果是kafka会通过NIO的offsetchannel提交当前的offset到broker kafka high level API测试启动kafka1bin/kafka-server-start.sh config/server.properties &amp; nohup kafka-server-start.sh /home/hadoop/apps/kafka_2.11-0.9.0.1/config/server.properties &amp; 创建topic_high_level_api_test1bin/kafka-topics.sh --create --zookeeper www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka --replication-factor 1 --partitions 3 --topic topic_high_level_api_test 启动kafka high level demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package kafka.javaapi.consumer;import kafka.consumer.Consumer;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.message.MessageAndMetadata;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;/** * Created by zhanghongming on 2016/11/26. */public class DemoHighLevelConsumer &#123; public static void main(String[] args) &#123; args = new String[]&#123;"www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka", "topic_high_level_api_test", "group1", "consumer1"&#125;; if (args == null || args.length != 4) &#123; System.err.print( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;zookeeper_list&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;consumer_id&#125;"); System.exit(1); &#125; String zk = args[0]; String topic = args[1]; String groupid = args[2]; String consumerid = args[3]; Properties props = new Properties(); props.put("zookeeper.connect", zk); props.put("zookeeper.session.timeout.ms", "3600000"); props.put("group.id", groupid); props.put("client.id", "test"); props.put("consumer.id", consumerid); props.put("auto.offset.reset","smallest");// "largest"); props.put("auto.commit.enable", "true"); props.put("auto.commit.interval.ms", "60000"); ConsumerConfig consumerConfig = new ConsumerConfig(props); ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(consumerConfig); Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(topic, 1); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumerConnector.createMessageStreams(topicCountMap); KafkaStream&lt;byte[], byte[]&gt; stream1 = consumerMap.get(topic).get(0); ConsumerIterator&lt;byte[], byte[]&gt; it1 = stream1.iterator(); while (it1.hasNext()) &#123; MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it1.next(); String message = String.format("Consumer ID:%s, Topic:%s, GroupID:%s, PartitionID:%s, Offset:%s, Message Key:%s, Message Payload: %s", consumerid, messageAndMetadata.topic(), groupid, messageAndMetadata.partition(), messageAndMetadata.offset(), new String(messageAndMetadata.key()),new String(messageAndMetadata.message())); System.out.println(message); &#125; &#125;&#125; 启动生产者用随机算法生产数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package kafka;import java.util.ArrayList;import java.util.List;import java.util.Properties;import java.util.Scanner;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import kafka.serializer.StringEncoder;public class ProducerDemo &#123; static private final String TOPIC = "topic_high_level_api_test"; static private final String ZOOKEEPER = "www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka"; static private final String BROKER_LIST = "www.hadoop01.com:9092,www.hadoop02.com:9092,www.hadoop03.com:9092";// static private final int PARTITIONS = TopicAdmin.partitionNum(ZOOKEEPER, TOPIC); static private final int PARTITIONS = 3; public static void main(String[] args) throws Exception &#123; Producer&lt;String, String&gt; producer = initProducer(); System.out.print("1111"); sendOne(producer, TOPIC); &#125; private static Producer&lt;String, String&gt; initProducer() &#123; Properties props = new Properties(); props.put("metadata.broker.list", BROKER_LIST); // props.put("serializer.class", "kafka.serializer.StringEncoder"); props.put("serializer.class", StringEncoder.class.getName()); props.put("partitioner.class", RoundRobinPartitioner.class.getName()); // props.put("partitioner.class", "kafka.producer.DefaultPartitioner");// props.put("compression.codec", "0"); props.put("producer.type", "async"); props.put("batch.num.messages", "3"); props.put("queue.buffer.max.ms", "10000000"); props.put("queue.buffering.max.messages", "1000000"); props.put("queue.enqueue.timeout.ms", "20000000"); ProducerConfig config = new ProducerConfig(props); Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config); return producer; &#125; public static void sendOne(Producer&lt;String, String&gt; producer, String topic) throws InterruptedException &#123; KeyedMessage&lt;String, String&gt; message1 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 31"); producer.send(message1); KeyedMessage&lt;String, String&gt; message2 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 32"); producer.send(message2); KeyedMessage&lt;String, String&gt; message3 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 33"); producer.send(message3); KeyedMessage&lt;String, String&gt; message4 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 34"); producer.send(message4); KeyedMessage&lt;String, String&gt; message5 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 35"); producer.send(message5); KeyedMessage&lt;String, String&gt; message6 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 36"); producer.send(message6); KeyedMessage&lt;String, String&gt; message7 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 37"); producer.send(message7); KeyedMessage&lt;String, String&gt; message8 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 38"); producer.send(message8); producer.close(); &#125;&#125; 查看kafka high level demo打印的信息1234567Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:0, Message Key:31, Message Payload: test 32Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:0, Message Key:31, Message Payload: test 31Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:0, Offset:0, Message Key:31, Message Payload: test 33Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:1, Message Key:31, Message Payload: test 35Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:2, Message Key:31, Message Payload: test 38Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:1, Message Key:31, Message Payload: test 34Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:2, Message Key:31, Message Payload: test 37 查看zookeeper里的信息 123456789101112131415161718192021222324252627282930313233343536373839[zk: localhost:2181(CONNECTED) 92] get /kafka/consumers/group1/offsets/topic_high_level_api_test/13cZxid = 0x36000000f4ctime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f7mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000f4cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0[zk: localhost:2181(CONNECTED) 93] get /kafka/consumers/group1/offsets/topic_high_level_api_test/23cZxid = 0x36000000f1ctime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f6mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000f1cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0[zk: localhost:2181(CONNECTED) 94] get /kafka/consumers/group1/offsets/topic_high_level_api_test/01cZxid = 0x36000000eectime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f5mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000eecversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0 可以对应的上，分区0消费了1条数据，分区2和3消费了3条数据 如果我们队上述数据做处理，kafka high level demo里的auto.commit.enable设置为false 启动kafka high level demo，发送数据，再重新启动kafka high level demo 还会消费之前发送的数据，是因为offset没有更改 此时我们可以在代码里增加手工提交的代码1consumerConnector.commitOffsets();]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark算子]]></title>
    <url>%2F2016%2F04%2F05%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[spark的算子分为两大类Transformations和ActionsTransformations： map(func) :返回一个新的分布式数据集，由每个原元素经过func函数转换后组成 filter(func) : 返回一个新的数据集，由经过func函数后返回值为true的原元素组成 flatMap(func) : 类似于map，但是每一个输入元素，会被映射为0到多个输出元素（因此，func函数的返回值是一个Seq，而不是单一元素） mapPartitions(func) 类似于map，但是运行在不同的RDD分区，func函数运行在一个RDD上必须是 Iterator =&gt; Iterator 类 mapPartitionsWithIndex(func) 类似于mapPartitions, 而且还提供了一个表示分区索引的整数值, func函数运行在一个RDD上必须是 (Int, Iterator) =&gt; Iterator 类型. sample(withReplacement, frac, seed) : 根据给定的随机种子seed，随机抽样出数量为frac的数据 union(otherDataset) : 返回一个新的数据集，由原数据集和参数联合而成 intersection(otherDataset) Return a new RDD that contains the intersection of elements in the source dataset and the argument. distinct([numTasks])) Return a new dataset that contains the distinct elements of the source dataset. groupByKey([numTasks]) : 在一个由（K,V）对组成的数据集上调用，返回一个（K，Seq[V])对的数据集。注意：默认情况下，使用8个并行任务进行分组，你可以传入numTask可选参数，根据数据量设置不同数目的Task reduceByKey(func, [numTasks]) : 在一个（K，V)对的数据集上使用，返回一个（K，V）对的数据集，key相同的值，都被使用指定的reduce函数聚合到一起。和groupbykey类似，任务的个数是可以通过第二个可选参数来配置的。 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral “zero” value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. sortByKey([ascending], [numTasks]) When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.-join(otherDataset, [numTasks]) : 在类型为（K,V)和（K,W)类型的数据集上调用，返回一个（K,(V,W))对，每个key中的所有元素都在一起的数据集 groupWith(otherDataset, [numTasks]) : 在类型为（K,V)和(K,W)类型的数据集上调用，返回一个数据集，组成元素为（K, Seq[V], Seq[W]) Tuples。这个操作在其它框架，称为CoGroup cogroup(otherDataset, [numTasks]) When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable, Iterable)) tuples. This operation is also called groupWith. cartesian(otherDataset) : 笛卡尔积。但在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。 pipe(command, [envVars]) Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process’s stdin and lines output to its stdout are returned as an RDD of strings. coalesce(numPartitions) Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset. repartition(numPartitions) Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. repartitionAndSortWithinPartitions(partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery. 以上Transformation算子详细又分为如下两类：1.Value数据类型的Transformation算子。输入分区与输出分区一对一型 map、flatMap、mapPartitions输入分区与输出分区多对一型 union、cartesian输入分区与输出分区多对多型 groupBy输出分区为输入分区子集型 filter、distinct、subtract、sample、takeSampleCache型 cache、persist 2.Key-Value数据类型的Transfromation算子。类型 算子输入分区与输出分区一对一 mapValues对单个RDD combineByKey、reduceByKey、partitionBy两个RDD聚集 Cogroup连接 join、leftOutJoin、rightOutJoin Actions： reduce(func) : 通过函数func聚集数据集中的所有元素。Func函数接受2个参数，返回一个值。这个函数必须是关联性的，确保可以被正确的并发执行 collect() : 在Driver的程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用，直接将整个RDD集Collect返回，很可能会让Driver程序OOM count() : 返回数据集的元素个数 take(n) : 返回一个数组，由数据集的前n个元素组成。注意，这个操作目前并非在多个节点上，并行执行，而是Driver程序所在机器，单机计算所有的元素(Gateway的内存压力会增大，需要谨慎使用） first() : 返回数据集的第一个元素（类似于take(1)） take(n) Return an array with the first n elements of the dataset. takeSample(withReplacement, num, [seed]) Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. takeOrdered(n, [ordering]) Return the first n elements of the RDD using either their natural order or a custom comparator. saveAsTextFile(path) : 将数据集的元素，以textfile的形式，保存到本地文件系统，hdfs或者任何其它hadoop支持的文件系统。Spark将会调用每个元素的toString方法，并将它转换为文件中的一行文本 saveAsSequenceFile(path) : 将数据集的元素，以sequencefile的格式，保存到指定的目录下，本地系统，hdfs或者任何其它hadoop支持的文件系统。RDD的元素必须由key-value对组成，并都实现了Hadoop的Writable接口，或隐式可以转换为Writable（Spark包括了基本类型的转换，例如Int，Double，String等等） saveAsObjectFile(path)(Java and Scala) Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile(). countByKey() Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. foreach(func) : 在数据集的每一个元素上，运行函数func。这通常用于更新一个累加器变量，或者和外部存储系统做交互]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windwos下启动spark]]></title>
    <url>%2F2016%2F04%2F02%2Fwindwos%E4%B8%8B%E5%90%AF%E5%8A%A8spark%2F</url>
    <content type="text"><![CDATA[安装scala（略）上文已经安装好了hadoop下面安装spark 下载和hadoop匹配版本的sparkhttp://spark.apache.org/downloads.html 博主下载的spark-2.1.0-bin-hadoop2.6.tgz解药到添加环境变量D:\tool\spark-2.1.0-bin-hadoop2.6\bin 到path下 小提示：如果运行spark代码的时候提示Could not locate executable null\bin\winutils.exe in the Hadoop binaries.解决方法：System.setProperty(“hadoop.home.dir”, “D:\tool\hadoop-2.6.0”); 如果和winutils相关的问题可能还需要下载winutils地址：https://github.com/steveloughran/winutils把对应hadoop版本的winutils放在hadoop的bin目录下 直接用spark-shell在cmd里启动即可 测试：加载文件val testlog=sc.textFile(“inst.ini”)获取第一行数据、testlog.first获取行数testlog.count过滤包含General的行val linesWithGeneral=testlog.filter(line =&gt; line.contains(“General”))过滤不包含General的行val linesNoWithGeneral=testlog.filter(line =&gt; !line.contains(“General”))循环打印linesNoWithGeneral.collect.foreach(println)求单词最多的行单词数testlog.map(line =&gt; line.split(“ “).size).reduce((a, b) =&gt; if (a &gt; b) a else b)也可以引用数学工具，写成textFile.map(line =&gt; line.split(“ “).size).reduce((a, b) =&gt; Math.max(a, b))计算wordcount（此处使用reduceByKey会生成ShuffledRDD）val wordCounts = textFile.flatMap(line =&gt; line.split(“ “)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b) 写个程序在本地跑一下12345678910111213141516171819202122package cn.zwjf.test/** * Created by Administrator on 2017/4/5. */import org.apache.spark.SparkContextimport org.apache.spark.SparkConfobject SimpleApp &#123; def main(args: Array[String]) &#123; System.setProperty("hadoop.home.dir", "D:\\tool\\hadoop-2.6.0"); val logFile = "D:\\tool\\spark-2.1.0-bin-hadoop2.6\\README.md" // Should be some file on your system val conf = new SparkConf().setAppName("Simple Application").setMaster("local[2]") val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line =&gt; line.contains("Spark")).count() val numBs = logData.filter(line =&gt; line.contains("http")).count() println(s"Lines with Spark: $numAs, Lines with http: $numBs") sc.stop() &#125;&#125; 输出Lines with a: 20, Lines with b: 10]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windwos下启动hadoop]]></title>
    <url>%2F2016%2F04%2F02%2Fwindwos%E4%B8%8B%E5%90%AF%E5%8A%A8hadoop%2F</url>
    <content type="text"><![CDATA[安装JDK（略） 下载hadoop2.6.0地址：https://archive.apache.org/dist/hadoop/common/hadoop-2.6.0/选择编译后的文件hadoop-2.6.0.tar.gz 下载解压，博主解压到D:\tool下 配置环境变量：HADOOP_HOMED:\tool\hadoop-2.6.0 修改hadoop配置文件core-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml没有就将mapred-site.xml.template重命名为mapred-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;hdfs://localhost:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011&lt;configuration&gt; &lt;!-- 这个参数设置为1，因为是单机版hadoop --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop-env.cmd 注释原来的JAVA_HOME，增加你本地的路径（注意JDK路径为C:\Program Files\Java\jdk1.8.0_74）123@rem set JAVA_HOME=%JAVA_HOME%set JAVA_HOME=C:\Progra~1\Java\jdk1.8.0_74 格式化hdfs namenode -format 到hadoop的sbin目录执行“start-all.cmd”，它将会启动以下进程。 打开http://localhost:8088/cluster]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-MLlib-数据类型实战]]></title>
    <url>%2F2016%2F03%2F30%2Fspark-MLlib-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[监督学习：给定数据集，知道要预测什么 非监督学习：不告诉计算机要做什么，让他自己去发现数据的内部机构 半监督学习：考虑如何用少量的标注样本和大量的未标注样本进行训练和分类 强化学习：强化学习通过观察来学习动作的完成，每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。在这种学习模式下，输入数据作为对模型的反馈 实战练习：本地向量// 创建一个密集向量 (1.0, 0.0, 3.0).val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)// 创建一个稀疏向量 (1.0, 0.0, 3.0) 通过制定对应于非零项的索引和值val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))// 创建稀疏向量 (1.0, 0.0, 3.0) 通过指定非零项val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0))) 行矩阵需要引入如下包123456789101112131415import org.apache.spark.mllib.linalg.&#123;Vector, Vectors&#125;import org.apache.spark.mllib.linalg.distributed.RowMatriximport org.apache.spark.rdd.RDD代码：val dv1 : Vector = Vectors.dense(1.0,2.0,3.0)val dv2 : Vector = Vectors.dense(2.0,3.0,4.0)val rows : RDD[Vector] = sc.parallelize(Array(dv1,dv2))val mat: RowMatrix = new RowMatrix(rows)// Get its size.val m = mat.numRows()val n = mat.numCols()print(m) //2print(n) //3 我们可以通过其自带的computeColumnSummaryStatistics()方法获取该矩阵的一些统计摘要信息，并可以对其进行QR分解，SVD分解和PCA分解1234567891011val summary = mat.computeColumnSummaryStatistics()val count = summary.countprintln(count) //2val max = summary.maxprintln(max) //[2.0,3.0,4.0]val variance = summary.varianceprintln(variance) //[0.5,0.5,0.5]val mean = summary.meanprintln(mean) //[1.5,2.5,3.5]val normL1 = summary.normL1println(normL1) //[3.0,5.0,7.0] 索引行矩阵需要引入12import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.distributed.&#123;IndexedRow, IndexedRowMatrix&#125; 代码12345val idxr1 = IndexedRow(1,dv1)val idxr2 = IndexedRow(2,dv2)val idxrows = sc.parallelize(Array(idxr1,idxr2))val idxmat: IndexedRowMatrix = new IndexedRowMatrix(idxrows)idxmat.rows.foreach(println) IndexedRow(2,[2.0,3.0,4.0])IndexedRow(1,[1.0,2.0,3.0]) 坐标矩阵MatrixEntry的参数意义：(i: Long, j: Long, value: Double)，其中i是行索引，j是列索引，value是该位置的值。坐标矩阵一般在矩阵的两个维度都很大，且矩阵非常稀疏的时候使用需要引入：1import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125; 代码：1234567val conf = new SparkConf().setAppName("Matrix").setMaster("local[2]")val sc = new SparkContext(conf)var ent1 = new MatrixEntry(0,1,0.5)var ent2 = new MatrixEntry(2,2,1.8)val entries : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2))val coordMat: CoordinateMatrix = new CoordinateMatrix(entries)coordMat.entries.foreach(println) MatrixEntry(2,2,1.8)MatrixEntry(0,1,0.5) 12val transMat: CoordinateMatrix = coordMat.transpose() //转置transMat.entries.foreach(println) MatrixEntry(1,0,0.5)MatrixEntry(2,2,1.8) 12val indexedRowMatrix = transMat.toIndexedRowMatrix() //转换成索引行矩阵indexedRowMatrix.rows.foreach(println) IndexedRow(2,(3,[2],[1.8]))IndexedRow(1,(3,[0],[0.5])) 分块矩阵分块矩阵是基于矩阵块MatrixBlock构成的RDD的分布式矩阵，其中每一个矩阵块MatrixBlock都是一个元组((Int, Int), Matrix)，其中(Int, Int)是块的索引，而Matrix则是在对应位置的子矩阵（sub-matrix），其尺寸由rowsPerBlock和colsPerBlock决定，默认值均为1024。分块矩阵支持和另一个分块矩阵进行加法操作和乘法操作，并提供了一个支持方法validate()来确认分块矩阵是否创建成功。分块矩阵可由索引行矩阵IndexedRowMatrix或坐标矩阵CoordinateMatrix调用toBlockMatrix()方法来进行转换，该方法将矩阵划分成尺寸默认为1024x1024的分块，可以在调用toBlockMatrix(rowsPerBlock, colsPerBlock)方法时传入参数来调整分块的尺寸。下面以矩阵A（如图）为例，先利用矩阵项MatrixEntry将其构造成坐标矩阵，再转化成如图所示的4个分块矩阵，最后对矩阵A与其转置进行乘法运算： 需要引入12import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.BlockMatrix 123456789101112131415161718192021222324252627282930313233343536373839ent1 = new MatrixEntry(0,0,1)ent2 = new MatrixEntry(1,1,1)val ent3 = new MatrixEntry(2,0,-1)val ent4 = new MatrixEntry(2,1,2)val ent5 = new MatrixEntry(2,2,1)val ent6 = new MatrixEntry(3,0,1)val ent7 = new MatrixEntry(3,1,1)val ent8 = new MatrixEntry(3,3,1)val entries2 : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2,ent3,ent4,ent5,ent6,ent7,ent8))val coordMat2: CoordinateMatrix = new CoordinateMatrix(entries2)val matA: BlockMatrix = coordMat2.toBlockMatrix(2,2).cache()//将坐标矩阵转换成2x2的分块矩阵并存储matA.validate()println("块矩阵")val localMatrix: Matrix = matA.toLocalMatrixprintln(localMatrix)/*1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */val numColBlocks = matA.numColBlocks //2println(numColBlocks)val numRowBlocks = matA.numColBlocks //2println(numRowBlocks)val ata = matA.transpose.multiply(matA)//计算其转置矩阵和矩阵的积矩阵val matrix = ata.toLocalMatrixprintln(matrix)/*3.0 -1.0 -1.0 1.0 -1.0 6.0 2.0 1.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ 整体测试代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package mllibimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.&#123;Matrix, Vector, Vectors&#125;import org.apache.spark.mllib.linalg.distributed.RowMatriximport org.apache.spark.rdd.RDDimport spark_streaming.LoggerLevelsimport org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.&#123;IndexedRow, IndexedRowMatrix&#125;import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.BlockMatrixclass VectorTets&#123;&#125;/** * Created by zhanghongming on 2017/4/8. */object VectorTets &#123; def main(args: Array[String]) &#123; val dv:Vector =Vectors.dense(1.0,2.0,3.0) /*val rows: RDD[Vector] = print(dv)*/ LoggerLevels.setStreamingLogLevels() //StreamingContext val conf = new SparkConf().setAppName("Matrix").setMaster("local[2]") val sc = new SparkContext(conf) val dv1 : Vector = Vectors.dense(1.0,2.0,3.0) val dv2 : Vector = Vectors.dense(2.0,3.0,4.0) val rows : RDD[Vector] = sc.parallelize(Array(dv1,dv2)) val mat: RowMatrix = new RowMatrix(rows) // Get its size. val m = mat.numRows() val n = mat.numCols() println(m) //2 println(n) //3 mat.rows.foreach(println) /* [2.0,3.0,4.0] [1.0,2.0,3.0] * */ val summary = mat.computeColumnSummaryStatistics() val count = summary.count println(count) //2 val max = summary.max println(max) //[2.0,3.0,4.0] val variance = summary.variance println(variance) //[0.5,0.5,0.5] val mean = summary.mean println(mean) //[1.5,2.5,3.5] val normL1 = summary.normL1 println(normL1) //[3.0,5.0,7.0] /////索引行矩阵 val idxr1 = IndexedRow(1,dv1) val idxr2 = IndexedRow(2,dv2) val idxrows = sc.parallelize(Array(idxr1,idxr2)) val idxmat: IndexedRowMatrix = new IndexedRowMatrix(idxrows) idxmat.rows.foreach(println) /////坐标矩阵 var ent1 = new MatrixEntry(0, 1, 0.5) var ent2 = new MatrixEntry(2, 2, 1.8) var entries : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2)) val coordMat: CoordinateMatrix = new CoordinateMatrix(entries) coordMat.entries.foreach(println) val transMat: CoordinateMatrix = coordMat.transpose() //转置 transMat.entries.foreach(println) val indexedRowMatrix = transMat.toIndexedRowMatrix() //转换成索引行矩阵 indexedRowMatrix.rows.foreach(println) //块矩阵 ent1 = new MatrixEntry(0,0,1) ent2 = new MatrixEntry(1,1,1) val ent3 = new MatrixEntry(2,0,-1) val ent4 = new MatrixEntry(2,1,2) val ent5 = new MatrixEntry(2,2,1) val ent6 = new MatrixEntry(3,0,1) val ent7 = new MatrixEntry(3,1,1) val ent8 = new MatrixEntry(3,3,1) val entries2 : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2,ent3,ent4,ent5,ent6,ent7,ent8)) val coordMat2: CoordinateMatrix = new CoordinateMatrix(entries2) val matA: BlockMatrix = coordMat2.toBlockMatrix(2,2).cache()//将坐标矩阵转换成2x2的分块矩阵并存储 matA.validate() println("块矩阵") val localMatrix: Matrix = matA.toLocalMatrix println(localMatrix) /* 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ val numColBlocks = matA.numColBlocks//2 println(numColBlocks) val numRowBlocks = matA.numColBlocks//2 println(numRowBlocks) println("计算其转置矩阵") println( matA.transpose.toLocalMatrix()) println("计算其转置矩阵和矩阵的积矩阵") val ata = matA.transpose.multiply(matA)//计算其转置矩阵和矩阵的积矩阵 val matrix = ata.toLocalMatrix println(matrix) /* 3.0 -1.0 -1.0 1.0 -1.0 6.0 2.0 1.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ &#125;&#125; pom文件1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.10.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设置模式-软件设计原则]]></title>
    <url>%2F2016%2F03%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99(1)%2F</url>
    <content type="text"><![CDATA[开闭原则Software entities should be open for extension,but closed for modificationfrom 《Object Oriented Software Construction》 by Bertrand Meyer at 1988 entities:实体对像extension:扩展 通俗来讲在设计一个模块的时候,应当使这个模块可以在不被修改的前提下被扩展例如：spring在装载一个类后，想要增加新功能扩展一个类，spring就会自动装载这个类 和面向接口编程统一起来 合成/聚合复用原则使用聚合，组合代替继承 but， what is 聚合组合？e.g.class Driver { //使用成员变量形式实现聚合关系 Car mycar; public void drive(){ mycar.run(); }}spring 装配即是这个原则 but, when use 继承？当子类是父类的一种特例，从语义来说是一个单独的父类，而不是为了复用代码 23种设计模式引用：代理模式：真实主题作为代理类的一部分。例如日志，安全，缓存可以放在代理类里，需要调用真实主题的时候才会去调用spring 的AOP就是代理模式 里氏代换原则子类可以在所有场合代替父类通俗来讲：子类可以扩展父类的功能，但不能改变父类原有的功能 23种设计模式引用：模板方法：do not call us,we will call you 依赖倒置原则Abstractions should not depend upon details. Details should depend upon abstractions.抽象不应当依赖于细节,细节应当依赖于抽象 e.g. 客户端依赖接口，接口有不同的实现。客户端不依赖实现爱人依赖人这个借口，人借口下面有鲁国人，秦国人等实现 接口隔离原则Clients should not be forced to depend upon interfaces that they don’t use——客户端不应该依赖它不需用的接口。The dependency of one class to another one should depend on the smallest possible interface——类间的依赖关系应该建立在最小的接口上 迪米特法则1）只与你直接的朋友们通信。2）不要跟“陌生人”说话。3）每一个软件单位对其他的单位都只有最少的知识，而且局限于那些与本单位密切相关的软件单位。 但并不是绝对的例如StringBuffer 的append方法可以一直append下去，但是每次返回的都是它本身，这样不会造成空指针等异常 23种设计模式引用：门面（外观）模式和调停者（中介者）模式实际上就是迪米特法则的具体应用。 单一职责原则一个类，应该只有一个职责。每一个职责都是变化的一个轴线，如果一个类有一个以上的职责，这些职责就耦合在了一起。这会导致脆弱的设计。当一个职责发生变化时，可能会影响其它的职责。另外，多个职责耦合在一起，会影响复用性。我们可能只需要复用该类的某一个职责，但这个职责跟其它职责耦合在了一起，很难分离出来]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-线程]]></title>
    <url>%2F2016%2F03%2F08%2Fjava%20thread%2F</url>
    <content type="text"><![CDATA[ThreadLocal：线程本地变量（ThreadLocal为变量在每个线程中都创建了一个副本） 先了解一下ThreadLocal类提供的几个方法： public T get() { }public void set(T value) { }public void remove() { }protected T initialValue() { } get()方法是用来获取ThreadLocal在当前线程中保存的变量副本，通过他获取ThreadLocalMap（当前线程t中的一个成员变量threadLocals。）ThreadLocalMap的Entry继承了WeakReferenceset()用来设置当前线程中变量的副本，remove()用来移除当前线程中变量的副本initialValue()是一个protected方法,目前返回null 从如上引出一个问题：WeakReference：当一个对象o被创建时, 它被放在Heap里. 当GC运行的时候, 如果发现没有任何引用指向o, o就会被回收以腾出内存空间. 或者换句话说, 一个对象被回收, 必须满足两个条件: 1)没有任何引用指向它 2)GC被运行. 当一个对象仅仅被weak reference指向, 而没有任何其他strong reference指向的时候, 如果GC运行, 那么这个对象就会被回收. 多线程的几个方法：yield()：做的是让当前运行线程回到可运行状态，以允许具有相同优先级的其他线程获得运行机会。 因此，使用yield()的目的是让相同优先级的线程之间能适当的轮转执行。但是，实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。 join(): 方法的主要作用就是同步，它可以使得线程之间的并行执行变为串行执行。原理是：join方法是通过调用线程的wait方法来达到同步的目的的。例如，A线程中调用了B线程的join方法，则相当于A线程调用了B线程的wait方法，在调用了B线程的wait方法后，A线程就会进入阻塞状态]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机科学导论]]></title>
    <url>%2F2016%2F02%2F07%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[记录为什么要用机器语言让计算机来执行指令而不用我们所学的英语或其他语言呢？举个例子：如果你让机器去执行biweekly。设定发工资是按照biweekly来执行，那么机器会去按照每两周发一次，还是每周发两次呢？自然语言的语义多种多样。 python不能使用字符串+数字，但是可以使用字符串*数字]]></content>
      <categories>
        <category>计算机科学导论</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAP原理]]></title>
    <url>%2F2015%2F03%2F03%2FCAP%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Consistency 一致性Ø 通过某个节点的写操作结果对后面通过其它节点的读操作可见Ø 如果更新数据后，并发访问情况下可立即感知该更新，称为强一致性Ø 如果允许之后部分或者全部感知不到该更新，称为弱一致性Ø 若在之后的一段时间（通常该时间不固定）后，一定可以感知该更新，称为最终一致性 Availability 可用性Ø 任何一个没有发生故障的节点必须在有限的时间内返回合理的结果 Partition tolerance 分区容忍性Ø 部分节点宕机或者无法与其它节点通信时，各分区间还可保持分布式系统的功能 结论：分布式存储系统中，一致性，可用性，分区容忍性只能满足两个，不能满足三个。由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。因此很多时候是在可用性和一致性之间权衡。。]]></content>
      <categories>
        <category>原理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jdk8 Optional]]></title>
    <url>%2F2014%2F12%2F04%2Fjdk8-Optional%2F</url>
    <content type="text"><![CDATA[java 8 Optional1.of方法：为非null的值创建一个Optional。 of方法通过工厂方法创建Optional类。需要注意的是，创建对象时传入的参数不能为null。如果传入参数为null，则抛出NullPointerException 。 //调用工厂方法创建Optional实例Optional name = Optional.of(“Sanaulla”);//ok//传入参数为null，抛出NullPointerException.Optional someNull = Optional.of(null);//java.lang.NullPointerException ofNullable为指定的值创建一个Optional，如果指定的值为null，则返回一个空的Optional。 ofNullable与of方法相似，唯一的区别是可以接受参数为null的情况。示例如下： Optional empty = Optional.ofNullable(null);isPresent如果值存在返回true，否则返回false。 Optional name = Optional.of(“name”);Optional name2 = Optional.ofNullable(null);if (name.isPresent()) { //在Optional实例内调用get()返回已存在的值 System.out.println(name.get());//names} orElse如果有值则将其返回，否则返回指定的其它值。 Optional name = Optional.ofNullable(null);System.out.println(name.orElse(“There is some value!”)); orElseGetorElseGet与orElse方法类似，区别在于得到的默认值。orElse方法将传入的字符串作为默认 System.out.println(name.orElseGet(() -&gt; “Default Value”));]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jdk8 引用方法]]></title>
    <url>%2F2014%2F12%2F03%2Fjdk8-%E5%BC%95%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[java 8 引用方法我们通常使用lambda表达式来创建匿名方法。然而，有时候我们仅仅是调用了一个已存在的方法。如下: Arrays.sort(stringsArray,(s1,s2)-&gt;s1.compareToIgnoreCase(s2)); 在Java8中，我们可以直接通过方法引用来简写lambda表达式中已经存在的方法。 Arrays.sort(stringsArray, String::compareToIgnoreCase); Person [] persons = new Person[10]; //使用匿名类Arrays.sort(persons, new Comparator() { @Override public int compare(Person o1, Person o2) { return o1.birthday.compareTo(o2.birthday); } }); //使用lambda表达式Arrays.sort(persons, (o1, o2) -&gt; o1.birthday.compareTo(o2.birthday)); //使用lambda表达式和类的静态方法Arrays.sort(persons, (o1, o2) -&gt; Person.compareByAge(o1,o2)); //使用方法引用//引用的是类的静态方法Arrays.sort(persons, Person::compareByAge);]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[解密JDK8 枚举]]></title>
    <url>%2F2014%2F12%2F02%2Fjdk8-enum%2F</url>
    <content type="text"><![CDATA[写一个枚举类123456public enum Season &#123; SPRING, SUMMER, AUTUMN, WINTER&#125; 然后我们使用javac编译上面的类,得到class文件然后,我们利用反编译的方法来看看字节码文件究竟是什么.这里使用的工具是javap的简单命令,先列举一下这个Season下的全部元素. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190Classfile /C:/Season.class Last modified 2016-12-8; size 956 bytes MD5 checksum 7f6dfb988d182327a1a73ee986a9d3fa Compiled from "Season.java"public final class cn.redis.model.Season extends java.lang.Enum&lt;cn.redis.model.Season&gt; minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_FINAL, ACC_SUPER, ACC_ENUMConstant pool: #1 = Fieldref #4.#38 // cn/redis/model/Season.$VALUES:[Lcn/redis/model/Season; #2 = Methodref #39.#40 // "[Lcn/redis/model/Season;".clone:()Ljava/lang/Object; #3 = Class #23 // "[Lcn/redis/model/Season;" #4 = Class #41 // cn/redis/model/Season #5 = Methodref #16.#42 // java/lang/Enum.valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; #6 = Methodref #16.#43 // java/lang/Enum."&lt;init&gt;":(Ljava/lang/String;I)V #7 = String #17 // SPRING #8 = Methodref #4.#43 // cn/redis/model/Season."&lt;init&gt;":(Ljava/lang/String;I)V #9 = Fieldref #4.#44 // cn/redis/model/Season.SPRING:Lcn/redis/model/Season; #10 = String #19 // SUMMER #11 = Fieldref #4.#45 // cn/redis/model/Season.SUMMER:Lcn/redis/model/Season; #12 = String #20 // AUTUMN #13 = Fieldref #4.#46 // cn/redis/model/Season.AUTUMN:Lcn/redis/model/Season; #14 = String #21 // WINTER #15 = Fieldref #4.#47 // cn/redis/model/Season.WINTER:Lcn/redis/model/Season; #16 = Class #48 // java/lang/Enum #17 = Utf8 SPRING #18 = Utf8 Lcn/redis/model/Season; #19 = Utf8 SUMMER #20 = Utf8 AUTUMN #21 = Utf8 WINTER #22 = Utf8 $VALUES #23 = Utf8 [Lcn/redis/model/Season; #24 = Utf8 values #25 = Utf8 ()[Lcn/redis/model/Season; #26 = Utf8 Code #27 = Utf8 LineNumberTable #28 = Utf8 valueOf #29 = Utf8 (Ljava/lang/String;)Lcn/redis/model/Season; #30 = Utf8 &lt;init&gt; #31 = Utf8 (Ljava/lang/String;I)V #32 = Utf8 Signature #33 = Utf8 ()V #34 = Utf8 &lt;clinit&gt; #35 = Utf8 Ljava/lang/Enum&lt;Lcn/redis/model/Season;&gt;; #36 = Utf8 SourceFile #37 = Utf8 Season.java #38 = NameAndType #22:#23 // $VALUES:[Lcn/redis/model/Season; #39 = Class #23 // "[Lcn/redis/model/Season;" #40 = NameAndType #49:#50 // clone:()Ljava/lang/Object; #41 = Utf8 cn/redis/model/Season #42 = NameAndType #28:#51 // valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; #43 = NameAndType #30:#31 // "&lt;init&gt;":(Ljava/lang/String;I)V #44 = NameAndType #17:#18 // SPRING:Lcn/redis/model/Season; #45 = NameAndType #19:#18 // SUMMER:Lcn/redis/model/Season; #46 = NameAndType #20:#18 // AUTUMN:Lcn/redis/model/Season; #47 = NameAndType #21:#18 // WINTER:Lcn/redis/model/Season; #48 = Utf8 java/lang/Enum #49 = Utf8 clone #50 = Utf8 ()Ljava/lang/Object; #51 = Utf8 (Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum;&#123; public static final cn.redis.model.Season SPRING; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season SUMMER; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season AUTUMN; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season WINTER; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static cn.redis.model.Season[] values(); descriptor: ()[Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=0, args_size=0 0: getstatic #1 // Field $VALUES:[Lcn/redis/model/Season; 3: invokevirtual #2 // Method "[Lcn/redis/model/Season;".clone:()Ljava/lang/Object; 6: checkcast #3 // class "[Lcn/redis/model/Season;" 9: areturn LineNumberTable: line 3: 0 public static cn.redis.model.Season valueOf(java.lang.String); descriptor: (Ljava/lang/String;)Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=1, args_size=1 0: ldc #4 // class cn/redis/model/Season 2: aload_0 3: invokestatic #5 // Method java/lang/Enum.valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; 6: checkcast #4 // class cn/redis/model/Season 9: areturn LineNumberTable: line 3: 0 static &#123;&#125;; descriptor: ()V flags: ACC_STATIC Code: stack=4, locals=0, args_size=0 0: new #4 // class cn/redis/model/Season 3: dup 4: ldc #7 // String SPRING 6: iconst_0 7: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 10: putstatic #9 // Field SPRING:Lcn/redis/model/Season; 13: new #4 // class cn/redis/model/Season 16: dup 17: ldc #10 // String SUMMER 19: iconst_1 20: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 23: putstatic #11 // Field SUMMER:Lcn/redis/model/Se ason; 26: new #4 // class cn/redis/model/Season 29: dup 30: ldc #12 // String AUTUMN 32: iconst_2 33: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 36: putstatic #13 // Field AUTUMN:Lcn/redis/model/Season; 39: new #4 // class cn/redis/model/Season 42: dup 43: ldc #14 // String WINTER 45: iconst_3 46: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 49: putstatic #15 // Field WINTER:Lcn/redis/model/Season; 52: iconst_4 53: anewarray #4 // class cn/redis/model/Season 56: dup 57: iconst_0 58: getstatic #9 // Field SPRING:Lcn/redis/model/Season; 61: aastore 62: dup 63: iconst_1 64: getstatic #11 // Field SUMMER:Lcn/redis/model/Season; 67: aastore 68: dup 69: iconst_2 70: getstatic #13 // Field AUTUMN:Lcn/redis/model/Season; 73: aastore 74: dup 75: iconst_3 76: getstatic #15 // Field WINTER:Lcn/redis/model/Season; 79: aastore 80: putstatic #1 // Field $VALUES:[Lcn/redis/model/Season; 83: return LineNumberTable: line 4: 0 line 5: 13 line 6: 26 line 7: 39 line 3: 52 &#125;Signature: #35 // Ljava/lang/Enum&lt;Lcn/redis/model/Season;&gt;;SourceFile: "Season.java" 从上反编译结果可知 java代码中的Season转换成了继承自的java.lang.enum的类 既然隐式继承自java.lang.enum,也就意味java代码中,Season不能再继承其他的类 Season被标记成了final,意味着它不能被继承 静态块static {}; 0~52为实例化SPRING, SUMMER, AUTUMN, WINTER 53~83为创建Season[]数组$VALUES,并将上面的四个对象放入数组的操作. 8: invokevirtual #4 在switch-case中,还是将Enum转成了int值(通过调用Enum.oridinal()方法)]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2014%2F11%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy 欢迎大家来到我的博客 More info: Deployment]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
</search>
