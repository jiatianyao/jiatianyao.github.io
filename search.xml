<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[flink sliding window join]]></title>
    <url>%2F2019%2F08%2F13%2Fflink%20sliding%20window%20join%2F</url>
    <content type="text"><![CDATA[æ¥ä¸Šç¯‡æ–‡ç« flink operator å…·ä½“åˆ†æä¸‹window joinå®˜ç½‘åœ°å€ï¼šhttps://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/operators/joining.html æ ¹æ®å®˜ç½‘ä¾‹å­ä»demoå®ç°ï¼š å·¥å…·ç±»ï¼šæ¨¡æ‹Ÿsourceï¼šGreenSource å‘é€ï¼šï¼ˆâ€tomâ€,0ï¼‰,ï¼ˆâ€jerryâ€,1ï¼‰ , ï¼ˆâ€aliceâ€,2ï¼‰ , ï¼ˆâ€tomâ€,3ï¼‰, ï¼ˆâ€tomâ€,4ï¼‰OrangeSource å‘é€ï¼šï¼ˆâ€tomâ€,0ï¼‰,ï¼ˆâ€tomâ€,1ï¼‰ , ï¼ˆâ€tomâ€,2ï¼‰ , ï¼ˆâ€tomâ€,3ï¼‰, ï¼ˆâ€tomâ€,4ï¼‰1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.opensourceteams.module.bigdata.flink.window;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import java.io.Serializable;import java.util.Iterator;import java.util.Random;@SuppressWarnings("serial")public class WindowJoinSampleData &#123; static final String[] KEY1 = &#123;"tom", "jerry", "alice", "tom", "tom"&#125;; static final String[] KEY2 = &#123;"tom", "tom", "tom", "tom", "tom"&#125;; static final int[] VALUE = &#123;0,1,2,3,4&#125;; public static class GreenSource implements Iterator&lt;Tuple2&lt;String, Integer&gt;&gt;, Serializable &#123; static int GRADE_COUNT = 0; private final Random rnd = new Random(hashCode()); @Override public boolean hasNext() &#123; return GRADE_COUNT == 5 ? false :true; &#125; @Override public Tuple2&lt;String, Integer&gt; next() &#123; Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = new Tuple2&lt;&gt;(KEY1[GRADE_COUNT], VALUE[GRADE_COUNT]); GRADE_COUNT ++; return stringIntegerTuple2; &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125; public static DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; getSource(StreamExecutionEnvironment env, long rate) &#123; return env.fromCollection(new ThrottledIterator&lt;&gt;(new GreenSource(), rate), TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;()&#123;&#125;)); &#125; &#125; /** * Continuously generates (name, salary). */ public static class OrangeSource implements Iterator&lt;Tuple2&lt;String, Integer&gt;&gt;, Serializable &#123; static int GRADE_COUNT = 0; private final Random rnd = new Random(hashCode()); @Override public boolean hasNext() &#123; return GRADE_COUNT == 5 ? false :true; &#125; @Override public Tuple2&lt;String, Integer&gt; next() &#123; Tuple2&lt;String, Integer&gt; stringIntegerTuple2 = new Tuple2&lt;&gt;(KEY2[GRADE_COUNT], VALUE[GRADE_COUNT]); GRADE_COUNT ++; return stringIntegerTuple2; &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125; public static DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; getSource(StreamExecutionEnvironment env, long rate) &#123; return env.fromCollection(new ThrottledIterator&lt;&gt;(new OrangeSource(), rate), TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;()&#123;&#125;)); &#125; &#125;&#125; æ¨¡æ‹Ÿé¢‘ç‡å‘é€ï¼Œæ¯éš”äº”ç§’å‘é€ä¸€æ¬¡æ•°æ® 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package com.opensourceteams.module.bigdata.flink.window;import java.io.Serializable;import java.util.Iterator;import static java.util.Objects.requireNonNull;public class ThrottledIterator&lt;T&gt; implements Iterator&lt;T&gt;, Serializable &#123; private static final long serialVersionUID = 1L; @SuppressWarnings("NonSerializableFieldInSerializableClass") private final Iterator&lt;T&gt; source; private final long sleepBatchSize; private final long sleepBatchTime; private long lastBatchCheckTime; private long num; public ThrottledIterator(Iterator&lt;T&gt; source, long elementsPerSecond) &#123; this.source = requireNonNull(source); if (!(source instanceof Serializable)) &#123; throw new IllegalArgumentException("source must be java.io.Serializable"); &#125; if (elementsPerSecond &gt;= 1) &#123; // how long does element take this.sleepBatchSize = 1; this.sleepBatchTime = 5000 / elementsPerSecond; &#125; else &#123; throw new IllegalArgumentException("'elements per second' must be positive and not zero"); &#125; &#125; @Override public boolean hasNext() &#123; return source.hasNext(); &#125; @Override public T next() &#123; // delay if necessary if (lastBatchCheckTime &gt; 0) &#123; if (++num &gt;= sleepBatchSize) &#123; num = 0; final long now = System.currentTimeMillis(); final long elapsed = now - lastBatchCheckTime; if (elapsed &lt; sleepBatchTime /2 ) &#123; try &#123; Thread.sleep(sleepBatchTime - elapsed); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); &#125; &#125;else &#123; try &#123; if (elapsed &lt; sleepBatchTime) &#123; Thread.sleep(sleepBatchTime - (sleepBatchTime-elapsed)); &#125;else if(elapsed == sleepBatchTime)&#123; Thread.sleep(sleepBatchTime); &#125;else&#123; Thread.sleep(sleepBatchTime - (elapsed - sleepBatchTime)); &#125; &#125; catch (InterruptedException e) &#123; // restore interrupt flag and proceed Thread.currentThread().interrupt(); &#125; &#125; lastBatchCheckTime = now; &#125; &#125; else &#123; lastBatchCheckTime = System.currentTimeMillis(); &#125; return source.next(); &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125;&#125; ä¸»å‡½æ•°ï¼šä¸ºè§‚å¯Ÿæ–¹ä¾¿ï¼Œå°†æ—¶é—´æ‰©å¤§5000å€ï¼šwindowæ»‘åŠ¨çª—å£5ç§’æ»‘åŠ¨ä¸€æ¬¡ï¼Œçª—å£å¤§å°ä¸º10ç§’12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.opensourceteams.module.bigdata.flink.window;import org.apache.flink.api.common.functions.JoinFunction;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;@SuppressWarnings("serial")public class SlingdingWindowJoin &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool params = ParameterTool.fromArgs(args); final long windowSize = params.getLong("windowSize", 10000); final long windowSlide = params.getLong("windowSize", 5000); final long rate = params.getLong("rate", 1L); // obtain execution environment, run this example in "ingestion time" StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); // make parameters available in the web interface env.getConfig().setGlobalJobParameters(params); // create the data sources for both grades and salaries DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; orangeStream = WindowJoinSampleData.OrangeSource.getSource(env, rate); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; greenStream = WindowJoinSampleData.GreenSource.getSource(env, rate); // run the actual window join program // for testability, this functionality is in a separate method. DataStream&lt;String&gt; joinedStream = runWindowJoin(orangeStream, greenStream, windowSize,windowSlide); // print the results with a single thread, rather than in parallel joinedStream.print().setParallelism(1); // execute program env.execute("Slingding Window Join Example"); &#125; public static DataStream&lt;String&gt; runWindowJoin( DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades, DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries, long windowSize,long windowSlide) &#123; return grades.join(salaries) .where(new NameKeySelector()) .equalTo(new NameKeySelector()) .window(SlidingEventTimeWindows.of(Time.milliseconds(windowSize), Time.milliseconds(windowSlide ))) .apply(new JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, String&gt;() &#123; @Override public String join( Tuple2&lt;String, Integer&gt; first, Tuple2&lt;String, Integer&gt; second) &#123; return first.f1 + "," + second.f1; &#125; &#125;); &#125; private static class NameKeySelector implements KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt; &#123; @Override public String getKey(Tuple2&lt;String, Integer&gt; value) &#123; return value.f0; &#125; &#125;&#125; é¢„æœŸç»“æœï¼š è¿è¡Œç»“æœï¼š]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink operator]]></title>
    <url>%2F2019%2F08%2F10%2Fflink%20operator%2F</url>
    <content type="text"><![CDATA[æ¥æºç åˆ†æäºŒã€‚è¯¦ç»†åˆ†æä¸‹Operator Transformation:1.DataStream -&gt; DataStream map flatMap filter map:1234567DataStream&lt;Integer&gt; dataStream = //...dataStream.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer value) throws Exception &#123; return 2 * value; &#125;&#125;); flatMap:123456789dataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for(String word: value.split(" "))&#123; out.collect(word); &#125; &#125;&#125;); filter123456dataStream.filter(new FilterFunction&lt;Integer&gt;() &#123; @Override public boolean filter(Integer value) throws Exception &#123; return value != 0; &#125;&#125;); 2.DataStream -&gt; KeyedDataStream keyBy 12dataStream.keyBy("someKey") // Key by field "someKey"dataStream.keyBy(0) // Key by the first element of a Tuple 3.KeyedDataStream -&gt; DataStream sum reduce fold min/minBy max/maxBy reduce:1234567keyedStream.reduce(new ReduceFunction&lt;Integer&gt;() &#123; @Override public Integer reduce(Integer value1, Integer value2) throws Exception &#123; return value1 + value2; &#125;&#125;); flod:1234567DataStream&lt;String&gt; result = keyedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String current, Integer value) &#123; return current + "-" + value; &#125; &#125;); aggregations: 12345678910keyedStream.sum(0);keyedStream.sum("key");keyedStream.min(0);keyedStream.min("key");keyedStream.max(0);keyedStream.max("key");keyedStream.minBy(0);keyedStream.minBy("key");keyedStream.maxBy(0);keyedStream.maxBy("key"); 4.WindowedStream -&gt; DataStream sum reduce fold min/minBy max/maxBy apply 1dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data apply:12345678910111213141516171819202122232425windowedStream.apply (new WindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Tuple, Window&gt;() &#123; public void apply (Tuple tuple, Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;);// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply (new AllWindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Window&gt;() &#123; public void apply (Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;); reduce:12345windowedStream.reduce (new ReduceFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123; public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception &#123; return new Tuple2&lt;String,Integer&gt;(value1.f0, value1.f1 + value2.f1); &#125;&#125;); fold:12345windowedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; public String fold(String current, Integer value) &#123; return current + "-" + value; &#125;&#125;) aggregations:12345678910windowedStream.sum(0);windowedStream.sum("key");windowedStream.min(0);windowedStream.min("key");windowedStream.max(0);windowedStream.max("key");windowedStream.minBy(0);windowedStream.minBy("key");windowedStream.maxBy(0);windowedStream.maxBy("key"); windowä¸‹aggregate ä¼šåœ¨æ•´ä¸ªwindowå†…aggregateã€‚éwindowsä¼šæ‰§è¡Œrolling aggregate 5.DataStream,DataStream -&gt; ConnectedStream connect 6.DataStream -&gt; SplitStream split 12345678910111213SplitStream&lt;Integer&gt; split = someDataStream.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) &#123; output.add("even"); &#125; else &#123; output.add("odd"); &#125; return output; &#125;&#125;); 7.SplitStream -&gt; DataStream select 1234SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select("even");DataStream&lt;Integer&gt; odd = split.select("odd");DataStream&lt;Integer&gt; all = split.select("even","odd"); Flink JoinTumbling Window Join 12345678910111213141516171819import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time; ...DataStream&lt;Integer&gt; orangeStream = ...DataStream&lt;Integer&gt; greenStream = ...orangeStream.join(greenStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(TumblingEventTimeWindows.of(Time.milliseconds(2))) .apply (new JoinFunction&lt;Integer, Integer, String&gt; ()&#123; @Override public String join(Integer first, Integer second) &#123; return first + "," + second; &#125; &#125;); Sliding Window Join 12345678910111213141516171819import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;...DataStream&lt;Integer&gt; orangeStream = ...DataStream&lt;Integer&gt; greenStream = ...orangeStream.join(greenStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(SlidingEventTimeWindows.of(Time.milliseconds(2) /* size */, Time.milliseconds(1) /* slide */)) .apply (new JoinFunction&lt;Integer, Integer, String&gt; ()&#123; @Override public String join(Integer first, Integer second) &#123; return first + "," + second; &#125; &#125;); Session Window Join 12345678910111213141516171819import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows;import org.apache.flink.streaming.api.windowing.time.Time; ...DataStream&lt;Integer&gt; orangeStream = ...DataStream&lt;Integer&gt; greenStream = ...orangeStream.join(greenStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(EventTimeSessionWindows.withGap(Time.milliseconds(1))) .apply (new JoinFunction&lt;Integer, Integer, String&gt; ()&#123; @Override public String join(Integer first, Integer second) &#123; return first + "," + second; &#125; &#125;); Interval Join 1234567891011121314151617181920import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;import org.apache.flink.streaming.api.windowing.time.Time;... DataStream&lt;Integer&gt; orangeStream = ... DataStream&lt;Integer&gt; greenStream = ... orangeStream .keyBy(&lt;KeySelector&gt;) .intervalJoin(greenStream.keyBy(&lt;KeySelector&gt;)) .between(Time.milliseconds(-2), Time.milliseconds(1)) .process (new ProcessJoinFunction&lt;Integer, Integer, String()&#123; @Override public void processElement(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out) &#123; out.collect(first + "," + second); &#125; &#125;); Async I/O API: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// This example implements the asynchronous request and callback with Futures that have the// interface of Java 8's futures (which is the same one followed by Flink's Future)/** * An implementation of the 'AsyncFunction' that sends requests and sets the callback. */class AsyncDatabaseRequest extends RichAsyncFunction&lt;String, Tuple2&lt;String, String&gt;&gt; &#123; /** The database specific client that can issue concurrent requests with callbacks */ private transient DatabaseClient client; @Override public void open(Configuration parameters) throws Exception &#123; client = new DatabaseClient(host, post, credentials); &#125; @Override public void close() throws Exception &#123; client.close(); &#125; @Override public void asyncInvoke(String key, final ResultFuture&lt;Tuple2&lt;String, String&gt;&gt; resultFuture) throws Exception &#123; // issue the asynchronous request, receive a future for result final Future&lt;String&gt; result = client.query(key); // set the callback to be executed once the request by the client is complete // the callback simply forwards the result to the result future CompletableFuture.supplyAsync(new Supplier&lt;String&gt;() &#123; @Override public String get() &#123; try &#123; return result.get(); &#125; catch (InterruptedException | ExecutionException e) &#123; // Normally handled explicitly. return null; &#125; &#125; &#125;).thenAccept( (String dbResult) -&gt; &#123; resultFuture.complete(Collections.singleton(new Tuple2&lt;&gt;(key, dbResult))); &#125;); &#125;&#125;// create the original streamDataStream&lt;String&gt; stream = ...;// apply the async I/O transformationDataStream&lt;Tuple2&lt;String, String&gt;&gt; resultStream = AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100);]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flinkæºç 3--StreamGraph->JobGraph->ExecutionGraph]]></title>
    <url>%2F2019%2F08%2F04%2Fflink%E6%BA%90%E7%A0%813--StreamGraph--JobGraph--ExecutionGraph%2F</url>
    <content type="text"><![CDATA[æˆ‘ä»¬æ¥ç€åˆ†æStreamExecutionEnvironmentè¿™ä¸ªç±»çš„å®ç°ç±»ï¼šå³æˆ‘ä»¬åœ¨è°ƒç”¨env.execute(â€œFlink StreamingChainingDemoâ€);æ­¤å¤„æœ€æ–°ç‰ˆçš„flinkä»£ç å’Œä»¥å‰çš„ç»“æ„ä¸åŒã€‚å…ˆæ˜¯è°ƒç”¨çš„StreamExecutionEnvironmentçš„executeä»£ç  public JobExecutionResult execute(String jobName) throws Exception { Preconditions.checkNotNull(jobName, &quot;Streaming Job name should not be null.&quot;); return execute(getStreamGraph(jobName)); } @Internal public StreamGraph getStreamGraph(String jobName) { return getStreamGraphGenerator().setJobName(jobName).generate(); } private StreamGraphGenerator getStreamGraphGenerator() { if (transformations.size() &lt;= 0) { throw new IllegalStateException(&quot;No operators defined in streaming topology. Cannot execute.&quot;); } return new StreamGraphGenerator(transformations, config, checkpointCfg) .setStateBackend(defaultStateBackend)//null .setChaining(isChainingEnabled)//true .setUserArtifacts(cacheFile) .setTimeCharacteristic(timeCharacteristic)//DEFAULT_TIME_CHARACTERISTIC = TimeCharacteristic.ProcessingTime; //IngestionTime .setDefaultBufferTimeout(bufferTimeout);//100 } æ ¹æ®transformations, config, checkpointCfgåˆå§‹åŒ–StreamGraphGenerator JobGraph:æ¥ç€ä¼šè¿›å…¥åˆ°LocalStreamEnvironment è¿™ä¸ªå®ç°ç±»é‡Œé¢ public JobExecutionResult execute(StreamGraph streamGraph) throws Exception { JobGraph jobGraph = streamGraph.getJobGraph(); jobGraph.setAllowQueuedScheduling(true); Configuration configuration = new Configuration(); configuration.addAll(jobGraph.getJobConfiguration()); configuration.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, &quot;0&quot;); // add (and override) the settings with what the user defined configuration.addAll(this.configuration); if (!configuration.contains(RestOptions.BIND_PORT)) { configuration.setString(RestOptions.BIND_PORT, &quot;0&quot;); } int numSlotsPerTaskManager = configuration.getInteger(TaskManagerOptions.NUM_TASK_SLOTS, jobGraph.getMaximumParallelism()); MiniClusterConfiguration cfg = new MiniClusterConfiguration.Builder() .setConfiguration(configuration) .setNumSlotsPerTaskManager(numSlotsPerTaskManager) .build(); if (LOG.isInfoEnabled()) { LOG.info(&quot;Running job on local embedded Flink mini cluster&quot;); } MiniCluster miniCluster = new MiniCluster(cfg); try { miniCluster.start(); configuration.setInteger(RestOptions.PORT, miniCluster.getRestAddress().get().getPort()); return miniCluster.executeJobBlocking(jobGraph); } finally { transformations.clear(); miniCluster.close(); } } streamGraph.getJobGraph é‡Œ @SuppressWarnings(&quot;deprecation&quot;) @Override public JobGraph getJobGraph(@Nullable JobID jobID) { // temporarily forbid checkpointing for iterative jobs if (isIterative() &amp;&amp; checkpointConfig.isCheckpointingEnabled() &amp;&amp; !checkpointConfig.isForceCheckpointing()) { throw new UnsupportedOperationException( &quot;Checkpointing is currently not supported by default for iterative jobs, as we cannot guarantee exactly once semantics. &quot; + &quot;State checkpoints happen normally, but records in-transit during the snapshot will be lost upon failure. &quot; + &quot;\nThe user can force enable state checkpoints with the reduced guarantees by calling: env.enableCheckpointing(interval,true)&quot;); } return StreamingJobGraphGenerator.createJobGraph(this, jobID); } public static JobGraph createJobGraph(StreamGraph streamGraph, @Nullable JobID jobID) { return new StreamingJobGraphGenerator(streamGraph, jobID).createJobGraph(); } private JobGraph createJobGraph() { // make sure that all vertices start immediately jobGraph.setScheduleMode(streamGraph.getScheduleMode()); // Generate deterministic hashes for the nodes in order to identify them across // submission iff they didn&apos;t change. Map&lt;Integer, byte[]&gt; hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph); // Generate legacy version hashes for backwards compatibility List&lt;Map&lt;Integer, byte[]&gt;&gt; legacyHashes = new ArrayList&lt;&gt;(legacyStreamGraphHashers.size()); for (StreamGraphHasher hasher : legacyStreamGraphHashers) { legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph)); } Map&lt;Integer, List&lt;Tuple2&lt;byte[], byte[]&gt;&gt;&gt; chainedOperatorHashes = new HashMap&lt;&gt;(); setChaining(hashes, legacyHashes, chainedOperatorHashes); setPhysicalEdges(); setSlotSharingAndCoLocation(); configureCheckpointing(); JobGraphGenerator.addUserArtifactEntries(streamGraph.getUserArtifacts(), jobGraph); // set the ExecutionConfig last when it has been finalized try { jobGraph.setExecutionConfig(streamGraph.getExecutionConfig()); } catch (IOException e) { throw new IllegalConfigurationException(&quot;Could not serialize the ExecutionConfig.&quot; + &quot;This indicates that non-serializable types (like custom serializers) were registered&quot;); } return jobGraph; } ä¸Šé¢setChaining(hashes, legacyHashes, chainedOperatorHashes);å…¶æ ¸å¿ƒä»£ç ä¸ºï¼šcreateChainçš„isChainable public static boolean isChainable(StreamEdge edge, StreamGraph streamGraph) { StreamNode upStreamVertex = streamGraph.getSourceVertex(edge);//è·å–StreamEdgeçš„æºå’Œç›®æ ‡StreamNode StreamNode downStreamVertex = streamGraph.getTargetVertex(edge); StreamOperatorFactory&lt;?&gt; headOperator = upStreamVertex.getOperatorFactory();//è·å–æºå’Œç›®æ ‡StreamNodeä¸­çš„StreamOperator StreamOperatorFactory&lt;?&gt; outOperator = downStreamVertex.getOperatorFactory(); //å¯ä»¥chainingçš„æ¡ä»¶ï¼š return downStreamVertex.getInEdges().size() == 1//ä¸‹æ¸¸èŠ‚ç‚¹åªæœ‰ä¸€ä¸ªè¾“å…¥ &amp;&amp; outOperator != null//ä¸‹æ¸¸èŠ‚ç‚¹çš„æ“ä½œç¬¦ä¸ä¸ºnull &amp;&amp; headOperator != null//ä¸Šæ¸¸èŠ‚ç‚¹çš„æ“ä½œç¬¦ä¸ä¸ºnull &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex)//ä¸Šä¸‹æ¸¸èŠ‚ç‚¹åœ¨ä¸€ä¸ªæ§½ä½å…±äº«ç»„å†… &amp;&amp; outOperator.getChainingStrategy() == ChainingStrategy.ALWAYS//ä¸‹æ¸¸èŠ‚ç‚¹çš„è¿æ¥ç­–ç•¥æ˜¯ ALWAYS &amp;&amp; (headOperator.getChainingStrategy() == ChainingStrategy.HEAD ||//ä¸Šæ¸¸èŠ‚ç‚¹çš„è¿æ¥ç­–ç•¥æ˜¯ HEAD æˆ–è€… ALWAYS headOperator.getChainingStrategy() == ChainingStrategy.ALWAYS) &amp;&amp; (edge.getPartitioner() instanceof ForwardPartitioner)//edge çš„åˆ†åŒºå‡½æ•°æ˜¯ ForwardPartitioner çš„å®ä¾‹ &amp;&amp; edge.getShuffleMode() != ShuffleMode.BATCH//è¾¹çš„shuffleæ¨¡å¼ä¸ºBATCH &amp;&amp; upStreamVertex.getParallelism() == downStreamVertex.getParallelism()//ä¸Šä¸‹æ¸¸èŠ‚ç‚¹çš„å¹¶è¡Œåº¦ç›¸ç­‰ &amp;&amp; streamGraph.isChainingEnabled();//isChainingEnabledä¸ºtrueï¼Œé»˜è®¤ä¸ºtrue } åªæœ‰ä¸Šè¿°çš„10ä¸ªæ¡ä»¶éƒ½åŒæ—¶æ»¡è¶³æ—¶ï¼Œæ‰èƒ½è¯´æ˜ä¸¤ä¸ªStreamEdgeçš„æºå’Œç›®æ ‡StreamNodeæ˜¯å¯ä»¥é“¾æ¥åœ¨ä¸€èµ·æ‰§è¡Œçš„ private List&lt;StreamEdge&gt; createChain( Integer startNodeId, Integer currentNodeId, Map&lt;Integer, byte[]&gt; hashes, List&lt;Map&lt;Integer, byte[]&gt;&gt; legacyHashes, int chainIndex, Map&lt;Integer, List&lt;Tuple2&lt;byte[], byte[]&gt;&gt;&gt; chainedOperatorHashes) { if (!builtVertices.contains(startNodeId)) { List&lt;StreamEdge&gt; transitiveOutEdges = new ArrayList&lt;StreamEdge&gt;(); List&lt;StreamEdge&gt; chainableOutputs = new ArrayList&lt;StreamEdge&gt;(); List&lt;StreamEdge&gt; nonChainableOutputs = new ArrayList&lt;StreamEdge&gt;(); StreamNode currentNode = streamGraph.getStreamNode(currentNodeId); for (StreamEdge outEdge : currentNode.getOutEdges()) { if (isChainable(outEdge, streamGraph)) { chainableOutputs.add(outEdge); } else { nonChainableOutputs.add(outEdge); } } for (StreamEdge chainable : chainableOutputs) { transitiveOutEdges.addAll( createChain(startNodeId, chainable.getTargetId(), hashes, legacyHashes, chainIndex + 1, chainedOperatorHashes)); } for (StreamEdge nonChainable : nonChainableOutputs) { transitiveOutEdges.add(nonChainable);//ä¸å¯è¿æ¥çš„StreamEdge,è¾“å‡ºStreamEdgeæ”¾å…¥transitiveOutEdges createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes, legacyHashes, 0, chainedOperatorHashes);//ç»§ç»­éå†å¯chainingçš„èŠ‚ç‚¹ } //è·å–å¤´èŠ‚ç‚¹æ•£åˆ—å€¼ï¼Œæ²¡æœ‰åˆå§‹åŒ–ä¸ºç©ºé“¾è¡¨ List&lt;Tuple2&lt;byte[], byte[]&gt;&gt; operatorHashes = chainedOperatorHashes.computeIfAbsent(startNodeId, k -&gt; new ArrayList&lt;&gt;()); //è·å–å½“å‰èŠ‚ç‚¹æ•£åˆ—å€¼ byte[] primaryHashBytes = hashes.get(currentNodeId); OperatorID currentOperatorId = new OperatorID(primaryHashBytes); //éå†legacyHashes ä¸primaryHashBytesç»„æˆäºŒå…ƒæ•°ç»„ï¼Œæ·»åŠ åˆ°é“¾è¡¨ä¸­ for (Map&lt;Integer, byte[]&gt; legacyHash : legacyHashes) { operatorHashes.add(new Tuple2&lt;&gt;(primaryHashBytes, legacyHash.get(currentNodeId))); } //é€šè¿‡-&gt; æ‹¼æ¥chainingåç§° chainedNames.put(currentNodeId, createChainedName(currentNodeId, chainableOutputs)); chainedMinResources.put(currentNodeId, createChainedMinResources(currentNodeId, chainableOutputs)); chainedPreferredResources.put(currentNodeId, createChainedPreferredResources(currentNodeId, chainableOutputs)); if (currentNode.getInputFormat() != null) { getOrCreateFormatContainer(startNodeId).addInputFormat(currentOperatorId, currentNode.getInputFormat()); } if (currentNode.getOutputFormat() != null) { getOrCreateFormatContainer(startNodeId).addOutputFormat(currentOperatorId, currentNode.getOutputFormat()); } //åˆ›å»ºjobVertexå¹¶è®¾ç½®å¹¶è¡Œåº¦è¿”å›StreamConfigå®ä¾‹ StreamConfig config = currentNodeId.equals(startNodeId) ? createJobVertex(startNodeId, hashes, legacyHashes, chainedOperatorHashes) : new StreamConfig(new Configuration()); //è®¾ç½®åºåˆ—åŒ–å™¨ï¼ŒStreamOperatorï¼Œcheckpointï¼ˆé»˜è®¤AT_LEAST_ONCEï¼‰ setVertexConfig(currentNodeId, config, chainableOutputs, nonChainableOutputs); if (currentNodeId.equals(startNodeId)) { config.setChainStart(); config.setChainIndex(0); config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName()); config.setOutEdgesInOrder(transitiveOutEdges); config.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges()); for (StreamEdge edge : transitiveOutEdges) { connect(startNodeId, edge);//å°†JobVertexå’ŒJobEdgeç›¸è¿ } //å°†chainä¸­æ‰€æœ‰å­èŠ‚ç‚¹çš„StreamConfigå†™å…¥åˆ° headOfChain èŠ‚ç‚¹çš„ chainedTaskConfig_ é…ç½®ä¸­ config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId)); } else { chainedConfigs.computeIfAbsent(startNodeId, k -&gt; new HashMap&lt;Integer, StreamConfig&gt;()); config.setChainIndex(chainIndex); StreamNode node = streamGraph.getStreamNode(currentNodeId); config.setOperatorName(node.getOperatorName()); chainedConfigs.get(startNodeId).put(currentNodeId, config); } config.setOperatorID(currentOperatorId); if (chainableOutputs.isEmpty()) { config.setChainEnd(); } return transitiveOutEdges; } else { return new ArrayList&lt;&gt;(); } } éå†transitiveOutEdgesï¼Œå¹¶å°†æ¯ä¸€æ¡StreamEdgeè¾¹ä½œä¸ºå‚æ•°ä¼ å…¥connect( )å‡½æ•°ä¸­: private void connect(Integer headOfChain, StreamEdge edge) { //å°†å½“å‰edgeè®°å½•ç‰©ç†è¾¹ç•Œé¡ºåºé›†åˆä¸­ physicalEdgesInOrder.add(edge); //è·å–å½“å‰edgeçš„ä¸‹æ¸¸èŠ‚ç‚¹ID Integer downStreamvertexID = edge.getTargetId(); //è·å–ä¸Šä¸‹æ¸¸JobVertexèŠ‚ç‚¹ JobVertex headVertex = jobVertices.get(headOfChain); JobVertex downStreamVertex = jobVertices.get(downStreamvertexID); //è·å–ä¸‹æ¸¸JobVertexçš„é…ç½® StreamConfig downStreamConfig = new StreamConfig(downStreamVertex.getConfiguration()); //ä¸‹æ¸¸JobVertexçš„è¾“å…¥è®¡æ•°å™¨åŠ 1 downStreamConfig.setNumberOfInputs(downStreamConfig.getNumberOfInputs() + 1); StreamPartitioner&lt;?&gt; partitioner = edge.getPartitioner(); //æ ¹æ®shuffleæ¨¡å¼ä¸åŒåˆ›å»ºä¸åŒçš„ResultPartitionType ResultPartitionType resultPartitionType; switch (edge.getShuffleMode()) { case PIPELINED://æœ‰é™çš„æˆ–æ— é™çš„ resultPartitionType = ResultPartitionType.PIPELINED_BOUNDED; break; case BATCH://ä»…åœ¨ç”Ÿæˆå®Œæ•´ç»“æœåå‘ä¸‹æ¸¸å‘é€æ•°æ® resultPartitionType = ResultPartitionType.BLOCKING; break; case UNDEFINED://blockingConnectionsBetweenChains ä¸ºtrue BLOCKING flase ä¸ºPIPELINED_BOUNDED resultPartitionType = streamGraph.isBlockingConnectionsBetweenChains() ? ResultPartitionType.BLOCKING : ResultPartitionType.PIPELINED_BOUNDED; break; default: throw new UnsupportedOperationException(&quot;Data exchange mode &quot; + edge.getShuffleMode() + &quot; is not supported yet.&quot;); } //æ ¹æ®ForwardPartitionerå’ŒRescalePartitionerä¸¤ç§åˆ†åŒºæ–¹å¼å»ºç«‹DistributionPattern.POINTWISEç±»å‹çš„JobEdge JobEdge jobEdge; if (partitioner instanceof ForwardPartitioner || partitioner instanceof RescalePartitioner) { jobEdge = downStreamVertex.connectNewDataSetAsInput( headVertex, DistributionPattern.POINTWISE, resultPartitionType); } else {//å…¶ä»–åˆ†åŒºæ–¹å¼åˆ™æ˜¯DistributionPattern.ALL_TO_ALLç±»å‹ jobEdge = downStreamVertex.connectNewDataSetAsInput( headVertex, DistributionPattern.ALL_TO_ALL, resultPartitionType); } // set strategy name so that web interface can show it. è®¾ç½®ç­–ç•¥åç§°æ–¹ä¾¿webæ¥å£æ˜¾ç¤º jobEdge.setShipStrategyName(partitioner.toString()); if (LOG.isDebugEnabled()) { LOG.debug(&quot;CONNECTED: {} - {} -&gt; {}&quot;, partitioner.getClass().getSimpleName(), headOfChain, downStreamvertexID); } } public JobEdge connectNewDataSetAsInput( JobVertex input, DistributionPattern distPattern, ResultPartitionType partitionType) { //JobVertexå’ŒJobEdgeä¹‹é—´é€šè¿‡åˆ›å»ºIntermediateDataSetæ¥è¿æ¥ IntermediateDataSet dataSet = input.createAndAddResultDataSet(partitionType); JobEdge edge = new JobEdge(dataSet, this, distPattern); this.inputs.add(edge); dataSet.addConsumer(edge); return edge; } æœ€åé™„ä¸Šä¸€å‰¯ executeæ–¹æ³•æœ€åæ‰§è¡Œã€‚miniCluster.executeJobBlocking(jobGraph);ä¸­é—´ä¸€äº›åˆ—Akka çš„RPCé€šè®¯çœç•¥ä¸è¡¨ï¼Œå¯¹å¹¶å‘ç¼–ç¨‹æœ‰å…´è¶£å¯ä»¥ç ”ç©¶ä¸‹Akka å’ŒActor ExecutionGraphï¼š public JobExecutionResult executeJobBlocking(JobGraph job) throws JobExecutionException, InterruptedException { checkNotNull(job, &quot;job is null&quot;); final CompletableFuture&lt;JobSubmissionResult&gt; submissionFuture = submitJob(job); final CompletableFuture&lt;JobResult&gt; jobResultFuture = submissionFuture.thenCompose( (JobSubmissionResult ignored) -&gt; requestJobResult(job.getJobID())); final JobResult jobResult; try { jobResult = jobResultFuture.get(); } catch (ExecutionException e) { throw new JobExecutionException(job.getJobID(), &quot;Could not retrieve JobResult.&quot;, ExceptionUtils.stripExecutionException(e)); } try { return jobResult.toJobExecutionResult(Thread.currentThread().getContextClassLoader()); } catch (IOException | ClassNotFoundException e) { throw new JobExecutionException(job.getJobID(), e); } } public CompletableFuture&lt;JobSubmissionResult&gt; submitJob(JobGraph jobGraph) { final CompletableFuture&lt;DispatcherGateway&gt; dispatcherGatewayFuture = getDispatcherGatewayFuture(); // we have to allow queued scheduling in Flip-6 mode because we need to request slots // from the ResourceManager jobGraph.setAllowQueuedScheduling(true); final CompletableFuture&lt;InetSocketAddress&gt; blobServerAddressFuture = createBlobServerAddress(dispatcherGatewayFuture); final CompletableFuture&lt;Void&gt; jarUploadFuture = uploadAndSetJobFiles(blobServerAddressFuture, jobGraph); final CompletableFuture&lt;Acknowledge&gt; acknowledgeCompletableFuture = jarUploadFuture .thenCombine( dispatcherGatewayFuture, (Void ack, DispatcherGateway dispatcherGateway) -&gt; dispatcherGateway.submitJob(jobGraph, rpcTimeout)) .thenCompose(Function.identity()); return acknowledgeCompletableFuture.thenApply( (Acknowledge ignored) -&gt; new JobSubmissionResult(jobGraph.getJobID())); }]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flinkæºç 2]]></title>
    <url>%2F2019%2F08%2F03%2Fflink%E6%BA%90%E7%A0%812%2F</url>
    <content type="text"><![CDATA[æ ¹æ®ä¸Šç¯‡æ–‡ç« å†…å®¹æ‰©å±•ä¸€ä¸‹chaining demo é¦–å…ˆå†™ä¸€ä¸ªstreamingçš„ wordcount public class StreamingChainingDemo { @Data @AllArgsConstructor @NoArgsConstructor public static class KeyCount{ private String key; private int count; } public static void main(String[] args) throws Exception { // set up the streaming execution environment Configuration configuration= new Configuration(){ { setInteger(&quot;rest.port&quot;,9191); setBoolean(&quot;local.start-webserver&quot;,true); } }; final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration); env.setParallelism(2).setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); DataStreamSource&lt;String&gt; dataStreamSource = env.socketTextStream(&quot;localhost&quot;, 9999); dataStreamSource.flatMap((String line, Collector&lt;KeyCount&gt; out) -&gt; { Stream.of(line.split(&quot;\\s+&quot;)).forEach(value -&gt; out.collect(new KeyCount(value,1))); } ).returns(Types.POJO(KeyCount.class)) .keyBy(new KeySelector&lt;KeyCount, Object&gt;() { @Override public Object getKey(KeyCount value) throws Exception { return value.getKey(); } }).timeWindow(Time.seconds(10)).sum(&quot;count&quot;).print(); // execute program env.execute(&quot;Flink StreamingChainingDemo&quot;); } } è¿è¡Œå¦‚ä¸Šä»£ç åæˆ‘ä»¬çœ‹WEB UIå¯ä»¥çœ‹åˆ°keyByæ“ä½œå’Œsinkæ˜¯chainingåœ¨ä¸€èµ·çš„ã€‚å¦‚æœæˆ‘ä»¬åœ¨print()çš„åé¢åŠ ä¸Š.disableChaining() å¯ä»¥çœ‹åˆ°keyByå’Œsinkæ˜¯forwardçš„å¹¶échainingåœ¨ä¸€èµ· æ­¤æ—¶æˆ‘ä»¬åœ¨returns(Types.POJO(KeyCount.class))åé¢å¢åŠ .filter(word-&gt; !â€â€.equals(word))å¤§å®¶çŒœflatå’Œfilterä¼šchainingåœ¨ä¸€èµ·å—ï¼Ÿ yeahï¼Œä½ ç­”å¯¹äº†å—ï¼Ÿflatå’Œfilterä¼šchainingåœ¨ä¸€èµ·ã€‚ è¿™æ—¶ æˆ‘å¼•å…¥å¹¶è¡Œåº¦çš„æ¦‚å¿µï¼šåœ¨.filter(word-&gt; !â€â€.equals(word))åé¢æœºä¸Š.setParallelism(3) å°±ä¼šå‘ç°flatå’Œfilteræ˜¯Reblanceçš„å…³ç³» StreamOperator æºç è§£æè¿™ä¸ªæ¥å£ç»§æ‰¿ CheckpointListener, KeyContext, Disposable, Serializable æä¾›äº†å¦‚ä¸‹æ–¹æ³•ï¼š ç”Ÿå‘½å‘¨æœŸç›¸å…³ï¼š open close dispose prepareSnapshotPreBarrier å®¹é”™ä¸çŠ¶æ€ snapshotState initializeState ä¸StreamRecordç›¸å…³ setKeyContextElement1 setKeyContextElement2 chainç›¸å…³ getChainingStrategy setChainingStrategy ç›‘æ§ç›¸å…³ getMetricGroup getOperatorID AbstractStreamOperatorï¼ŒOneInputStreamOperatorä¸TwoInputStreamOperatoræ¥å£ç»§æ‰¿è‡ªStreamOperator OneInputStreamOperatoræœ‰3ä¸ªæ–¹æ³• processElement processWatermark processLatencyMarker TwoInputStreamOperatoræœ‰6ä¸ªæ–¹æ³• processElement1 processElement2 processWatermark1 processWatermark2 processLatencyMarker1 processLatencyMarker2 AbstractStreamOperator é‡è¦çš„å˜é‡ï¼šåé¢ä¼šå°†å…·ä½“çš„ç”¨æ³•ç”¨å¤„å†³å®šæ˜¯å¦åœ¨ç”ŸæˆJobGraphæ—¶å¯¹ç®—å­è¿›è¡ŒChainingä¼˜åŒ–ï¼š protected ChainingStrategy chainingStrategy = ChainingStrategy.HEAD; 3ä¸ªä¸çŠ¶æ€ç›¸å…³çš„å˜é‡ private transient AbstractKeyedStateBackend&lt;?&gt; keyedStateBackend; private transient DefaultKeyedStateStore keyedStateStore; private transient OperatorStateBackend operatorStateBackend; ç›‘æ§ç›¸å…³çš„å˜é‡ protected transient OperatorMetricGroup metrics; protected transient LatencyStats latencyStats; æ–¹æ³•ä½œç”¨å’Œçˆ¶ç±»å¤§åŒå°å¼‚æ­¤å¤„ç•¥ AbstractStreamOperatorçš„å­ç±»æŠ½è±¡ç±»AbstractUdfStreamOperatorè¿™ä¸ªæŠ½è±¡ç±»åŒæ—¶å®ç°äº†OutputTypeConfigurableæ¥å£å¹¶é‡å†™äº†setOutputTypeæ–¹æ³•è®¾ç½®äº†è¾“å‡ºç±»å‹ æœ€åæˆ‘ä»¬æ¥çœ‹OneInputStreamOperatorè¿™ä¸ªç±»çš„å®ç°ç±»ï¼š StreamFilterï¼ŒStreamMapä¸StreamFlatMapç®—å­åœ¨å®ç°çš„processElementåˆ†åˆ«è°ƒç”¨ä¼ å…¥çš„FilterFunctionï¼ŒMapFunctionï¼Œ FlatMapFunctionçš„udfå°†elementä¼ åˆ°ä¸‹æ¸¸ã€‚å…¶ä¸­StreamFlatMapç”¨åˆ°äº†TimestampedCollectorï¼Œå®ƒæ˜¯outputçš„ä¸€å±‚å°è£…ï¼Œå°†timestampåŠ å…¥åˆ°StreamRecordä¸­å‘é€åˆ°ä¸‹æ¸¸ã€‚ StreamGroupedReduceä¸StreamGroupedFoldç®—å­ç›¸ä¼¼çš„ç‚¹æ˜¯éƒ½æ¶‰åŠåˆ°äº†æ“ä½œçŠ¶æ€, æ‰€ä»¥åœ¨è¦†ç›–openæ–¹æ³•æ—¶é€šè¿‡åˆ›å»ºä¸€ä¸ªçŠ¶æ€çš„æè¿°ç¬¦ä»¥åŠè°ƒç”¨AbstractStreamOperatorå®ç°çš„getPartitionedStateæ–¹æ³•è·å–äº†ä¸€ä¸ªstateBackendçš„æ“ä½œå¥æŸ„ã€‚åœ¨processElementæ–¹æ³•ä¸­å€ŸåŠ©è¿™ä¸ªå¥æŸ„è·å–å½“å‰çŠ¶æ€å€¼ï¼Œåœ¨ç”¨UDFå°†æ–°çš„å…ƒç´ èšåˆè¿›å»å¹¶æ›´æ–°çŠ¶æ€å€¼ï¼Œæœ€åè¾“å‡ºåˆ°ä¸‹æ¸¸ã€‚ä¸åŒçš„æ˜¯Foldçš„è¾“å‡ºç±»å‹å¯èƒ½ä¸ä¸€æ ·ï¼ˆæ‰€ä»¥å®ç°äº†OutputTypeConfigurableæ¥å£çš„setOutputTypeæ–¹æ³•ï¼‰ï¼Œå¹¶ä¸”æœ‰åˆå§‹å€¼ã€‚ ProcessOperatorï¼Œ LegacyKeyedProcessOperatorï¼ˆ@Deprecatedï¼‰ProcessFunctionæ˜¯æ¯”è¾ƒçµæ´»çš„UDFï¼Œå…è®¸ç”¨æˆ·é€šè¿‡åœ¨processElementçš„æ—¶å€™å¯ä»¥é€šè¿‡ä¼ å…¥çš„ctxæ“ä½œTimerServiceæ³¨å†ŒProcessingTimeTimerå’ŒEventTimeTimerï¼Œå¹¶ä¸”é€šè¿‡å®ç°æ–¹æ³•onTimerå°±å¯ä»¥åœ¨Timerè¢«è§¦å‘çš„æ—¶å€™æ‰§è¡Œå›è°ƒçš„é€»è¾‘ã€‚ StreamSinkï¼šSimpleContextï¼Œå¯ä»¥è·å–processingTimeï¼Œwatermarkå’Œelementçš„æ—¶é—´æˆ³ã€‚GenericWriteAheadSinkæä¾›äº†ä¸€ä¸ªå¯ä»¥è¢«å®ç°ä¸ºExactly onceçš„sinkçš„æŠ½è±¡ç±»AsyncWaitOperatoræä¾›äº†å¼‚æ­¥å¤„ç†çš„èƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªæ¯”è¾ƒç‰¹æ®Šçš„ç®—å­ï¼Œå¯¹å…ƒç´ çš„å¤„ç†å’Œå¤‡ä»½æ¢å¤éƒ½æ¯”è¾ƒç‰¹æ®Šã€‚elementçš„è¾“å‡ºé€šè¿‡ä¸€ä¸ªEmitterå¯¹è±¡æ¥å®ç°TimestampsAndPeriodicWatermarksOperatorï¼ŒTimestampsAndPunctuatedWatermarksOperatoré€šè¿‡TimestampAssigneræå–timestampå¹¶ç”ŸæŒ‰ç…§è§„åˆ™ç”Ÿæˆwatermark å’ŒTwoInputStreamOperatorè¿™ä¸ªç±»çš„å®ç°ç±»CoStreamMapï¼Œ CoStreamFlatMapåŸºæœ¬ä¸å•æµçš„é€»è¾‘æ²¡ä»€ä¹ˆåŒºåˆ«ï¼Œåªæ˜¯é’ˆå¯¹ä¸¤ä¸ªæµçš„Functionåšç±»ä¼¼çš„å¤„ç†ã€‚IntervalJoinOperatorå¯¹åŒæµçš„å…ƒç´ æ ¹æ®æä¾›çš„ProcessJoinFunctionåšå†…è¿æ¥ï¼Œå¹¶ä¸”æ¯ä¸ªå…ƒç´ éƒ½æœ‰å¤±æ•ˆæ—¶é—´ã€‚åœ¨processElementæ–¹æ³•ä¸­ï¼Œæ¯å½“ä¸€ä¸ªæµçš„å…ƒç´ åˆ°è¾¾ï¼Œä¼šå°†å®ƒåŠ å…¥å¯¹åº”æµçš„bufferï¼Œå¹¶ä¸”éå†å¦ä¸€ä¸ªæµçš„bufferæ‰¾åˆ°æ‰€æœ‰joinçš„é€‰é¡¹ã€‚æœ€åå†æ ¹æ®å¤±æ•ˆæ—¶é—´æ³¨å†Œä¸€ä¸ªçŠ¶æ€æ¸…ç†çš„Timeré˜²æ­¢bufferæ— é™å¢é•¿ã€‚ CoBroadcastWithKeyedOperatorå’ŒCoBroadcastWithNonKeyedOperatoræä¾›äº†å¯¹(Keyed)BroadcastProcessFunctionçš„æ”¯æŒï¼Œå’ŒCoProcessæœ‰ä¸€äº›ç±»ä¼¼ï¼Œåªæ˜¯Broadcastçš„Streamåªæœ‰è¯»æƒé™ï¼Œæ²¡æœ‰å†™æƒé™ã€‚å¹¶ä¸”å¯ä»¥é€šè¿‡contextç›´æ¥è·å¾—BroadcastState CoProcessOperatorå’ŒKeyedCoProcessOperatoræœ¬è´¨ä¸Šä¸å•æµçš„å¤„ç†ä¹Ÿæ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œä½†æ˜¯æä¾›äº†åŒæµä¹‹é—´å…±äº«çŠ¶æ€çš„å¯èƒ½ã€‚CoProcessOperatorä¹Ÿè¢«ç”¨æ¥å®ç°NonWindowJoin]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flinkæºç 1]]></title>
    <url>%2F2019%2F08%2F02%2Fflink%E6%BA%90%E7%A0%811%2F</url>
    <content type="text"><![CDATA[nc -lk 9000 bin/flink run examples/streaming/SocketWindowWordCount.jar â€“hostname localhost â€“port 9000 ç¼–è¯‘flinkgit clone https://github.com/apache/flink.gitcd flinkmvn clean package -DskipTests # this will take up to 10 minutes Flinkæºç é˜…è¯»ï¼šä»è¯»å–æ–‡ä»¶å¼€å§‹ï¼šä¾‹å¦‚env.readFileStreamå…±æœ‰å¦‚ä¸‹DataSourceï¼š fromElements fromElements fromCollection fromCollection fromCollection fromCollection fromParallelCollection fromParallelCollection fromParallelCollection readTextFile readTextFile readFile readFile readFile readFileStream readFile socketTextStream socketTextStream socketTextStream socketTextStream socketTextStream createInput createInput createInput createFileInput addSource addSource addSource addSource public DataStream&lt;String&gt; readFileStream(String filePath, long intervalMillis, FileMonitoringFunction.WatchType watchType) { DataStream&lt;Tuple3&lt;String, Long, Long&gt;&gt; source = addSource(new FileMonitoringFunction( filePath, intervalMillis, watchType), &quot;Read File Stream source&quot;); return source.flatMap(new FileReadFunction()); } ç¬¬ä¸‰ä¸ªå‚æ•°åˆ†ä¸ºï¼š ONLY_NEW_FILES, // Only new files will be processed. ä»…å¤„ç†æ–°å¢æ–‡ä»¶ REPROCESS_WITH_APPENDED, // When some files are appended, all contents of the files will be processed. å½“æ–‡ä»¶å†…å®¹å¢åŠ äº†ä¹‹åä¼šé‡æ–°å¤„ç†æ•´ä¸ªæ–‡ä»¶ã€‚ PROCESS_ONLY_APPENDED // When some files are appended, only appended contents will be processed. å½“æ–‡ä»¶å†…å®¹å¢åŠ äº†ä¹‹ååªå¤„ç†æ–°å¢åŠ å†…å®¹ FileMonitoringFunction ç»§æ‰¿äºSourceFunctionæ¥å£SourceFunctionæœ‰ä¸¤ä¸ªæ–¹æ³•ï¼šrun ä¸šåŠ¡é€»è¾‘æ–¹æ³•cancel å–æ¶ˆæ•°æ®æºçš„æ•°æ®äº§ç”ŸFileMonitoringFunctionå®ç°äº†è¿™ä¸¤ä¸ªæ–¹æ³• @Override public void run(SourceContext&lt;Tuple3&lt;String, Long, Long&gt;&gt; ctx) throws Exception { FileSystem fileSystem = FileSystem.get(new URI(path)); while (isRunning) { List&lt;String&gt; files = listNewFiles(fileSystem);//åˆ—å‡ºæ–°å¢æ–‡ä»¶ for (String filePath : files) { if (watchType == WatchType.ONLY_NEW_FILES || watchType == WatchType.REPROCESS_WITH_APPENDED) { ctx.collect(new Tuple3&lt;String, Long, Long&gt;(filePath, 0L, -1L));//ä»å¤´åˆ°å°¾æ”¶é›†æ•°æ® offsetOfFiles.put(filePath, -1L); } else if (watchType == WatchType.PROCESS_ONLY_APPENDED) { long offset = 0; long fileSize = fileSystem.getFileStatus(new Path(filePath)).getLen(); if (offsetOfFiles.containsKey(filePath)) { offset = offsetOfFiles.get(filePath); } //åªæ”¶é›†æ–°å¢éƒ¨åˆ†æ•°æ®ï¼Œå³ä»ä¸Šæ¬¡è·å–çš„offsetåˆ°è¿™æ¬¡æ–‡ä»¶æœ«å°¾filesize ctx.collect(new Tuple3&lt;String, Long, Long&gt;(filePath, offset, fileSize)); offsetOfFiles.put(filePath, fileSize); LOG.info(&quot;File processed: {}, {}, {}&quot;, filePath, offset, fileSize); } } Thread.sleep(interval); } } private List&lt;String&gt; listNewFiles(FileSystem fileSystem) throws IOException { List&lt;String&gt; files = new ArrayList&lt;String&gt;(); FileStatus[] statuses = fileSystem.listStatus(new Path(path));//åˆ—å‡ºç»™å®šè·¯å¾„ä¸­æ–‡ä»¶/ç›®å½•çš„çŠ¶æ€ï¼ˆå¦‚æœè·¯å¾„ä¸ºä¸€ä¸ªç›®å½•ã€‚) //FileStatus æœ‰getLenï¼ŒgetBlockSize,getReplication,getModificationTimeï¼ŒgetAccessTimeï¼ŒisDir,getPathæ–¹æ³• if (statuses == null) { LOG.warn(&quot;Path does not exist: {}&quot;, path); } else { for (FileStatus status : statuses) { Path filePath = status.getPath(); String fileName = filePath.getName(); long modificationTime = status.getModificationTime(); if (!isFiltered(fileName, modificationTime)) { //å½“WatchType ä¸ºONLY_NEW_FILESå¹¶ä¸”modificationTimesè¿™ä¸ªmapåŒ…å«è¯»å–çš„æ–‡ä»¶æ—¶å³æ–°å¢æ–‡ä»¶ // æˆ–æ–‡ä»¶ä¿®æ”¹æ—¶é—´å¤§äºmodificationTimeæ—¶ä¸ºtrue files.add(filePath.toString()); modificationTimes.put(fileName, modificationTime); } } } return files; } æ¥ä¸‹æ¥ä¸ºä»¬çœ‹canalæ–¹æ³•åªåšäº†ä¸€ä»¶äº‹æŒºç›´running @Override public void cancel() { isRunning = false; } å¯¹FileMonitoringFunctionçš„å®ç°æ¸…æ¥šä¹‹åï¼Œå›åˆ°StreamExecutionEnvironmentä¸­ï¼Œçœ‹addSourceæ–¹æ³•ã€‚ public &lt;OUT&gt; DataStreamSource&lt;OUT&gt; addSource(SourceFunction&lt;OUT&gt; function, String sourceName) { return addSource(function, sourceName, null); } public &lt;OUT&gt; DataStreamSource&lt;OUT&gt; addSource(SourceFunction&lt;OUT&gt; function, String sourceName, TypeInformation&lt;OUT&gt; typeInfo) { if (function instanceof ResultTypeQueryable) { //å¦‚æœä¼ å…¥çš„functionå®ç°äº†ResultTypeQueryableæ¥å£, åˆ™ç›´æ¥é€šè¿‡æ¥å£è·å– typeInfo = ((ResultTypeQueryable&lt;OUT&gt;) function).getProducedType(); } if (typeInfo == null) { try { typeInfo = TypeExtractor.createTypeInfo( SourceFunction.class, function.getClass(), 0, null, null);//è¿™ä¸ªæ–¹æ³•æœ‰ç‚¹é•¿ï¼Œå®é™…æ˜¯typeInfoä¸ºç©ºé€šè¿‡åå°„æœºåˆ¶æ¥æå–typeInfo } catch (final InvalidTypesException e) { //è·å–å¤±è´¥è¿”å›MissingTypeInfoå®ä¾‹ï¼Œé‡Œé¢ä¸¤ä¸ªå˜é‡ï¼šfunctionNameï¼ŒtypeException typeInfo = (TypeInformation&lt;OUT&gt;) new MissingTypeInfo(sourceName, e); } } //æ ¹æ®functionæ˜¯å®ç°äº†ParallelSourceFunctionæ¥åˆ¤æ–­æ˜¯å¦æ˜¯ä¸€ä¸ªå¹¶è¡Œæ•°æ®æºèŠ‚ç‚¹ boolean isParallel = function instanceof ParallelSourceFunction; //é—­åŒ…æ¸…ç†, å¯å‡å°‘åºåˆ—åŒ–å†…å®¹, ä»¥åŠé˜²æ­¢åºåˆ—åŒ–å‡ºé”™ clean(function); //åˆå§‹åŒ–ä¸€ä¸ªChainingStrategy.HEADèŠ‚ç‚¹ final StreamSource&lt;OUT, ?&gt; sourceOperator = new StreamSource&lt;&gt;(function); return new DataStreamSource&lt;&gt;(this, typeInfo, sourceOperator, isParallel, sourceName);//è¿”å›DataStreamSource } ç”±äºFileMonitoringFunctionç»§æ‰¿çš„æ˜¯SourceFunctionä¸æ˜¯ ParallelSourceFunction æ•…isParallelä¸ºflaseï¼Œå³å¹¶è¡Œåº¦ä¸º1ä¸Šé¢çœ‹åˆ°ChainingStrategyè¿™ä¸ªæšä¸¾ç±»å®é™…æœ‰ä¸‰ä¸ªå±æ€§ï¼šALWAYS è¡¨ç¤ºå°½å¯èƒ½çš„ä¸å‰åoperator chainingNEVER è¡¨ç¤ºä¸ä¼šchainingHEAD è¡¨ç¤ºåªä¼šchainingåé¢ã€‚å…·ä½“åé¢è¯¦ç»†è®²è§£å…¶ä½œç”¨ æ¥ä¸‹æ¥çœ‹æœ€åä¸€ä¸ªå‡½æ•°DataStreamSource public DataStreamSource(StreamExecutionEnvironment environment, TypeInformation&lt;T&gt; outTypeInfo, StreamSource&lt;T, ?&gt; operator, boolean isParallel, String sourceName) { super(environment, new SourceTransformation&lt;&gt;(sourceName, operator, outTypeInfo, environment.getParallelism())); this.isParallel = isParallel; if (!isParallel) { setParallelism(1); } } protected SingleOutputStreamOperator(StreamExecutionEnvironment environment, Transformation&lt;T&gt; transformation) { super(environment, transformation); } public DataStream(StreamExecutionEnvironment environment, Transformation&lt;T&gt; transformation) { this.environment = Preconditions.checkNotNull(environment, &quot;Execution Environment must not be null.&quot;); this.transformation = Preconditions.checkNotNull(transformation, &quot;Stream Transformation must not be null.&quot;); } åé¢å°±æ˜¯ä¸€ç³»åˆ—transform åé¢ç« èŠ‚è¯¦ç»†ä»‹ç»]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -ç¥ç»ç½‘ç»œ]]></title>
    <url>%2F2018%2F08%2F25%2FMachine%20learning%20Artificial%20Neural%20Network%2F</url>
    <content type="text"><![CDATA[ä»€ä¹ˆæ˜¯äººå·¥ç¥ç»ç½‘ç»œæ¨¡å‹äººå·¥ç¥ç»ç½‘ç»œ(Artificial Neural Network, ANN)æ²¡æœ‰ä¸€ä¸ªä¸¥æ ¼çš„æ­£å¼å®šä¹‰ã€‚å®ƒçš„åŸºæœ¬ç‰¹ç‚¹ï¼Œæ˜¯è¯•å›¾æ¨¡ä»¿å¤§è„‘çš„ç¥ç»å…ƒä¹‹é—´ä¼ é€’ï¼Œå¤„ç†ä¿¡æ¯çš„æ¨¡å¼ã€‚ ä¸€ä¸ªè®¡ç®—æ¨¡å‹ï¼Œè¦è¢«ç§°ä¸ºä¸ºç¥ç»ç½‘ç»œï¼Œé€šå¸¸éœ€è¦å¤§é‡å½¼æ­¤è¿æ¥çš„èŠ‚ç‚¹ ï¼ˆä¹Ÿç§° â€˜ç¥ç»å…ƒâ€™ï¼‰ï¼Œå¹¶ä¸”å…·å¤‡ä¸¤ä¸ªç‰¹æ€§ï¼šæ¯ä¸ªç¥ç»å…ƒï¼Œé€šè¿‡æŸç§ç‰¹å®šçš„è¾“å‡ºå‡½æ•° ï¼ˆä¹Ÿå«æ¿€åŠ±å‡½æ•° activation functionï¼‰ï¼Œè®¡ç®—å¤„ç†æ¥è‡ªå…¶å®ƒç›¸é‚»ç¥ç»å…ƒçš„åŠ æƒè¾“å…¥å€¼ç¥ç»å…ƒä¹‹é—´çš„ä¿¡æ¯ä¼ é€’çš„å¼ºåº¦ï¼Œç”¨æ‰€è°“åŠ æƒå€¼æ¥å®šä¹‰ï¼Œç®—æ³•ä¼šä¸æ–­è‡ªæˆ‘å­¦ä¹ ï¼Œè°ƒæ•´è¿™ä¸ªåŠ æƒå€¼æ€»ç»“ï¼šç¥ç»ç½‘ç»œç®—æ³•çš„æ ¸å¿ƒå°±æ˜¯ï¼šè®¡ç®—ã€è¿æ¥ã€è¯„ä¼°ã€çº é”™ã€å­¦ä¹  ç¥ç»ç½‘ç»œæ¨¡å‹å¯ä»¥åˆ†ä¸ºï¼šå‰å‘ç½‘ç»œç½‘ç»œä¸­å„ä¸ªç¥ç»å…ƒæ¥å—å‰ä¸€çº§çš„è¾“å…¥ï¼Œå¹¶è¾“å‡ºåˆ°ä¸‹ä¸€çº§ï¼Œç½‘ç»œä¸­æ²¡æœ‰åé¦ˆï¼Œå¯ä»¥ç”¨ä¸€ä¸ªæœ‰å‘æ— ç¯è·¯å›¾è¡¨ç¤ºã€‚è¿™ç§ç½‘ç»œå®ç°ä¿¡å·ä»è¾“å…¥ç©ºé—´åˆ°è¾“å‡ºç©ºé—´çš„å˜æ¢ï¼Œå®ƒçš„ä¿¡æ¯å¤„ç†èƒ½åŠ›æ¥è‡ªäºç®€å•éçº¿æ€§å‡½æ•°çš„å¤šæ¬¡å¤åˆã€‚ç½‘ç»œç»“æ„ç®€å•ï¼Œæ˜“äºå®ç°ã€‚åä¼ ç½‘ç»œæ˜¯ä¸€ç§å…¸å‹çš„å‰å‘ç½‘ç»œã€‚ åé¦ˆç½‘ç»œç½‘ç»œå†…ç¥ç»å…ƒé—´æœ‰åé¦ˆï¼Œå¯ä»¥ç”¨ä¸€ä¸ªæ— å‘çš„å®Œå¤‡å›¾è¡¨ç¤ºã€‚è¿™ç§ç¥ç»ç½‘ç»œçš„ä¿¡æ¯å¤„ç†æ˜¯çŠ¶æ€çš„å˜æ¢ï¼Œå¯ä»¥ç”¨åŠ¨åŠ›å­¦ç³»ç»Ÿç†è®ºå¤„ç†ã€‚ç³»ç»Ÿçš„ç¨³å®šæ€§ä¸è”æƒ³è®°å¿†åŠŸèƒ½æœ‰å¯†åˆ‡å…³ç³»ã€‚Hopfieldç½‘ç»œã€æ³¢è€³å…¹æ›¼æœºå‡å±äºè¿™ç§ç±»å‹ã€‚ æ¿€æ´»å‡½æ•°ç”¨äºå¤„ç†å¤æ‚çš„éçº¿æ€§åˆ†ç±»æƒ…å†µã€‚æ¯”çº¿æ€§å›å½’ã€logisticå›å½’çµæ´»ã€‚è®­ç»ƒçš„æ—¶å€™æ³¨æ„è¿‡æ‹Ÿåˆã€‚éçº¿æ€§æ¿€æ´»å‡½æ•°Sigmoidğ‘“(ğ‘¥)=1/(1+expâ¡(âˆ’ğ‘¥))ç‰¹ç‚¹ï¼šå½“xè¶‹è¿‘è´Ÿæ— ç©·æ—¶ï¼Œyè¶‹è¿‘äº0ï¼›è¶‹è¿‘äºæ­£æ— ç©·æ—¶ï¼Œyè¶‹è¿‘äº1ï¼›xè¶…å‡º[-6,6]çš„èŒƒå›´åï¼Œå‡½æ•°å€¼åŸºæœ¬ä¸Šæ²¡æœ‰å˜åŒ–ï¼Œå€¼éå¸¸æ¥è¿‘0æˆ–è€…1è¯¥å‡½æ•°çš„å€¼åŸŸèŒƒå›´é™åˆ¶åœ¨(0,1)ä¹‹é—´ï¼Œè¿™æ ·sigmoidå‡½æ•°å°±èƒ½ä¸ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒè”ç³»èµ·æ¥äº†ã€‚ğ‘“^â€² (ğ‘¥)=ğ‘“(ğ‘¥)(1âˆ’ğ‘“(ğ‘¥)) åŒæ›²æ­£åˆ‡tanhâ¡(ğ‘¥)=(ğ‘’^ğ‘¥âˆ’ğ‘’^(âˆ’ğ‘¥))/(ğ‘’^ğ‘¥+ğ‘’^(âˆ’ğ‘¥) )ç‰¹ç‚¹ï¼šå½“xè¶‹è¿‘è´Ÿæ— ç©·æ—¶ï¼Œyè¶‹è¿‘äº-1ï¼›è¶‹è¿‘äºæ­£æ— ç©·æ—¶ï¼Œyè¶‹è¿‘äº1ï¼›xè¶…å‡º[-3,3]çš„èŒƒå›´åï¼Œå‡½æ•°å€¼åŸºæœ¬ä¸Šæ²¡æœ‰å˜åŒ–ï¼Œå€¼éå¸¸æ¥è¿‘-1æˆ–è€…1è¯¥å‡½æ•°çš„å€¼åŸŸèŒƒå›´é™åˆ¶åœ¨(-1,1)ä¹‹é—´tanh^â€² (ğ‘¥)=1âˆ’tanh(x)^2 ä¿®æ­£çº¿æ€§å•å…ƒRectifier Linear Unitsï¼ˆReLUï¼‰ğ‘“(ğ‘¥)=maxâ¡(0,ğ‘¥)ç‰¹ç‚¹ï¼šåªæœ‰æœ‰ä¸€åŠéšå«å±‚æ˜¯å¤„äºæ¿€æ´»çŠ¶æ€ï¼Œå…¶ä½™éƒ½æ˜¯è¾“å‡ºä¸º0ä¸ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼ˆå³åœ¨sigmoidæ¥è¿‘é¥±å’ŒåŒºæ—¶ï¼Œå¯¼æ•°è¶‹äº0ï¼Œè¿™ç§æƒ…å†µä¼šé€ æˆä¿¡æ¯ä¸¢å¤±ï¼‰åªéœ€æ¯”è¾ƒã€ä¹˜åŠ è¿ç®—ï¼Œå› æ­¤è®¡ç®—æ–¹ä¾¿ï¼Œè®¡ç®—é€Ÿåº¦å¿«ï¼ŒåŠ é€Ÿäº†ç½‘ç»œçš„è®­ç»ƒReLUæ¯”sigmoidæ›´æ¥è¿‘ç”Ÿç‰©å­¦çš„æ¿€æ´»æ¨¡å‹è¿˜æœ‰ä¸€äº›æ”¹è¿›æˆ–çš„å˜ä½“ Softplusğ‘“(ğ‘¥)=logâ¡(1+ğ‘’^ğ‘¥ )ç‰¹ç‚¹ï¼šxè¶‹äºè´Ÿæ— ç©·æ—¶ï¼Œsoftplusè¶‹äº0ï¼›xè¶‹äºæ­£æ— ç©·æ—¶ï¼Œ softplusè¶‹äºxå®ƒæ˜¯ReLUçš„å¹³æ»‘ç‰ˆå®ƒæ˜¯sigmoidçš„åŸå‡½æ•° æŸå¤±å‡½æ•°ç”¨äºå›å½’ä¸­çš„å‡æ–¹æŸå¤±ï¼šğ¸=1/2 (ğ‘¦âˆ’ğ‘¦ Ì‚ )^2ç”¨äºåˆ†ç±»ä¸­çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼šğ¸=âˆ’âˆ‘â–’ã€–ğ‘¦ğ‘˜ ğ‘™ğ‘œğ‘”((ğ‘¦ğ‘˜ ) Ì‚ )â€ â€œ ã€—k=1,2,â€¦,mè¡¨ç¤ºmç§ç±»åˆ«ã€‚åœ¨è¿çº¦é¢„æµ‹ä¸­m=2 åŸºäº Anaconda çš„å®‰è£…å®‰è£…tensorflowå»ºç«‹ä¸€ä¸ª conda è®¡ç®—ç¯å¢ƒåå­—å«tensorflow: Python 2.7$ conda create -n tensorflow python=2.7 Python 3.4$ conda create -n tensorflow python=3.4 activate tensorflow å®‰è£…tensorflowconda install â€“channel https://conda.anaconda.org/conda-forge tensorflow import tensorflow as tfé€€å‡ºpython3ç¯å¢ƒæˆ–å½“ä½ ä¸ç”¨ TensorFlow çš„æ—¶å€™,å…³é—­ç¯å¢ƒ:(tensorflow)$ deactivate$ # Your prompt should change back windowsä¸‹å®‰è£…å‡çº§pippython -m pip install â€“upgrade pipå®‰è£…tensorflowpip3 install â€“upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0-cp35-cp35m-win_amd64.whl]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow å®‰è£…ä½¿ç”¨]]></title>
    <url>%2F2018%2F08%2F18%2FInstall%20TensorFlow%2F</url>
    <content type="text"><![CDATA[åŸºäºAnacondaçš„Tensorflowå®‰è£…Anacondaæ ¹æ®å®˜ç½‘é€‰æ‹©åŸºäºä¸åŒçš„pythonç‰ˆæœ¬å®‰è£…ï¼šhttps://www.anaconda.com/download/#windows åšä¸»é€‰æ‹©Python 3.6 versionï¼Œwindows 64bit å®‰è£…å®Œæˆåéœ€è¦é…ç½®ç¯å¢ƒå˜é‡ï¼Œæ ¹ç›®å½•å’ŒScriptsç›®å½•åŠ å…¥åˆ°Pathä¸‹é¢G:\ProgramData\Anaconda3;G:\ProgramData\Anaconda3\Scripts 1.æ£€æµ‹anacondaç¯å¢ƒæ˜¯å¦å®‰è£…æˆåŠŸï¼šconda â€“version2.æ£€æµ‹ç›®å‰å®‰è£…äº†å“ªäº›ç¯å¢ƒå˜é‡ï¼šconda info â€“envs 3.å®‰è£…pythonç‰ˆæœ¬ï¼ˆåšä¸»é€‰æ‹©3.5ï¼‰ï¼šconda create â€“name tensorflow python=3.5å®‰è£…åæ˜¯3.5.64.æ¿€æ´»tensflowçš„ç¯å¢ƒï¼šactivate tensorflow5.æ£€æµ‹tensflowçš„ç¯å¢ƒæ·»åŠ åˆ°äº†Anacondaé‡Œé¢ï¼šconda info â€“envs6.å®‰è£…tensorflow gruç‰ˆæœ¬pip install â€“ignore-installed â€“upgrade tensorflow-gpu å®‰è£…å…¶ä»–ç»„ä»¶ï¼špip install pandasconda install scikit-learnconda install matplotlib IDEæƒ³è¦ä½¿ç”¨tensorflow éœ€è¦åˆ¶å®štensorflowçš„pythonç‰ˆæœ¬]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -éšæœºæ£®æ—]]></title>
    <url>%2F2018%2F08%2F16%2FMathematical%20%20Random%20Forest%2F</url>
    <content type="text"><![CDATA[sklearn.ensemble.RandomForestClassifier n_estimators : integer, optional (default=10) æ£®æ—é‡Œï¼ˆå†³ç­–ï¼‰æ ‘çš„æ•°ç›® criterion : string, optional (default=â€giniâ€) è¡¡é‡åˆ†è£‚è´¨é‡çš„æ€§èƒ½ï¼ˆå‡½æ•°ï¼‰ã€‚ å—æ”¯æŒçš„æ ‡å‡†æ˜¯åŸºå°¼ä¸çº¯åº¦çš„â€giniâ€,å’Œä¿¡æ¯å¢ç›Šçš„â€entropyâ€ï¼ˆç†µï¼‰ã€‚æ³¨æ„ï¼šè¿™ä¸ªå‚æ•°æ˜¯ç‰¹å®šæ ‘çš„ max_features : int, float, string or None, optional (default=â€autoâ€) é€‰æ‹©æœ€é€‚å±æ€§æ—¶åˆ’åˆ†çš„ç‰¹å¾ä¸èƒ½è¶…è¿‡æ­¤å€¼: å¦‚æœæ˜¯intï¼Œå°±è¦è€ƒè™‘æ¯ä¸€æ¬¡åˆ†å‰²å¤„çš„max_featureç‰¹å¾ å¦‚æœæ˜¯floatï¼Œé‚£ä¹ˆmax_featureså°±æ˜¯ä¸€ä¸ªç™¾åˆ†æ¯”ï¼Œé‚£ä¹ˆï¼ˆmax_feature*n_featuresï¼‰ç‰¹å¾æ•´æ•°å€¼æ˜¯åœ¨æ¯ä¸ªåˆ†å‰²å¤„è€ƒè™‘çš„ã€‚ å¦‚æœæ˜¯autoï¼Œé‚£ä¹ˆmax_features=sqrt(n_features)ï¼Œå³n_featuresçš„å¹³æ–¹æ ¹å€¼ã€‚ å¦‚æœæ˜¯log2ï¼Œé‚£ä¹ˆmax_features=log2(n_features) å¦‚æœæ˜¯None,é‚£ä¹ˆmax_features=n_features æ³¨æ„ï¼šå¯»æ‰¾åˆ†å‰²ç‚¹ä¸ä¼šåœæ­¢ï¼Œç›´åˆ°æ‰¾åˆ°æœ€å°‘ä¸€ä¸ªæœ‰æ•ˆçš„èŠ‚ç‚¹åˆ’åˆ†åŒºï¼Œå³ä½¿å®ƒéœ€è¦æœ‰æ•ˆæ£€æŸ¥è¶…è¿‡max_featuresçš„ç‰¹å¾ã€‚ max_depth : integer or None, optional (default=None)ï¼ˆå†³ç­–ï¼‰æ ‘çš„æœ€å¤§æ·±åº¦ã€‚å¦‚æœå€¼ä¸ºNoneï¼Œé‚£ä¹ˆä¼šæ‰©å±•èŠ‚ç‚¹ï¼Œç›´åˆ°æ‰€æœ‰çš„å¶å­æ˜¯çº¯å‡€çš„ï¼Œæˆ–è€…ç›´åˆ°æ‰€æœ‰å¶å­åŒ…å«å°‘äºmin_sample_splitçš„æ ·æœ¬ã€‚ min_samples_split : int, float, optional (default=2) æ ¹æ®å±æ€§åˆ’åˆ†èŠ‚ç‚¹æ—¶ï¼Œæ¯ä¸ªåˆ’åˆ†æœ€å°‘çš„æ ·æœ¬æ•°ã€‚ å¦‚æœä¸ºintï¼Œé‚£ä¹ˆè€ƒè™‘min_samples_splitä½œä¸ºæœ€å°çš„æ•°å­—ã€‚ å¦‚æœä¸ºfloatï¼Œé‚£ä¹ˆmin_samples_splitæ˜¯ä¸€ä¸ªç™¾åˆ†æ¯”ï¼Œå¹¶ä¸”æŠŠceil(min_samples_split*n_samples)æ˜¯æ¯ä¸€ä¸ªåˆ†å‰²æœ€å°çš„æ ·æœ¬æ•°é‡ã€‚ åœ¨ç‰ˆæœ¬0.18ä¸­æ›´æ”¹ï¼šä¸ºç™¾åˆ†æ¯”æ·»åŠ æµ®ç‚¹å€¼ã€‚ å¶å­èŠ‚ç‚¹æœ€å°‘çš„æ ·æœ¬æ•°ã€‚ å¦‚æœä¸ºintï¼Œé‚£ä¹ˆè€ƒè™‘min_samples_leafä½œä¸ºæœ€å°çš„æ•°å­—ã€‚ å¦‚æœä¸ºfloatï¼Œé‚£ä¹ˆmin_samples_leafä¸ºä¸€ä¸ªç™¾åˆ†æ¯”ï¼Œå¹¶ä¸”ceil(min_samples_leaf*n_samples)æ˜¯æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°é‡ã€‚ åœ¨ç‰ˆæœ¬0.18ä¸­æ›´æ”¹ï¼šä¸ºç™¾åˆ†æ¯”æ·»åŠ æµ®ç‚¹å€¼ã€‚ min_weight_fraction_leaf : float, optional (default=0.) ä¸€ä¸ªå¶å­èŠ‚ç‚¹æ‰€éœ€è¦çš„æƒé‡æ€»å’Œï¼ˆæ‰€æœ‰çš„è¾“å…¥æ ·æœ¬ï¼‰çš„æœ€å°åŠ æƒåˆ†æ•°ã€‚å½“sample_weightæ²¡æœ‰æä¾›æ—¶ï¼Œæ ·æœ¬å…·æœ‰ç›¸åŒçš„æƒé‡ max_leaf_nodes : int or None, optional (default=None) å¶å­æ ‘çš„æœ€å¤§æ ·æœ¬æ•°ã€‚ ä»¥æœ€ä¼˜çš„æ–¹æ³•ä½¿ç”¨max_leaf_nodesæ¥ç”Ÿé•¿æ ‘ã€‚æœ€å¥½çš„èŠ‚ç‚¹è¢«å®šä¹‰ä¸ºä¸çº¯åº¦ä¸Šçš„ç›¸å¯¹å‡å°‘ã€‚å¦‚æœä¸ºNone,é‚£ä¹ˆä¸é™åˆ¶å¶å­èŠ‚ç‚¹çš„æ•°é‡ã€‚ min_impurity_split : float, æ ‘æ—©æœŸç”Ÿé•¿çš„é˜ˆå€¼ã€‚å¦‚æœä¸€ä¸ªèŠ‚ç‚¹çš„ä¸çº¯åº¦è¶…è¿‡é˜ˆå€¼é‚£ä¹ˆè¿™ä¸ªèŠ‚ç‚¹å°†ä¼šåˆ†è£‚ï¼Œå¦åˆ™å®ƒè¿˜æ˜¯ä¸€ç‰‡å¶å­ã€‚ è‡ª0.19ç‰ˆä»¥åä¸æ¨èä½¿ç”¨ï¼šmin_impurity_splitå·²è¢«å¼ƒç”¨ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯0.19ä¸­çš„min_impurity_decreaseã€‚min_impurity_splitå°†åœ¨0.21ä¸­è¢«åˆ é™¤ã€‚ ä½¿ç”¨min_impurity_decrease min_impurity_decrease : float, optional (default=0.) å¦‚æœèŠ‚ç‚¹çš„åˆ†è£‚å¯¼è‡´çš„ä¸çº¯åº¦çš„ä¸‹é™ç¨‹åº¦å¤§äºæˆ–è€…ç­‰äºè¿™ä¸ªèŠ‚ç‚¹çš„å€¼ï¼Œé‚£ä¹ˆè¿™ä¸ªèŠ‚ç‚¹å°†ä¼šè¢«åˆ†è£‚ã€‚ ä¸çº¯åº¦åŠ æƒå‡å°‘æ–¹ç¨‹å¼å¦‚ä¸‹ï¼š N_t / N (impurity - N_t_R / N_t right_impurity- N_t_L / N_t * left_impurity) Næ˜¯æ ·æœ¬æ€»çš„æ•°é‡ï¼ŒN_tæ˜¯å½“å‰èŠ‚ç‚¹å¤„çš„æ ·æœ¬æ•°é‡ï¼ŒN_t_Læ˜¯å·¦å­©å­èŠ‚ç‚¹æ ·æœ¬çš„æ•°é‡,è¿˜æœ‰N_t_Ræ˜¯å³å­©å­èŠ‚ç‚¹çš„æ ·æœ¬æ•°é‡ã€‚ Nï¼ŒN_tï¼ŒN_t_Rå’ŒN_t_Lå…¨éƒ¨æ˜¯æŒ‡åŠ æƒæ€»å’Œï¼Œå¦‚æœsample_weighté€šè¿‡çš„è¯ã€‚ 0.19ç‰ˆæœ¬æ–°åŠ çš„å‚æ•°ã€‚ bootstrap : boolean, optional (default=True) å»ºç«‹å†³ç­–æ ‘æ—¶ï¼Œæ˜¯å¦ä½¿ç”¨æœ‰æ”¾å›æŠ½æ ·ã€‚ oob_score : bool (default=False) æ˜¯å¦ä½¿ç”¨è¢‹å¤–æ ·æœ¬æ¥ä¼°è®¡æ³›åŒ–ç²¾åº¦ã€‚ n_jobs : integer, optional (default=1) ç”¨äºæ‹Ÿåˆå’Œé¢„æµ‹çš„å¹¶è¡Œè¿è¡Œçš„å·¥ä½œï¼ˆä½œä¸šï¼‰æ•°é‡ã€‚å¦‚æœå€¼ä¸º-1ï¼Œé‚£ä¹ˆå·¥ä½œæ•°é‡è¢«è®¾ç½®ä¸ºæ ¸çš„æ•°é‡ã€‚ random_state : int, RandomState instance or None, optional (default=None) RandomStateIf intï¼Œrandom_stateæ˜¯éšæœºæ•°ç”Ÿæˆå™¨ä½¿ç”¨çš„ç§å­; å¦‚æœæ˜¯RandomStateå®ä¾‹ï¼Œrandom_stateå°±æ˜¯éšæœºæ•°ç”Ÿæˆå™¨; å¦‚æœä¸ºNoneï¼Œåˆ™éšæœºæ•°ç”Ÿæˆå™¨æ˜¯np.randomä½¿ç”¨çš„RandomStateå®ä¾‹ã€‚ verbose : int, optional (default=0) æ§åˆ¶å†³ç­–æ ‘å»ºç«‹è¿‡ç¨‹çš„å†—ä½™åº¦ã€‚ warm_start : bool, optional (default=False) å½“è¢«è®¾ç½®ä¸ºTrueæ—¶ï¼Œé‡æ–°ä½¿ç”¨ä¹‹å‰å‘¼å«çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨æ¥ç»™å…¨ä½“æ‹Ÿåˆå’Œæ·»åŠ æ›´å¤šçš„ä¼°è®¡å™¨ï¼Œåä¹‹ï¼Œä»…ä»…åªæ˜¯ä¸ºäº†æ‹Ÿåˆä¸€ä¸ªå…¨æ–°çš„æ£®æ—ã€‚ class_weight : dict, list of dicts, â€œbalancedâ€, â€œbalanced_subsampleâ€ æˆ–è€…None,ï¼ˆé»˜è®¤å€¼ä¸ºNoneï¼‰,ä¸æ ¼å¼{class_label: weight}ç›¸å…³è”çš„ç±»çš„å¯é€‰çš„æƒå€¼ã€‚å¦‚æœæ²¡æœ‰ç»™å€¼ï¼Œæ‰€æœ‰çš„ç±»åˆ°éƒ½åº”è¯¥æœ‰ä¸€ä¸ªæƒå€¼ã€‚å¯¹äºå¤šè¾“å‡ºé—®é¢˜ï¼Œä¸€ä¸ªå­—å…¸åº åˆ—å¯ä»¥æŒ‰ç…§yçš„åˆ—çš„é¡ºåˆ©è¢«æä¾›ã€‚ è¯·æ³¨æ„ï¼Œå¯¹äºå¤šè¾“å‡ºï¼ˆåŒ…æ‹¬å¤šæ ‡ç­¾ï¼‰ï¼Œå…¶æƒå€¼åº”è¯¥è¢«å®šä¹‰ä¸ºå®ƒè‡ªå·±å­—å…¸çš„æ¯ä¸€åˆ—çš„æ¯ä¸€ä¸ªç±»ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå››ç±»å¤šæ ‡ç­¾åˆ†ç±»ï¼Œæƒå€¼åº”è¯¥å¦‚[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] è¿™æ ·ï¼Œè€Œä¸æ˜¯[{1:1}, {2:5}, {3:1}, {4:1}].è¿™æ ·ã€‚ â€œbalancedâ€æ¨¡å¼ä½¿ç”¨yçš„å€¼æ¥è‡ªåŠ¨çš„è°ƒæ•´æƒå€¼ï¼Œä¸è¾“å…¥æ•°æ®ä¸­ç±»åˆ«é¢‘ç‡æˆåæ¯”ï¼Œå¦‚ï¼šn_samples / (n_classes * np.bincount(y)) â€œbalanced_subsampleâ€æ¨¡å¼å’Œâ€balancedâ€ç›¸åŒï¼Œé™¤äº†æƒå€¼æ˜¯åŸºäºæ¯æ£µæˆé•¿æ ‘æœ‰æ”¾å›æŠ½æ ·è®¡ç®—çš„ã€‚ å¯¹äºå¤šè¾“å‡ºï¼Œyçš„æ¯åˆ—æƒå€¼å°†ç›¸ä¹˜ã€‚ è¯·æ³¨æ„ï¼Œå¦‚æœæŒ‡å®šäº†sample_weight,è¿™äº›æƒå€¼å°†ä¼šå’Œsample_weightç›¸ä¹˜ï¼ˆé€šè¿‡æ‹Ÿåˆæ–¹æ³•ä¼ é€’ï¼‰ã€‚ Attributes: å±æ€§ estimators_ : å†³ç­–æ ‘åˆ†ç±»å™¨çš„åºåˆ— æ‹Ÿåˆçš„å­ä¼°è®¡å™¨çš„é›†åˆã€‚ classes_ : æ•°ç»„ç»´åº¦=[n_classes]çš„æ•°ç»„æˆ–è€…ä¸€ä¸ªè¿™æ ·æ•°ç»„çš„åºåˆ—ã€‚ ç±»åˆ«æ ‡ç­¾ï¼ˆå•ä¸€è¾“å‡ºé—®é¢˜ï¼‰ï¼Œæˆ–è€…ç±»åˆ«æ ‡ç­¾çš„æ•°ç»„åºåˆ—ï¼ˆå¤šè¾“å‡ºé—®é¢˜ï¼‰ã€‚ nclasses : int or list ç±»åˆ«çš„æ•°é‡ï¼ˆå•è¾“å‡ºé—®é¢˜ï¼‰ï¼Œæˆ–è€…ä¸€ä¸ªåºåˆ—ï¼ŒåŒ…å«æ¯ä¸€ä¸ªè¾“å‡ºçš„ç±»åˆ«æ•°é‡ï¼ˆå¤šè¾“å‡ºé—®é¢˜ï¼‰ nfeatures : int æ‰§è¡Œæ‹Ÿåˆæ—¶çš„ç‰¹å¾æ•°é‡ã€‚ noutputs : int æ‰§è¡Œæ‹Ÿåˆæ—¶çš„è¾“å‡ºæ•°é‡ã€‚ featureimportances : array of shape = [n_features] ç‰¹å¾çš„é‡è¦æ€§ï¼ˆå€¼è¶Šé«˜ï¼Œç‰¹å¾è¶Šé‡è¦ï¼‰ oobscore : floatä½¿ç”¨è¢‹å¤–ä¼°è®¡è·å¾—çš„è®­ç»ƒæ•°æ®é›†çš„å¾—åˆ†ã€‚ oob_decisionfunction :ç»´åº¦=[n_samples,n_classes]çš„æ•°ç»„ åœ¨è®­ç»ƒé›†ä¸Šç”¨è¢‹å¤–ä¼°è®¡è®¡ç®—çš„å†³ç­–å‡½æ•°ã€‚å¦‚æœn_estimatorså¾ˆå°çš„è¯ï¼Œé‚£ä¹ˆåœ¨æœ‰æ”¾å›æŠ½æ ·ä¸­ï¼Œä¸€ä¸ªæ•°æ®ç‚¹ä¹Ÿä¸ä¼šè¢«å¿½ç•¥æ˜¯å¯èƒ½çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œoob_decisionfunction å¯èƒ½åŒ…æ‹¬NaNã€‚ å‚æ•°çš„é»˜è®¤å€¼æ§åˆ¶å†³ç­–æ ‘çš„å¤§å°ï¼ˆä¾‹å¦‚ï¼Œmax_depthï¼Œï¼Œmin_samples_leafç­‰ç­‰ï¼‰ï¼Œå¯¼è‡´å®Œå…¨çš„ç”Ÿé•¿å’Œåœ¨æŸäº›æ•°æ®é›†ä¸Šå¯èƒ½éå¸¸å¤§çš„æœªä¿®å‰ªçš„æ ‘ã€‚ä¸ºäº†é™ä½å†…å®¹æ¶ˆè€—ï¼Œå†³ç­–æ ‘çš„å¤æ‚åº¦å’Œå¤§å°åº”è¯¥é€šè¿‡è®¾ç½®è¿™äº›å‚æ•°å€¼æ¥æ§åˆ¶ã€‚è¿™äº›ç‰¹å¾æ€»æ˜¯åœ¨æ¯ä¸ªåˆ†å‰²ä¸­éšæœºæ’åˆ—ã€‚ å› æ­¤ï¼Œå³ä½¿ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œmax_features = n_featureså’Œbootstrap = Falseï¼Œå¦‚æœåœ¨æœç´¢æœ€ä½³åˆ†å‰²æœŸé—´æ‰€åˆ—ä¸¾çš„è‹¥å¹²åˆ†å‰²çš„å‡†åˆ™çš„æ”¹è¿›æ˜¯ç›¸åŒçš„ï¼Œé‚£ä¹ˆæ‰¾åˆ°çš„æœ€ä½³åˆ†å‰²ç‚¹å¯èƒ½ä¼šä¸åŒã€‚ ä¸ºäº†åœ¨æ‹Ÿåˆè¿‡ç¨‹ä¸­è·å¾—ä¸€ä¸ªç¡®å®šçš„è¡Œä¸ºï¼Œrandom_stateå°†ä¸å¾—ä¸è¢«ä¿®æ­£ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -æ¢¯åº¦ä¸‹é™]]></title>
    <url>%2F2018%2F04%2F27%2FMathematical%20regression%20gradient%20descent%2F</url>
    <content type="text"><![CDATA[æ¢¯åº¦ä¸‹é™ï¼šæ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆBatch Gradient Descentï¼Œç®€ç§°BGDï¼‰ ä¼˜ç‚¹ï¼šå…¨å±€æœ€ä¼˜è§£ï¼›æ˜“äºå¹¶è¡Œå®ç°ï¼› ç¼ºç‚¹ï¼šå½“æ ·æœ¬æ•°ç›®å¾ˆå¤šæ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹ä¼šå¾ˆæ…¢ã€‚éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆStochastic Gradient Descentï¼Œç®€ç§°SGDï¼‰ ä¼˜ç‚¹ï¼šè®­ç»ƒé€Ÿåº¦å¿«ï¼›è¿­ä»£æ¬¡æ•°å°‘ ç¼ºç‚¹ï¼šå‡†ç¡®åº¦ä¸‹é™ï¼Œå¹¶ä¸æ˜¯å…¨å±€æœ€ä¼˜ï¼›ä¸æ˜“äºå¹¶è¡Œå®ç°ã€‚å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆMBGDï¼‰å¦‚æœæŸå¤±å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œæ¢¯åº¦ä¸‹é™æ³•å¾—åˆ°çš„è§£å°±ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜è§£ã€‚ å‡¸å‡¹å‡½æ•°ï¼šè®¾f(x)åœ¨åŒºé—´Dä¸Šè¿ç»­ï¼Œå¦‚æœå¯¹Dä¸Šä»»æ„ä¸¤ç‚¹aã€bæ’æœ‰fï¼ˆï¼ˆa+bï¼‰/2ï¼‰&lt;(f(a)+f(b))/2é‚£ä¹ˆç§°f(x)åœ¨Dä¸Šçš„å›¾å½¢æ˜¯ï¼ˆå‘ä¸Šï¼‰å‡¹çš„ï¼ˆæˆ–å‡¹å¼§ï¼‰ï¼›å¦‚æœæ’æœ‰fï¼ˆï¼ˆa+bï¼‰/2ï¼‰&gt;(f(a)+f(b))/2é‚£ä¹ˆç§°f(x)åœ¨Dä¸Šçš„å›¾å½¢æ˜¯ï¼ˆå‘ä¸Šï¼‰å‡¸çš„ï¼ˆæˆ–å‡¸å¼§ï¼‰ æ¢¯åº¦ä¸‹é™ç›¸å…³æ¦‚å¿µï¼š æ­¥é•¿ï¼ˆLearning rateï¼‰ï¼šæ­¥é•¿å†³å®šäº†åœ¨æ¢¯åº¦ä¸‹é™è¿­ä»£çš„è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€æ­¥æ²¿æ¢¯åº¦è´Ÿæ–¹å‘å‰è¿›çš„é•¿åº¦ã€‚ç”¨ä¸Šé¢ä¸‹å±±çš„ä¾‹å­ï¼Œæ­¥é•¿å°±æ˜¯åœ¨å½“å‰è¿™ä¸€æ­¥æ‰€åœ¨ä½ç½®æ²¿ç€æœ€é™¡å³­æœ€æ˜“ä¸‹å±±çš„ä½ç½®èµ°çš„é‚£ä¸€æ­¥çš„é•¿åº¦ã€‚ 2.ç‰¹å¾ï¼ˆfeatureï¼‰ï¼šæŒ‡çš„æ˜¯æ ·æœ¬ä¸­è¾“å…¥éƒ¨åˆ†ï¼Œæ¯”å¦‚2ä¸ªå•ç‰¹å¾çš„æ ·æœ¬ï¼ˆx(0),y(0)ï¼‰,ï¼ˆx(1),y(1)ï¼‰ï¼ˆx(0),y(0)ï¼‰,ï¼ˆx(1),y(1)ï¼‰,åˆ™ç¬¬ä¸€ä¸ªæ ·æœ¬ç‰¹å¾ä¸ºx(0)x(0)ï¼Œç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å‡ºä¸ºy(0)y(0)ã€‚ å‡è®¾å‡½æ•°ï¼ˆhypothesis functionï¼‰ï¼šåœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œä¸ºäº†æ‹Ÿåˆè¾“å…¥æ ·æœ¬ï¼Œè€Œä½¿ç”¨çš„å‡è®¾å‡½æ•°ï¼Œè®°ä¸ºhÎ¸(x)hÎ¸(x)ã€‚æ¯”å¦‚å¯¹äºå•ä¸ªç‰¹å¾çš„mä¸ªæ ·æœ¬ï¼ˆx(i),y(i)ï¼‰(i=1,2,â€¦m)ï¼ˆx(i),y(i)ï¼‰(i=1,2,â€¦m),å¯ä»¥é‡‡ç”¨æ‹Ÿåˆå‡½æ•°å¦‚ä¸‹ï¼š hÎ¸(x)=Î¸0+Î¸1xhÎ¸(x)=Î¸0+Î¸1xã€‚ æŸå¤±å‡½æ•°ï¼ˆloss functionï¼‰ï¼šä¸ºäº†è¯„ä¼°æ¨¡å‹æ‹Ÿåˆçš„å¥½åï¼Œé€šå¸¸ç”¨æŸå¤±å‡½æ•°æ¥åº¦é‡æ‹Ÿåˆçš„ç¨‹åº¦ã€‚æŸå¤±å‡½æ•°æå°åŒ–ï¼Œæ„å‘³ç€æ‹Ÿåˆç¨‹åº¦æœ€å¥½ï¼Œå¯¹åº”çš„æ¨¡å‹å‚æ•°å³ä¸ºæœ€ä¼˜å‚æ•°ã€‚åœ¨çº¿æ€§å›å½’ä¸­ï¼ŒæŸå¤±å‡½æ•°é€šå¸¸ä¸ºæ ·æœ¬è¾“å‡ºå’Œå‡è®¾å‡½æ•°çš„å·®å–å¹³æ–¹ã€‚æ¯”å¦‚å¯¹äºmä¸ªæ ·æœ¬ï¼ˆxi,yiï¼‰(i=1,2,â€¦m)ï¼ˆxi,yiï¼‰(i=1,2,â€¦m),é‡‡ç”¨çº¿æ€§å›å½’ï¼ŒæŸå¤±å‡½æ•°ä¸ºï¼š J(Î¸0,Î¸1)=âˆ‘i=1m(hÎ¸(xi)âˆ’yi)2J(Î¸0,Î¸1)=âˆ‘i=1m(hÎ¸(xi)âˆ’yi)2 å…¶ä¸­xixiè¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬ç‰¹å¾ï¼Œyiyiè¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬å¯¹åº”çš„è¾“å‡ºï¼ŒhÎ¸(xi)hÎ¸(xi)ä¸ºå‡è®¾å‡½æ•°ã€‚ å±€éƒ¨åŠ æƒå›å½’ç®€å•æ¥è¯´ï¼Œè¿™ä¸ªè¿‡ç¨‹å…¶å®æ˜¯åœ¨å…ˆæ‹Ÿåˆå‡ºä¸€æ¡æ›²çº¿ï¼Œç„¶åå†ç”¨è¿™ä¸ªæ›²çº¿å»é¢„æµ‹éœ€è¦é¢„æµ‹çš„ç‚¹ã€‚(æºè‡ªç™¾åº¦)ä¸ºä»€ä¹ˆæ”¹è¿›è¦ç”¨åŠ æƒå›å½’å‘¢ï¼Ÿ å¾ˆç®€å•ï¼Œå› ä¸ºéçº¿æ€§æ‹Ÿåˆå‡ºç›´çº¿è¯¯å·®ä¼šå¾ˆå¤§ï¼Œè¿™é‡Œçš„å±€éƒ¨åŠ æƒç±»ä¼¼äºknnç®—æ³•çš„æƒé‡ï¼Œå³è·ç¦»ä¸­å¿ƒç‚¹è¶Šè¿‘çš„æƒé‡è¶Šå¤§ï¼Œå¯¹æ‹Ÿåˆæ›²çº¿çš„å½±å“ä¹Ÿå°±è¶Šå¤§ï¼Œæ‰€ä»¥ä¹Ÿæœ‰äº†å±€éƒ¨åŠ æƒè¿™ä¸€åè¯ å‚è€ƒæ–‡çŒ®ï¼šhttps://blog.csdn.net/Gentle_Guan/article/details/76586689?locationNum=8&amp;fps=1]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -é€»è¾‘å›å½’æ¨¡å‹ç‰¹å¾å¤„ç†]]></title>
    <url>%2F2018%2F04%2F08%2FMathematical%20Feature%20processing%20of%20logistic%20regression%20model%2F</url>
    <content type="text"><![CDATA[å¦‚æœæœ‰å¼‚å¸¸å€¼ï¼Œä½¿ç”¨æå¤§-æå°å½’ä¸€åŒ–æˆ–å‡å€¼-æ ‡å‡†å·®å½’ä¸€åŒ–ï¼Œè®¡ç®—ä¹‹å‰éœ€è¦å°†æç«¯å€¼æ’é™¤åœ¨å¤–ã€‚ä¾‹å¦‚ï¼šxâ€™=xâˆ’min/ maxâˆ’minè®¡ç®—maxä¸minæ—¶éœ€è¦ç”¨P1ä¸P99æ¥ä»£æ›¿ã€‚æ–°ç”Ÿæˆçš„å€¼å¦‚æœè¶…è¿‡1ç”¨1è¡¨ç¤ºï¼Œå¦‚æœå°äº0 ç”¨0è¡¨ç¤º]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -å›å½’ç®—æ³•å®ä¾‹]]></title>
    <url>%2F2018%2F03%2F27%2FMathematical%20regression%20%2F</url>
    <content type="text"><![CDATA[æ¦‚å¿µæ¢³ç†ï¼šæ•°å­¦æœŸæœ›ï¼šåœ¨æ¦‚ç‡è®ºå’Œç»Ÿè®¡å­¦ä¸­ï¼Œæ•°å­¦æœŸæœ›(mean)ï¼ˆæˆ–å‡å€¼ï¼Œäº¦ç®€ç§°æœŸæœ›ï¼‰æ˜¯è¯•éªŒä¸­æ¯æ¬¡å¯èƒ½ç»“æœçš„æ¦‚ç‡ä¹˜ä»¥å…¶ç»“æœçš„æ€»å’Œï¼Œæ˜¯æœ€åŸºæœ¬çš„æ•°å­¦ç‰¹å¾ä¹‹ä¸€ã€‚å®ƒåæ˜ éšæœºå˜é‡å¹³å‡å–å€¼çš„å¤§å° æ–¹å·®ï¼šï¼ˆvariance)æ˜¯åœ¨æ¦‚ç‡è®ºå’Œç»Ÿè®¡æ–¹å·®è¡¡é‡éšæœºå˜é‡æˆ–ä¸€ç»„æ•°æ®æ—¶ç¦»æ•£ç¨‹åº¦çš„åº¦é‡ã€‚æ¦‚ç‡è®ºä¸­æ–¹å·®ç”¨æ¥åº¦é‡éšæœºå˜é‡å’Œå…¶æ•°å­¦æœŸæœ›ï¼ˆå³å‡å€¼ï¼‰ä¹‹é—´çš„åç¦»ç¨‹åº¦ã€‚ç»Ÿè®¡ä¸­çš„æ–¹å·®ï¼ˆæ ·æœ¬æ–¹å·®ï¼‰æ˜¯æ¯ä¸ªæ ·æœ¬å€¼ä¸å…¨ä½“æ ·æœ¬å€¼çš„å¹³å‡æ•°ä¹‹å·®çš„å¹³æ–¹å€¼çš„å¹³å‡æ•°ã€‚åœ¨è®¸å¤šå®é™…é—®é¢˜ä¸­ï¼Œç ”ç©¶æ–¹å·®å³åç¦»ç¨‹åº¦æœ‰ç€é‡è¦æ„ä¹‰ã€‚ æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼šåœ¨æ•°å­¦ä¸­ï¼Œè¿ç»­å‹éšæœºå˜é‡çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆåœ¨ä¸è‡³äºæ··æ·†æ—¶å¯ä»¥ç®€ç§°ä¸ºå¯†åº¦å‡½æ•°ï¼‰æ˜¯ä¸€ä¸ªæè¿°è¿™ä¸ªéšæœºå˜é‡çš„è¾“å‡ºå€¼ï¼Œåœ¨æŸä¸ªç¡®å®šçš„å–å€¼ç‚¹é™„è¿‘çš„å¯èƒ½æ€§çš„å‡½æ•°ã€‚è€Œéšæœºå˜é‡çš„å–å€¼è½åœ¨æŸä¸ªåŒºåŸŸä¹‹å†…çš„æ¦‚ç‡åˆ™ä¸ºæ¦‚ç‡å¯†åº¦å‡½æ•°åœ¨è¿™ä¸ªåŒºåŸŸä¸Šçš„ç§¯åˆ†ã€‚å½“æ¦‚ç‡å¯†åº¦å‡½æ•°å­˜åœ¨çš„æ—¶å€™ï¼Œç´¯ç§¯åˆ†å¸ƒå‡½æ•°æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°çš„ç§¯åˆ†ã€‚æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸€èˆ¬ä»¥å°å†™æ ‡è®°æ­£æ€åˆ†å¸ƒæ˜¯é‡è¦çš„æ¦‚ç‡åˆ†å¸ƒã€‚å®ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ï¼šéšç€å‚æ•°Î¼å’ŒÏƒå˜åŒ–ï¼Œæ¦‚ç‡åˆ†å¸ƒä¹Ÿäº§ç”Ÿå˜åŒ–ã€‚æœŸæœ›ï¼šÎ¼æ–¹å·®ï¼šÏƒ^2ä¸­ä½æ•°ï¼šÎ¼ä¼—44o6fdeswq DFGI-æ•°ï¼šÎ¼ååº¦ï¼š0å³°åº¦ï¼š3\]æ­£æ€åˆ†å¸ƒï¼šåˆç§°ä¸ºå¸¸æ€åˆ†å¸ƒï¼Œé«˜æ–¯åˆ†å¸ƒã€‚è‹¥éšæœºå˜é‡Xæœä»ä¸€ä¸ªæ•°å­¦æœŸæœ›ä¸ºÎ¼ã€æ–¹å·®ä¸ºÏƒ^2çš„æ­£æ€åˆ†å¸ƒï¼Œè®°ä¸ºN(Î¼ï¼ŒÏƒ^2)ã€‚å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºæ­£æ€åˆ†å¸ƒçš„æœŸæœ›å€¼Î¼å†³å®šäº†å…¶ä½ç½®ï¼Œå…¶æ ‡å‡†å·®Ïƒå†³å®šäº†åˆ†å¸ƒçš„å¹…åº¦ã€‚å½“Î¼ = 0,Ïƒ = 1æ—¶çš„æ­£æ€åˆ†å¸ƒæ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚ çº¿æ€§å›å½’ï¼šçº¿æ€§å›å½’æ˜¯åˆ©ç”¨æ•°ç†ç»Ÿè®¡ä¸­å›å½’åˆ†æï¼Œæ¥ç¡®å®šä¸¤ç§æˆ–ä¸¤ç§ä»¥ä¸Šå˜é‡é—´ç›¸äº’ä¾èµ–çš„å®šé‡å…³ç³»çš„ä¸€ç§ç»Ÿè®¡åˆ†ææ–¹æ³•ï¼Œè¿ç”¨ååˆ†å¹¿æ³›ã€‚å…¶è¡¨è¾¾å½¢å¼ä¸ºy = wâ€™x+eï¼Œeä¸ºè¯¯å·®æœä»å‡å€¼ä¸º0çš„æ­£æ€åˆ†å¸ƒã€‚ å›å½’æ•°æ®ï¼šhttp://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption å±æ€§ä¿¡æ¯ï¼š 1.dateï¼šæ ¼å¼dd/mm/yyyyæ—¥æœŸ2.timeï¼šæ ¼å¼HHæ—¶é—´ï¼šMMï¼šSS3.global_active_powerï¼šå®¶ç”¨å…¨çƒåˆ†é’Ÿå¹³å‡æœ‰åŠŸåŠŸç‡ï¼ˆåƒç“¦ï¼‰4.global_reactive_power: å®¶ç”¨å…¨çƒåˆ†é’Ÿå¹³å‡æ— åŠŸåŠŸç‡ï¼ˆåƒç“¦ï¼‰5.voltageï¼šåˆ†é’Ÿå¹³å‡ç”µå‹ï¼ˆä¼ç‰¹ï¼‰6.global_intensityï¼šå®¶ç”¨å…¨çƒåˆ†é’Ÿå¹³å‡ç”µæµå¼ºåº¦ï¼ˆå®‰åŸ¹ï¼‰7.sub_metering_1ï¼šèƒ½è€—åˆ†é¡¹è®¡é‡1å·ï¼ˆä¸­æœ‰åŠŸç”µèƒ½ç”µèƒ½ï¼‰ã€‚å®ƒä¸å¨æˆ¿ç›¸å¯¹åº”ï¼Œä¸»è¦åŒ…æ‹¬æ´—ç¢—æœºã€çƒ¤ç®±å’Œå¾®æ³¢ç‚‰ï¼ˆçƒ­æ¿ä¸æ˜¯ç”µåŠ¨çš„ï¼Œè€Œæ˜¯ç‡ƒæ°”é©±åŠ¨çš„ï¼‰ã€‚8.sub_metering_2ï¼šèƒ½è€—åˆ†é¡¹è®¡é‡2å·ï¼ˆä¸­æœ‰åŠŸç”µèƒ½ç”µèƒ½ï¼‰ã€‚å®ƒå¯¹åº”æ´—è¡£æˆ¿ï¼ŒåŒ…æ‹¬æ´—è¡£æœºã€æ»šç­’çƒ˜å¹²æœºã€å†°ç®±å’Œç¯ã€‚9.sub_metering_3ï¼šèƒ½è€—åˆ†é¡¹è®¡é‡3å·ï¼ˆä¸­æœ‰åŠŸç”µèƒ½ç”µèƒ½ï¼‰ã€‚å®ƒç›¸å½“äºä¸€ä¸ªç”µçƒ­æ°´å™¨å’Œä¸€ä¸ªç©ºè°ƒã€‚ 1234567891011121314151617import pandas as pdimport numpy as npimport timeimport sklearnimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitpath='C:/Users/zhanghongming/Documents/data/100.txt'names = ['Date','Time','Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']df=pd.read_csv(path,sep=';')print(df.head()) Date Time Global_active_power Global_reactive_power Voltage \ 0 16/12/2006 17:24:00 4.216 0.418 234.841 16/12/2006 17:25:00 5.360 0.436 233.632 16/12/2006 17:26:00 5.374 0.498 233.293 16/12/2006 17:27:00 5.388 0.502 233.744 16/12/2006 17:28:00 3.666 0.528 235.68 Global_intensity Sub_metering_1 Sub_metering_2 Sub_metering_30 18.4 0.0 1.0 17.01 23.0 0.0 1.0 16.02 23.0 0.0 2.0 17.03 23.0 0.0 1.0 17.04 15.8 0.0 1.0 17.0 çœ‹æ‰€æœ‰çš„å˜é‡å€¼123for i in df.columns: print(df[i].value_counts()) Name: Date, dtype: int6419:01:00 117:27:00 118:24:00 1 .. Name: Time, Length: 99, dtype: int644.230 22.912 24.218 26.072 15.412 1 .. Name: Global_active_power, Length: 96, dtype: int640.000 330.090 70.054 40.144 3 .. Name: Global_reactive_power, dtype: int64235.84 3234.20 2235.68 2233.74 2 .. Name: Voltage, Length: 90, dtype: int6412.4 713.8 515.8 5 ..Name: Global_intensity, dtype: int640.0 99Name: Sub_metering_1, dtype: int641.0 500.0 262.0 8 Name: Sub_metering_2, dtype: int6417.0 7716.0 1818.0 4 Name: Sub_metering_3, dtype: int64 12345678910111213141516171819#ç©ºå€¼å¤„ç†new_df= df.replace('?',np.nan)datas = new_df.dropna(how='any')#å®šä¹‰æ—¶é—´æ ¼å¼åŒ–def datae_format(dt): t = time.strptime(' '.join(dt),'%d/%m/%Y %H:%M:%S') return (t.tm_year,t.tm_mon,t.tm_mday,t.tm_hour,t.tm_min,t.tm_sec)##åˆ†æåŠŸç‡å’Œæ—¶é—´çš„çº¿æ€§å…³ç³»ã€‚å°†æ—¶é—´è½¬æ¢ä¸ºè¿ç»­çš„X = datas[names[0:2]]X = X.apply(lambda x :pd.Series(datae_format(x)),axis=1)Y = datas[names[2]]print(X.head(5))print(Y.head(5)) 0 1 2 3 4 5 0 2006 12 16 17 24 01 2006 12 16 17 25 02 2006 12 16 17 26 03 2006 12 16 17 27 04 2006 12 16 17 28 00 4.2161 5.3602 5.3743 5.3884 3.666Name: Global_active_power, dtype: float64 å‡½æ•°è®²è§£sklearn.model_selection.train_test_splitéšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸€èˆ¬å½¢å¼ï¼štrain_test_splitæ˜¯äº¤å‰éªŒè¯ä¸­å¸¸ç”¨çš„å‡½æ•°ï¼ŒåŠŸèƒ½æ˜¯ä»æ ·æœ¬ä¸­éšæœºçš„æŒ‰æ¯”ä¾‹é€‰å–train dataå’Œtestdataï¼Œå½¢å¼ä¸ºï¼šX_train,X_test, y_train, y_test =cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)å‚æ•°è§£é‡Šï¼štrain_dataï¼šæ‰€è¦åˆ’åˆ†çš„æ ·æœ¬ç‰¹å¾é›†train_targetï¼šæ‰€è¦åˆ’åˆ†çš„æ ·æœ¬ç»“æœtest_sizeï¼šæ ·æœ¬å æ¯”ï¼Œå¦‚æœæ˜¯æ•´æ•°çš„è¯å°±æ˜¯æ ·æœ¬çš„æ•°é‡random_stateï¼šæ˜¯éšæœºæ•°çš„ç§å­ã€‚éšæœºæ•°ç§å­ï¼šå…¶å®å°±æ˜¯è¯¥ç»„éšæœºæ•°çš„ç¼–å·ï¼Œåœ¨éœ€è¦é‡å¤è¯•éªŒçš„æ—¶å€™ï¼Œä¿è¯å¾—åˆ°ä¸€ç»„ä¸€æ ·çš„éšæœºæ•°ã€‚æ¯”å¦‚ä½ æ¯æ¬¡éƒ½å¡«1ï¼Œå…¶ä»–å‚æ•°ä¸€æ ·çš„æƒ…å†µä¸‹ä½ å¾—åˆ°çš„éšæœºæ•°ç»„æ˜¯ä¸€æ ·çš„ã€‚ä½†å¡«0æˆ–ä¸å¡«ï¼Œæ¯æ¬¡éƒ½ä¼šä¸ä¸€æ ·ã€‚éšæœºæ•°çš„äº§ç”Ÿå–å†³äºç§å­ï¼Œéšæœºæ•°å’Œç§å­ä¹‹é—´çš„å…³ç³»éµä»ä»¥ä¸‹ä¸¤ä¸ªè§„åˆ™ï¼šç§å­ä¸åŒï¼Œäº§ç”Ÿä¸åŒçš„éšæœºæ•°ï¼›ç§å­ç›¸åŒï¼Œå³ä½¿å®ä¾‹ä¸åŒä¹Ÿäº§ç”Ÿç›¸åŒçš„éšæœºæ•°ã€‚ 1234567891011121314151617X_train,X_test,Y_train,Y_test = train_test_split( X, Y, test_size=0.2, random_state=42)## æ•°æ®æ ‡å‡†æ¢è¡Œss = StandardScaler()X_train = ss.fit_transform(X_train)X_test = ss.fit_transform(X_test)##è®­ç»ƒæ•°æ®lr = LinearRegression()lr.fit(X_train,Y_train)##é¢„æµ‹Yå€¼y_predict = lr.predict(X_test)print("å‡†ç¡®ç‡:",lr.score(X_test,Y_test)) æ ·æœ¬æ•°æ®100æ¡ï¼šå‡†ç¡®ç‡: 0.0226499044921æ ·æœ¬æ•°æ®1000æ¡ï¼š0.103073016594 æ¨¡å‹ä¿å­˜åŠåŠ è½½ï¼š12345678from sklearn.externals import joblib## æ¨¡å‹ä¿å­˜ï¼šjoblib.dump(ss,"data_ss.model")joblib.dump(lr,"data_lr.model")## åŠ è½½æ¨¡å‹joblib,load("data_ss.model")joblib,load("data_lr.model") plotæ–‡æ¡£ï¼šhttps://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot 123456789101112## è§£å†³ä¸­æ–‡é—®é¢˜mpl.rcParams['font.sans-serif'] = [u'SimHei'];mpl.rcParams['axes.unicode_minus'] = Falset=np.arange(len(X_test))plt.figure(facecolor='w')plt.plot(t,Y_test,'r--',linewidth=2,label=u'çœŸå®å€¼')plt.plot(t,y_predict,'g--',linewidth=2,label=u'é¢„æµ‹å€¼')plt.legend(loc ='lower right')plt.title(u'çº¿æ€§å›å½’æ—¶é—´ä¸ç”µå‹çš„å…³ç³»',fontsize=20 )plt.grid(b=True)plt.show() 100æ¡æ•°æ®ï¼š 1000æ¡æ•°æ®ï¼š linearå¤šé¡¹å¼123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.grid_search import GridSearchCVmodels = [Pipeline([('Poly',PolynomialFeatures()),('Linear',LinearRegression())])]model = models[0]##è·å–Xï¼ŒYå˜é‡ï¼Œå¹¶å°†æ—¶é—´å˜é‡è½¬æ¢ä¸ºæ•°å€¼å‹è¿ç»­çš„X = datas[names[0:2]]X = X.apply(lambda x :pd.Series(datae_format(x)),axis=1)Y = datas[names[4]]## å¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†X_train,X_test,Y_train,Y_test = train_test_split( X, Y, test_size=0.2, random_state=0)## æ•°æ®æ ‡å‡†åŒ–ss = StandardScaler()X_train = ss.fit_transform(X_train)X_test = ss.fit_transform(X_test)## æ¨¡å‹è®­ç»ƒt=np.arange(len(X_test))N =5d_pool= np.arange(1,N,1)m=d_pool.sizeclrs = [] # é¢œè‰²for c in np.linspace(16711680, 255, m,dtype='int64'): clrs.append('#%06x' % c)line_width = 3plt.figure(figsize=(12,6),facecolor='w')for i,d in enumerate(d_pool): plt.subplot(N-1,1,i+1) plt.plot(t,Y_test,'r--',label=u'çœŸå®å€¼',ms=10,zorder=N) model.set_params(Poly__degree=d) # è®¾ç½®å¤šé¡¹å¼çš„é˜¶ model.fit(X_train,Y_train) lin = model.get_params('Linear')['Linear'] output =u'%dé˜¶ï¼Œç³»æ•°ä¸ºï¼š'%d print( output,lin.coef_.ravel()) y_hat = model.predict(X_test) s = model.score(X_test,Y_test) z=N-1 if (d==2) else 0 label=u'%dé˜¶,å‡†ç¡®ç‡=%.3f'%(d,s) plt.plot(t,y_hat,color=clrs[i],lw=line_width,alpha = 0.75,label=label,zorder=z) plt.legend(loc = 'upper left') plt.grid(True) plt.ylabel(u'%dé˜¶ç»“æœ'%d,fontsize=12)# é¢„æµ‹å€¼å’ŒçœŸå®å€¼ç”»å›¾æ¯”è¾ƒplt.legend(loc = 'lower right')plt.suptitle(u'çº¿æ€§å›å½’æ—¶é—´ä¸ç”µå‹ä¹‹é—´å¤šé¡¹å¼å…³ç³»')plt.grid(b=True)plt.show() 1é˜¶ï¼Œç³»æ•°ä¸ºï¼š [ 0.00000000e+00 5.55111512e-17 0.00000000e+00 0.00000000e+00 -4.22939297e-01 -4.34494704e-01 0.00000000e+00]2é˜¶ï¼Œç³»æ•°ä¸ºï¼š [ 2.47983335e-17 1.11022302e-16 -2.22044605e-16 -1.11022302e-16 -5.05820937e-01 -3.46571423e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 -8.58357173e-01 -7.57689882e-01 0.00000000e+00 -1.60364055e-01 0.00000000e+00 0.00000000e+00]3é˜¶ï¼Œç³»æ•°ä¸ºï¼š [ -1.69309011e-15 -2.99760217e-15 3.33066907e-16 5.55111512e-16 -4.41970713e-02 -3.57278153e-01 0.00000000e+00 2.22044605e-16 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.43644538e-01 4.86208530e-01 0.00000000e+00 3.26242425e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.48140740e-01 6.37890832e-01 0.00000000e+00 -7.45035081e-01 0.00000000e+00 0.00000000e+00 -5.02511111e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]4é˜¶ï¼Œç³»æ•°ä¸ºï¼š [ 2.22002972e-013 -8.01497757e-013 5.37209166e-014 -4.53519167e-013 7.19933381e-003 2.05441337e-001 3.86510268e-013 -5.94066463e-013 6.66133815e-015 6.66133815e-016 -1.88737914e-015 6.27276009e-015 -5.30131494e-015 -1.01585407e-014 3.35287353e-014 8.21565038e-015 -2.55351296e-015 -2.22044605e-016 -6.31088724e-030 3.02922588e-028 1.00974196e-028 -5.04870979e-029 -4.89052469e-002 3.28220946e-001 0.00000000e+000 6.15440583e-003 0.00000000e+000 0.00000000e+000 1.26217745e-029 0.00000000e+000 -5.60519386e-044 -8.40779079e-045 -7.00649232e-045 -2.24207754e-044 -5.60519386e-045 2.80259693e-045 1.40129846e-045 0.00000000e+000 0.00000000e+000 4.97841222e-060 1.55575382e-061 -1.24460306e-060 -2.48920611e-060 0.00000000e+000 -3.11150764e-061 -1.16681536e-061 -3.11150764e-061 -4.66726146e-061 2.21085915e-075 -5.52714788e-076 -1.38178697e-076 2.76357394e-076 -1.38178697e-076 -3.45446742e-077 0.00000000e+000 1.34940134e-079 0.00000000e+000 1.91761463e-093 1.49813643e-095 -5.99254573e-095 4.49440930e-095 1.12360233e-095 0.00000000e+000 7.49068217e-096 -2.34083818e-097 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 -1.66326556e-111 4.15816391e-112 -2.07908195e-112 0.00000000e+000 -6.13741990e-002 -8.70197287e-001 8.39734513e-140 -1.48604949e+000 0.00000000e+000 0.00000000e+000 -4.67097255e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000 -2.42848401e-001 -9.28403263e-001 0.00000000e+000 -8.91115491e-001 0.00000000e+000 0.00000000e+000 1.33924630e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 2.81909059e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn pca]]></title>
    <url>%2F2018%2F03%2F03%2Fscikit-learn%20pca%2F</url>
    <content type="text"><![CDATA[from sklearn import datasets digits = datasets.load_digits()x = digits.datay = digits.target from sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=666) print(x_train.shape) #(1347, 64)from sklearn.neighbors import KNeighborsClassifierimport timestart = time.clock()knn_clf = KNeighborsClassifier() knn_clf.fit(x_train,y_train)end = time.clock() print(end-start) #0.009107513739889835score = knn_clf.score(x_test,y_test) print(score) #0.986666666667 from sklearn.decomposition import PCA pca = PCA(n_components=2)pca.fit(x_train)X_train_reduction = pca.transform(x_train)X_test_reduction = pca.transform(x_test) start2 = time.clock()knn_clf = KNeighborsClassifier()knn_clf.fit(X_train_reduction,y_train)end2 = time.clock() print(end2-start2) #0.0019209365663966915score = knn_clf.score(X_test_reduction,y_test)print(score) #0.606666666667 print(pca.explainedvariance) pca = PCA(n_components=x_train.shape[1]) pca3 = PCA(0.95)pca3.fit(x_train)print(pca3.ncomponents) #28 start3 = time.clock()knn_clf3 = KNeighborsClassifier()X_train_reduction = pca3.transform(x_train)X_test_reduction = pca3.transform(x_test)knn_clf3.fit(X_train_reduction,y_train)end3 = time.clock() print(end3-start3) #0.006395458395239972score = knn_clf3.score(X_test_reduction,y_test)print(score) #0.98]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn KNN]]></title>
    <url>%2F2018%2F03%2F02%2Fscikit-learn%20KNN%2F</url>
    <content type="text"><![CDATA[åœ°å€ï¼šhttp://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier 1234567891011from sklearn.neighbors import KNeighborsClassifierX = [[0], [1], [2], [3]]y = [0, 0, 1, 1]neigh = KNeighborsClassifier(n_neighbors=3)neigh.fit(X, y)print(neigh.predict([[1.1]]))# [0]print(neigh.predict_proba([[0.9]]))#[[ 0.66666667 0.33333333]] æ‰‹å†™æ•°å­—ç»ƒä¹ ï¼š 12345678910111213141516171819from sklearn import datasetsdigits = datasets.load_digits()x= digits.datay= digits.targetprint(x.shape)#(1797, 64)from sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=666)from sklearn.neighbors import KNeighborsClassifierknn_clf = KNeighborsClassifier(n_neighbors=3)knn_clf.fit(x_train,y_train)score= knn_clf.score(x_test,y_test)print(score)#0.988888888889 æŸ¥æ‰¾æœ€ä½³è¶…å‚æ•°123456789101112beat_score=0break_k =-1for k in range(1,11): knn_clf2 = KNeighborsClassifier(n_neighbors=k) knn_clf2.fit(x_train,y_train) score= knn_clf2.score(x_test,y_test) if(score &gt;beat_score): break_k =k beat_score = scoreprint("beat_score",beat_score)print("break_k",break_k) beat_score 0.991666666667break_k 4 123456789101112131415beat_score=0break_k =-1beat_method=''for method in ['uniform','distance']: for k in range(1,11): knn_clf2 = KNeighborsClassifier(n_neighbors=k,weights=method) knn_clf2.fit(x_train,y_train) score= knn_clf2.score(x_test,y_test) if(score &gt;beat_score): beat_method = method break_k =k beat_score = scoreprint("beat_score",beat_score)print("break_k",break_k) ç½‘æ ¼æœç´ ï¼š123456789101112131415161718192021222324252627import timestart = time.clock()param_grid=[ &#123; 'weights':['uniform'], 'n_neighbors':[i for i in range(1,11)] &#125;, &#123; 'weights':['distance'], 'n_neighbors':[i for i in range(1,11)], 'p':[i for i in range(1,6)] &#125;]knn_clf = KNeighborsClassifier()from sklearn.model_selection import GridSearchCVgrid_search = GridSearchCV(knn_clf,param_grid)grid_search.fit(x_train,y_train)print("grid_search.best_estimator_:",grid_search.best_estimator_)print("grid_search.best_score_:",grid_search.best_score_)print("grid_search.best_params_:",grid_search.best_params_)knn_clf = grid_search.best_estimator_knn_clf_score = knn_clf.score(x_test,y_test)print(knn_clf_score)end = time.clock()print(end-start) grid_search.bestestimator: KNeighborsClassifier(algorithm=â€™autoâ€™, leaf_size=30, metric=â€™minkowskiâ€™, metric_params=None, n_jobs=1, n_neighbors=3, p=3, weights=â€™distanceâ€™)grid_search.bestscore: 0.985386221294grid_search.bestparams: {â€˜n_neighborsâ€™: 3, â€˜pâ€™: 3, â€˜weightsâ€™: â€˜distanceâ€™}0.983333333333296.18877317471 å¢åŠ å¹¶è¡ŒåŒ–å¤„ç†ï¼š1grid_search = GridSearchCV(knn_clf,param_grid,n_jobs=2,verbose=2) é€Ÿåº¦å˜ä¸ºï¼š156.67693082041933 æœ€å€¼å½’ä¸€åŒ–ï¼šé€‚ç”¨äºåˆ†å¸ƒæœ‰æ˜æ˜¾è¾¹ç•Œçš„æƒ…å†µï¼Œå—outlierå½±å“è¾ƒå¤§å‡å€¼æ–¹å·®å½’ä¸€åŒ–ï¼šé€‚ç”¨äºåˆ†å¸ƒæ²¡æœ‰æ˜æ˜¾è¾¹ç•Œçš„æƒ…å†µï¼Œæœ‰å¯èƒ½å­˜åœ¨æç«¯æ•°å€¼]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn regression]]></title>
    <url>%2F2018%2F03%2F01%2Fscikit-learn%20regression%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122from sklearn import datasetsboston = datasets.load_boston()x= boston.datay= boston.targefrom sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=666)from sklearn import linear_modelclf = linear_model.LinearRegression()clf.fit(x_train,y_train)y_predict = clf.predict(x_test)from sklearn.metrics import mean_squared_errorfrom sklearn.metrics import mean_absolute_errorMSE = mean_squared_error(y_test,y_predict)print(MSE)MAE = mean_absolute_error(y_test,y_predict)print(MAE)]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -å†³ç­–æ ‘ç®—æ³•å®ä¾‹]]></title>
    <url>%2F2018%2F02%2F18%2FMathematical%20decision%20tree%2F</url>
    <content type="text"><![CDATA[å†³ç­–æ ‘ï¼šæœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•æ˜¯ä¸€ç§é¢„æµ‹æ¨¡å‹æ˜¯åœ¨å·²çŸ¥å„ç§æƒ…å†µå‘ç”Ÿæ¦‚ç‡åŸºç¡€ä¸Šï¼Œé€šè¿‡æ„å»ºå†³ç­–æ ‘æ¥è¿›è¡Œåˆ†æçš„ä¸€ç§æ–¹æ³• æ ‘å½¢ç»“æ„ä»è·ŸèŠ‚ç‚¹å¼€å§‹ï¼Œé¢„æµ‹å¾…åˆ†ç±»é¡¹å¯¹åº”çš„ç‰¹å¾å±æ€§ï¼ŒæŒ‰ç…§å€¼é€‰æ‹©è¾“å‡ºåˆ†æ”¯ï¼Œç›´åˆ°å¶å­èŠ‚ç‚¹ï¼Œå°†å¶å­èŠ‚ç‚¹çš„å­˜æ”¾ç±»åˆ«ä½œä¸ºæ ‘çš„ç»“æœ å†³ç­–æ ‘åˆ†ä¸ºä¸¤ç±»ï¼šåˆ†ç±»ï¼Œå›å½’å‰è€…ç”¨äºåˆ†ç±»æ ‡ç­¾å€¼ï¼Œåè€…ç”¨äºé¢„æµ‹è¿ç»­å€¼å¸¸ç”¨ç®—æ³•ID3ï¼ŒC4,5ï¼ŒCART æ•°æ®æ ‡å‡†åŒ–ï¼šStandardScaler (åŸºäºç‰¹å¾çŸ©é˜µçš„åˆ—ï¼Œå°†å±æ€§å€¼è½¬æ¢è‡³æœä»æ­£æ€åˆ†å¸ƒ)æ ‡å‡†åŒ–æ˜¯ä¾ç…§ç‰¹å¾çŸ©é˜µçš„åˆ—å¤„ç†æ•°æ®ï¼Œå…¶é€šè¿‡æ±‚z-scoreçš„æ–¹æ³•ï¼Œå°†æ ·æœ¬çš„ç‰¹å¾å€¼è½¬æ¢åˆ°åŒä¸€é‡çº²ä¸‹å¸¸ç”¨ä¸åŸºäºæ­£æ€åˆ†å¸ƒçš„ç®—æ³•ï¼Œæ¯”å¦‚å›å½’æ•°æ®å½’ä¸€åŒ–MinMaxScaler ï¼ˆåŒºé—´ç¼©æ”¾ï¼ŒåŸºäºæœ€å¤§æœ€å°å€¼ï¼Œå°†æ•°æ®è½¬æ¢åˆ°0,1åŒºé—´ä¸Šçš„ï¼‰æå‡æ¨¡å‹æ”¶æ•›é€Ÿåº¦ï¼Œæå‡æ¨¡å‹ç²¾åº¦å¸¸è§ç”¨äºç¥ç»ç½‘ç»œNormalizer ï¼ˆåŸºäºçŸ©é˜µçš„è¡Œï¼Œå°†æ ·æœ¬å‘é‡è½¬æ¢ä¸ºå•ä½å‘é‡ï¼‰å…¶ç›®çš„åœ¨äºæ ·æœ¬å‘é‡åœ¨ç‚¹ä¹˜è¿ç®—æˆ–å…¶ä»–æ ¸å‡½æ•°è®¡ç®—ç›¸ä¼¼æ€§æ—¶ï¼Œæ‹¥æœ‰ç»Ÿä¸€çš„æ ‡å‡†å¸¸è§ç”¨äºæ–‡æœ¬åˆ†ç±»å’Œèšç±»ã€logisticå›å½’ä¸­ä¹Ÿä¼šä½¿ç”¨ï¼Œæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆ ç‰¹å¾é€‰æ‹©ï¼šä»å·²æœ‰çš„ç‰¹å¾ä¸­é€‰æ‹©å‡ºå½±å“ç›®æ ‡å€¼æœ€å¤§çš„ç‰¹å¾å±æ€§ å¸¸ç”¨æ–¹æ³•ï¼š{ åˆ†ç±»ï¼šFç»Ÿè®¡é‡ã€å¡æ–¹ç³»æ•°ï¼Œäº’ä¿¡æ¯mutual_info_classif{ è¿ç»­ï¼šçš®å°”é€Šç›¸å…³ç³»æ•° Fç»Ÿè®¡é‡ äº’ä¿¡æ¯mutual_info_classif SelectKBestï¼ˆå¡æ–¹ç³»æ•°ï¼‰]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®¾è®¡æ¨¡å¼-ç»“æ„æ¨¡å¼]]></title>
    <url>%2F2018%2F02%2F11%2FDesign%20pattern%20structural%2F</url>
    <content type="text"><![CDATA[1.é€‚é…å™¨æ•ˆæœåŠä¼˜ç¼ºç‚¹ï¼šå¯¹äºç±»é€‚é…å™¨ï¼š ç”¨ä¸€ä¸ªå…·ä½“çš„Adapterç±»å¯¹Adapteeå’ŒTagetè¿›è¡ŒåŒ¹é…ã€‚ç»“æœæ˜¯å½“æˆ‘ä»¬æƒ³è¦åŒ¹é…ä¸€ä¸ªç±»ä»¥åŠæ‰€æœ‰å®ƒçš„å­ç±»æ—¶ï¼Œç±»Adapterå°†ä¸èƒ½èƒœä»»å·¥ä½œã€‚ ä½¿å¾—Adapterå¯ä»¥overrideï¼ˆé‡å®šä¹‰ï¼‰ Adapteeçš„éƒ¨åˆ†è¡Œä¸ºï¼Œå› ä¸ºAdapteræ˜¯Adapteeçš„ä¸€ä¸ªå­ç±»ã€‚å¯¹äºå¯¹è±¡é€‚é…å™¨ï¼š å…è®¸ä¸€ä¸ªAdapterä¸å¤šä¸ªAdapteeï¼Œå³Adapteeæœ¬èº«ä»¥åŠå®ƒçš„æ‰€æœ‰å­ç±»ï¼ˆå¦‚æœæœ‰å­ç±»çš„è¯ï¼‰åŒæ—¶å·¥ä½œã€‚Adapterä¹Ÿå¯ä»¥ä¸€æ¬¡ç»™æ‰€æœ‰çš„Adapteeæ·»åŠ åŠŸèƒ½ã€‚ ä½¿å¾—overrideï¼ˆé‡å®šä¹‰ï¼‰Adapteeçš„è¡Œä¸ºæ¯”è¾ƒå›°éš¾ã€‚å¦‚æœä¸€å®šè¦override Adapteeçš„æ–¹æ³•ï¼Œå°±åªå¥½å…ˆåšä¸€ä¸ªAdapteeçš„å­ç±»ä»¥override Adapteeçš„æ–¹æ³•ï¼Œç„¶åå†æŠŠè¿™ä¸ªå­ç±»å½“ä½œçœŸæ­£çš„Adapteeæºè¿›è¡Œé€‚é…ã€‚ 2.æ¡¥æ¥ç»§æ‰¿æ˜¯ä¸€ç§å¼ºè€¦åˆçš„ç»“æœï¼Œçˆ¶ç±»å˜ï¼Œå­ç±»å°±å¿…é¡»è¦å˜ã€‚å¯ä»¥ä½¿ç”¨ç»„åˆ/ç»§æ‰¿æ¥è§£è€¦åˆã€‚å°†æŠ½è±¡å’Œä»–çš„å®ç°åˆ†ç¦» 3.ç»„åˆå°†å¯¹è±¡ç»„åˆæˆå±æ€§ç»“æ„ä»¥è¡¨ç¤ºâ€˜éƒ¨åˆ†-æ•´ä½“â€™çš„å±‚æ¬¡ç»“æ„ã€‚ç»„åˆæ¨¡å¼ä½¿å¾—ç”¨æˆ·å¯¹å•ä¸ªå¯¹è±¡å’Œç»„åˆå¯¹è±¡çš„ä½¿ç”¨å…·æœ‰ä¸€è‡´æ€§ã€‚ ç»„åˆæ¨¡å¼æè¿°äº†å¦‚ä½•ä½¿ç”¨é€’å½’çš„ç»„åˆï¼Œä½¿å®¢æˆ·ä¸ç”¨åŒºåˆ†è¿™äº›ç±» 4.è£…é…å™¨5.å¤–è§‚6.äº«å…ƒæ¨¡å¼7.ä»£ç†æ¨¡å¼Copy-on-writedaiä»£ç†ï¼šå³å†™å³å¤åˆ¶â€œå¿«ç…§â€è™šæ‹Ÿä»£ç†çš„ä¸€ç§ï¼ŒæŠŠå¤åˆ¶æ‹–å»¶åˆ°åªæœ‰å®¢æˆ·ç«¯éœ€è¦æ—¶ï¼Œæ‰çœŸæ­£æ‰§è¡Œä¿æŠ¤ä»£ç†ï¼šå…è®¸åœ¨è®¿é—®å¯¹è±¡æ—¶é™„åŠ ç®¡ç†ä»»åŠ¡ 1.ä»€ä¹ˆæ˜¯ä»£ç†æ¨¡å¼ï¼šä¾‹å¦‚æˆ‘ä»¬æ‰¾æˆ¿å­æ‰¾ä¸­ä»‹2.ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ä»£ç†ï¼šæˆ‘ä»¬ä¸éœ€è¦è‡ªå·±æ‰¾æˆ¿å­]]></content>
      <categories>
        <category>è®¾è®¡æ¨¡å¼</category>
      </categories>
      <tags>
        <tag>è®¾è®¡æ¨¡å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®¾è®¡æ¨¡å¼-è¡Œä¸ºæ¨¡å¼]]></title>
    <url>%2F2018%2F02%2F10%2FDesign%20pattern%20behavior%2F</url>
    <content type="text"><![CDATA[è´£ä»»é“¾æ¨¡å¼å‘½ä»¤æ¨¡å¼å‘½ä»¤Command â€”â€”å£°æ˜æ‰§è¡Œæ“ä½œçš„æ¥å£ã€‚å…·ä½“å‘½ä»¤ConcreteCommand â€”â€”å®šä¹‰æ¥æ”¶å¯¹è±¡å’ŒåŠ¨ä½œä¹‹é—´çš„ç»‘å®šå…³ç³»ã€‚ â€”â€”é€šè¿‡å¼•èµ·æ¥æ”¶è€…çš„ç›¸åº”åŠ¨ä½œæ¥å®ç°æ‰§è¡Œã€‚å®¢æˆ·Client â€”â€”äº§ç”Ÿä¸€ä¸ªConcreteCommandå¯¹è±¡ï¼Œå¹¶è®¾ç½®æ¥æ”¶è€…ã€‚å¼•å‘è€…Invoker â€”â€”è¦æ±‚å‘½ä»¤æ‰§è¡Œè¯·æ±‚ã€‚æ¥æ”¶è€…Receiver â€”â€”çŸ¥é“å¦‚ä½•æ‰§è¡Œä¸è¯·æ±‚ç›¸è”ç³»çš„æ“ä½œã€‚ è¿­ä»£å™¨æ¨¡å¼æ¨¡æ¿æ–¹æ³•æ¨¡å¼å‡†å¤‡ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œå®šä¹‰ä¸€ä¸ªç®—æ³•çš„å¤§ä½“æ¡†æ¶å°†éƒ¨åˆ†é€»è¾‘ä»¥å…·ä½“æ–¹æ³•ä»¥åŠå…·ä½“æ„é€ å­çš„å½¢å¼å®ç°å‰©ä½™çš„é€»è¾‘é€šè¿‡å£°æ˜ä¸€äº›æŠ½è±¡æ–¹æ³•æ¥æè¿°è¿™äº›æŠ½è±¡æ–¹æ³•è¦æ±‚å­ç±»å®ç°ï¼Œä¸åŒçš„å­ç±»å¯ä»¥ä»¥ä¸åŒçš„æ–¹å¼å®ç°è¿™äº›æŠ½è±¡æ–¹æ³•ï¼Œä»è€Œå¯¹å‰©ä½™çš„é€»è¾‘æœ‰ä¸åŒçš„å®ç°ã€‚å­ç±»ä¸æ”¹å˜ç®—æ³•çš„ç»“æ„è€Œé‡å®šä¹‰ç®—æ³• è§‚å¯Ÿè€…æ¨¡å¼åŒä¸€åº”ç”¨å¯¹è±¡ä¸åŒå±•ç¤ºå½¢å¼ï¼Œå¦‚ä¸€ç»„æ•°æ®æ˜ å°„ä¸ºè¡¨æ ¼å’ŒæŸ±çŠ¶å›¾ã€‚ç”¨æˆ·æ›´æ”¹è¡¨æ ¼æ•°æ®ï¼ŒæŸ±çŠ¶å›¾è¦åŒæ­¥ä¿®æ”¹ å…³é”®å¯¹è±¡ï¼šæŠ½è±¡ä¸»é¢˜Subjectæä¾›ä¸€ä¸ªè¿æ¥è§‚å¯Ÿè€…å¯¹è±¡å’Œè§£é™¤è¿æ¥çš„æ¥å£ã€‚çŸ¥é“å®ƒçš„è§‚å¯Ÿè€…ã€‚å¯æœ‰ä»»æ„æ•°ç›®çš„è§‚å¯Ÿè€…å¯¹è±¡è§‚å¯Ÿä¸€ä¸ªä¸»é¢˜ã€‚å¯ä»¥å¢åŠ å’Œåˆ é™¤è§‚å¯Ÿè€…å¯¹è±¡ï¼Œå…·ä½“ä¸»é¢˜ConcreteSubjectï¼šé€šå¸¸ç”¨ä¸€ä¸ªå…·ä½“å­ç±»å®ç°ã€‚è´Ÿè´£å®ç°å¯¹è§‚å¯Ÿè€…å¼•ç”¨çš„èšé›†çš„ç®¡ç†åŠ›æ³¨ã€‚å°†æœ‰å…³çŠ¶æ€å­˜å…¥ConcreteObserverå¯¹è±¡ã€‚åœ¨å…·ä½“ä¸»é¢˜å†…éƒ¨çŠ¶æ€æ”¹å˜æ—¶å‘å®ƒçš„è§‚å¯Ÿè€…å‘é€é€šçŸ¥ã€‚ æŠ½è±¡è§‚å¯Ÿè€…Observer ï¼šä¸€èˆ¬ç”¨ä¸€ä¸ªæŠ½è±¡ç±»æˆ–è€…ä¸€ä¸ªæ¥å£å®ç°ï¼Œä¸ºæ‰€æœ‰çš„å…·ä½“è§‚å¯Ÿè€…å®šä¹‰ä¸€ä¸ªæ›´æ–°æ¥å£æ›´æ–°æ¥å£åŒ…å«çš„æ–¹æ³•å«æ›´æ–°æ–¹æ³•ã€‚å…·ä½“è§‚å¯Ÿè€…ConcreteObserveré€šå¸¸ç”¨ä¸€ä¸ªå…·ä½“å­ç±»å®ç°ï¼Œä¿å­˜ä¸€ä¸ªæŒ‡å‘ConcreteSubjectå¯¹è±¡çš„å¼•ç”¨ã€‚å­˜å‚¨è¦ä¸ä¸»é¢˜ä¸€è‡´çš„çŠ¶æ€ã€‚å®ç°æŠ½è±¡è§‚å¯Ÿè€…è§’è‰²æ‰€è¦æ±‚çš„æ›´æ–°æ¥å£ï¼Œä»¥ä¾¿ä½¿æœ¬èº«çš„çŠ¶æ€ä¸ä¸»é¢˜çš„çŠ¶æ€ç›¸åè°ƒã€‚ çŠ¶æ€æ¨¡å¼ç­–ç•¥æ¨¡å¼è®¿é—®è€…æ¨¡å¼è§£é‡Šå™¨æ¨¡å¼]]></content>
      <categories>
        <category>è®¾è®¡æ¨¡å¼</category>
      </categories>
      <tags>
        <tag>è®¾è®¡æ¨¡å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018å­¦ä¹ è®¡åˆ’]]></title>
    <url>%2F2018%2F02%2F07%2F2018%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[å¤§æ•°æ®åŠæœºå™¨å­¦ä¹ å­¦ä¹ è®¡åˆ’ ç¼–ç¨‹åŸºç¡€ï¼šPythonhttps://cn.udacity.com/course/programming-foundations-with-python--ud036 è®¡ç®—æœºç§‘å­¦å¯¼è®º 72å°æ—¶https://cn.udacity.com/course/intro-to-computer-science--cs101 æ¨è®ºç»Ÿè®¡å­¦ 48å°æ—¶https://cn.udacity.com/course/intro-to-inferential-statistics--ud201 æè¿°ç»Ÿè®¡å­¦ 48å°æ—¶https://cn.udacity.com/course/intro-to-inferential-statistics--ud201 æœºå™¨å­¦ä¹  240å°æ—¶https://cn.udacity.com/course/machine-learning--ud262 ç»Ÿè®¡å­¦å…¥é—¨https://cn.udacity.com/course/intro-to-statistics--st101 åŸºç¡€çº¿æ€§ä»£æ•°https://cn.udacity.com/course/linear-algebra-refresher-course--ud953 æœºå™¨å­¦ä¹ https://cn.udacity.com/course/machine-learning-engineer-nanodegree--nd009 Apache Storm è¿›è¡Œå®æ—¶åˆ†æ 48å°æ—¶https://cn.udacity.com/course/real-time-analytics-with-apache-storm--ud381 Bashè„šæœ¬ 40+å°æ—¶]]></content>
      <categories>
        <category>å­¦ä¹ è®¡åˆ’</category>
      </categories>
      <tags>
        <tag>å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -æ”¯æŒå‘é‡æœº]]></title>
    <url>%2F2018%2F02%2F05%2FMathematical%20Support%20Vector%20Machine%2F</url>
    <content type="text"><![CDATA[æ”¯æŒå‘é‡æœºï¼ˆSupport Vector Machineï¼ŒSVMï¼‰çš„åŸºæœ¬æ¦‚å¿µï¼šç‚¹åˆ°è¶…å¹³é¢çš„è·ç¦»åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¸ºäº†è·å–ç¨³å¥çš„çº¿æ€§åˆ†ç±»å™¨ï¼Œä¸€ä¸ªå¾ˆè‡ªç„¶çš„æƒ³æ³•æ˜¯ï¼Œæ‰¾å‡ºä¸€æ¡åˆ†å‰²çº¿ä½¿å¾—ä¸¤ä¾§æ ·æœ¬ä¸è¯¥åˆ†å‰²çº¿çš„å¹³å‡è·ç¦»è¶³å¤Ÿçš„è¿œã€‚åœ¨æ¬§å¼ç©ºé—´ä¸­ï¼Œå®šä¹‰ä¸€ä¸ªç‚¹ğ’™åˆ°ç›´çº¿ï¼ˆæˆ–è€…é«˜ç»´ç©ºé—´ä¸­çš„è¶…å¹³é¢ï¼‰ğ’˜^ğ‘‡ ğ’™+ğ‘=0çš„è·ç¦»å…¬å¼æ˜¯ï¼š ğ‘Ÿ(ğ‘¥)= (|ğ’˜^ğ‘‡ ğ’™+ğ‘|)/(||ğ’˜||)åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¦‚æœè¿™æ ·çš„åˆ†å‰²çº¿æˆ–è€…åˆ†å‰²å¹³é¢èƒ½å¤Ÿå‡†ç¡®åœ°å°†æ ·æœ¬åˆ†å¼€ï¼Œå¯¹äºæ ·æœ¬{ğ’™ğ‘–,ğ‘¦ğ‘–}âˆˆğ·, ğ‘¦ğ‘–=Â±1 è€Œè¨€ï¼Œè‹¥ğ‘¦ğ‘–=1ï¼Œåˆ™æœ‰ğ’˜^ğ‘‡ ğ’™ğ’Š+ğ‘â‰¥1ï¼Œåä¹‹è‹¥ğ‘¦ğ‘–=-1ï¼Œåˆ™æœ‰ğ’˜^ğ‘‡ ğ’™_ğ’Š+ğ‘â‰¤âˆ’1.]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®¾è®¡æ¨¡å¼-å·¥å‚æ¨¡å¼]]></title>
    <url>%2F2018%2F01%2F27%2FDesign%20pattern%20factory%2F</url>
    <content type="text"><![CDATA[åˆ›å»ºå‡ ä¸ªå¥—çš®è‚¤ï¼Œæ‰€æœ‰çš„UIæ§ä»¶ å¦‚æŒ‰é’®ï¼Œæ»šåŠ¨æ¡ï¼Œçª—å£ éƒ½è¦åˆ›å»ºå‡ºæ¥ã€‚ç°åœ¨éœ€è¦çº¢è‰²ä¸»é¢˜ï¼Œé»‘è‰²ä¸»é¢˜ï¼Œå’Œè“è‰²ä¸»é¢˜3å¥—çš®è‚¤ã€‚ æ¥å£ç±»ï¼š1234567891011121314public interface Button &#123; public void display();&#125;public interface ScrollBar &#123; public void display();&#125;public interface Window &#123; public void display();&#125;public interface SkinFactory &#123; public ScrollBar createScrollBar(); public Button createButton(); public Window createWindow();&#125; çº¢è‰²çš®è‚¤å·¥å‚123456789101112131415161718192021222324252627282930public class RedSkinFactory implements SkinFactory &#123; public ScrollBar createScrollBar() &#123; return new RedScrollBar(); &#125; public Button createButton() &#123; return new RedButton(); &#125; public Window createWindow() &#123; return new RedWindow(); &#125;&#125;public class RedScrollBar implements ScrollBar &#123; public void display() &#123; System.out.println("åˆ›å»ºçº¢è‰²æ»šåŠ¨æ¡ã€‚"); &#125;&#125;public class RedButton implements Button &#123; public void display() &#123; System.out.println("åˆ›å»ºçº¢è‰²æŒ‰é’®"); &#125;&#125;public class RedWindow implements Window &#123; public void display() &#123; System.out.println("åˆ›å»ºçº¢è‰²çª—å£ã€‚"); &#125;&#125; å®ç°ç±»123456789101112131415161718public class SkinClient &#123; public static void main(String[] args) &#123; SkinFactory BlackSkinFactory = new BlackSkinFactory(); BlackSkinFactory.createButton().display(); BlackSkinFactory.createScrollBar().display(); BlackSkinFactory.createWindow().display(); SkinFactory RedSkinFactory = new RedSkinFactory(); RedSkinFactory.createButton().display(); RedSkinFactory.createScrollBar().display(); RedSkinFactory.createWindow().display(); SkinFactory BlueSkinFactory = new BlueSkinFactory(); BlueSkinFactory.createButton().display(); BlueSkinFactory.createScrollBar().display(); BlueSkinFactory.createWindow().display(); &#125;&#125; shè¾“å‡ºç»“æœ123456789åˆ›å»ºé»‘è‰²æŒ‰é’®åˆ›å»ºé»‘è‰²æ»šåŠ¨æ¡ã€‚åˆ›å»ºé»‘è‰²çª—å£ã€‚åˆ›å»ºçº¢è‰²æŒ‰é’®åˆ›å»ºçº¢è‰²æ»šåŠ¨æ¡ã€‚åˆ›å»ºçº¢è‰²çª—å£ã€‚åˆ›å»ºè“è‰²æŒ‰é’®åˆ›å»ºè“è‰²æ»šåŠ¨æ¡ã€‚åˆ›å»ºè“è‰²çª—å£ã€‚ å…¶ä»–é¢œè‰²åŒä¸Šä»£ç ç»“æ„æˆªå›¾ï¼šç»“æœæˆªå›¾ï¼š]]></content>
      <categories>
        <category>è®¾è®¡æ¨¡å¼</category>
      </categories>
      <tags>
        <tag>è®¾è®¡æ¨¡å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®¾è®¡æ¨¡å¼ç›®å½•]]></title>
    <url>%2F2018%2F01%2F27%2FDesign%20pattern%20catalog%2F</url>
    <content type="text"><![CDATA[åˆ›å»ºæ€§æ¨¡å¼ï¼š1.ç±»çš„åˆ›å»ºæ¨¡å¼â€”â€”ä½¿ç”¨ç»§æ‰¿å…³ç³»ï¼ŒæŠŠç±»çš„åˆ›å»ºå»¶è¿Ÿåˆ°å­ç±»2.å¯¹è±¡çš„åˆ›å»ºæ¨¡å¼â€”â€”æŠŠå¯¹è±¡çš„åˆ›å»ºè¿‡ç¨‹åŠ¨æ€åœ°å§”æ´¾ç»™å¦ä¸€ä¸ªå¯¹è±¡ å°è£…è¦åˆ›å»ºçš„å…·ä½“ç±»ï¼ˆç±»çš„å®ä¾‹ï¼‰çš„ä¿¡æ¯ éšè—è¿™äº›ç±»ï¼ˆç±»çš„å®ä¾‹ï¼‰è¢«åˆ›å»ºå’Œç»„åˆçš„è¿‡ç¨‹ åŒ…å«æŠ½è±¡å·¥å‚ã€å»ºé€ è€…ã€å·¥å‚æ–¹å¼ã€åŸå‹ã€å•ä¾‹ ç»“æ„æ€§æ¨¡å¼ï¼šè€ƒè™‘å¦‚ä½•ç»„åˆç±»å’Œå¯¹è±¡æ„æˆè¾ƒå¤§çš„ç»“æ„ã€‚1.ç»“æ„æ€§ç±»æ¨¡å¼ï¼šä½¿ç”¨ç»§æ‰¿æ¥ç»„åˆæ¥å£æˆ–å®ç°2.ç»“æ„æ€§å¯¹è±¡æ¨¡å¼ï¼šå¯¹è±¡åˆæˆå®ç°æ–°åŠŸèƒ½ã€‚åŒ…å«ï¼šé€‚é…å™¨ã€æ¡¥æ¥ã€ç»„åˆã€è£…é¥°ç€ã€å¤–è§‚ã€è½»é‡ã€ä»£ç† è¡Œä¸ºæ¨¡å¼ï¼šä¸»è¦è§£å†³ç®—æ³•å’Œå¯¹è±¡ä¹‹é—´çš„è´£ä»»åˆ†é…é—®é¢˜ã€‚å¯¹è±¡æˆ–ç±»çš„æ¨¡å¼å®ƒä»¬ä¹‹é—´çš„é€šä¿¡æ¨¡å¼ã€‚åŒ…å«ï¼šè´£ä»»é“¾ã€å‘½ä»¤ã€è§£é‡Šå™¨ã€è¿­ä»£ã€ä¸­ä»‹è€…ã€å¤‡å¿˜å½•ã€è§‚å¯Ÿè€…ã€çŠ¶æ€ã€ç­–ç•¥ã€æ¨¡æ¿æ–¹æ³•ã€è§‚å¯Ÿè€… å·¥å‚æ–¹æ³•ä¸»è¦é’ˆå¯¹ä¸€ä¸ªäº§å“ç­‰çº§ç»“æ„æŠ½è±¡å·¥å‚æ¨¡å¼éœ€è¦é¢å¯¹å¤šä¸ªäº§å“ç­‰çº§ç»“æ„]]></content>
      <categories>
        <category>è®¾è®¡æ¨¡å¼</category>
      </categories>
      <tags>
        <tag>è®¾è®¡æ¨¡å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kNNå®ç°æ‰‹å†™æ•°å­—è¯†åˆ«]]></title>
    <url>%2F2018%2F01%2F25%2FkNN%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[éœ€æ±‚åˆ©ç”¨ä¸€ä¸ªæ‰‹å†™æ•°å­—â€œå…ˆéªŒæ•°æ®â€é›†ï¼Œä½¿ç”¨knnç®—æ³•æ¥å®ç°å¯¹æ‰‹å†™æ•°å­—çš„è‡ªåŠ¨è¯†åˆ«ï¼›å…ˆéªŒæ•°æ®ï¼ˆè®­ç»ƒæ•°æ®ï¼‰é›†ï¼š æ•°æ®ç»´åº¦æ¯”è¾ƒå¤§ï¼Œæ ·æœ¬æ•°æ¯”è¾ƒå¤šã€‚ æ•°æ®é›†åŒ…æ‹¬æ•°å­—0-9çš„æ‰‹å†™ä½“ã€‚ æ¯ä¸ªæ•°å­—å¤§çº¦æœ‰200ä¸ªæ ·æœ¬ã€‚ æ¯ä¸ªæ ·æœ¬ä¿æŒåœ¨ä¸€ä¸ªtxtæ–‡ä»¶ä¸­ã€‚ æ‰‹å†™ä½“å›¾åƒæœ¬èº«çš„å¤§å°æ˜¯32x32çš„äºŒå€¼å›¾ï¼Œè½¬æ¢åˆ°txtæ–‡ä»¶ä¿å­˜åï¼Œå†…å®¹ä¹Ÿæ˜¯32x32ä¸ªæ•°å­—ï¼Œ0æˆ–è€…1ï¼Œå¦‚ä¸‹ï¼š é¦–å…ˆå‡†å¤‡æµ‹è¯•æ–‡ä»¶:1934ä¸ªè®­ç»ƒæ•°æ®946ä¸ªæµ‹è¯•æ•°æ® åˆ†æï¼š1ã€æ‰‹å†™ä½“å› ä¸ºæ¯ä¸ªäººï¼Œç”šè‡³æ¯æ¬¡å†™çš„å­—éƒ½ä¸ä¼šå®Œå…¨ç²¾ç¡®ä¸€è‡´ï¼Œæ‰€ä»¥ï¼Œè¯†åˆ«æ‰‹å†™ä½“çš„å…³é”®æ˜¯â€œç›¸ä¼¼åº¦â€2ã€æ—¢ç„¶æ˜¯è¦æ±‚æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œé‚£ä¹ˆï¼Œé¦–å…ˆéœ€è¦å°†æ ·æœ¬è¿›è¡ŒæŠ½è±¡ï¼Œå°†æ¯ä¸ªæ ·æœ¬å˜æˆä¸€ç³»åˆ—ç‰¹å¾æ•°æ®ï¼ˆå³ç‰¹å¾å‘é‡ï¼‰3ã€æ‰‹å†™ä½“åœ¨ç›´è§‚ä¸Šå°±æ˜¯ä¸€ä¸ªä¸ªçš„å›¾ç‰‡ï¼Œè€Œå›¾ç‰‡æ˜¯ç”±ä¸Šè¿°å›¾ç¤ºä¸­çš„åƒç´ ç‚¹æ¥æè¿°çš„ï¼Œæ ·æœ¬çš„ç›¸ä¼¼åº¦å…¶å®å°±æ˜¯åƒç´ çš„ä½ç½®å’Œé¢œè‰²ä¹‹é—´çš„ç»„åˆçš„ç›¸ä¼¼åº¦4ã€å› æ­¤ï¼Œå°†å›¾ç‰‡çš„åƒç´ æŒ‰ç…§å›ºå®šé¡ºåºè¯»å–åˆ°ä¸€ä¸ªä¸ªçš„å‘é‡ä¸­ï¼Œå³å¯å¾ˆå¥½åœ°è¡¨ç¤ºæ‰‹å†™ä½“æ ·æœ¬5ã€æŠ½è±¡å‡ºäº†æ ·æœ¬å‘é‡ï¼ŒåŠç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå³å¯åº”ç”¨KNNæ¥å®ç° ä»£ç ï¼š1) ä¸€ä¸ªç”¨æ¥ç”Ÿæˆå°†æ¯ä¸ªæ ·æœ¬çš„txtæ–‡ä»¶è½¬æ¢ä¸ºå¯¹åº”çš„ä¸€ä¸ªå‘é‡ï¼Œ2) ä¸€ä¸ªç”¨æ¥åŠ è½½æ•´ä¸ªæ•°æ®é›†ï¼Œ3) ä¸€ä¸ªå®ç°kNNåˆ†ç±»ç®—æ³•ã€‚4) æœ€åå°±æ˜¯å®ç°åŠ è½½ã€æµ‹è¯•çš„å‡½æ•°ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125########################################## kNN: k Nearest Neighbors# å‚æ•°: inX: vector to compare to existing dataset (1xN)# dataSet: size m data set of known vectors (NxM)# labels: data set labels (1xM vector)# k: number of neighbors to use for comparison # è¾“å‡º: å¤šæ•°ç±»#########################################from numpy import *import operatorimport os# KNNåˆ†ç±»æ ¸å¿ƒæ–¹æ³•def kNNClassify(newInput, dataSet, labels, k): numSamples = dataSet.shape[0] # shape[0]ä»£è¡¨è¡Œæ•° ## step 1: è®¡ç®—æ¬§å¼è·ç¦» # tile(A, reps): å°†Aé‡å¤repsæ¬¡æ¥æ„é€ ä¸€ä¸ªçŸ©é˜µ # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) - dataSet # Subtract element-wise squaredDiff = diff ** 2 # squared for the subtract squaredDist = sum(squaredDiff, axis = 1) # sum is performed by row distance = squaredDist ** 0.5 ## step 2: å¯¹è·ç¦»æ’åº # argsort()è¿”å›æ’åºåçš„ç´¢å¼• sortedDistIndices = argsort(distance) classCount = &#123;&#125; # å®šä¹‰ä¸€ä¸ªç©ºçš„å­—å…¸ for i in xrange(k): ## step 3: é€‰æ‹©kä¸ªæœ€å°è·ç¦» voteLabel = labels[sortedDistIndices[i]] ## step 4: è®¡ç®—ç±»åˆ«çš„å‡ºç°æ¬¡æ•° # when the key voteLabel is not in dictionary classCount, get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ## step 5: è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«ä½œä¸ºåˆ†ç±»ç»“æœ maxCount = 0 for key, value in classCount.items(): if value &gt; maxCount: maxCount = value maxIndex = key return maxIndex # å°†å›¾ç‰‡è½¬æ¢ä¸ºå‘é‡def img2vector(filename): rows = 32 cols = 32 imgVector = zeros((1, rows * cols)) fileIn = open(filename) for row in xrange(rows): lineStr = fileIn.readline() for col in xrange(cols): imgVector[0, row * 32 + col] = int(lineStr[col]) return imgVector# åŠ è½½æ•°æ®é›†def loadDataSet(): ## step 1: è¯»å–è®­ç»ƒæ•°æ®é›† print "---Getting training set..." dataSetDir = 'E:/Python/ml/knn/' trainingFileList = os.listdir(dataSetDir + 'trainingDigits') # åŠ è½½æµ‹è¯•æ•°æ® numSamples = len(trainingFileList) train_x = zeros((numSamples, 1024)) train_y = [] for i in xrange(numSamples): filename = trainingFileList[i] # get train_x train_x[i, :] = img2vector(dataSetDir + 'trainingDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 train_y.append(label) ## step 2:è¯»å–æµ‹è¯•æ•°æ®é›† print "---Getting testing set..." testingFileList = os.listdir(dataSetDir + 'testDigits') # load the testing set numSamples = len(testingFileList) test_x = zeros((numSamples, 1024)) test_y = [] for i in xrange(numSamples): filename = testingFileList[i] # get train_x test_x[i, :] = img2vector(dataSetDir + 'testDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 test_y.append(label) return train_x, train_y, test_x, test_y# æ‰‹å†™è¯†åˆ«ä¸»æµç¨‹def testHandWritingClass(): ## step 1: åŠ è½½æ•°æ® print "step 1: load data..." train_x, train_y, test_x, test_y = loadDataSet() ## step 2: æ¨¡å‹è®­ç»ƒ. print "step 2: training..." pass ## step 3: æµ‹è¯• print "step 3: testing..." numTestSamples = test_x.shape[0] matchCount = 0 for i in xrange(numTestSamples): predict = kNNClassify(test_x[i], train_x, train_y, 3) if predict == test_y[i]: matchCount += 1 accuracy = float(matchCount) / numTestSamples ## step 4: è¾“å‡ºç»“æœ print "step 4: show the result..." print 'The classify accuracy is: %.2f%%' % (accuracy * 100) æµ‹è¯•12import kNNkNN.testHandWritingClass() æ‰§è¡Œç»“æœï¼š1234567tep 1: load data...---Getting training set...---Getting testing set...step 2: training...step 3: testing...step 4: show the result...The classify accuracy is: 98.84%]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ ä¹‹KNNç®—æ³•æ¨æ¼”]]></title>
    <url>%2F2018%2F01%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN%E7%AE%97%E6%B3%95%E6%8E%A8%E6%BC%94%2F</url>
    <content type="text"><![CDATA[ä»è®­ç»ƒé›†ä¸­æ‰¾åˆ°å’Œæ–°æ•°æ®æœ€æ¥è¿‘çš„kæ¡è®°å½•ï¼Œç„¶åæ ¹æ®å¤šæ•°ç±»æ¥å†³å®šæ–°æ•°æ®ç±»åˆ«ã€‚ç®—æ³•æ¶‰åŠ3ä¸ªä¸»è¦å› ç´ ï¼š1) è®­ç»ƒæ•°æ®é›†2) è·ç¦»æˆ–ç›¸ä¼¼åº¦çš„è®¡ç®—è¡¡é‡3) kçš„å¤§å°ç»¿è‰²åœ†è¦è¢«å†³å®šèµ‹äºˆå“ªä¸ªç±»ï¼Œæ˜¯çº¢è‰²ä¸‰è§’å½¢è¿˜æ˜¯è“è‰²å››æ–¹å½¢ï¼Ÿå¦‚æœK=3ï¼Œç”±äºçº¢è‰²ä¸‰è§’å½¢æ‰€å æ¯”ä¾‹ä¸º2/3ï¼Œç»¿è‰²åœ†å°†è¢«èµ‹äºˆçº¢è‰²ä¸‰è§’å½¢é‚£ä¸ªç±»ï¼Œå¦‚æœK=5ï¼Œç”±äºè“è‰²å››æ–¹å½¢æ¯”ä¾‹ä¸º3/5ï¼Œå› æ­¤ç»¿è‰²åœ†è¢«èµ‹äºˆè“è‰²å››æ–¹å½¢ç±»ã€‚ KNNåˆ†ç±»ç®—æ³•Pythonå®æˆ˜æœ‰ä»¥ä¸‹å…ˆéªŒæ•°æ®ï¼Œä½¿ç”¨knnç®—æ³•å¯¹æœªçŸ¥ç±»åˆ«æ•°æ®åˆ†ç±» xè½´ yè½´ ç±»å‹ 1.0 0.9 A 1.0 1.0 A 0.0 0.1 B 0.1 0.2 B æœªçŸ¥ç±»åˆ«æ•°æ® xè½´ yè½´ ç±»å‹ 1.2 1.0 ï¼Ÿ 0.1 0.3 ï¼Ÿ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960########################################## kNN: k Nearest Neighbors# è¾“å…¥: newInput: (1xN)çš„å¾…åˆ†ç±»å‘é‡# dataSet: (NxM)çš„è®­ç»ƒæ•°æ®é›†# labels: è®­ç»ƒæ•°æ®é›†çš„ç±»åˆ«æ ‡ç­¾å‘é‡# k: è¿‘é‚»æ•° # è¾“å‡º: å¯èƒ½æ€§æœ€å¤§çš„åˆ†ç±»æ ‡ç­¾#########################################from numpy import *import operator#åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ï¼ŒåŒ…å«2ä¸ªç±»åˆ«å…±4ä¸ªæ ·æœ¬def createDataSet(): # ç”Ÿæˆä¸€ä¸ªçŸ©é˜µï¼Œæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ group = array([[1.0, 0.9], [1.0, 1.0], [0.1, 0.2], [0.0, 0.1]]) # 4ä¸ªæ ·æœ¬åˆ†åˆ«æ‰€å±çš„ç±»åˆ« labels = ['A', 'A', 'B', 'B'] return group, labels# KNNåˆ†ç±»ç®—æ³•å‡½æ•°å®šä¹‰def kNNClassify(newInput, dataSet, labels, k): numSamples = dataSet.shape[0] # shape[0]è¡¨ç¤ºè¡Œæ•° ## step 1: è®¡ç®—è·ç¦» # tile(A, reps): æ„é€ ä¸€ä¸ªçŸ©é˜µï¼Œé€šè¿‡Aé‡å¤repsæ¬¡å¾—åˆ°(tile(A, ï¼ˆrepsXï¼ŒrepsYï¼‰)æ­¤å¤„åœ¨è¡Œçš„æ–¹å‘é‡å¤repsXæ¬¡ï¼Œåœ¨Yçš„æ–¹å‘é‡å¤repsYæ¬¡) # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) - dataSet # æŒ‰å…ƒç´ æ±‚å·®å€¼ squaredDiff = diff ** 2 #å°†å·®å€¼å¹³æ–¹ squaredDist = sum(squaredDiff, axis = 1) # æŒ‰è¡Œç´¯åŠ  distance = squaredDist ** 0.5 #å°†å·®å€¼å¹³æ–¹å’Œæ±‚å¼€æ–¹ï¼Œå³å¾—è·ç¦» ## step 2: å¯¹è·ç¦»æ’åº ## æ­¤å¤„æ’åºéœ€è¦æ³¨æ„æ ·æœ¬æ ‡ç­¾çš„é¡ºåºã€‚æ’åºåå­˜çš„æ˜¯è§’æ ‡å·ã€‚åç»­ç›´æ¥ä»æ’åºåçš„æ ‡ç­¾åºåˆ—æ‰¾ä¸‹æ ‡æ˜¯æ’åºå†…å®¹çš„æ•°æ® # argsort() è¿”å›æ’åºåçš„ç´¢å¼•å€¼ sortedDistIndices = argsort(distance) #æˆ–distance.argsort() classCount = &#123;&#125; # define a dictionary (can be append element) for i in xrange(k): ## step 3: é€‰æ‹©kä¸ªæœ€è¿‘é‚» voteLabel = labels[sortedDistIndices[i]] ## step 4: è®¡ç®—kä¸ªæœ€è¿‘é‚»ä¸­å„ç±»åˆ«å‡ºç°çš„æ¬¡æ•° # when the key voteLabel is not in dictionary classCount, get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ## step 5: è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«æ ‡ç­¾ maxCount = 0 for key, value in classCount.items(): if value &gt; maxCount: maxCount = value maxIndex = key return maxIndex ##step 5 å¯ä»¥æ”¹æˆï¼š # sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True) #return sortedClassCount[0][0] ç„¶åè°ƒç”¨ç®—æ³•è¿›è¡Œæµ‹è¯•ï¼š1234567891011121314import kNNfrom numpy import * #ç”Ÿæˆæ•°æ®é›†å’Œç±»åˆ«æ ‡ç­¾dataSet, labels = kNN.createDataSet()#å®šä¹‰ä¸€ä¸ªæœªçŸ¥ç±»åˆ«çš„æ•°æ®testX = array([1.2, 1.0])k = 3#è°ƒç”¨åˆ†ç±»å‡½æ•°å¯¹æœªçŸ¥æ•°æ®åˆ†ç±»outputLabel = kNN.kNNClassify(testX, dataSet, labels, 3)print "Your input is:", testX, "and classified to class: ", outputLabeltestX = array([0.1, 0.3])outputLabel = kNN.kNNClassify(testX, dataSet, labels, 3)print "Your input is:", testX, "and classified to class: ", outputLabel è¿™æ—¶å€™ä¼šè¾“å‡º12Your input is: [ 1.2 1.0] and classified to class: AYour input is: [ 0.1 0.3] and classified to class: B]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -æ¦‚å¿µ]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[ç›‘ç£å­¦ä¹ ï¼šæˆ‘ä»¬å‘ç³»ç»Ÿè¾“å…¥æˆ‘ä»¬æ‰€ç§°çš„â€œå·²æ ‡è®°æ ·æœ¬â€å·²æ ‡è®°æ ·æœ¬æŒ‡æˆ‘ä»¬ç»™ç³»ç»Ÿä¸€äº›ä¿¡æ¯ï¼Œè®©å®ƒç”¨äºç†è§£æˆ‘ä»¬å‘å®ƒè¾“å…¥çš„æ•°æ®ã€‚ä¾‹å¦‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç³»ç»Ÿæ¥è¯†åˆ«æˆ‘çš„è„¸ï¼Œæˆ‘ä»¬å°†ç»™ä»–ä¸€äº›æˆ‘çš„é¢éƒ¨ç…§ç‰‡ï¼Œä¸€äº›åˆ«äººçš„é¢éƒ¨ç…§ç‰‡ï¼Œæˆ‘ä»¬ä¼šå‘Šè¯‰ç³»ç»Ÿ è¿™äº›ç…§ç‰‡æ˜¯Davidçš„ï¼Œé‚£äº›ç…§ç‰‡æ˜¯åˆ«äººçš„ã€‚éšæ—¶é—´æ¨ç§»ï¼Œå®ƒä¼šåˆ©ç”¨è¿™äº›ä¿¡æ¯æå‡å®ƒçš„ç†è§£ç¨‹åº¦ã€‚â€ç›‘ç£â€œè¡¨ç¤ºä½ æœ‰å¾ˆå¤šæ ·æœ¬ï¼Œä½ äº†è§£è¿™äº›æ ·æœ¬çš„æ­£ç¡®ç­”æ¡ˆ éç›‘ç£å­¦ä¹ æ¯”å¦‚åœ¨åŒ»ç–—ä¿å¥æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å·²ç»äº†è§£æŸç§ç–¾ç—…ï¼Œç›‘ç£å­¦ä¹ å¯ç”¨ä»¥å¸®åŠ©ç³»ç»Ÿï¼Œè¯†åˆ«å“ªäº›äººæœ‰æ‚£è¯¥ç—…çš„é£é™©ï¼Œè€Œéç›‘ç£å­¦ä¹ å®é™…ä¸Šå¯ä»¥æ ¹æ®å¸¸è§ç—‡çŠ¶çš„æ¨¡å¼å¸®åŠ©æˆ‘ä»¬å‘ç°ç”šè‡³æˆ‘ä»¬è¿˜ä¸çŸ¥é“çš„ç–¾ç—…ã€‚ èšç±»ï¼šK-MEANS æœ´ç´ è´å¶æ–¯ï¼šæ¨å¯¼è¿‡ç¨‹å’Œç”¨æ³•ï¼šhttp://scikit-learn.org/stable/modules/naive_bayes.html è¿˜æœ‰é«˜æ–¯æœ´ç´ è´å¶æ–¯ pythonæœ‰ä¸€ä¸ªåº“å«sklearn]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ -é¢„æµ‹æ³°å¦å°¼å…‹å·ä¹˜å®¢ç”Ÿè¿˜ç‡]]></title>
    <url>%2F2018%2F01%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E8%BF%98%E7%8E%87%2F</url>
    <content type="text"><![CDATA[æœºå™¨å­¦ä¹ é¡¹ç›® 0: é¢„æµ‹æ³°å¦å°¼å…‹å·ä¹˜å®¢ç”Ÿè¿˜ç‡1912å¹´ï¼Œæ³°å¦å°¼å…‹å·åœ¨ç¬¬ä¸€æ¬¡èˆªè¡Œä¸­å°±ä¸å†°å±±ç›¸æ’æ²‰æ²¡ï¼Œå¯¼è‡´äº†å¤§éƒ¨åˆ†ä¹˜å®¢å’Œèˆ¹å‘˜èº«äº¡ã€‚åœ¨è¿™ä¸ªå…¥é—¨é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢éƒ¨åˆ†æ³°å¦å°¼å…‹å·æ—…å®¢åå•ï¼Œæ¥ç¡®å®šå“ªäº›ç‰¹å¾å¯ä»¥æœ€å¥½åœ°é¢„æµ‹ä¸€ä¸ªäººæ˜¯å¦ä¼šç”Ÿè¿˜ã€‚ä¸ºäº†å®Œæˆè¿™ä¸ªé¡¹ç›®ï¼Œä½ å°†éœ€è¦å®ç°å‡ ä¸ªåŸºäºæ¡ä»¶çš„é¢„æµ‹å¹¶å›ç­”ä¸‹é¢çš„é—®é¢˜ã€‚æˆ‘ä»¬å°†æ ¹æ®ä»£ç çš„å®Œæˆåº¦å’Œå¯¹é—®é¢˜çš„è§£ç­”æ¥å¯¹ä½ æäº¤çš„é¡¹ç›®çš„è¿›è¡Œè¯„ä¼°ã€‚ å¼€å§‹å½“æˆ‘ä»¬å¼€å§‹å¤„ç†æ³°å¦å°¼å…‹å·ä¹˜å®¢æ•°æ®æ—¶ï¼Œä¼šå…ˆå¯¼å…¥æˆ‘ä»¬éœ€è¦çš„åŠŸèƒ½æ¨¡å—ä»¥åŠå°†æ•°æ®åŠ è½½åˆ° pandas DataFrameã€‚è¿è¡Œä¸‹é¢åŒºåŸŸä¸­çš„ä»£ç åŠ è½½æ•°æ®ï¼Œå¹¶ä½¿ç”¨ .head() å‡½æ•°æ˜¾ç¤ºå‰å‡ é¡¹ä¹˜å®¢æ•°æ®ã€‚ 1234567891011121314151617import numpy as npimport pandas as pd# RMS Titanic data visualization code # æ•°æ®å¯è§†åŒ–ä»£ç from titanic_visualizations import survival_statsfrom IPython.display import display%matplotlib inline# Load the dataset # åŠ è½½æ•°æ®é›†in_file = 'titanic_data.csv'full_data = pd.read_csv(in_file)# Print the first few entries of the RMS Titanic data # æ˜¾ç¤ºæ•°æ®åˆ—è¡¨ä¸­çš„å‰å‡ é¡¹ä¹˜å®¢æ•°æ®display(full_data.head()) åºå· PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Thâ€¦ female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S ä»æ³°å¦å°¼å…‹å·çš„æ•°æ®æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°èˆ¹ä¸Šæ¯ä½æ—…å®¢çš„ç‰¹å¾ Survivedï¼šæ˜¯å¦å­˜æ´»ï¼ˆ0ä»£è¡¨å¦ï¼Œ1ä»£è¡¨æ˜¯ï¼‰ Pclassï¼šç¤¾ä¼šé˜¶çº§ï¼ˆ1ä»£è¡¨ä¸Šå±‚é˜¶çº§ï¼Œ2ä»£è¡¨ä¸­å±‚é˜¶çº§ï¼Œ3ä»£è¡¨åº•å±‚é˜¶çº§ï¼‰ Nameï¼šèˆ¹ä¸Šä¹˜å®¢çš„åå­— Sexï¼šèˆ¹ä¸Šä¹˜å®¢çš„æ€§åˆ« Age:èˆ¹ä¸Šä¹˜å®¢çš„å¹´é¾„ï¼ˆå¯èƒ½å­˜åœ¨ NaNï¼‰ SibSpï¼šä¹˜å®¢åœ¨èˆ¹ä¸Šçš„å…„å¼Ÿå§å¦¹å’Œé…å¶çš„æ•°é‡ Parchï¼šä¹˜å®¢åœ¨èˆ¹ä¸Šçš„çˆ¶æ¯ä»¥åŠå°å­©çš„æ•°é‡ Ticketï¼šä¹˜å®¢èˆ¹ç¥¨çš„ç¼–å· Fareï¼šä¹˜å®¢ä¸ºèˆ¹ç¥¨æ”¯ä»˜çš„è´¹ç”¨ Cabinï¼šä¹˜å®¢æ‰€åœ¨èˆ¹èˆ±çš„ç¼–å·ï¼ˆå¯èƒ½å­˜åœ¨ NaNï¼‰ Embarkedï¼šä¹˜å®¢ä¸Šèˆ¹çš„æ¸¯å£ï¼ˆC ä»£è¡¨ä» Cherbourg ç™»èˆ¹ï¼ŒQ ä»£è¡¨ä» Queenstown ç™»èˆ¹ï¼ŒS ä»£è¡¨ä» Southampton ç™»èˆ¹ï¼‰ å› ä¸ºæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯æ¯ä¸ªä¹˜å®¢æˆ–èˆ¹å‘˜æ˜¯å¦åœ¨äº‹æ•…ä¸­æ´»äº†ä¸‹æ¥ã€‚å¯ä»¥å°† Survived è¿™ä¸€ç‰¹å¾ä»è¿™ä¸ªæ•°æ®é›†ç§»é™¤ï¼Œå¹¶ä¸”ç”¨ä¸€ä¸ªå•ç‹¬çš„å˜é‡ outcomes æ¥å­˜å‚¨ã€‚å®ƒä¹Ÿåšä¸ºæˆ‘ä»¬è¦é¢„æµ‹çš„ç›®æ ‡ã€‚ è¿è¡Œè¯¥ä»£ç ï¼Œä»æ•°æ®é›†ä¸­ç§»é™¤ Survived è¿™ä¸ªç‰¹å¾ï¼Œå¹¶å°†å®ƒå­˜å‚¨åœ¨å˜é‡ outcomes ä¸­ã€‚ 123456789101112# Store the 'Survived' feature in a new variable and remove it from the dataset # ä»æ•°æ®é›†ä¸­ç§»é™¤ 'Survived' è¿™ä¸ªç‰¹å¾ï¼Œå¹¶å°†å®ƒå­˜å‚¨åœ¨ä¸€ä¸ªæ–°çš„å˜é‡ä¸­ã€‚outcomes = full_data['Survived']data = full_data.drop('Survived', axis = 1)# Show the new dataset with 'Survived' removed# æ˜¾ç¤ºå·²ç§»é™¤ 'Survived' ç‰¹å¾çš„æ•°æ®é›†display(data.head())display(outcomes[1])display(data.loc[1]) åºå· PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 Cumings, Mrs. John Bradley (Florence Briggs Thâ€¦ female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 1 PassengerId 2 Pclass 1 Name Cumings, Mrs. John Bradley (Florence Briggs Th... Sex female Age 38 SibSp 1 Parch 0 Ticket PC 17599 Fare 71.2833 Cabin C85 Embarked C Name: 1, dtype: object è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•å°†æ³°å¦å°¼å…‹å·çš„ Survived æ•°æ®ä» DataFrame ç§»é™¤ã€‚æ³¨æ„åˆ° dataï¼ˆä¹˜å®¢æ•°æ®ï¼‰å’Œ outcomes ï¼ˆæ˜¯å¦å­˜æ´»ï¼‰ç°åœ¨å·²ç»åŒ¹é…å¥½ã€‚è¿™æ„å‘³ç€å¯¹äºä»»ä½•ä¹˜å®¢çš„ data.loc[i] éƒ½æœ‰å¯¹åº”çš„å­˜æ´»çš„ç»“æœ outcome[i]ã€‚ ä¸ºäº†éªŒè¯æˆ‘ä»¬é¢„æµ‹çš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ ‡å‡†æ¥ç»™æˆ‘ä»¬çš„é¢„æµ‹æ‰“åˆ†ã€‚å› ä¸ºæˆ‘ä»¬æœ€æ„Ÿå…´è¶£çš„æ˜¯æˆ‘ä»¬é¢„æµ‹çš„å‡†ç¡®ç‡ï¼Œæ—¢æ­£ç¡®é¢„æµ‹ä¹˜å®¢å­˜æ´»çš„æ¯”ä¾‹ã€‚è¿è¡Œä¸‹é¢çš„ä»£ç æ¥åˆ›å»ºæˆ‘ä»¬çš„ accuracy_score å‡½æ•°ä»¥å¯¹å‰äº”åä¹˜å®¢çš„é¢„æµ‹æ¥åšæµ‹è¯•ã€‚ æ€è€ƒé¢˜ï¼šä»ç¬¬å…­ä¸ªä¹˜å®¢ç®—èµ·ï¼Œå¦‚æœæˆ‘ä»¬é¢„æµ‹ä»–ä»¬å…¨éƒ¨éƒ½å­˜æ´»ï¼Œä½ è§‰å¾—æˆ‘ä»¬é¢„æµ‹çš„å‡†ç¡®ç‡æ˜¯å¤šå°‘ï¼Ÿ 123456789101112131415161718192021222324def accuracy_score(truth, pred): """ Returns accuracy score for input truth and predictions. """ # Ensure that the number of predictions matches number of outcomes # ç¡®ä¿é¢„æµ‹çš„æ•°é‡ä¸ç»“æœçš„æ•°é‡ä¸€è‡´ if len(truth) == len(pred): # print "kaish1"# print truth# print "kaish2"# print pred# print "end" # Calculate and return the accuracy as a percent # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ return "Predictions have an accuracy of &#123;:.2f&#125;%.".format((truth == pred).mean()*100) else: return "Number of predictions does not match number of outcomes!" # Test the 'accuracy_score' function# æµ‹è¯• 'accuracy_score' å‡½æ•°# print np.ones(5, dtype = int)# print outcomes[:5]predictions = pd.Series(np.ones(5, dtype = int))# print accuracy_score(outcomes[:5], predictions) æç¤ºï¼šå¦‚æœä½ ä¿å­˜ iPython Notebookï¼Œä»£ç è¿è¡Œçš„è¾“å‡ºä¹Ÿå°†è¢«ä¿å­˜ã€‚ä½†æ˜¯ï¼Œä¸€æ—¦ä½ é‡æ–°æ‰“å¼€é¡¹ç›®ï¼Œä½ çš„å·¥ä½œåŒºå°†ä¼šè¢«é‡ç½®ã€‚è¯·ç¡®ä¿æ¯æ¬¡éƒ½ä»ä¸Šæ¬¡ç¦»å¼€çš„åœ°æ–¹è¿è¡Œä»£ç æ¥é‡æ–°ç”Ÿæˆå˜é‡å’Œå‡½æ•°ã€‚ é¢„æµ‹å¦‚æœæˆ‘ä»¬è¦é¢„æµ‹æ³°å¦å°¼å…‹å·ä¸Šçš„ä¹˜å®¢æ˜¯å¦å­˜æ´»ï¼Œä½†æ˜¯æˆ‘ä»¬åˆå¯¹ä»–ä»¬ä¸€æ— æ‰€çŸ¥ï¼Œé‚£ä¹ˆæœ€å¥½çš„é¢„æµ‹å°±æ˜¯èˆ¹ä¸Šçš„äººæ— ä¸€å¹¸å…ã€‚è¿™æ˜¯å› ä¸ºï¼Œæˆ‘ä»¬å¯ä»¥å‡å®šå½“èˆ¹æ²‰æ²¡çš„æ—¶å€™å¤§å¤šæ•°ä¹˜å®¢éƒ½é‡éš¾äº†ã€‚ä¸‹é¢çš„ predictions_0 å‡½æ•°å°±é¢„æµ‹èˆ¹ä¸Šçš„ä¹˜å®¢å…¨éƒ¨é‡éš¾ã€‚ 1234567891011121314151617def predictions_0(data): """ Model with no features. Always predicts a passenger did not survive. """ predictions = [] for _, passenger in data.iterrows(): # Predict the survival of 'passenger' # é¢„æµ‹ 'passenger' çš„ç”Ÿè¿˜ç‡ predictions.append(0) # Return our predictions # è¿”å›é¢„æµ‹ç»“æœ return pd.Series(predictions)# Make the predictions# è¿›è¡Œé¢„æµ‹predictions = predictions_0(data) é—®é¢˜1å¯¹æ¯”çœŸå®çš„æ³°å¦å°¼å…‹å·çš„æ•°æ®ï¼Œå¦‚æœæˆ‘ä»¬åšä¸€ä¸ªæ‰€æœ‰ä¹˜å®¢éƒ½æ²¡æœ‰å­˜æ´»çš„é¢„æµ‹ï¼Œä½ è®¤ä¸ºè¿™ä¸ªé¢„æµ‹çš„å‡†ç¡®ç‡èƒ½è¾¾åˆ°å¤šå°‘ï¼Ÿ æç¤ºï¼šè¿è¡Œä¸‹é¢çš„ä»£ç æ¥æŸ¥çœ‹é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚ 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 61.62%. å›ç­”: è¯·ç”¨ä¸Šé¢å‡ºç°çš„é¢„æµ‹ç»“æœæ¥æ›¿æ¢æ‰è¿™é‡Œçš„æ–‡å­— 61.62% æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ survival_stats å‡½æ•°æ¥çœ‹çœ‹ Sex è¿™ä¸€ç‰¹å¾å¯¹ä¹˜å®¢çš„å­˜æ´»ç‡æœ‰å¤šå¤§å½±å“ã€‚è¿™ä¸ªå‡½æ•°å®šä¹‰åœ¨åä¸º titanic_visualizations.py çš„ Python è„šæœ¬æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬çš„é¡¹ç›®æä¾›äº†è¿™ä¸ªæ–‡ä»¶ã€‚ä¼ é€’ç»™å‡½æ•°çš„å‰ä¸¤ä¸ªå‚æ•°åˆ†åˆ«æ˜¯æ³°å¦å°¼å…‹å·çš„ä¹˜å®¢æ•°æ®å’Œä¹˜å®¢çš„ ç”Ÿè¿˜ç»“æœã€‚ç¬¬ä¸‰ä¸ªå‚æ•°è¡¨æ˜æˆ‘ä»¬ä¼šä¾æ®å“ªä¸ªç‰¹å¾æ¥ç»˜åˆ¶å›¾å½¢ã€‚ è¿è¡Œä¸‹é¢çš„ä»£ç ç»˜åˆ¶å‡ºä¾æ®ä¹˜å®¢æ€§åˆ«è®¡ç®—å­˜æ´»ç‡çš„æŸ±å½¢å›¾ã€‚ 1survival_stats(data, outcomes, 'Sex') è§‚å¯Ÿæ³°å¦å°¼å…‹å·ä¸Šä¹˜å®¢å­˜æ´»çš„æ•°æ®ç»Ÿè®¡ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å¤§éƒ¨åˆ†ç”·æ€§ä¹˜å®¢åœ¨èˆ¹æ²‰æ²¡çš„æ—¶å€™éƒ½é‡éš¾äº†ã€‚ç›¸åçš„ï¼Œå¤§éƒ¨åˆ†å¥³æ€§ä¹˜å®¢éƒ½åœ¨äº‹æ•…ä¸­ç”Ÿè¿˜ã€‚è®©æˆ‘ä»¬åœ¨å…ˆå‰æ¨æ–­çš„åŸºç¡€ä¸Šç»§ç»­åˆ›å»ºï¼šå¦‚æœä¹˜å®¢æ˜¯ç”·æ€§ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±é¢„æµ‹ä»–ä»¬é‡éš¾ï¼›å¦‚æœä¹˜å®¢æ˜¯å¥³æ€§ï¼Œé‚£ä¹ˆæˆ‘ä»¬é¢„æµ‹ä»–ä»¬åœ¨äº‹æ•…ä¸­æ´»äº†ä¸‹æ¥ã€‚ å°†ä¸‹é¢çš„ä»£ç è¡¥å……å®Œæ•´ï¼Œè®©å‡½æ•°å¯ä»¥è¿›è¡Œæ­£ç¡®é¢„æµ‹ã€‚ æç¤ºï¼šæ‚¨å¯ä»¥ç”¨è®¿é—® dictionaryï¼ˆå­—å…¸ï¼‰çš„æ–¹æ³•æ¥è®¿é—®èˆ¹ä¸Šä¹˜å®¢çš„æ¯ä¸ªç‰¹å¾å¯¹åº”çš„å€¼ã€‚ä¾‹å¦‚ï¼Œ passenger[&#39;Sex&#39;] è¿”å›ä¹˜å®¢çš„æ€§åˆ«ã€‚ 123456789101112131415161718192021222324252627def predictions_1(data): """ Model with one feature: - Predict a passenger survived if they are female. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # ç§»é™¤ä¸‹æ–¹çš„ 'pass' å£°æ˜ # and write your prediction conditions here # è¾“å…¥ä½ è‡ªå·±çš„é¢„æµ‹æ¡ä»¶ # print passenger['Sex'] if(passenger['Sex'] == "male") : predictions.append(0) else : predictions.append(1) # Return our predictions # è¿”å›é¢„æµ‹ç»“æœ return pd.Series(predictions)# Make the predictions# è¿›è¡Œé¢„æµ‹predictions = predictions_1(data)# print predictions é—®é¢˜2å½“æˆ‘ä»¬é¢„æµ‹èˆ¹ä¸Šå¥³æ€§ä¹˜å®¢å…¨éƒ¨å­˜æ´»ï¼Œè€Œå‰©ä¸‹çš„äººå…¨éƒ¨é‡éš¾ï¼Œé‚£ä¹ˆæˆ‘ä»¬é¢„æµ‹çš„å‡†ç¡®ç‡ä¼šè¾¾åˆ°å¤šå°‘ï¼Ÿ æç¤ºï¼šè¿è¡Œä¸‹é¢çš„ä»£ç æ¥æŸ¥çœ‹æˆ‘ä»¬é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚ 12#print predictionsprint accuracy_score(outcomes, predictions) Predictions have an accuracy of 78.68%. å›ç­”: ç”¨ä¸Šé¢å‡ºç°çš„é¢„æµ‹ç»“æœæ¥æ›¿æ¢æ‰è¿™é‡Œçš„æ–‡å­— 78.68%. ä»…ä»…ä½¿ç”¨ä¹˜å®¢æ€§åˆ«ï¼ˆSexï¼‰è¿™ä¸€ç‰¹å¾ï¼Œæˆ‘ä»¬é¢„æµ‹çš„å‡†ç¡®æ€§å°±æœ‰äº†æ˜æ˜¾çš„æé«˜ã€‚ç°åœ¨å†çœ‹ä¸€ä¸‹ä½¿ç”¨é¢å¤–çš„ç‰¹å¾èƒ½å¦æ›´è¿›ä¸€æ­¥æå‡æˆ‘ä»¬çš„é¢„æµ‹å‡†ç¡®åº¦ã€‚ä¾‹å¦‚ï¼Œç»¼åˆè€ƒè™‘æ‰€æœ‰åœ¨æ³°å¦å°¼å…‹å·ä¸Šçš„ç”·æ€§ä¹˜å®¢ï¼šæˆ‘ä»¬æ˜¯å¦æ‰¾åˆ°è¿™äº›ä¹˜å®¢ä¸­çš„ä¸€ä¸ªå­é›†ï¼Œä»–ä»¬çš„å­˜æ´»æ¦‚ç‡è¾ƒé«˜ã€‚è®©æˆ‘ä»¬å†æ¬¡ä½¿ç”¨ survival_stats å‡½æ•°æ¥çœ‹çœ‹æ¯ä½ç”·æ€§ä¹˜å®¢çš„å¹´é¾„ï¼ˆAgeï¼‰ã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç¬¬å››ä¸ªå‚æ•°æ¥é™å®šæŸ±å½¢å›¾ä¸­åªæœ‰ç”·æ€§ä¹˜å®¢ã€‚ è¿è¡Œä¸‹é¢è¿™æ®µä»£ç ï¼ŒæŠŠç”·æ€§åŸºäºå¹´é¾„çš„ç”Ÿå­˜ç»“æœç»˜åˆ¶å‡ºæ¥ã€‚ 1survival_stats(data, outcomes, 'Age', ["Sex == 'male'"]) ä»”ç»†è§‚å¯Ÿæ³°å¦å°¼å…‹å·å­˜æ´»çš„æ•°æ®ç»Ÿè®¡ï¼Œåœ¨èˆ¹æ²‰æ²¡çš„æ—¶å€™ï¼Œå¤§éƒ¨åˆ†å°äº10å²çš„ç”·å­©éƒ½æ´»ç€ï¼Œè€Œå¤§å¤šæ•°10å²ä»¥ä¸Šçš„ç”·æ€§éƒ½éšç€èˆ¹çš„æ²‰æ²¡è€Œé‡éš¾ã€‚è®©æˆ‘ä»¬ç»§ç»­åœ¨å…ˆå‰é¢„æµ‹çš„åŸºç¡€ä¸Šæ„å»ºï¼šå¦‚æœä¹˜å®¢æ˜¯å¥³æ€§ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±é¢„æµ‹å¥¹ä»¬å…¨éƒ¨å­˜æ´»ï¼›å¦‚æœä¹˜å®¢æ˜¯ç”·æ€§å¹¶ä¸”å°äº10å²ï¼Œæˆ‘ä»¬ä¹Ÿä¼šé¢„æµ‹ä»–ä»¬å…¨éƒ¨å­˜æ´»ï¼›æ‰€æœ‰å…¶å®ƒæˆ‘ä»¬å°±é¢„æµ‹ä»–ä»¬éƒ½æ²¡æœ‰å¹¸å­˜ã€‚ å°†ä¸‹é¢ç¼ºå¤±çš„ä»£ç è¡¥å……å®Œæ•´ï¼Œè®©æˆ‘ä»¬çš„å‡½æ•°å¯ä»¥å®ç°é¢„æµ‹ã€‚æç¤º: æ‚¨å¯ä»¥ç”¨ä¹‹å‰ predictions_1 çš„ä»£ç ä½œä¸ºå¼€å§‹æ¥ä¿®æ”¹ä»£ç ï¼Œå®ç°æ–°çš„é¢„æµ‹å‡½æ•°ã€‚ 12345678910111213141516171819202122232425262728293031def predictions_2(data): """ Model with two features: - Predict a passenger survived if they are female. - Predict a passenger survived if they are male and younger than 10. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # ç§»é™¤ä¸‹æ–¹çš„ 'pass' å£°æ˜ # and write your prediction conditions here # è¾“å…¥ä½ è‡ªå·±çš„é¢„æµ‹æ¡ä»¶ if(passenger['Sex'] == "male") : if(passenger['Age'] &lt; 10) : predictions.append(1) else : predictions.append(0) else : predictions.append(1) # Return our predictions # è¿”å›é¢„æµ‹ç»“æœ return pd.Series(predictions)# Make the predictions# è¿›è¡Œé¢„æµ‹predictions = predictions_2(data)# print predictions é—®é¢˜3å½“é¢„æµ‹æ‰€æœ‰å¥³æ€§ä»¥åŠå°äº10å²çš„ç”·æ€§éƒ½å­˜æ´»çš„æ—¶å€™ï¼Œé¢„æµ‹çš„å‡†ç¡®ç‡ä¼šè¾¾åˆ°å¤šå°‘ï¼Ÿ æç¤ºï¼šè¿è¡Œä¸‹é¢çš„ä»£ç æ¥æŸ¥çœ‹é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚ 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 79.35%. å›ç­”: ç”¨ä¸Šé¢å‡ºç°çš„é¢„æµ‹ç»“æœæ¥æ›¿æ¢æ‰è¿™é‡Œçš„æ–‡å­— 79.35% æ·»åŠ å¹´é¾„ï¼ˆAgeï¼‰ç‰¹å¾ä¸æ€§åˆ«ï¼ˆSexï¼‰çš„ç»“åˆæ¯”å•ç‹¬ä½¿ç”¨æ€§åˆ«ï¼ˆSexï¼‰ä¹Ÿæé«˜äº†ä¸å°‘å‡†ç¡®åº¦ã€‚ç°åœ¨è¯¥ä½ æ¥åšé¢„æµ‹äº†ï¼šæ‰¾åˆ°ä¸€ç³»åˆ—çš„ç‰¹å¾å’Œæ¡ä»¶æ¥å¯¹æ•°æ®è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—é¢„æµ‹ç»“æœæé«˜åˆ°80%ä»¥ä¸Šã€‚è¿™å¯èƒ½éœ€è¦å¤šä¸ªç‰¹æ€§å’Œå¤šä¸ªå±‚æ¬¡çš„æ¡ä»¶è¯­å¥æ‰ä¼šæˆåŠŸã€‚ä½ å¯ä»¥åœ¨ä¸åŒçš„æ¡ä»¶ä¸‹å¤šæ¬¡ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾ã€‚Pclassï¼ŒSexï¼ŒAgeï¼ŒSibSp å’Œ Parch æ˜¯å»ºè®®å°è¯•ä½¿ç”¨çš„ç‰¹å¾ã€‚ ä½¿ç”¨ survival_stats å‡½æ•°æ¥è§‚æµ‹æ³°å¦å°¼å…‹å·ä¸Šä¹˜å®¢å­˜æ´»çš„æ•°æ®ç»Ÿè®¡ã€‚æç¤º: è¦ä½¿ç”¨å¤šä¸ªè¿‡æ»¤æ¡ä»¶ï¼ŒæŠŠæ¯ä¸€ä¸ªæ¡ä»¶æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨é‡Œä½œä¸ºæœ€åä¸€ä¸ªå‚æ•°ä¼ é€’è¿›å»ã€‚ä¾‹å¦‚: [&quot;Sex == &#39;male&#39;&quot;, &quot;Age &lt; 18&quot;] 1survival_stats(data, outcomes, 'Embarked', ["Sex == 'female'","Pclass == 3"]) å½“æŸ¥çœ‹å’Œç ”ç©¶äº†å›¾å½¢åŒ–çš„æ³°å¦å°¼å…‹å·ä¸Šä¹˜å®¢çš„æ•°æ®ç»Ÿè®¡åï¼Œè¯·è¡¥å…¨ä¸‹é¢è¿™æ®µä»£ç ä¸­ç¼ºå¤±çš„éƒ¨åˆ†ï¼Œä½¿å¾—å‡½æ•°å¯ä»¥è¿”å›ä½ çš„é¢„æµ‹ã€‚åœ¨åˆ°è¾¾æœ€ç»ˆçš„é¢„æµ‹æ¨¡å‹å‰è¯·ç¡®ä¿è®°å½•ä½ å°è¯•è¿‡çš„å„ç§ç‰¹å¾å’Œæ¡ä»¶ã€‚æç¤º: æ‚¨å¯ä»¥ç”¨ä¹‹å‰ predictions_2 çš„ä»£ç ä½œä¸ºå¼€å§‹æ¥ä¿®æ”¹ä»£ç ï¼Œå®ç°æ–°çš„é¢„æµ‹å‡½æ•°ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839def predictions_3(data): """ Model with multiple features. Makes a prediction with an accuracy of at least 80%. """ predictions = [] for _, passenger in data.iterrows(): # Remove the 'pass' statement below # and write your prediction conditions here if(passenger['Sex'] == "male") : if(passenger['Age'] &lt; 18) : if(passenger['Pclass'] &lt; 3 ) : predictions.append(1) else : predictions.append(0) else : predictions.append(0) # print passenger['Pclass']# if(passenger['Pclass'] &gt; 1 ) :# predictions.append(0)# else :# predictions.append(1) else : if(passenger['Pclass'] == 3 ) : if(passenger['Embarked'] == 'S' ) : predictions.append(0) else : predictions.append(1) else : predictions.append(1) # Return our predictions return pd.Series(predictions)# Make the predictionspredictions = predictions_3(data) ç»“è®ºè¯·æè¿°ä½ å®ç°80%å‡†ç¡®åº¦çš„é¢„æµ‹æ¨¡å‹æ‰€ç»å†çš„æ­¥éª¤ã€‚æ‚¨è§‚å¯Ÿè¿‡å“ªäº›ç‰¹å¾ï¼ŸæŸäº›ç‰¹æ€§æ˜¯å¦æ¯”å…¶ä»–ç‰¹å¾æ›´æœ‰å¸®åŠ©ï¼Ÿä½ ç”¨äº†ä»€ä¹ˆæ¡ä»¶æ¥é¢„æµ‹ç”Ÿè¿˜ç»“æœï¼Ÿä½ æœ€ç»ˆçš„é¢„æµ‹çš„å‡†ç¡®ç‡æ˜¯å¤šå°‘ï¼Ÿæç¤º:è¿è¡Œä¸‹é¢çš„ä»£ç æ¥æŸ¥çœ‹ä½ çš„é¢„æµ‹å‡†ç¡®åº¦ã€‚ 1print accuracy_score(outcomes, predictions) Predictions have an accuracy of 82.38%. å›ç­”: ç”¨ä¸Šé¢é—®é¢˜çš„ç­”æ¡ˆæ¥æ›¿æ¢æ‰è¿™é‡Œçš„æ–‡å­— ç»“è®ºç»è¿‡äº†æ•°æ¬¡å¯¹æ•°æ®çš„æ¢ç´¢å’Œåˆ†ç±»ï¼Œä½ åˆ›å»ºäº†ä¸€ä¸ªé¢„æµ‹æ³°å¦å°¼å…‹å·ä¹˜å®¢å­˜æ´»ç‡çš„æœ‰ç”¨çš„ç®—æ³•ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ä½ æ‰‹åŠ¨åœ°å®ç°äº†ä¸€ä¸ªç®€å•çš„æœºå™¨å­¦ä¹ æ¨¡å‹â€”â€”å†³ç­–æ ‘ï¼ˆdecision treeï¼‰ã€‚å†³ç­–æ ‘æ¯æ¬¡æŒ‰ç…§ä¸€ä¸ªç‰¹å¾æŠŠæ•°æ®åˆ†å‰²æˆè¶Šæ¥è¶Šå°çš„ç¾¤ç»„ï¼ˆè¢«ç§°ä¸º nodesï¼‰ã€‚æ¯æ¬¡æ•°æ®çš„ä¸€ä¸ªå­é›†è¢«åˆ†å‡ºæ¥ï¼Œå¦‚æœåˆ†å‰²ç»“æœçš„å­é›†ä¸­çš„æ•°æ®æ¯”ä¹‹å‰æ›´åŒè´¨ï¼ˆåŒ…å«è¿‘ä¼¼çš„æ ‡ç­¾ï¼‰ï¼Œæˆ‘ä»¬çš„é¢„æµ‹ä¹Ÿå°±æ›´åŠ å‡†ç¡®ã€‚ç”µè„‘æ¥å¸®åŠ©æˆ‘ä»¬åšè¿™ä»¶äº‹ä¼šæ¯”æ‰‹åŠ¨åšæ›´å½»åº•ï¼Œæ›´ç²¾ç¡®ã€‚è¿™ä¸ªé“¾æ¥æä¾›äº†å¦ä¸€ä¸ªä½¿ç”¨å†³ç­–æ ‘åšæœºå™¨å­¦ä¹ å…¥é—¨çš„ä¾‹å­ã€‚ å†³ç­–æ ‘æ˜¯è®¸å¤šç›‘ç£å­¦ä¹ ç®—æ³•ä¸­çš„ä¸€ç§ã€‚åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å…³å¿ƒçš„æ˜¯ä½¿ç”¨æ•°æ®çš„ç‰¹å¾å¹¶æ ¹æ®æ•°æ®çš„ç»“æœæ ‡ç­¾è¿›è¡Œé¢„æµ‹æˆ–å»ºæ¨¡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸€ç»„æ•°æ®éƒ½æœ‰ä¸€ä¸ªçœŸæ­£çš„ç»“æœå€¼ï¼Œä¸è®ºæ˜¯åƒæ³°å¦å°¼å…‹å·ç”Ÿå­˜æ•°æ®é›†ä¸€æ ·çš„æ ‡ç­¾ï¼Œæˆ–è€…æ˜¯è¿ç»­çš„æˆ¿ä»·é¢„æµ‹ã€‚ é—®é¢˜5æƒ³è±¡ä¸€ä¸ªçœŸå®ä¸–ç•Œä¸­åº”ç”¨ç›‘ç£å­¦ä¹ çš„åœºæ™¯ï¼Œä½ æœŸæœ›é¢„æµ‹çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿä¸¾å‡ºä¸¤ä¸ªåœ¨è¿™ä¸ªåœºæ™¯ä¸­èƒ½å¤Ÿå¸®åŠ©ä½ è¿›è¡Œé¢„æµ‹çš„æ•°æ®é›†ä¸­çš„ç‰¹å¾ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python numpy]]></title>
    <url>%2F2018%2F01%2F10%2Fpython%20numpy%2F</url>
    <content type="text"><![CDATA[jupyter notebook:1.arangeç”Ÿæˆæ•°ç»„np.arange(10)array([0,1,2,3,4,5,6,7,8,9]) 2.ç­‰å·®æ•°åˆ—æ˜¯æŒ‡ä»ç¬¬äºŒé¡¹èµ·ï¼Œæ¯ä¸€é¡¹ä¸å®ƒçš„å‰ä¸€é¡¹çš„å·®ç­‰äºåŒä¸€ä¸ªå¸¸æ•°çš„ä¸€ç§æ•°åˆ—ï¼Œå¸¸ç”¨Aã€Pè¡¨ç¤ºç­‰å·®æ•°åˆ— linspacenp.linspace(1,10,5)array([1. , 3.25 ,5.5,5.75,10]) np.linspace(1,10,5,endpoint=False) ç›¸å½“äºç”Ÿæˆ6ä¸ªæ•°åªæ˜¾ç¤ºå‰äº”ä¸ªarray([1. , 2.8 ,4.6,6.5,8.2]) 3.ç­‰æ¯”æ•°åˆ—æ˜¯æŒ‡ä»ç¬¬äºŒé¡¹èµ·ï¼Œæ¯ä¸€é¡¹ä¸å®ƒçš„å‰ä¸€é¡¹çš„æ¯”å€¼ç­‰äºåŒä¸€ä¸ªå¸¸æ•°çš„ä¸€ç§æ•°åˆ—ç­‰æ¯”æ•°åˆ—logspacenp.logspace(1,10,5) 4.shapearr = np.array([ [1,2,3], [2,3,4] ]) arr.shape(2, 3) 5.zeroshelp(np.zeros)zeros(shape, dtype=float, order=â€™Câ€™) Examples12345678910111213141516171819&gt;&gt;&gt; np.zeros(5)array([ 0., 0., 0., 0., 0.])&gt;&gt;&gt; np.zeros((5,), dtype=np.int)array([0, 0, 0, 0, 0])&gt;&gt;&gt; np.zeros((2, 1))array([[ 0.], [ 0.]])&gt;&gt;&gt; s = (2,2)&gt;&gt;&gt; np.zeros(s)array([[ 0., 0.], [ 0., 0.]])&gt;&gt;&gt; np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtypearray([(0, 0), (0, 0)], dtype=[('x', '&lt;i4'), ('y', '&lt;i4')]) 6.onesç±»ä¼¼zeros,åªä¸è¿‡å¡«å……çš„æ˜¯1 7.emptyç±»ä¼¼zeros,åªä¸è¿‡å¡«å……çš„æ˜¯éšæœºå€¼ 8.reshapeæŠŠä¸€ç»´æ•°ç»„è½¬ç½®ä¸ºå¤šç»´æ•°ç»„reshape ä¸ä¼šæ”¹å˜åŸæ¥çš„ndarrayï¼Œä½†æ˜¯å¾—åˆ°æ–°çš„ndarrayæ˜¯åŸæ•°ç»„çš„è§†å›¾å¯¹äºndarrayçš„ä¸€äº›æ–¹æ³•æ“ä½œï¼Œé¦–å…ˆè¦åŒºåˆ†æ˜¯å¦ä¼šæ”¹å˜åŸæ¥å˜é‡ï¼Œä»¥æ­¤æ¥åˆ¤æ–­æ˜¯è§†å›¾è¿˜æ˜¯å‰¯æœ¬]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonâ€”pandas]]></title>
    <url>%2F2018%2F01%2F08%2Fpython%E2%80%94pandas%2F</url>
    <content type="text"><![CDATA[from pandas import Series,DataFrameimport pandas as pd Seriesï¼šä¸€ç§ç±»ä¼¼äºä¸€ç»´æ•°ç»„çš„å¯¹è±¡ï¼Œæ˜¯ç”±ä¸€ç»„æ•°æ®ï¼ˆå„ç§NumPyæ•°æ®ç±»å‹ï¼‰ä»¥åŠä¸€ç»„ä¸ä¹‹ç›¸å…³çš„æ•°æ®æ ‡ç­¾ï¼ˆå³ç´¢å¼•ï¼‰ç»„æˆã€‚ ç´¢å¼•å¯é‡å¤1234567from pandas import Series,DataFrameimport pandas as pdimport numpy as nparr = np.array([1,3,5,np.NaN,10])serises0=Series(arr)serises0 0 1.01 3.02 5.03 NaN4 10.0dtype: float641serises0.dtype dtype(â€˜float64â€™) 1serises0.index RangeIndex(start=0, stop=5, step=1) 1serises0.values array([ 1., 3., 5., nan, 10.]) 12serises1=Series(data=[91,92,93],dtype=np.float64,index=[u'æ•°å­¦',u'è¯­æ–‡',u'å¤–è¯­'])serises1 æ•°å­¦ 91.0è¯­æ–‡ 92.0å¤–è¯­ 93.0dtype: float64 12dict0=&#123;u'æ•°å­¦':91.0,u'è¯­æ–‡':92,u'å¤–è¯­':93&#125;dict0 {uâ€™\u5916\u8bedâ€™: 93, uâ€™\u6570\u5b66â€™: 91.0, uâ€™\u8bed\u6587â€™: 92} 12serises2=Series(dict0)serises2 å¤–è¯­ 93.0æ•°å­¦ 91.0è¯­æ–‡ 92.0dtype: float64 1serises2[0] 93.0 1serises2[u'å¤–è¯­'] 93.0 1serises2['å¤–è¯­':'è¯­æ–‡'] å¤–è¯­ 93.0æ•°å­¦ 91.0è¯­æ–‡ 92.0dtype: float64 Seriesè¿ç®—ï¼Œè‡ªåŠ¨å¯¹é½ç´¢å¼•123serises2=Series(data=[11,12,13],dtype=np.float64,index=['p1','p2','p3'])serises3=Series(data=[22,23,24,25],dtype=np.float64,index=['p2','p3','p4','p5'])serises2 +serises3 p1 NaNp2 34.0p3 36.0p4 NaNp5 NaNdtype: float64 123serises1.name='name'serises1.index.name='è€ƒè¯•ç§‘ç›®'serises1 è€ƒè¯•ç§‘ç›®æ•°å­¦ 91.0è¯­æ–‡ 92.0å¤–è¯­ 93.0Name: name, dtype: float64 DataFrame ï¼š è¡¨æ ¼å½¢å¼çš„æ•°æ®ç»“æ„ï¼ŒåŒ…å«ä¸€ç»„æœ‰åºçš„åˆ—ï¼Œæ¯åˆ—å¯ä»¥æ˜¯ä¸åŒçš„å€¼ç±»å‹ï¼ŒDataFrameæ—¢æœ‰è¡Œç´¢å¼•åˆæœ‰åˆ—ç´¢å¼•ï¼Œå¯ä»¥çœ‹åšæ˜¯ç”±Seriesç»„æˆçš„å­—å…¸é€šè¿‡äºŒç»´æ•°ç»„åˆ›å»º12df01=DataFrame([['Tom','John'],[88,90]])df01 0 1 0 Tom John1 88 90 é€šè¿‡å­—å…¸åˆ›å»º123data=&#123;'Tom':[88,55],'John':[90,22]&#125;df02=DataFrame(data)df02 John Tom0 90 881 22 55 DataFrame å¯ä»¥å¢åŠ æ•°æ®1df02.ix['2']=np.NaN John Tom 0 90.0 88.01 22.0 55.02 NaN NaN æ•°æ®åˆ é™¤12df02=df02.dropna()df02 John Tom 0 90.0 88.01 22.0 55.0 123456arr1=np.random.randint(5,10,(4,4))df1=pd.DataFrame(arr1)df1df1.ix[:2,1]=np.NANdf1.ix[:1,2]=np.NANdf1 0 1 2 3 0 8 NaN NaN 71 5 NaN NaN 92 5 NaN 5.0 53 9 8.0 7.0 5 loc ilociloc å¯¹äºä¸‹æ ‡è¿›è¡Œæ“ä½œloc å¯¹äºç´¢å¼•å€¼è¿›è¡Œæ“ä½œ]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark æœºå™¨å­¦ä¹ å…¥é—¨(äºŒ)]]></title>
    <url>%2F2017%2F10%2F19%2Fspark%20ml2%2F</url>
    <content type="text"><![CDATA[Spark MLlib æ ¸å¿ƒç»„ä»¶Ã˜ DataFrame åŒ…å«ä¸åŒçš„åˆ—ï¼Œå¯å­˜å‚¨æ–‡æœ¬ã€ å›¾ç‰‡ã€ æ ‡ç­¾ã€ ç‰¹å¾å‘é‡ã€ é¢„æµ‹å€¼ç­‰Ã˜ Estimator ç”¨äºä»è®­ç»ƒæ•°æ®ç®—å‡ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¾“å…¥DataFrameè¾“å‡ºModelÃ˜ Transformer è¾“å…¥DataFrameè¾“å‡ºDataFrameâ€¢ Modelå³ä¸ºä¸€ç§Transformerï¼Œç”¨äºä»æµ‹è¯•æ•°æ®ç”Ÿæˆé¢„æµ‹ç»“æœâ€¢ ç‰¹å¾è½¬æ¢Transformerè½¬æ¢ä¸€ä¸ªæˆ–å¤šä¸ªç‰¹å¾ï¼Œå¹¶å°†è½¬æ¢åçš„ç‰¹å¾åˆ—è¿½åŠ åˆ°è¾“å‡ºDataFrameä¸­Ã˜ Pipelineâ€¢ å°†å¤šä¸ªTransformerå’ŒEstimatoré“¾æ¥èµ·æ¥å½¢æˆä¸€ä¸ªæœºå™¨å­¦ä¹ workflowâ€¢ Pipelineä¹Ÿæ˜¯ä¸€ç§Estimatorï¼Œç”Ÿæˆä¸€ä¸ªPipelineModelÃ˜ Parameter æ‰€æœ‰çš„Transformerå’ŒEstimatorå…±äº«è¯¥é€šç”¨çš„APIæ¥æŒ‡å®šå‚æ•°Ã˜ PipelineModel ä¿è¯å¯¹æµ‹è¯•æ•°æ®ä½¿ç”¨ä¸è®­ç»ƒæ•°æ®å®Œå…¨ç›¸åŒçš„æ•°æ®è½¬æ¢ è®­ç»ƒæ¨¡å‹ï¼š Tokenizer and HashingTF æ˜¯TransformersLogisticRegression æ˜¯Estimator é¢„æµ‹ç»“æœPipeline æ˜¯ Estimatorfit()æ–¹æ³•ä¼šäº§ç”Ÿä¸€ä¸ªPipelineModeltransform()æ–¹æ³•Pipelines and PipelineModelså¸®åŠ©ç¡®ä¿è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ç»è¿‡ç›¸åŒç‰¹å¾çš„å¤„ç†æ­¥éª¤ã€‚ SparseMatrixä»‹ç»æ¨èåšä¸»ï¼šhttp://blog.csdn.net/sinat_29508201/article/details/54089771]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark æœºå™¨å­¦ä¹ å…¥é—¨(ä¸€)]]></title>
    <url>%2F2017%2F10%2F18%2Fspark%20ml1%2F</url>
    <content type="text"><![CDATA[ç›®å‰åŸºäºRDDçš„MLlibå·²ç»è¿›å…¥ç»´æŠ¤è«æ–¯ã€‚å¤§æ¦‚åœ¨spark2.3åŸºäºRDDçš„MLlib APIå°†è¦è¢«åºŸå¼ƒã€‚æœªæ¥æ˜¯åŸºäºDataFrameçš„API 1.åŸºæœ¬ç»Ÿè®¡è®¡ç®—ä¸¤ç»„æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§ çš®å°”æ£®ç›¸å…³ç³»æ•°ï¼ˆPearson correlation coefficientï¼‰ä¹Ÿç§°çš®å°”æ£®ç§¯çŸ©ç›¸å…³ç³»æ•°(Pearson product-moment correlation coefficient) ï¼Œæ˜¯ä¸€ç§çº¿æ€§ç›¸å…³ç³»æ•°ã€‚çš®å°”æ£®ç›¸å…³ç³»æ•°æ˜¯ç”¨æ¥åæ˜ ä¸¤ä¸ªå˜é‡çº¿æ€§ç›¸å…³ç¨‹åº¦çš„ç»Ÿè®¡é‡ã€‚ç›¸å…³ç³»æ•°ç”¨rè¡¨ç¤ºï¼Œå…¶ä¸­nä¸ºæ ·æœ¬é‡ï¼Œåˆ†åˆ«ä¸ºä¸¤ä¸ªå˜é‡çš„è§‚æµ‹å€¼å’Œå‡å€¼ã€‚ræè¿°çš„æ˜¯ä¸¤ä¸ªå˜é‡é—´çº¿æ€§ç›¸å…³å¼ºå¼±çš„ç¨‹åº¦ã€‚rçš„ç»å¯¹å€¼è¶Šå¤§è¡¨æ˜ç›¸å…³æ€§è¶Šå¼º æŒ‰ç…§é«˜ä¸­æ•°å­¦æ°´å¹³æ¥ç†è§£, å®ƒå¾ˆç®€å•, å¯ä»¥çœ‹åšå°†ä¸¤ç»„æ•°æ®é¦–å…ˆåšZåˆ†æ•°å¤„ç†ä¹‹å, ç„¶åä¸¤ç»„æ•°æ®çš„ä¹˜ç§¯å’Œé™¤ä»¥æ ·æœ¬æ•° Zåˆ†æ•°ä¸€èˆ¬ä»£è¡¨æ­£æ€åˆ†å¸ƒä¸­, æ•°æ®åç¦»ä¸­å¿ƒç‚¹çš„è·ç¦».ç­‰äºå˜é‡å‡æ‰å¹³å‡æ•°å†é™¤ä»¥æ ‡å‡†å·®.(å°±æ˜¯é«˜è€ƒçš„æ ‡å‡†åˆ†ç±»ä¼¼çš„å¤„ç†) æ ‡å‡†å·®åˆ™ç­‰äºå˜é‡å‡æ‰å¹³å‡æ•°çš„å¹³æ–¹å’Œ,å†é™¤ä»¥æ ·æœ¬æ•°,æœ€åå†å¼€æ–¹. æ‰€ä»¥, æ ¹æ®è¿™ä¸ªæœ€æœ´ç´ çš„ç†è§£,æˆ‘ä»¬å¯ä»¥å°†å…¬å¼ä¾æ¬¡ç²¾ç®€ä¸º: spearmanç›¸å…³ç³»æ•°ï¼šæ˜¯è¡¡é‡åˆ†çº§å®šåºå˜é‡ä¹‹é—´çš„ç›¸å…³ç¨‹åº¦çš„ç»Ÿè®¡é‡ï¼Œå¯¹ä¸æœä»æ­£æ€åˆ†å¸ƒçš„èµ„æ–™ã€åŸå§‹èµ„æ–™ç­‰çº§èµ„æ–™ã€ä¸€ä¾§å¼€å£èµ„æ–™ã€æ€»ä½“åˆ†å¸ƒç±»å‹æœªçŸ¥çš„èµ„æ–™ä¸ç¬¦åˆä½¿ç”¨ç§¯çŸ©ç›¸å…³ç³»æ•°æ¥æè¿°å…³è”æ€§ã€‚æ­¤æ—¶å¯é‡‡ç”¨ç§©ç›¸å…³ï¼ˆrank correlationï¼‰ï¼Œä¹Ÿç§°ç­‰çº§ç›¸å…³ï¼Œæ¥æè¿°ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦ä¸æ–¹å‘ã€‚ 1234567891011121314151617181920212223242526272829303132import org.apache.spark.ml.linalg.&#123;Matrix, Vectors&#125;import org.apache.spark.ml.stat.Correlationimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.Rowobject ml_1 &#123;def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder .master("local") .getOrCreate() import spark.implicits._ // $example on$ val data = Seq( Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))), Vectors.dense(4.0, 5.0, 0.0, 3.0), Vectors.dense(6.0, 7.0, 0.0, 8.0), Vectors.sparse(4, Seq((0, 9.0), (3, 1.0))) ) val df = data.map(Tuple1.apply).toDF("features") val Row(coeff1: Matrix) = Correlation.corr(df, "features").head println("Pearson correlation matrix:\n" + coeff1.toString) val Row(coeff2: Matrix) = Correlation.corr(df, "features", "spearman").head println("Spearman correlation matrix:\n" + coeff2.toString) // $example off$ spark.stop() &#125;&#125; 1.0ä¸ºç¬¬ä¸€åˆ—å’Œç¬¬ä¸€åˆ—è®¡ç®—0.055641488407465814ä¸ºç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—è®¡ç®—0.4004714203168137 ä¸ºç¬¬ä¸‰åˆ—å’Œç¬¬å››åˆ—è®¡ç®— è®¡ç®—ç»“æœ:12345678910Pearson correlation matrix:1.0 0.055641488407465814 NaN 0.4004714203168137 0.055641488407465814 1.0 NaN 0.9135958615342522 NaN NaN 1.0 NaN 0.4004714203168137 0.9135958615342522 NaN 1.0 Spearman correlation matrix:1.0 0.10540925533894532 NaN 0.40000000000000174 0.10540925533894532 1.0 NaN 0.9486832980505141 NaN NaN 1.0 NaN 0.40000000000000174 0.9486832980505141 NaN 1.0 å¡æ–¹æ£€éªŒå¡æ–¹æ£€éªŒæ˜¯ç”¨é€”éå¸¸å¹¿çš„ä¸€ç§å‡è®¾æ£€éªŒæ–¹æ³•ï¼Œå®ƒåœ¨åˆ†ç±»èµ„æ–™ç»Ÿè®¡æ¨æ–­ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ï¼šä¸¤ä¸ªç‡æˆ–ä¸¤ä¸ªæ„æˆæ¯”æ¯”è¾ƒçš„å¡æ–¹æ£€éªŒï¼›å¤šä¸ªç‡æˆ–å¤šä¸ªæ„æˆæ¯”æ¯”è¾ƒçš„å¡æ–¹æ£€éªŒä»¥åŠåˆ†ç±»èµ„æ–™çš„ç›¸å…³åˆ†æç­‰ã€‚å¡æ–¹æ£€éªŒå°±æ˜¯ç»Ÿè®¡æ ·æœ¬çš„å®é™…è§‚æµ‹å€¼ä¸ç†è®ºæ¨æ–­å€¼ä¹‹é—´çš„åç¦»ç¨‹åº¦ï¼Œå®é™…è§‚æµ‹å€¼ä¸ç†è®ºæ¨æ–­å€¼ä¹‹é—´çš„åç¦»ç¨‹åº¦å°±å†³å®šå¡æ–¹å€¼çš„å¤§å°ï¼Œå¡æ–¹å€¼è¶Šå¤§ï¼Œè¶Šä¸ç¬¦åˆï¼›å¡æ–¹å€¼è¶Šå°ï¼Œåå·®è¶Šå°ï¼Œè¶Šè¶‹äºç¬¦åˆï¼Œè‹¥ä¸¤ä¸ªå€¼å®Œå…¨ç›¸ç­‰æ—¶ï¼Œå¡æ–¹å€¼å°±ä¸º0ï¼Œè¡¨æ˜ç†è®ºå€¼å®Œå…¨ç¬¦åˆã€‚ 1234567891011121314151617181920212223242526272829303132333435import org.apache.spark.ml.linalg.Vectorimport org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.ml.stat.ChiSquareTestimport org.apache.spark.sql.SparkSession/** * Created by Administrator on 2017/10/18. */object ChiSquareTest &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder .master("local") .getOrCreate() import spark.implicits._ val data = Seq( (0.0, Vectors.dense(0.5, 10.0)), (0.0, Vectors.dense(1.5, 20.0)), (1.0, Vectors.dense(1.5, 30.0)), (0.0, Vectors.dense(3.5, 30.0)), (0.0, Vectors.dense(3.5, 40.0)), (1.0, Vectors.dense(3.5, 40.0)) ) val df = data.toDF("label", "features") val chi = ChiSquareTest.test(df, "features", "label").head println("pValues = " + chi.getAs[Vector](0)) println("degreesOfFreedom = " + chi.getSeq[Int](1).mkString("[", ",", "]")) println("statistics = " + chi.getAs[Vector](2)) spark.stop() &#125;&#125; ç»“æœï¼š123pValues = [0.6872892787909721,0.6822703303362126]degreesOfFreedom = [2,3]statistics = [0.75,1.5]]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark shuffle è°ƒä¼˜]]></title>
    <url>%2F2017%2F08%2F30%2Fspark%20shuffle%20%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[shuffleå®ç°çš„å…·ä½“è¿‡ç¨‹1.Sparkç³»ç»Ÿåœ¨è¿è¡Œå«shuffleè¿‡ç¨‹çš„åº”ç”¨æ—¶ï¼ŒExecutorè¿›ç¨‹é™¤äº†è¿è¡Œtaskï¼Œè¿˜è¦è´Ÿè´£å†™shuffle æ•°æ®ï¼Œç»™å…¶ä»–Executoræä¾›shuffleæ•°æ®ã€‚ å½“Executorè¿›ç¨‹ä»»åŠ¡è¿‡é‡ï¼Œå¯¼è‡´GCè€Œä¸èƒ½ä¸ºå…¶ ä»–Executoræä¾›shuffleæ•°æ®æ—¶ï¼Œä¼šå½±å“ä»»åŠ¡è¿è¡Œã€‚ è¿™é‡Œå®é™…ä¸Šæ˜¯åˆ©ç”¨External Shuffle Service æ¥æå‡æ€§èƒ½ï¼ŒExternal shuffle Serviceæ˜¯é•¿æœŸå­˜åœ¨äºNodeManagerè¿›ç¨‹ä¸­çš„ä¸€ä¸ªè¾…åŠ©æœåŠ¡ã€‚ é€šè¿‡è¯¥æœåŠ¡ æ¥æŠ“å–shuffleæ•°æ®ï¼Œå‡å°‘äº†Executorçš„å‹åŠ›ï¼Œåœ¨Executor GCçš„æ—¶å€™ä¹Ÿä¸ä¼šå½±å“å…¶ä»– Executorçš„ä»»åŠ¡è¿è¡Œã€‚ å¯ç”¨æ–¹æ³•ï¼š ä¸€. åœ¨NodeManagerä¸­å¯åŠ¨External shuffle Serviceã€‚a. åœ¨â€œyarn-site.xmlâ€ä¸­æ·»åŠ å¦‚ä¸‹é…ç½®é¡¹ï¼š &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;spark_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt; &lt;/property&gt; é…ç½®å‚æ•°æè¿° yarn.nodemanager.aux-services ï¼šNodeManagerä¸­ä¸€ä¸ªé•¿æœŸè¿è¡Œçš„è¾…åŠ©æœåŠ¡ï¼Œç”¨äºæå‡Shuffleè®¡ç®—æ€§èƒ½ã€‚ yarn.nodemanager.auxservices.spark_shuffle.class ï¼šNodeManagerä¸­è¾…åŠ©æœåŠ¡å¯¹åº”çš„ç±»ã€‚ spark.shuffle.service.port ï¼šShuffleæœåŠ¡ç›‘å¬æ•°æ®è·å–è¯·æ±‚çš„ç«¯å£ã€‚å¯é€‰é…ç½®ï¼Œé»˜è®¤å€¼ä¸ºâ€œ7337â€ã€‚ b. æ·»åŠ ä¾èµ–çš„jaråŒ… æ‹·è´â€œ${SPARK_HOME}/lib/spark-1.3.0-yarn-shuffle.jarâ€åˆ°â€œ${HADOOP_HOME}/share/hadoop/yarn/lib/â€ç›®å½•ä¸‹ã€‚ c. é‡å¯NodeManagerè¿›ç¨‹ï¼Œä¹Ÿå°±å¯åŠ¨äº†External shuffle Serviceã€‚ äºŒ. Sparkåº”ç”¨ä½¿ç”¨External shuffle Serviceã€‚åœ¨â€œspark-defaults.confâ€ä¸­å¿…é¡»æ·»åŠ å¦‚ä¸‹é…ç½®é¡¹ï¼š spark.shuffle.service.enabled true spark.shuffle.service.port 7337 è¯´æ˜ 1.å¦‚æœ1.å¦‚æœâ€œyarn.nodemanager.aux-servicesâ€é…ç½®é¡¹å·²å­˜åœ¨ï¼Œåˆ™åœ¨valueä¸­æ·»åŠ  â€œspark_shuffleâ€ï¼Œä¸”ç”¨é€—å·å’Œå…¶ä»–å€¼åˆ†å¼€ã€‚ 2.â€œspark.shuffle.service.portâ€çš„å€¼éœ€è¦å’Œä¸Šé¢â€œyarn-site.xmlâ€ä¸­çš„å€¼ä¸€æ ·ã€‚ é…ç½®å‚æ•°æè¿° spark.shuffle.service.enabled ï¼šNodeManagerä¸­ä¸€ä¸ªé•¿æœŸè¿è¡Œçš„è¾…åŠ©æœåŠ¡ï¼Œç”¨äºæå‡Shuffle è®¡ç®—æ€§èƒ½ã€‚é»˜è®¤ä¸ºfalseï¼Œè¡¨ç¤ºä¸å¯ç”¨è¯¥åŠŸèƒ½ã€‚ spark.shuffle.service.port ï¼šShuffleæœåŠ¡ç›‘å¬æ•°æ®è·å–è¯·æ±‚çš„ç«¯å£ã€‚å¯é€‰é…ç½®ï¼Œé»˜è®¤å€¼ ä¸ºâ€œ7337â€ã€‚ Hash Shuffleä¸è¶³map taskä¼šæ ¹æ®reduceçš„æ•°é‡ï¼ˆpartitionï¼‰ ç”Ÿæˆç›¸åº”çš„bucket å†™shuffle blockFile å¦‚æœmap å’Œreduceæ•°é‡è¿‡å¤šï¼Œä¼šå†™å¾ˆå¤šblockFileï¼Œé€ æˆé—®é¢˜1ï¼šè¶…è¿‡æ“ä½œç³»ç»Ÿæ‰€èƒ½æ‰“å¼€æœ€å¤§æ–‡ä»¶æ•°ï¼Œé—®é¢˜2ï¼šå¤§é‡éšæœºå†™éšæœºè¯» è§£å†³æ–¹æ¡ˆï¼š 1.shuffle å‚æ•°ï¼šspark.shuffle.consolidateFiles é»˜è®¤ä¸ºfalseå¦‚æœè®¾ç½®ä¸ºâ€trueâ€ï¼Œåœ¨shuffleæœŸé—´ï¼Œåˆå¹¶çš„ä¸­é—´æ–‡ä»¶å°†ä¼šè¢«åˆ›å»ºã€‚åˆ›å»ºæ›´å°‘çš„æ–‡ä»¶å¯ä»¥æä¾›æ–‡ä»¶ç³»ç»Ÿçš„shuffleçš„æ•ˆ ç‡ã€‚è¿™äº›shuffleéƒ½ä¼´éšç€å¤§é‡é€’å½’ä»»åŠ¡ã€‚å½“ç”¨ext4å’Œdfsæ–‡ä»¶ç³»ç»Ÿæ—¶ï¼Œæ¨èè®¾ç½®ä¸ºâ€trueâ€ã€‚åœ¨ext3ä¸­ï¼Œå› ä¸ºæ–‡ä»¶ç³»ç»Ÿçš„é™åˆ¶ï¼Œè¿™ä¸ªé€‰é¡¹å¯ èƒ½æœºå™¨ï¼ˆå¤§äº8æ ¸ï¼‰é™ä½æ•ˆç‡ 2.sort shuffleæ¯ä¸ªmapåªå†™åˆ°ä¸€ä¸ªæ–‡ä»¶ï¼Œå’Œä¸Šé¢çš„å†™åˆ°reduceä¸ªæ•°ä¸ªæ–‡ä»¶ä¸åŒ ä¸åŒç®—å­å½±å“shuffleè¡¨ç°å½¢å¼ä¸åŒreduceByKey(func, numPartitions=None)ä¹Ÿå°±æ˜¯ï¼ŒreduceByKeyç”¨äºå¯¹æ¯ä¸ªkeyå¯¹åº”çš„å¤šä¸ªvalueè¿›è¡Œmergeæ“ä½œï¼Œæœ€é‡è¦çš„æ˜¯å®ƒèƒ½å¤Ÿåœ¨æœ¬åœ°å…ˆè¿›è¡Œmergeæ“ä½œï¼Œå¹¶ä¸”mergeæ“ä½œå¯ä»¥é€šè¿‡å‡½æ•°è‡ªå®šä¹‰ã€‚ groupByKey(numPartitions=None)ä¹Ÿå°±æ˜¯ï¼ŒgroupByKeyä¹Ÿæ˜¯å¯¹æ¯ä¸ªkeyè¿›è¡Œæ“ä½œï¼Œä½†åªç”Ÿæˆä¸€ä¸ªsequenceã€‚éœ€è¦ç‰¹åˆ«æ³¨æ„â€œNoteâ€ä¸­çš„è¯ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ï¼šå¦‚æœéœ€è¦å¯¹sequenceè¿›è¡Œaggregationæ“ä½œï¼ˆæ³¨æ„ï¼ŒgroupByKeyæœ¬èº«ä¸èƒ½è‡ªå®šä¹‰æ“ä½œå‡½æ•°ï¼‰ï¼Œé‚£ä¹ˆï¼Œé€‰æ‹©reduceByKey/aggregateByKeyæ›´å¥½ã€‚è¿™æ˜¯å› ä¸ºgroupByKeyä¸èƒ½è‡ªå®šä¹‰å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦å…ˆç”¨groupByKeyç”ŸæˆRDDï¼Œç„¶åæ‰èƒ½å¯¹æ­¤RDDé€šè¿‡mapè¿›è¡Œè‡ªå®šä¹‰å‡½æ•°æ“ä½œã€‚]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark æœºå™¨å­¦ä¹ å…¥é—¨(äºŒ)]]></title>
    <url>%2F2017%2F08%2F29%2Fspark%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[éƒ¨ç½²ä¼˜åŒ–ï¼šç£ç›˜ï¼šæŒ‚è½½ç£ç›˜æ—¶ä½¿ç”¨noatimeå’Œnodiratimeé€‰é¡¹å‡å°‘å†™çš„å¼€é”€ linuxæ¯ä¸ªæ–‡ä»¶éƒ½ä¼šä¿ç•™3ä¸ªæ—¶é—´æˆ³ç”¨stat æ–‡ä»¶å æ¥æŸ¥çœ‹Acessï¼šæ–‡ä»¶è®¿é—®æ—¶é—´Modfiyï¼šå†…å®¹ä¿®æ”¹æ—¶é—´Changeï¼šæ–‡ä»¶åä¿®æ”¹æ—¶é—´ å‚æ•°å«ä¹‰ï¼šç£ç›˜ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ä¸æ›´æ–°è®¿é—®æ—¶é—´å†…å­˜ï¼šJVM å†…å­˜ä¸å»ºè®®æ¯ä¸ªexecutor è¶…è¿‡200G CPUæ¯å°æœºå™¨çš„Vcoreæ•°ä¸å»ºè®®å°äº8 JOBè°ƒåº¦ï¼šFail Schedule å¯æœ€å¤§ç¨‹åº¦ä¿è¯å„ä¸ªJobéƒ½æœ‰æœºä¼šè·å–èµ„æº æ•°æ®åºåˆ—åŒ–ï¼šKyro serializationåºåˆ—åŒ–é€Ÿåº¦æ›´å¿«ï¼Œç»“æœæ›´ç´§å‡‘ä¸ºäº†æ›´å¥½çš„æ€§èƒ½ï¼Œéœ€æå‰æ³¨å†Œè¢«åºåˆ—åŒ–çš„ç±»ï¼Œå¦åˆ™ä¼šå­˜åœ¨å¤§é‡çš„ç©ºé—´æµªè´¹é€šè¿‡spark.serializeræŒ‡å®š å‡å°‘å†…å­˜æ¶ˆè€—ï¼šå°½é‡ä½¿ç”¨åŸºæœ¬æ•°æ®ç±»å‹å’Œæ•°ç»„ï¼Œé¿å…ä½¿ç”¨javaé›†åˆç±»å°½é‡å‡å°‘åŒ…å«å¤§é‡å°å¯¹è±¡çš„åµŒå¥—ç»“æ„Keyå°½é‡ä½¿ç”¨æ•°å€¼æˆ–æšä¸¾ç±»å‹è€Œä¸æ˜¯å­—ç¬¦ä¸²RAMå°äº32GBæ—¶ï¼Œä½¿ç”¨-XX:+UseCompressedOopsä½¿ç”¨4å­—èŠ‚ï¼ˆè€Œé8å­—èŠ‚ï¼‰çš„æŒ‡é’ˆ è°ƒæ•´å¹¶è¡Œåº¦ï¼šè°ƒæ•´Mapä¾§å¹¶è¡Œåº¦å¯¹äºkafka direct stream å¯é€šè¿‡è°ƒæ•´Topicçš„Patitionä¸ªæ•°è°ƒæ•´Spark Mapä¾§å¹¶è¡Œåº¦å¯¹äºspark.textFileï¼Œé€šè¿‡å‚æ•°è°ƒæ•´ è°ƒæ•´Reduceä¾§å¹¶è¡Œåº¦é€šè¿‡spark.default.parallelismè®¾ç½®shuffleæ—¶é»˜è®¤å¹¶è¡Œåº¦]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¼€æºçˆ¬è™«ä»‹ç»]]></title>
    <url>%2F2017%2F08%2F05%2F%E5%BC%80%E6%BA%90%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[https://github.com/Chyroc/WechatSogou å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«ã€‚åŸºäºæœç‹—å¾®ä¿¡æœç´¢çš„å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«æ¥å£ï¼Œå¯ä»¥æ‰©å±•æˆåŸºäºæœç‹—æœç´¢çš„çˆ¬è™«ï¼Œè¿”å›ç»“æœæ˜¯åˆ—è¡¨ï¼Œæ¯ä¸€é¡¹å‡æ˜¯å…¬ä¼—å·å…·ä½“ä¿¡æ¯å­—å…¸ã€‚ https://github.com/lanbing510/DouBanSpider è±†ç“£è¯»ä¹¦çˆ¬è™«ã€‚å¯ä»¥çˆ¬ä¸‹è±†ç“£è¯»ä¹¦æ ‡ç­¾ä¸‹çš„æ‰€æœ‰å›¾ä¹¦ï¼ŒæŒ‰è¯„åˆ†æ’åä¾æ¬¡å­˜å‚¨ï¼Œå­˜å‚¨åˆ°Excelä¸­ï¼Œå¯æ–¹ä¾¿å¤§å®¶ç­›é€‰æœç½—ï¼Œæ¯”å¦‚ç­›é€‰è¯„ä»·äººæ•°&gt;1000çš„é«˜åˆ†ä¹¦ç±ï¼›å¯ä¾æ®ä¸åŒçš„ä¸»é¢˜å­˜å‚¨åˆ°Excelä¸åŒçš„Sheet ï¼Œé‡‡ç”¨User Agentä¼ªè£…ä¸ºæµè§ˆå™¨è¿›è¡Œçˆ¬å–ï¼Œå¹¶åŠ å…¥éšæœºå»¶æ—¶æ¥æ›´å¥½çš„æ¨¡ä»¿æµè§ˆå™¨è¡Œä¸ºï¼Œé¿å…çˆ¬è™«è¢«å°ã€‚ https://github.com/LiuRoy/zhihu_spiderçŸ¥ä¹çˆ¬è™«ã€‚æ­¤é¡¹ç›®çš„åŠŸèƒ½æ˜¯çˆ¬å–çŸ¥ä¹ç”¨æˆ·ä¿¡æ¯ä»¥åŠäººé™…æ‹“æ‰‘å…³ç³»ï¼Œçˆ¬è™«æ¡†æ¶ä½¿ç”¨scrapyï¼Œæ•°æ®å­˜å‚¨ä½¿ç”¨mongo https://github.com/airingursb/bilibili-userBilibiliç”¨æˆ·çˆ¬è™«ã€‚æ€»æ•°æ®æ•°ï¼š20119918ï¼ŒæŠ“å–å­—æ®µï¼šç”¨æˆ·idï¼Œæ˜µç§°ï¼Œæ€§åˆ«ï¼Œå¤´åƒï¼Œç­‰çº§ï¼Œç»éªŒå€¼ï¼Œç²‰ä¸æ•°ï¼Œç”Ÿæ—¥ï¼Œåœ°å€ï¼Œæ³¨å†Œæ—¶é—´ï¼Œç­¾åï¼Œç­‰çº§ä¸ç»éªŒå€¼ç­‰ã€‚æŠ“å–ä¹‹åç”ŸæˆBç«™ç”¨æˆ·æ•°æ®æŠ¥å‘Šã€‚ https://github.com/LiuXingMing/SinaSpideræ–°æµªå¾®åšçˆ¬è™«ã€‚ä¸»è¦çˆ¬å–æ–°æµªå¾®åšç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ã€å¾®åšä¿¡æ¯ã€ç²‰ä¸å’Œå…³æ³¨ã€‚ä»£ç è·å–æ–°æµªå¾®åšCookieè¿›è¡Œç™»å½•ï¼Œå¯é€šè¿‡å¤šè´¦å·ç™»å½•æ¥é˜²æ­¢æ–°æµªçš„åæ‰’ã€‚ä¸»è¦ä½¿ç”¨ scrapy çˆ¬è™«æ¡†æ¶ https://github.com/gnemoug/distribute_crawlerå°è¯´ä¸‹è½½åˆ†å¸ƒå¼çˆ¬è™«ã€‚ä½¿ç”¨scrapy,Redis, MongoDB,graphiteå®ç°çš„ä¸€ä¸ªåˆ†å¸ƒå¼ç½‘ç»œçˆ¬è™«,åº•å±‚å­˜å‚¨mongodbé›†ç¾¤,åˆ†å¸ƒå¼ä½¿ç”¨rediså®ç°,çˆ¬è™«çŠ¶æ€æ˜¾ç¤ºä½¿ç”¨graphiteå®ç°ï¼Œä¸»è¦é’ˆå¯¹ä¸€ä¸ªå°è¯´ç«™ç‚¹ã€‚ https://github.com/yanzhou/CnkiSpiderä¸­å›½çŸ¥ç½‘çˆ¬è™«ã€‚è®¾ç½®æ£€ç´¢æ¡ä»¶åï¼Œæ‰§è¡Œsrc/CnkiSpider.pyæŠ“å–æ•°æ®ï¼ŒæŠ“å–æ•°æ®å­˜å‚¨åœ¨/dataç›®å½•ä¸‹ï¼Œæ¯ä¸ªæ•°æ®æ–‡ä»¶çš„ç¬¬ä¸€è¡Œä¸ºå­—æ®µåç§°ã€‚ https://github.com/lanbing510/LianJiaSpideré“¾å®¶ç½‘çˆ¬è™«ã€‚çˆ¬å–åŒ—äº¬åœ°åŒºé“¾å®¶å†å¹´äºŒæ‰‹æˆ¿æˆäº¤è®°å½•ã€‚æ¶µç›–é“¾å®¶çˆ¬è™«ä¸€æ–‡çš„å…¨éƒ¨ä»£ç ï¼ŒåŒ…æ‹¬é“¾å®¶æ¨¡æ‹Ÿç™»å½•ä»£ç ã€‚ https://github.com/taizilongxu/scrapy_jingdongäº¬ä¸œçˆ¬è™«ã€‚åŸºäºscrapyçš„äº¬ä¸œç½‘ç«™çˆ¬è™«ï¼Œä¿å­˜æ ¼å¼ä¸ºcsvã€‚ https://github.com/caspartse/QQ-Groups-SpiderQQ ç¾¤çˆ¬è™«ã€‚æ‰¹é‡æŠ“å– QQ ç¾¤ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç¾¤åç§°ã€ç¾¤å·ã€ç¾¤äººæ•°ã€ç¾¤ä¸»ã€ç¾¤ç®€ä»‹ç­‰å†…å®¹ï¼Œæœ€ç»ˆç”Ÿæˆ XLS(X) / CSV ç»“æœæ–‡ä»¶ã€‚ https://github.com/hanc00l/wooyun_publicä¹Œäº‘çˆ¬è™«ã€‚ ä¹Œäº‘å…¬å¼€æ¼æ´ã€çŸ¥è¯†åº“çˆ¬è™«å’Œæœç´¢ã€‚å…¨éƒ¨å…¬å¼€æ¼æ´çš„åˆ—è¡¨å’Œæ¯ä¸ªæ¼æ´çš„æ–‡æœ¬å†…å®¹å­˜åœ¨mongodbä¸­ï¼Œå¤§æ¦‚çº¦2Gå†…å®¹ï¼›å¦‚æœæ•´ç«™çˆ¬å…¨éƒ¨æ–‡æœ¬å’Œå›¾ç‰‡ä½œä¸ºç¦»çº¿æŸ¥è¯¢ï¼Œå¤§æ¦‚éœ€è¦10Gç©ºé—´ã€2å°æ—¶ï¼ˆ10Mç”µä¿¡å¸¦å®½ï¼‰ï¼›çˆ¬å–å…¨éƒ¨çŸ¥è¯†åº“ï¼Œæ€»å…±çº¦500Mç©ºé—´ã€‚æ¼æ´æœç´¢ä½¿ç”¨äº†Flaskä½œä¸ºweb serverï¼Œbootstrapä½œä¸ºå‰ç«¯ã€‚ https://github.com/fankcoder/findtripæœºç¥¨çˆ¬è™«ï¼ˆå»å“ªå„¿å’Œæºç¨‹ç½‘ï¼‰ã€‚Findtripæ˜¯ä¸€ä¸ªåŸºäºScrapyçš„æœºç¥¨çˆ¬è™«ï¼Œç›®å‰æ•´åˆäº†å›½å†…ä¸¤å¤§æœºç¥¨ç½‘ç«™ï¼ˆå»å“ªå„¿ + æºç¨‹ï¼‰ã€‚ https://github.com/leyle/163spideråŸºäºrequestsã€MySQLdbã€torndbçš„ç½‘æ˜“å®¢æˆ·ç«¯å†…å®¹çˆ¬è™« https://github.com/fanpei91/doubanspidersè±†ç“£ç”µå½±ã€ä¹¦ç±ã€å°ç»„ã€ç›¸å†Œã€ä¸œè¥¿ç­‰çˆ¬è™«é›† https://github.com/LiuXingMing/QQSpiderQQç©ºé—´çˆ¬è™«ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€è¯´è¯´ã€ä¸ªäººä¿¡æ¯ç­‰ï¼Œä¸€å¤©å¯æŠ“å– 400 ä¸‡æ¡æ•°æ®ã€‚ https://github.com/Shu-Ji/baidu-music-spiderç™¾åº¦mp3å…¨ç«™çˆ¬è™«ï¼Œä½¿ç”¨redisæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€‚ https://github.com/pakoo/tbcrawleræ·˜å®å’Œå¤©çŒ«çš„çˆ¬è™«,å¯ä»¥æ ¹æ®æœç´¢å…³é”®è¯,ç‰©å“idæ¥æŠ“å»é¡µé¢çš„ä¿¡æ¯ï¼Œæ•°æ®å­˜å‚¨åœ¨mongodbã€‚ https://github.com/benitoro/stockholmä¸€ä¸ªè‚¡ç¥¨æ•°æ®ï¼ˆæ²ªæ·±ï¼‰çˆ¬è™«å’Œé€‰è‚¡ç­–ç•¥æµ‹è¯•æ¡†æ¶ã€‚æ ¹æ®é€‰å®šçš„æ—¥æœŸèŒƒå›´æŠ“å–æ‰€æœ‰æ²ªæ·±ä¸¤å¸‚è‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®ã€‚æ”¯æŒä½¿ç”¨è¡¨è¾¾å¼å®šä¹‰é€‰è‚¡ç­–ç•¥ã€‚æ”¯æŒå¤šçº¿ç¨‹å¤„ç†ã€‚ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶ã€CSVæ–‡ä»¶ã€‚ https://github.com/k1995/BaiduyunSpiderç™¾åº¦äº‘ç›˜çˆ¬è™«]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH å®‰è£…spark2.2]]></title>
    <url>%2F2017%2F08%2F05%2FCDH%20%E5%AE%89%E8%A3%85spark2.2%2F</url>
    <content type="text"><![CDATA[å®˜æ–¹é“¾æ¥https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html 1.ä¸‹è½½Spark2 CSDhttps://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging 1.1.1 CSDç¬”è€…ä¸‹è½½2.2ç‰ˆæœ¬http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar1.1.2 Parcelhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/ 1.1.2.1 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcelhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel 1.1.2.2 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel.sha1ç„¶åå°†SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1æ”¹åä¸ºSPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha 1.1.2.3 manifest.jsonhttp://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/manifest.json åœæ­¢æœåŠ¡/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server stop/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent stop å°†csdæ–‡ä»¶æ”¾åˆ°/soft/bigdata/clouderamanager/cloudera/csd å°†parcelæ–‡ä»¶æ”¾åˆ°/soft/bigdata/clouderamanager/cloudera/parcel-repo ä¿®æ”¹æƒé™chgrp cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jarchown cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jar å¼€å¯æœåŠ¡/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start ä¸»æœº-&gt; Parcelå°±èƒ½çœ‹åˆ°spark2äº† åˆ†é…ï¼Œæ¿€æ´»ã€‚ç„¶åå°±å¯ä»¥æ·»åŠ æœåŠ¡äº† å¦‚æœæ·»åŠ æœåŠ¡ä¸æˆåŠŸéœ€è¦æŠŠjaræ–‡ä»¶æ”¾åˆ°/opt/cloudera/csd]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 2.2.0æºç ç¼–è¯‘]]></title>
    <url>%2F2017%2F08%2F02%2Fspark%202.2.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[å®˜ç½‘ä¸‹è½½sparkæºç https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz ç„¶ååœ¨ideaä¸­å¯¼å…¥sparkæºç é¡¹ç›®(idea mavené…ç½®æ­£ç¡®)ï¼Œç„¶åå¯¹sparké¡¹ç›®buildã€‚BuildæˆåŠŸååœ¨è¿›è¡Œç¼–è¯‘ã€‚Buildè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š æ‰¾ä¸åˆ°org.apache.spark.streaming.flume.sink.SparkFlumeProtocolæ‰¾ä¸åˆ°org.apache.spark.sql.catalyst.parser.SqlBaseParser è®¾ç½®mavençš„å‚æ•°ï¼Œå¦åˆ™ä¸€ç›´å‡ºç°outofMemoryåœ¨apache-maven-3.3.9-bin\binä¸‹é¢çš„mvn.cmdæ–‡ä»¶é‡Œçš„ï¼š@REM set MAVEN_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000ä¸‹é¢æ·»åŠ set MAVEN_OPTS= -Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m åœ¨git bash é‡Œç¼–è¯‘è¿›å…¥sparkæºç ç›®å½•mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½¿ç”¨Ambariå®‰è£…éƒ¨ç½²Sparké›†ç¾¤]]></title>
    <url>%2F2017%2F06%2F30%2F%E4%BD%BF%E7%94%A8Ambari%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Spark%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[ä¸‹è½½è¿›å…¥å®˜ç½‘ï¼Œé€‰æ‹©äº§å“é‡Œé¢çš„ä¸‹è½½é¡µé¢æˆ–è€…ç›´æ¥ç™»å½•https://hortonworks.com/downloads/é€‰æ‹©HDPÂ® 2.6: Ready for the enterpriseä¸‹é¢çš„Automated Install Guide å› ä¸ºåšä¸»æ˜¯CENTOS 7åœ¨è¿™ä¸ªé¡µé¢ç›´æ¥é€‰æ‹©ï¼šRHEL/CentOS/Oracle Linux 7 wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo -O /etc/yum.repos.d/ambari.repoyum repolist yum install ambari-server è¿›å…¥â€‹Setup Optionsambari-server setup â€“j /usr/java/defaultæç¤ºå‚æ•°åªèƒ½æ˜¯ä¸€ä¸ªï¼Œçœ‹æ¥jdkè¦åè®¾ç½® ambari-server setupå„ç§å›è½¦ï¼Œjdkçš„æ—¶å€™é€‰æ‹©å®‰è£…1.8çš„å¦‚æœè¾“å…¥è‡ªå®šä¹‰çš„jdkè¦æ³¨æ„æƒé™é—®é¢˜ å¯åŠ¨ï¼šambari-server start æŸ¥çœ‹ï¼š[root@storm01 storm]# lsof -i:8080COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 7648 storm 1438u IPv6 124470 0t0 TCP *:webcache (LISTEN) å…³é—­é˜²ç«å¢™systemctl disable firewalld.servicesystemctl stop firewalld.service åœ¨æµè§ˆå™¨è¾“å…¥http://storm01:8080å³å¯è¿›å…¥UIç”¨æˆ·å¯†ç éƒ½ä¸ºadmin åœ¨æ¯ä¸ªæœºå™¨ä¸Šå®‰è£…agentyum install -y ambari-agent ä¿®æ”¹é…ç½®æ–‡ä»¶ vi /etc/ambari-agent/conf/ambari-agent.ini hostname=localhostæ›´æ”¹ä¸º hostname=storm01 å¯åŠ¨agent service ambari-agent start è®¾ç½®hiveç”¨æˆ·å¯†ç éƒ½ä¸ºhive éœ€è¦PostgreSQLæ”¯æŒè¿œç¨‹è¿æ¥find / -name pg_hba.conf vi /var/lib/pgsql/data/pg_hba.confå°†local all postgres peeræ”¹æˆlocal all postgres trusthost all postgres 127.0.0.1/32 identæ”¹æˆhost all postgres 127.0.0.1/32 trustlocal all ambari,mapred md5æ”¹æˆlocal all ambari,mapred,hive trusthost all ambari,mapred 0.0.0.0/0 md5æ”¹æˆhost all ambari,mapred,hive 0.0.0.0/0 md5host all ambari,mapred ::/0 md5æ”¹æˆhost all ambari,mapred,hive ::/0 md5 é‡å¯æ•°æ®åº“service postgresql restart æ£€æŸ¥lsof -i:5432 å»ä»“åº“ä¸‹è½½postgresql http://mvnrepository.com/ ä¸‹è½½ PostgreSQL JDBC Driver JDBC 4.2çš„9.2-1002-jdbc4 æˆ–è€…ç›´æ¥è¾“å…¥ï¼šwget http://central.maven.org/maven2/org/postgresql/postgresql/9.2-1002-jdbc4/postgresql-9.2-1002-jdbc4.jar ambari-server setup â€“jdbc-db=postgres â€“jdbc-driver=/root/postgresql-9.2-1002-jdbc4.jar åˆ›å»ºç”¨æˆ·psql -U postgres -d postgrescreate user hive;alter user hive password â€˜hiveâ€™;create database hive;grant all on database hive to hive;alter database hive owner to hive; æ­¤æ—¶test ConnectionConnection å°±å¯ä»¥æµ‹è¯•æˆåŠŸ è®¾ç½®Ambari Metricså’ŒSmart Senseç”¨æˆ·å¯†ç éƒ½ä¸ºadmin å¦‚æœå‘ç°å¤±è´¥ï¼Œéœ€è¦è®¾ç½®/etc/hostsæ–‡ä»¶192.168.247.180 spark01192.168.247.181 spark02192.168.247.182 spark03 å®‰è£…å®Œæˆ]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phoenixåˆ›å»ºäºŒçº§ç´¢å¼•]]></title>
    <url>%2F2017%2F05%2F25%2Fphoenix%E5%88%9B%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[ç´¢å¼•åˆ†ä¸ºä¸¤ç§ï¼šGlobal IndexingLocal Indexing Global Indexing ï¼šä½¿ç”¨äºè¯»å¤šï¼Œå†™å°‘çš„åœºæ™¯.select æŸ¥å‡ºæ¥çš„æ•°æ®å¿…é¡»æ˜¯ç´¢å¼•å­—æ®µæ‰èƒ½ä½¿ç”¨åˆ°ç´¢å¼• Local Indexing ï¼šhbase bulk loadæ•°æ®ä¸Šä¼ åˆ°hdfsï¼š/phoenix_data/fanpu.csv HADOOP_CLASSPATH=$(hbase classpath) hadoop jar phoenix-4.8.0-cdh5.8.0-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool-libjars /soft/phoenix/phoenix-4.8.0-cdh5.8.0/lib/commons-csv-1.0.jar,/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib/joda-time-2.7.jar â€“table fanpu â€“input /phoenix_data/fanpu.csv æˆ‘å¦‚æœä½¿ç”¨Global Indexingï¼šåˆ›å»ºç´¢å¼•ï¼šcreate index fanpu_index on fanpu (â€œfamilyâ€.â€id_noâ€,â€familyâ€.â€nameâ€,â€familyâ€.â€mobileâ€); select from fanpu where â€œfamilyâ€.â€id_noâ€=â€™xxxxâ€™ and â€œfamilyâ€.â€nameâ€=â€™é©¬æ–‡å­¦â€™ and â€œfamilyâ€.â€mobileâ€=â€™xxxxâ€™; æ˜¯ä¸èƒ½ä½¿ç”¨ç´¢å¼•çš„*Ã— select â€œfamilyâ€.â€id_noâ€,â€familyâ€.â€nameâ€ from fanpu where â€œfamilyâ€.â€id_noâ€=â€™xxxxâ€™ and â€œfamilyâ€.â€nameâ€=â€™é©¬æ–‡å­¦â€™ and â€œfamilyâ€.â€mobileâ€=â€™xxxxâ€™; æ‰å¯ä»¥âˆš 12345678910111213141516171819202122230: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_RECEIPT_NO&quot;);76,392 rows affected (6.256 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER on &quot;LENDER-LEN_TRANSFER&quot; (&quot;family&quot;.&quot;LEN_LOAN_SELL_CONFIRM_ID&quot;);99,937 rows affected (6.256 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER_DTL on &quot;LENDER-LEN_TRANSFER_DTL&quot; (&quot;family&quot;.&quot;LENDERINFOID&quot;,&quot;family&quot;.&quot;LOAN_TRANSFER_MODEL_ID&quot;);153,029 rows affected (6.239 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL2 on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_RECEIPT_NO&quot;);76,392 rows affected (6.231 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_LOAN_DTL3 on &quot;LENDER-LEN_LENDER_LOAN_DTL&quot; (&quot;family&quot;.&quot;LOAN_ID&quot;);76,392 rows affected (6.234 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LOAN_MAIN on &quot;LENDER-LEN_LOAN_MAIN&quot; (&quot;family&quot;.&quot;ASSUME_DEBTOR_ID&quot;);20,865 rows affected (6.233 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LOAN_MAIN2 on &quot;LENDER-LEN_LOAN_MAIN&quot; (&quot;family&quot;.&quot;LOAN_CONTRACT_ID&quot;);20,865 rows affected (6.231 seconds)0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_LENDER_INFO on &quot;LENDER-LEN_LENDER_INFO&quot; (&quot;family&quot;.&quot;CUSTOMER_ID&quot;);17,053 rows affected (6.235 seconds) 0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_SELL_CONFIRM on â€œLENDER-LEN_SELL_CONFIRMâ€ (â€œIDâ€);9,417 rows affected (6.245 seconds) 0: jdbc:phoenix:slave1:2181&gt; create index my_index_LEN_TRANSFER3 on â€œLENDER-LEN_TRANSFERâ€ (â€œfamilyâ€.â€SELLER_LOAN_DTL_IDâ€);99,937 rows affected (6.251 seconds)]]></content>
      <categories>
        <category>phoenix</category>
      </categories>
      <tags>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH oozie]]></title>
    <url>%2F2017%2F05%2F18%2FCDH%20oozie%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[å®‰è£…å®Œoozieï¼Œæ‰“å¼€UI:http://master1:11000/oozie/ æ˜¾ç¤ºOozie web console is disabled.è§£å†³æ–¹æ¡ˆ;åŸå› æ˜¯oozieçš„/var/lib/oozieç›®å½•é‡Œç¼ºå°‘EXTçš„åŒ… ç‚¹å‡»Documentationé“¾æ¥é‡Œquickstartç»™å‡ºäº†è§£å†³æ–¹æ¡ˆDownload ExtJS library (it must be version 2.2) ä¸‹è½½åœ°å€http://dev.sencha.com/deploy/ext-2.2.zip å¦‚æœä¸‹è½½ä¸ä¸‹æ¥å¯ä»¥è¯•è¯•CSDNhttp://download.csdn.net/download/start_baby/6280675æˆ–è€…ï¼šhttp://archive.cloudera.com/gplextras/misc/ext-2.2.zip unzipè§£å‹ç„¶ååˆ·æ–°é¡µé¢æˆåŠŸè¿›å…¥oozieçš„webç•Œé¢ ç¼–å†™job.propertiesï¼ˆnameNodeè¦å½“æ—¶activeçš„ï¼‰123456nameNode=hdfs://master2:8020jobTracker=master2:8032queueName=defaultexamplesRoot=user/oozie/my-apps/shelloozie.wf.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/workflow.xmlEXEC=emp-join-demp.sh ç¼–å†™workflow.xml1234567891011121314151617181920212223&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt; &lt;start to=&quot;shell-node&quot;/&gt; &lt;action name=&quot;shell-node&quot;&gt; &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt; &lt;file&gt;$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;!--Copy the executable to compute node&apos;s current working directory --&gt; &lt;/shell&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; ç¼–å†™emp-join-demp.sh12#!/bin/shjava -cp PhoenixAPI-1.0-SNAPSHOT.jar com.zwjf.Month [hdfs@master1 data]$ hadoop fs -mkdir -p /user/oozie/my-apps/shell[hdfs@master1 data]$ hadoop fs -put workflow.xml /user/oozie/my-apps/shell[hdfs@master1 data]$ hadoop fs -put emp-join-demp.sh /user/oozie/my-apps/shell ä¸Šä¼ æˆ‘shellè„šæœ¬é‡Œæ‰§è¡Œçš„jaråŒ…ï¼Œä½ ä»¬æ ¹æ®è‡ªå·±çš„shellå†³å®šå¦‚ä½•æ“ä½œhadoop fs -put PhoenixAPI-1.0-SNAPSHOT.jar /user/oozie/my-apps/shell [hdfs@master1 data]$ /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie job -oozie http://master1:11000/oozie -config /soft/data/job.properties -run æäº¤å‡ºé”™ï¼šå»å†å²æœåŠ¡å™¨æŸ¥çœ‹é—®é¢˜http://master2:19888/jobhistory æŠ¥é”™ï¼šLauncher ERROR, reason: Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1] æ˜¯å› ä¸ºåˆ†å‘çš„æ—¶å€™æ‰¾ä¸åˆ°ç”¨æˆ·çš„jaråŒ…ï¼Œåœ¨workflow.xmlçš„ ${nameNode}/${examplesRoot}/${EXEC}#${EXEC}æ·»åŠ ä½ çš„jaråŒ…å¹¶ä¸Šä¼ hdfså°±å¯ä»¥## ä¾‹å¦‚ï¼š${nameNode}/${examplesRoot}/PhoenixAPI-1.0-SNAPSHOT.jar#PhoenixAPI-1.0-SNAPSHOT.jar å¯å‚è€ƒå¦‚ä¸‹æ–‡ç« http://blog.sina.com.cn/s/blog_e699b42b0102xh3o.html å¢åŠ å®šæ—¶ä»»åŠ¡job.propertieså¢åŠ 123oozie.coord.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/coordinator.xmlstart=2017-05-18T16:30Zend=2019-07-30T16:00Z oozie.wf.application.path=${nameNode}/${examplesRoot}/workflow.xmlæ”¹æˆworkflowAppUri=${nameNode}/${examplesRoot}/workflow.xml æ–°å»ºcoordinator.xml12345678910111213141516171819202122232425&lt;coordinator-app name=&quot;coordinator&quot; frequency=&quot;$&#123;coord:minutes(10)&#125;&quot; start=&quot;$&#123;start&#125;&quot; end=&quot;$&#123;end&#125;&quot; timezone=&quot;Asia/Shanghai&quot; xmlns=&quot;uri:oozie:coordinator:0.2&quot;&gt; &lt;action&gt; &lt;workflow&gt; &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;jobTracker&lt;/name&gt; &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;EXEC&lt;/name&gt; &lt;value&gt;$&#123;EXEC&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;nameNode&lt;/name&gt; &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;queueName&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/workflow&gt; &lt;/action&gt;&lt;/coordinator-app&gt; ä¸Šä¼ hadoop fs -put coordinator.xml /user/oozie/my-apps/shell åœæ­¢ä»»åŠ¡ï¼š/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie job -oozie http://master1:11000/oozie -kill 0000005-170518154227460-oozie-oozi-C æ³¨ï¼šè®¾ç½®çš„æ—¶é—´ä¸èƒ½å°äºå½“å‰æ—¶é—´ï¼Œå¦åˆ™ä¼šæŠŠä¹‹å‰æ²¡æ‰§è¡Œçš„éƒ½æ‰§è¡Œ]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafkaæºç é˜…è¯»2]]></title>
    <url>%2F2017%2F05%2F08%2Fkafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB2%2F</url>
    <content type="text"><![CDATA[å¯¼å…¥IDEAå³å¯çœ‹kafkaæºç ï¼šå¯åŠ¨ä¹‹å‰éœ€è¦å®‰è£…zookeeperåœ°å€ï¼šhttp://apache.fayea.com/zookeeper/http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz è§£å‹åå†å½“å‰ç›®å½•å¢åŠ dataLogDirå’Œdataç›®å½•å¤åˆ¶ä¸€ä»½é…ç½®æ–‡ä»¶æ”¹åä¸ºzoo.cfgä¿®æ”¹é…ç½®æ–‡ä»¶ï¼šzoo.cfgä¿®æ”¹å¹¶å¢åŠ dataDir=D:\tool\zookeeper-3.4.6\datadataLogDir=D:\tool\zookeeper-3.4.6\dataLogDir å¯åŠ¨zkServer.cmd å¯åŠ¨ZkCli.cmd kafkaå¯åŠ¨ã€‚åœ¨é…ç½®æ–‡ä»¶ä¿®æ”¹Program argumentsï¼šconfig/server.properties ç„¶åä¿®æ”¹server.propertiesé‡Œé¢çš„å‚æ•°å³å¯ã€‚ å¯åŠ¨å‰ï¼š å¯åŠ¨åï¼š æºç é˜…è¯»ï¼ˆä¸€ï¼‰ä»å¯åŠ¨å…¥å£åˆ†æï¼šKafka.scala 12345678910111213141516171819202122232425262728293031323334def main(args: Array[String]): Unit = &#123; try &#123; /*ä»é…ç½®æ–‡ä»¶è¯»å–kafkaæœåŠ¡å™¨å¯åŠ¨å‚æ•°--å°†ä¼ å…¥çš„å‚æ•°è½¬æ¢æˆProperties æ–‡ä»¶ï¼Œå¦‚æœå‚æ•°ä¸ºç©ºå°†æç¤ºï¼š * USAGE: java [options] KafkaServer server.properties [--override property=value]* * Option Description * ------ ----------- * --override &lt;String&gt; Optional property that should override values set in * server.properties file * * åˆ¤æ–­å‚æ•°æ˜¯å¦å¤§äº1ï¼Œå°†åé¢çš„å‚æ•°æ”¾åˆ°Propertiesé‡Œ * */ val serverProps = getPropsFromArgs(args) //åˆ›å»ºKafkaServerStartableå¯¹è±¡ val kafkaServerStartable = KafkaServerStartable.fromProps(serverProps) // attach shutdown handler to catch control-c Runtime.getRuntime().addShutdownHook(new Thread() &#123; override def run() = &#123; kafkaServerStartable.shutdown &#125; &#125;) kafkaServerStartable.startup kafkaServerStartable.awaitShutdown &#125; catch &#123; case e: Throwable =&gt; fatal(e) System.exit(1) &#125; System.exit(0) &#125; è¿™ä¸Šé¢æœ‰ä¸ªå°çŸ¥è¯†ç‚¹ï¼š_* å‘Šè¯‰ç¼–è¯‘å™¨ä½ å¸Œæœ›å°†æŸä¸ªå‚æ•°å½“ä½œå‚æ•°åºåˆ—å¤„ç†123456def echo(args: String*) = for (arg &lt;- args) println(arg)def main(args: Array[String]) = &#123; var args = Array("config/server.properties","canshu1","canshu2") echo(args.slice(1, args.length): _*)&#125; è¾“å‡ºæ˜¯canshu1canshu2 kafkaServerStartableå°è£…äº†KafkaServer 1.å…·ä½“çš„å¯åŠ¨ç±»åœ¨ï¼šKafkaServerStartableçš„startupæ–¹æ³•123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124def startup() &#123; try &#123; info("starting") if(isShuttingDown.get) throw new IllegalStateException("Kafka server is still shutting down, cannot re-start!") if(startupComplete.get) return val canStartup = isStartingUp.compareAndSet(false, true) if (canStartup) &#123; brokerState.newState(Starting) /* start scheduler */ kafkaScheduler.startup() /* setup zookeeper */ zkUtils = initZk() /* Get or create cluster_id */ _clusterId = getOrGenerateClusterId(zkUtils) info(s"Cluster ID = $clusterId") /* generate brokerId */ config.brokerId = getBrokerId this.logIdent = "[Kafka Server " + config.brokerId + "], " /* create and configure metrics */ val reporters = config.getConfiguredInstances(KafkaConfig.MetricReporterClassesProp, classOf[MetricsReporter], Map[String, AnyRef](KafkaConfig.BrokerIdProp -&gt; (config.brokerId.toString)).asJava) reporters.add(new JmxReporter(jmxPrefix)) val metricConfig = KafkaServer.metricConfig(config) metrics = new Metrics(metricConfig, reporters, time, true) quotaManagers = QuotaFactory.instantiate(config, metrics, time) notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala) /* start log manager */ logManager = createLogManager(zkUtils.zkClient, brokerState) logManager.startup() metadataCache = new MetadataCache(config.brokerId) credentialProvider = new CredentialProvider(config.saslEnabledMechanisms) socketServer = new SocketServer(config, metrics, time, credentialProvider) socketServer.startup() /* start replica manager */ replicaManager = new ReplicaManager(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower) replicaManager.startup() /* start kafka controller */ kafkaController = new KafkaController(config, zkUtils, brokerState, time, metrics, threadNamePrefix) kafkaController.startup() adminManager = new AdminManager(config, metrics, metadataCache, zkUtils) /* start group coordinator */ // Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM) groupCoordinator.startup() /* Get the authorizer and initialize it if one is specified.*/ authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt; val authZ = CoreUtils.createObject[Authorizer](authorizerClassName) authZ.configure(config.originals()) authZ &#125; /* start processing requests */ apis = new KafkaApis(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator, kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers, clusterId, time) requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, time, config.numIoThreads) Mx4jLoader.maybeLoad() /* start dynamic config manager */ dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers), ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers), ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider), ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers)) // Create the config manager. start listening to notifications dynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers) dynamicConfigManager.startup() /* tell everyone we are alive */ val listeners = config.advertisedListeners.map &#123; endpoint =&gt; if (endpoint.port == 0) endpoint.copy(port = socketServer.boundPort(endpoint.listenerName)) else endpoint &#125; kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack, config.interBrokerProtocolVersion) kafkaHealthcheck.startup() // Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it checkpointBrokerId(config.brokerId) /* register broker metrics */ registerStats() brokerState.newState(RunningAsBroker) shutdownLatch = new CountDownLatch(1) startupComplete.set(true) isStartingUp.set(false) AppInfoParser.registerAppInfo(jmxPrefix, config.brokerId.toString) info("started") &#125; &#125; catch &#123; case e: Throwable =&gt; fatal("Fatal error during KafkaServer startup. Prepare to shutdown", e) isStartingUp.set(false) shutdown() throw e &#125; &#125; Starting ç»§æ‰¿BrokerStatesï¼ŒBrokerStatesæ˜¯ä¸€ä¸ªsealed trait1234567sealed trait BrokerStates &#123; def state: Byte &#125;case object NotRunning extends BrokerStates &#123; val state: Byte = 0 &#125;case object Starting extends BrokerStates &#123; val state: Byte = 1 &#125;case object RecoveringFromUncleanShutdown extends BrokerStates &#123; val state: Byte = 2 &#125;case object RunningAsBroker extends BrokerStates &#123; val state: Byte = 3 &#125;case object PendingControlledShutdown extends BrokerStates &#123; val state: Byte = 6 &#125;case object BrokerShuttingDown extends BrokerStates &#123; val state: Byte = 7 &#125; traitå®šä¹‰ä¸ºsealed æœ‰ä¸¤å±‚å«ä¹‰1.å…¶ä¿®é¥°çš„trait classåªèƒ½åœ¨å½“å‰æ–‡ä»¶é‡Œé¢è¢«ç»§æ‰¿2.ç”¨sealedä¿®é¥°è¿™æ ·åšçš„ç›®çš„æ˜¯å‘Šè¯‰scalaç¼–è¯‘å™¨åœ¨æ£€æŸ¥æ¨¡å¼åŒ¹é…çš„æ—¶å€™ï¼Œè®©scalaçŸ¥é“è¿™äº›caseçš„æ‰€æœ‰æƒ…å†µï¼Œscalaå°±èƒ½å¤Ÿåœ¨ç¼–è¯‘çš„æ—¶å€™è¿›è¡Œæ£€æŸ¥ï¼Œçœ‹ä½ å†™çš„ä»£ç æ˜¯å¦æœ‰æ²¡æœ‰æ¼æ‰ä»€ä¹ˆæ²¡caseåˆ°ï¼Œå‡å°‘ç¼–ç¨‹çš„é”™è¯¯ã€‚ 2.start scheduler:1kafkaScheduler.startup() 12345678910111213141516171819202122class KafkaScheduler(val threads: Int, val threadNamePrefix: String = "kafka-scheduler-", daemon: Boolean = true) extends Scheduler with Logging &#123; private var executor: ScheduledThreadPoolExecutor = null private val schedulerThreadId = new AtomicInteger(0) override def startup() &#123; debug("Initializing task scheduler.") this synchronized &#123; if(isStarted) throw new IllegalStateException("This scheduler has already been started!") executor = new ScheduledThreadPoolExecutor(threads) executor.setContinueExistingPeriodicTasksAfterShutdownPolicy(false) executor.setExecuteExistingDelayedTasksAfterShutdownPolicy(false) executor.setThreadFactory(new ThreadFactory() &#123; def newThread(runnable: Runnable): Thread = Utils.newThread(threadNamePrefix + schedulerThreadId.getAndIncrement(), runnable, daemon) &#125;) &#125; &#125; ...&#125; 1.æ­¤å¤„ä½¿ç”¨äº†åŒæ­¥é”ï¼Œå¦‚æœå·²ç»å¯åŠ¨ï¼Œç›´æ¥æŠ›ä¸ªIllegalStateExceptionå¼‚å¸¸ï¼Œç”±å¤–é¢é€šç”¨å¼‚å¸¸Throwableæ•è·ã€‚2.æ ¹æ®é…ç½®æ–‡ä»¶çš„background.threads åˆ›å»ºä¸€ä¸ªScheduledThreadPoolExecutor(threads)ã€java.util.concurrentåŒ…ä¸‹çš„ã€‘ 3.setup zookeeper1zkUtils = initZk() 123456789101112131415161718192021222324252627282930313233private def initZk(): ZkUtils = &#123; info(s"Connecting to zookeeper on $&#123;config.zkConnect&#125;") val chrootIndex = config.zkConnect.indexOf("/") val chrootOption = &#123; if (chrootIndex &gt; 0) Some(config.zkConnect.substring(chrootIndex)) else None &#125; val secureAclsEnabled = config.zkEnableSecureAcls val isZkSecurityEnabled = JaasUtils.isZkSecurityEnabled() if (secureAclsEnabled &amp;&amp; !isZkSecurityEnabled) throw new java.lang.SecurityException(s"$&#123;KafkaConfig.ZkEnableSecureAclsProp&#125; is true, but the verification of the JAAS login file failed.") chrootOption.foreach &#123; chroot =&gt; val zkConnForChrootCreation = config.zkConnect.substring(0, chrootIndex) val zkClientForChrootCreation = ZkUtils(zkConnForChrootCreation, sessionTimeout = config.zkSessionTimeoutMs, connectionTimeout = config.zkConnectionTimeoutMs, secureAclsEnabled) zkClientForChrootCreation.makeSurePersistentPathExists(chroot) info(s"Created zookeeper path $chroot") zkClientForChrootCreation.zkClient.close() &#125; val zkUtils = ZkUtils(config.zkConnect, sessionTimeout = config.zkSessionTimeoutMs, connectionTimeout = config.zkConnectionTimeoutMs, secureAclsEnabled) zkUtils.setupCommonPaths() zkUtils &#125; 1.åˆ›å»ºè¿æ¥123456789val persistentZkPaths = Seq(ConsumersPath, BrokerIdsPath, BrokerTopicsPath, ConfigChangesPath, getEntityConfigRootPath(ConfigType.Topic), getEntityConfigRootPath(ConfigType.Client), DeleteTopicsPath, BrokerSequenceIdPath, IsrChangeNotificationPath) 2.è®¾ç½®é€šç”¨è·¯å¾„/consumers/brokers/ids/brokers/topics/config/changes/config/topics/config/clients/admin/delete_topics/brokers/seqid/isr_change_notification ISRï¼šKafkaåœ¨Zookeeperä¸­åŠ¨æ€ç»´æŠ¤äº†ä¸€ä¸ªISRï¼ˆin-sync replicasï¼‰ setï¼Œè¿™ä¸ªseté‡Œçš„æ‰€æœ‰replicaéƒ½è·Ÿä¸Šäº†leaderï¼Œåªæœ‰ISRé‡Œçš„æˆå‘˜æ‰æœ‰è¢«é€‰ä¸ºleaderçš„å¯èƒ½ 4.Get or create cluster_id12_clusterId = getOrGenerateClusterId(zkUtils)info(s"Cluster ID = $clusterId") 1234567891011121314151617181920212223242526 def getOrGenerateClusterId(zkUtils: ZkUtils): String = &#123; zkUtils.getClusterId.getOrElse(zkUtils.createOrGetClusterId(CoreUtils.generateUuidAsBase64)) &#125; def createOrGetClusterId(proposedClusterId: String): String = &#123; try &#123; createPersistentPath(ClusterIdPath, ClusterId.toJson(proposedClusterId)) proposedClusterId &#125; catch &#123; case _: ZkNodeExistsException =&gt; getClusterId.getOrElse(throw new KafkaException("Failed to get cluster id from Zookeeper. This can only happen if /cluster/id is deleted from Zookeeper.")) &#125; &#125;/** * Create an persistent node with the given path and data. Create parents if necessary. */ def createPersistentPath(path: String, data: String = "", acls: java.util.List[ACL] = UseDefaultAcls): Unit = &#123; val acl = if (acls eq UseDefaultAcls) ZkUtils.defaultAcls(isSecure, path) else acls try &#123; ZkPath.createPersistent(zkClient, path, data, acl) &#125; catch &#123; case _: ZkNoNodeException =&gt; createParentPath(path) ZkPath.createPersistent(zkClient, path, data, acl) &#125; &#125; æ­¤å¤„ä¼šåˆ›å»ºä¸€ä¸ªpersistentèŠ‚ç‚¹/cluster/id å¦‚æœèŠ‚ç‚¹å·²ç»å­˜åœ¨ï¼Œåˆ™åˆ¨é™¤å¼‚å¸¸ï¼Œä¸Šæ¬¡è·å–å¼‚å¸¸ï¼Œç„¶åå»èŠ‚ç‚¹ä¸‹è·å–_clusterIdï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºçš„proposedClusterIdè¿”å› æ­¤å¤„éœ€è¦æ³¨æ„zookeeperçš„èŠ‚ç‚¹ç±»å‹åˆ†ä¸ºï¼šæŒä¹…èŠ‚ç‚¹ï¼ˆPERSISTENTï¼‰æŒä¹…é¡ºåºèŠ‚ç‚¹ï¼ˆPERSISTENT_SEQUENTIALï¼‰ä¸´æ—¶èŠ‚ç‚¹ï¼ˆEPHEMERALï¼‰ä¸´æ—¶é¡ºåºèŠ‚ç‚¹ï¼ˆEPHEMERAL_SEQUENTIALï¼‰é¡ºåºèŠ‚ç‚¹å³åˆ›å»ºæœ‰åºçš„èŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ååŠ ä¸Šä¸€ä¸ªæ•°å­—åç¼€ã€‚ä¸´æ—¶èŠ‚ç‚¹å’Œå®¢æˆ·ç«¯ç»‘å®šï¼Œä¼šè¯å¤±æ•ˆï¼ˆéè¿æ¥æ–­å¼€ï¼‰åˆ™è‡ªåŠ¨æ¸…æ¥š ä¸´æ—¶é¡ºåºèŠ‚ç‚¹å¯ç”¨æ¥å®ç°åˆ†å¸ƒå¼é”1.å®¢æˆ·ç«¯è°ƒç”¨create()æ–¹æ³•åˆ›å»ºåä¸ºâ€œlocknode/guid-lock-â€çš„èŠ‚ç‚¹ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡ŒèŠ‚ç‚¹çš„åˆ›å»ºç±»å‹éœ€è¦è®¾ç½®ä¸ºEPHEMERAL_SEQUENTIALã€‚2.å®¢æˆ·ç«¯è°ƒç”¨getChildren(â€œlocknodeâ€)æ–¹æ³•æ¥è·å–æ‰€æœ‰å·²ç»åˆ›å»ºçš„å­èŠ‚ç‚¹ï¼Œæ³¨æ„ï¼Œè¿™é‡Œä¸æ³¨å†Œä»»ä½•Watcherã€‚3.å®¢æˆ·ç«¯è·å–åˆ°æ‰€æœ‰å­èŠ‚ç‚¹pathä¹‹åï¼Œå¦‚æœå‘ç°è‡ªå·±åœ¨æ­¥éª¤1ä¸­åˆ›å»ºçš„èŠ‚ç‚¹åºå·æœ€å°ï¼Œé‚£ä¹ˆå°±è®¤ä¸ºè¿™ä¸ªå®¢æˆ·ç«¯è·å¾—äº†é”ã€‚4.å¦‚æœåœ¨æ­¥éª¤3ä¸­å‘ç°è‡ªå·±å¹¶éæ‰€æœ‰å­èŠ‚ç‚¹ä¸­æœ€å°çš„ï¼Œè¯´æ˜è‡ªå·±è¿˜æ²¡æœ‰è·å–åˆ°é”ã€‚æ­¤æ—¶å®¢æˆ·ç«¯éœ€è¦æ‰¾åˆ°æ¯”è‡ªå·±å°çš„é‚£ä¸ªèŠ‚ç‚¹ï¼Œç„¶åå¯¹å…¶è°ƒç”¨exist()æ–¹æ³•ï¼ŒåŒæ—¶æ³¨å†Œäº‹ä»¶ç›‘å¬ã€‚5.ä¹‹åå½“è¿™ä¸ªè¢«å…³æ³¨çš„èŠ‚ç‚¹è¢«ç§»é™¤äº†ï¼Œå®¢æˆ·ç«¯ä¼šæ”¶åˆ°ç›¸åº”çš„é€šçŸ¥ã€‚è¿™ä¸ªæ—¶å€™å®¢æˆ·ç«¯éœ€è¦å†æ¬¡è°ƒç”¨getChildren(â€œlocknodeâ€)æ–¹æ³•æ¥è·å–æ‰€æœ‰å·²ç»åˆ›å»ºçš„å­èŠ‚ç‚¹ï¼Œç¡®ä¿è‡ªå·±ç¡®å®æ˜¯æœ€å°çš„èŠ‚ç‚¹äº†ï¼Œç„¶åè¿›å…¥æ­¥éª¤3ã€‚ 5.generate brokerId12config.brokerId = getBrokerIdthis.logIdent = "[Kafka Server " + config.brokerId + "], " ç•¥ 6.create and configure metricså†…éƒ¨çŠ¶æ€çš„ç›‘æ§æ¨¡å—12345678val reporters = config.getConfiguredInstances(KafkaConfig.MetricReporterClassesProp, classOf[MetricsReporter], Map[String, AnyRef](KafkaConfig.BrokerIdProp -&gt; (config.brokerId.toString)).asJava)reporters.add(new JmxReporter(jmxPrefix))val metricConfig = KafkaServer.metricConfig(config)metrics = new Metrics(metricConfig, reporters, time, true)quotaManagers = QuotaFactory.instantiate(config, metrics, time)notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala) ä»é…ç½®å‚æ•°metric.reporters è·å–ç›‘æ§ç±» æ­¤å¤„æœ‰ä¸ªå°æ¦‚å¿µ;é›†åˆå…è®¸ä½¿ç”¨asScalaå’ŒasJavaæ–¹æ³•æ¥åšscalaå’Œjavaä¹‹é—´çš„è½¬æ¢ metricConfigé‡Œå°è£…äº†metrics.num.samplesï¼ˆ ç”¨äºç»´æŠ¤metricsçš„æ ·æœ¬æ•°ï¼‰metrics.recording.levelmetrics.sample.window.msï¼ˆmetricsç³»ç»Ÿç»´æŠ¤å¯é…ç½®çš„æ ·æœ¬æ•°é‡ï¼Œåœ¨ä¸€ä¸ªå¯ä¿®æ­£çš„window sizeã€‚è¿™é¡¹é…ç½®é…ç½®äº†çª—å£å¤§å°ï¼Œä¾‹å¦‚ã€‚æˆ‘ä»¬å¯èƒ½åœ¨30sçš„æœŸé—´ç»´æŠ¤ä¸¤ä¸ªæ ·æœ¬ã€‚å½“ä¸€ä¸ªçª—å£æ¨å‡ºåï¼Œæˆ‘ä»¬ä¼šæ“¦é™¤å¹¶é‡å†™æœ€è€çš„çª—å£ï¼‰ å°æ¦‚å¿µï¼š4ç§æ“ä½œç¬¦çš„åŒºåˆ«å’Œè”ç³»:: è¯¥æ–¹æ³•è¢«ç§°ä¸ºconsï¼Œæ„ä¸ºæ„é€ ï¼Œå‘é˜Ÿåˆ—çš„å¤´éƒ¨è¿½åŠ æ•°æ®ï¼Œåˆ›é€ æ–°çš„åˆ—è¡¨ã€‚ç”¨æ³•ä¸º x::list,å…¶ä¸­xä¸ºåŠ å…¥åˆ°å¤´éƒ¨çš„å…ƒç´ ï¼Œæ— è®ºxæ˜¯åˆ—è¡¨ä¸å¦ï¼Œå®ƒéƒ½åªå°†æˆä¸ºæ–°ç”Ÿæˆåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œä¹Ÿå°±æ˜¯è¯´æ–°ç”Ÿæˆçš„åˆ—è¡¨é•¿åº¦ä¸ºlistçš„é•¿åº¦ï¼‹1(BTW ï¼Œ x::listç­‰ä»·äºlist.::(x)):+å’Œ+: ä¸¤è€…çš„åŒºåˆ«åœ¨äº:+æ–¹æ³•ç”¨äºåœ¨å°¾éƒ¨è¿½åŠ å…ƒç´ ï¼Œ+:æ–¹æ³•ç”¨äºåœ¨å¤´éƒ¨è¿½åŠ å…ƒç´ ï¼Œå’Œ::å¾ˆç±»ä¼¼ï¼Œä½†æ˜¯::å¯ä»¥ç”¨äºpattern match ï¼Œè€Œ+:åˆ™ä¸è¡Œ. å…³äº+:å’Œ:+,åªè¦è®°ä½å†’å·æ°¸è¿œé è¿‘é›†åˆç±»å‹å°±OKäº†ã€‚++ è¯¥æ–¹æ³•ç”¨äºè¿æ¥ä¸¤ä¸ªé›†åˆï¼Œlist1++list2::: è¯¥æ–¹æ³•åªèƒ½ç”¨äºè¿æ¥ä¸¤ä¸ªListç±»å‹çš„é›†åˆ 7.start log manager12345678logManager = createLogManager(zkUtils.zkClient, brokerState)logManager.startup()metadataCache = new MetadataCache(config.brokerId)credentialProvider = new CredentialProvider(config.saslEnabledMechanisms)socketServer = new SocketServer(config, metrics, time, credentialProvider)socketServer.startup() æ ¹æ®ä¸€ç³»åˆ—é…ç½®å‚æ•°ï¼Œå¯åŠ¨LogManagerã€‚è¯¦æƒ…è§kafka.log.CleanerConfigå’Œkafka.log.LogManager startupåˆ›å»ºäº†4ä¸ªçº¿ç¨‹ï¼Œåˆ†åˆ«è´Ÿè´£åˆ›å»ºæ—¥å¿—ï¼Œå†™æ—¥å¿—ï¼Œæ£€ç´¢æ—¥å¿—ï¼Œæ¸…ç†æ—¥å¿—1234567891011121314151617181920212223242526272829def startup() &#123; /* Schedule the cleanup task to delete old logs */ if(scheduler != null) &#123; info("Starting log cleanup with a period of %d ms.".format(retentionCheckMs)) scheduler.schedule("kafka-log-retention", cleanupLogs, delay = InitialTaskDelayMs, period = retentionCheckMs, TimeUnit.MILLISECONDS) info("Starting log flusher with a default period of %d ms.".format(flushCheckMs)) scheduler.schedule("kafka-log-flusher", flushDirtyLogs, delay = InitialTaskDelayMs, period = flushCheckMs, TimeUnit.MILLISECONDS) scheduler.schedule("kafka-recovery-point-checkpoint", checkpointRecoveryPointOffsets, delay = InitialTaskDelayMs, period = flushCheckpointMs, TimeUnit.MILLISECONDS) scheduler.schedule("kafka-delete-logs", deleteLogs, delay = InitialTaskDelayMs, period = defaultConfig.fileDeleteDelayMs, TimeUnit.MILLISECONDS) &#125; if(cleanerConfig.enableCleaner) cleaner.startup() &#125; 8.start replica manager123replicaManager = new ReplicaManager(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower)replicaManager.startup() å¯åŠ¨isr-expirationçº¿ç¨‹å¯åŠ¨isr-change-propagationçº¿ç¨‹åœ¨/controllerä¸‹å»ºäº†ä¸€ä¸ªç›‘å¬ æ­¤å¤„æœ‰ä¸ªå°æŠ€å·§12345678def inLock[T](lock: Lock)(fun: =&gt; T): T = &#123; lock.lock() try &#123; fun &#125; finally &#123; lock.unlock() &#125; &#125; : =&gt;æ³¨æ„:åé¢è¦æœ‰ç©ºæ ¼ï¼Œæ­¤å¤„æ ‡æ˜è°ƒç”¨çš„æ—¶å€™æ‰æ‰§è¡Œï¼Œå¦åˆ™åœ¨ç”¨inlockçš„å‡½æ•°æ—¶å€™funå·²ç»åœ¨é”å¤–é¢æ‰§è¡Œäº†. å‚è€ƒï¼šhttp://www.jianshu.com/p/f53e0b54a44a 9.start kafka controller1234kafkaController = new KafkaController(config, zkUtils, brokerState, time, metrics, threadNamePrefix)kafkaController.startup()adminManager = new AdminManager(config, metrics, metadataCache, zkUtils) 123456789def startup() = &#123; inLock(controllerContext.controllerLock) &#123; info("Controller starting up") registerSessionExpirationListener() isRunning = true controllerElector.startup info("Controller startup complete") &#125;&#125; åˆ†æ”¯1.registerSessionExpirationListener-&gt;SessionExpirationListener-&gt;handleNewSession å½“ä¼šè¯è¶…æ—¶ï¼Œé‡æ–°è¿æ¥ä¸Šçš„æ—¶å€™ï¼Œè°ƒç”¨ä¹‹å‰æ³¨å†Œåœ¨ZookeeperLeaderElectorçš„onControllerResignationå‡½æ•°controllerElector.elect é‡æ–°é€‰ä¸¾ åˆ†æ”¯2.ZookeeperLeaderElector-&gt;ï¼ˆonControllerFailoverï¼ŒonControllerResignationï¼‰-&gt;LeaderChangeListenercontrollerElectorå°±æ˜¯ZookeeperLeaderElector æ˜¯kafkaçš„é€‰ä¸¾æœºåˆ¶ZookeeperLeaderElectorï¼šé€šè¿‡zkåˆ›å»ºEphemeral Nodeï¼ˆä¸´æ—¶èŠ‚ç‚¹ï¼‰çš„æ–¹å¼æ¥è¿›è¡Œé€‰ä¸¾ï¼Œå³å¦‚æœå­˜åœ¨å¹¶å‘æƒ…å†µä¸‹å‘zkçš„åŒä¸€ä¸ªè·¯å¾„åˆ›å»ºnodeçš„è¯ï¼Œæœ‰ä¸”åªæœ‰1ä¸ªå®¢æˆ·ç«¯ä¼šåˆ›å»ºæˆåŠŸï¼Œå…¶å®ƒå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥ï¼Œä½†æ˜¯å½“åˆ›å»ºæˆåŠŸçš„å®¢æˆ·ç«¯å’Œzkçš„é“¾æ¥æ–­å¼€ä¹‹åï¼Œè¿™ä¸ªnodeä¹Ÿä¼šæ¶ˆå¤±ï¼Œå…¶å®ƒçš„å®¢æˆ·ç«¯ä»è€Œç»§ç»­ç«äº‰ 123456def startup &#123; inLock(controllerContext.controllerLock) &#123; controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener) elect &#125; &#125; 1.ç›‘å¬electionPathï¼ˆ/controllerï¼‰2.electé€‰ä¸¾ 1234val ControllerPath = "/controller"val electString = Json.encode(Map("version" -&gt; 1, "brokerid" -&gt; brokerId, "timestamp" -&gt; timestamp))val zkCheckedEphemeral = new ZKCheckedEphemeral(electionPath,lectString, controllerContext.zkUtils.zkConnection.getZookeeper, JaasUtils.isZkSecurityEnabled())leaderId = getControllerID æ­¤å¤„ä¼šåœ¨/controller ä¸‹é¢å†™ä¸€ä¸ªç±»ä¼¼å¦‚ä¸‹çš„å†…å®¹ï¼š{â€œversionâ€:1,â€brokeridâ€:102,â€timestampâ€:â€1495880001272â€}é€šè¿‡getControllerIDè·å–å½“å‰çš„leaderIdç„¶åé€šè¿‡amILeaderçœ‹è‡ªå·±æ˜¯å¦æ˜¯leader ZookeeperLeaderElecto12345678910111213141516171819202122232425262728293031323334353637383940class LeaderChangeListener extends IZkDataListener with Logging &#123; /** * Called when the leader information stored in zookeeper has changed. Record the new leader in memory * @throws Exception On any error. */ @throws[Exception] def handleDataChange(dataPath: String, data: Object) &#123; val shouldResign = inLock(controllerContext.controllerLock) &#123; val amILeaderBeforeDataChange = amILeader leaderId = KafkaController.parseControllerId(data.toString) info("New leader is %d".format(leaderId)) // The old leader needs to resign leadership if it is no longer the leader amILeaderBeforeDataChange &amp;&amp; !amILeader &#125; if (shouldResign) onResigningAsLeader() &#125; /** * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader * @throws Exception * On any error. */ @throws[Exception] def handleDataDeleted(dataPath: String) &#123; val shouldResign = inLock(controllerContext.controllerLock) &#123; debug("%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader" .format(brokerId, dataPath)) amILeader &#125; if (shouldResign) onResigningAsLeader() inLock(controllerContext.controllerLock) &#123; elect &#125; &#125; &#125; å¦‚æœèŠ‚ç‚¹ä¸‹çº¿ä¼šè°ƒç”¨handleDataDeletedã€‚çœ‹è‡ªå·±æ˜¯å¦æ˜¯leaderï¼Œå¦‚æœæ˜¯éœ€è¦å…ˆé€€ä¼‘onResigningAsLeaderã€‚ç„¶åé€‰ä¸¾123456789101112131415161718192021222324try &#123; val zkCheckedEphemeral = new ZKCheckedEphemeral(electionPath, electString, controllerContext.zkUtils.zkConnection.getZookeeper, JaasUtils.isZkSecurityEnabled()) zkCheckedEphemeral.create() info(brokerId + " successfully elected as leader") leaderId = brokerId onBecomingLeader() &#125; catch &#123; case _: ZkNodeExistsException =&gt; // If someone else has written the path, then leaderId = getControllerID if (leaderId != -1) debug("Broker %d was elected as leader instead of broker %d".format(leaderId, brokerId)) else warn("A leader has been elected but just resigned, this will result in another round of election") case e2: Throwable =&gt; error("Error while electing or becoming leader on broker %d".format(brokerId), e2) resign() &#125; amILeader åˆ›å»ºä¸´æ—¶èŠ‚ç‚¹ onControllerFailover:12345678910111213141516171819202122232425262728293031323334353637383940def onControllerFailover() &#123; if(isRunning) &#123; info("Broker %d starting become controller state transition".format(config.brokerId)) readControllerEpochFromZookeeper() incrementControllerEpoch(zkUtils.zkClient) // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks registerReassignedPartitionsListener() registerIsrChangeNotificationListener() registerPreferredReplicaElectionListener() partitionStateMachine.registerListeners() replicaStateMachine.registerListeners() initializeControllerContext() // We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines // are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before // they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and // partitionStateMachine.startup(). sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq) replicaStateMachine.startup() partitionStateMachine.startup() // register the partition change listeners for all existing topics on failover controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic)) info("Broker %d is ready to serve as the new controller with epoch %d".format(config.brokerId, epoch)) maybeTriggerPartitionReassignment() maybeTriggerPreferredReplicaElection() if (config.autoLeaderRebalanceEnable) &#123; info("starting the partition rebalance scheduler") autoRebalanceScheduler.startup() autoRebalanceScheduler.schedule("partition-rebalance-thread", checkAndTriggerPartitionRebalance, 5, config.leaderImbalanceCheckIntervalSeconds.toLong, TimeUnit.SECONDS) &#125; deleteTopicManager.start() &#125; else info("Controller has been shut down, aborting startup/failover") &#125; åœ¨/admin/reassign_partitionsç›®å½•æ³¨å†ŒPartitionsReassignedListenerç›‘å¬å‡½æ•°åœ¨/isr_change_notificationç›®å½•æ³¨å†ŒIsrChangeNotificationListenerç›‘å¬å‡½æ•°åœ¨/admin/preferred_replica_electionç›®å½•æ³¨å†ŒPreferredReplicaElectionListenerç›‘å¬å‡½æ•°åœ¨/brokers/topicsç›®å½•æ³¨å†ŒTopicChangeListenerç›‘å¬å‡½æ•°åœ¨/admin/delete_topicsç›®å½•æ³¨å†ŒDeleteTopicsListenerç›‘å¬å‡½æ•°åœ¨/brokers/idsç›®å½•æ³¨å†ŒBrokerChangeListenerç›‘å¬å‡½æ•° ç›‘å¬æ˜¯è°ƒç”¨zkçš„zkUtils.zkClient.subscribeChildChangeså‡½æ•°ï¼Œå‚æ•°æ˜¯è·¯å¾„å’Œç›‘å¬å‡½æ•°ç›‘å¬å‡½æ•°å®ç°IZkChildListeneræ¥å£å®ç°handleChildChangeæ–¹æ³• åˆå§‹åŒ–ControllerContextä¸Šä¸‹æ–‡,é‡Œé¢åŒ…å«å­˜æ´»çš„brokerï¼Œæ‰€æœ‰ä¸»é¢˜ï¼Œåˆ†åŒºå‰¯æœ¬ï¼Œåˆ†åŒºçš„leaderå’Œå·²ç»ä¸‹çº¿çš„brokerã€‚æ›´æ–°leaderå’Œisrç¼“å­˜ã€‚å¯åŠ¨ControllerChannelManageråˆå§‹åŒ–æ‰€æœ‰çš„replicaçŠ¶æ€åˆå§‹åŒ–æ‰€æœ‰çš„partitionçŠ¶æ€å¦‚æœauto.leader.rebalance.enable ä¸ºtrueä¼šå¯åŠ¨Rebalanceè°ƒåº¦æœ€ååˆ é™¤ä¸»é¢˜ é€šè¿‡replicaStateMachineåˆå§‹åŒ–æ‰€æœ‰çš„replicaçŠ¶æ€replicaStateMachineçš„handleStateChanges12345678910111213def handleStateChanges(replicas: Set[PartitionAndReplica], targetState: ReplicaState, callbacks: Callbacks = (new CallbackBuilder).build) &#123; if(replicas.nonEmpty) &#123; info("Invoking state change to %s for replicas %s".format(targetState, replicas.mkString(","))) try &#123; brokerRequestBatch.newBatch() replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks)) brokerRequestBatch.sendRequestsToBrokers(controller.epoch) &#125;catch &#123; case e: Throwable =&gt; error("Error while moving some replicas to %s state".format(targetState), e) &#125; &#125; &#125; é€šè¿‡partitionStateMachineåˆå§‹åŒ–æ‰€æœ‰çš„partitionçŠ¶æ€1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private def handleStateChange(topic: String, partition: Int, targetState: PartitionState, leaderSelector: PartitionLeaderSelector, callbacks: Callbacks) &#123; val topicAndPartition = TopicAndPartition(topic, partition) if (!hasStarted.get) throw new StateChangeFailedException(("Controller %d epoch %d initiated state change for partition %s to %s failed because " + "the partition state machine has not started") .format(controllerId, controller.epoch, topicAndPartition, targetState)) val currState = partitionState.getOrElseUpdate(topicAndPartition, NonExistentPartition) try &#123; targetState match &#123; case NewPartition =&gt; // pre: partition did not exist before this assertValidPreviousStates(topicAndPartition, List(NonExistentPartition), NewPartition) partitionState.put(topicAndPartition, NewPartition) val assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(",") stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, assignedReplicas)) // post: partition has been assigned replicas case OnlinePartition =&gt; assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OnlinePartition) partitionState(topicAndPartition) match &#123; case NewPartition =&gt; // initialize leader and isr path for new partition initializeLeaderAndIsrForPartition(topicAndPartition) case OfflinePartition =&gt; electLeaderForPartition(topic, partition, leaderSelector) case OnlinePartition =&gt; // invoked when the leader needs to be re-elected electLeaderForPartition(topic, partition, leaderSelector) case _ =&gt; // should never come here since illegal previous states are checked above &#125; partitionState.put(topicAndPartition, OnlinePartition) val leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader stateChangeLogger.trace("Controller %d epoch %d changed partition %s from %s to %s with leader %d" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader)) // post: partition has a leader case OfflinePartition =&gt; // pre: partition should be in New or Online state assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OfflinePartition) // should be called when the leader for a partition is no longer alive stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState)) partitionState.put(topicAndPartition, OfflinePartition) // post: partition has no alive leader case NonExistentPartition =&gt; // pre: partition should be in Offline state assertValidPreviousStates(topicAndPartition, List(OfflinePartition), NonExistentPartition) stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState)) partitionState.put(topicAndPartition, NonExistentPartition) // post: partition state is deleted from all brokers and zookeeper &#125; &#125; catch &#123; case t: Throwable =&gt; stateChangeLogger.error("Controller %d epoch %d initiated state change for partition %s from %s to %s failed" .format(controllerId, controller.epoch, topicAndPartition, currState, targetState), t) &#125; &#125; PartitionStateMachineå®ç°äº†topicçš„åˆ†åŒºçŠ¶æ€åˆ‡æ¢åŠŸèƒ½ï¼ŒPartitionå­˜åœ¨çš„çŠ¶æ€å¦‚ä¸‹ï¼šNewPartition åˆ†åŒºä¹‹å‰ä¸å­˜åœ¨ï¼Œåˆ›å»ºåè¢«åˆ†é…äº†replicasï¼Œä½†æ˜¯è¿˜æ²¡æœ‰leader/isrOnlinePartition partitionåœ¨replicasä¸­é€‰ä¸¾æŸä¸ªæˆä¸ºleaderä¹‹åOfflinePartition partitionçš„replicasä¸­çš„leaderä¸‹çº¿ä¹‹åï¼Œæ²¡æœ‰é‡æ–°é€‰ä¸¾æ–°çš„leaderä¹‹å‰ æˆ– partitionåˆ›å»ºä¹‹åç›´æ¥è¢«ä¸‹çº¿NonExistentPartition partitioné‡æ¥æ²¡æœ‰è¢«åˆ›å»º æˆ– partitionåˆ›å»ºä¹‹åè¢«åˆ é™¤ scalaå°çŸ¥è¯†ï¼šmkString1234567891011121314151617scala &gt; val a = Array("apple", "banana", "cherry")a: Array[String] = Array(apple, banana, cherry)scala &gt; a.mkString(",")res2: String = apple,banana,cherryscala&gt; a.mkString("[", ", ", "]")res3: String = [apple, banana, cherry]å¦‚æœæ˜¯æ•°ç»„éœ€è¦å…ˆå±•å¼€æ•°ç»„scala&gt; val b = Array(Array("a", "b"), Array("c", "d"))b: Array[Array[String]] = Array(Array(a, b), Array(c, d))é”™è¯¯çš„scala&gt; b.mkString(",")res4: String = [Ljava.lang.String;@64a9fca7,[Ljava.lang.String;@22f756c5æ­£ç¡®çš„scala&gt; b.flatten.mkString(",")res5: String = a,b,c,d OnlinePartition ï¼šæ£€æŸ¥å‰ç½®çŠ¶æ€æ˜¯å¦ä¸ºNewPartition, OnlinePartition, OfflinePartitionä¸­çš„ä¸€ç§ï¼Œ1.å¦‚æœæ˜¯NewPartitionï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344private def initializeLeaderAndIsrForPartition(topicAndPartition: TopicAndPartition) &#123; val replicaAssignment = controllerContext.partitionReplicaAssignment(topicAndPartition) val liveAssignedReplicas = replicaAssignment.filter(r =&gt; controllerContext.liveBrokerIds.contains(r)) liveAssignedReplicas.size match &#123; case 0 =&gt; val failMsg = ("encountered error during state change of partition %s from New to Online, assigned replicas are [%s], " + "live brokers are [%s]. No assigned replica is alive.") .format(topicAndPartition, replicaAssignment.mkString(","), controllerContext.liveBrokerIds) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) case _ =&gt; debug("Live assigned replicas for partition %s are: [%s]".format(topicAndPartition, liveAssignedReplicas)) // make the first replica in the list of assigned replicas, the leader //æ ¹æ®partitionReplicaAssignmentä¸­ä¿¡æ¯é€‰æ‹©ç¬¬ä¸€ä¸ªliveçš„replicaä¸ºleader,å…¶ä½™ä¸ºisr val leader = liveAssignedReplicas.head val leaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(new LeaderAndIsr(leader, liveAssignedReplicas.toList), controller.epoch) debug("Initializing leader and isr for partition %s to %s".format(topicAndPartition, leaderIsrAndControllerEpoch)) try &#123; //å°†leaderå’ŒisræŒä¹…åŒ–åˆ°zookeeper zkUtils.createPersistentPath( getTopicPartitionLeaderAndIsrPath(topicAndPartition.topic, topicAndPartition.partition), zkUtils.leaderAndIsrZkData(leaderIsrAndControllerEpoch.leaderAndIsr, controller.epoch)) // NOTE: the above write can fail only if the current controller lost its zk session and the new controller // took over and initialized this partition. This can happen if the current controller went into a long // GC pause //æ›´æ–°controllerContextä¸­çš„partitionLeadershipInfo controllerContext.partitionLeadershipInfo.put(topicAndPartition, leaderIsrAndControllerEpoch) //å°è£…å‘é€ç»™è¿™äº›replicaæ‰€åœ¨çš„brokerçš„LeaderAndIsrRequestè¯·æ±‚ï¼Œäº¤ç”±ControllerBrokerRequestBatch(brokerRequestBatch)å¤„ç† brokerRequestBatch.addLeaderAndIsrRequestForBrokers(liveAssignedReplicas, topicAndPartition.topic, topicAndPartition.partition, leaderIsrAndControllerEpoch, replicaAssignment) &#125; catch &#123; case _: ZkNodeExistsException =&gt; // read the controller epoch val leaderIsrAndEpoch = ReplicationUtils.getLeaderIsrAndEpochForPartition(zkUtils, topicAndPartition.topic, topicAndPartition.partition).get val failMsg = ("encountered error while changing partition %s's state from New to Online since LeaderAndIsr path already " + "exists with value %s and controller epoch %d") .format(topicAndPartition, leaderIsrAndEpoch.leaderAndIsr.toString(), leaderIsrAndEpoch.controllerEpoch) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) &#125; &#125; &#125; 2.å¦‚æœæ˜¯OfflinePartitionï¼ŒOnlinePartition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071def electLeaderForPartition(topic: String, partition: Int, leaderSelector: PartitionLeaderSelector) &#123; val topicAndPartition = TopicAndPartition(topic, partition) // handle leader election for the partitions whose leader is no longer alive stateChangeLogger.trace("Controller %d epoch %d started leader election for partition %s" .format(controllerId, controller.epoch, topicAndPartition)) try &#123; var zookeeperPathUpdateSucceeded: Boolean = false var newLeaderAndIsr: LeaderAndIsr = null var replicasForThisPartition: Seq[Int] = Seq.empty[Int] while(!zookeeperPathUpdateSucceeded) &#123; val currentLeaderIsrAndEpoch = getLeaderIsrAndEpochOrThrowException(topic, partition) val currentLeaderAndIsr = currentLeaderIsrAndEpoch.leaderAndIsr val controllerEpoch = currentLeaderIsrAndEpoch.controllerEpoch if (controllerEpoch &gt; controller.epoch) &#123; val failMsg = ("aborted leader election for partition [%s,%d] since the LeaderAndIsr path was " + "already written by another controller. This probably means that the current controller %d went through " + "a soft failure and another controller was elected with epoch %d.") .format(topic, partition, controllerId, controllerEpoch) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg) &#125; // elect new leader or throw exception val (leaderAndIsr, replicas) = leaderSelector.selectLeader(topicAndPartition, currentLeaderAndIsr) val (updateSucceeded, newVersion) = ReplicationUtils.updateLeaderAndIsr(zkUtils, topic, partition, leaderAndIsr, controller.epoch, currentLeaderAndIsr.zkVersion) //æ ¹æ®ä¸åŒçš„leaderSelectoré€‰ä¸¾æ–°çš„leaderï¼Œè¿™é‡Œä¸€èˆ¬è°ƒç”¨çš„æ˜¯OfflinePartitionLeaderSelector newLeaderAndIsr = leaderAndIsr newLeaderAndIsr.zkVersion = newVersion //å°†leaderå’ŒisræŒä¹…åŒ–åˆ°zookeeper zookeeperPathUpdateSucceeded = updateSucceeded replicasForThisPartition = replicas &#125; val newLeaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(newLeaderAndIsr, controller.epoch) // update the leader cache //æ›´æ–°controllerContextä¸­çš„partitionLeadershipInfo controllerContext.partitionLeadershipInfo.put(TopicAndPartition(topic, partition), newLeaderIsrAndControllerEpoch) stateChangeLogger.trace("Controller %d epoch %d elected leader %d for Offline partition %s" .format(controllerId, controller.epoch, newLeaderAndIsr.leader, topicAndPartition)) val replicas = controllerContext.partitionReplicaAssignment(TopicAndPartition(topic, partition)) // store new leader and isr info in cache //å°è£…å‘é€ç»™è¿™äº›replicaæ‰€åœ¨çš„brokerçš„LeaderAndIsrRequestè¯·æ±‚ï¼Œäº¤ç”±ControllerBrokerRequestBatch(brokerRequestBatch)å¤„ç† brokerRequestBatch.addLeaderAndIsrRequestForBrokers(replicasForThisPartition, topic, partition, newLeaderIsrAndControllerEpoch, replicas) &#125; catch &#123; case _: LeaderElectionNotNeededException =&gt; // swallow case nroe: NoReplicaOnlineException =&gt; throw nroe case sce: Throwable =&gt; val failMsg = "encountered error while electing leader for partition %s due to: %s.".format(topicAndPartition, sce.getMessage) stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg) throw new StateChangeFailedException(failMsg, sce) &#125; debug("After leader election, leader cache is updated to %s".format(controllerContext.partitionLeadershipInfo.map(l =&gt; (l._1, l._2)))) &#125;Â·Â·Â·åœ¨brokers/topics/***(å…·ä½“çš„topicåå­—)/ç›®å½•ä¸‹æ³¨å†ŒPartitionModificationsListener-&gt;AddPartitionsListenerç›‘å¬é€šè¿‡å¤„ç†ä¹‹å‰å¯åŠ¨ç•™ä¸‹çš„partitioné‡åˆ†é…çš„æƒ…å†µå¤„ç†ä¹‹å‰å¯åŠ¨ç•™ä¸‹çš„replicaé‡æ–°é€‰ä¸¾çš„æƒ…å†µå‘å…¶å®ƒKafkaServerå‘é€é›†ç¾¤topicçš„å…ƒæ•°æ®ä¿¡æ¯å·²è¿›è¡Œæ•°æ®çš„åŒæ­¥æ›´æ–°æ ¹æ®é…ç½®æ˜¯å¦å¼€å¯è‡ªåŠ¨å‡è¡¡å¼€å§‹åˆ é™¤topic### 10.start group coordinator```scala// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issuegroupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM)groupCoordinator.startup() 11.Get the authorizer and initialize it if one is specified.12345authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt; val authZ = CoreUtils.createObject[Authorizer](authorizerClassName) authZ.configure(config.originals()) authZ&#125; 12.start processing requests1234567apis = new KafkaApis(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator, kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers, clusterId, time)requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, time, config.numIoThreads)Mx4jLoader.maybeLoad()&#125; 13.start dynamic config manager123456 dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config, quotaManagers),ConfigType.Client -&gt; new ClientIdConfigHandler(quotaManagers),ConfigType.User -&gt; new UserConfigHandler(quotaManagers, credentialProvider),ConfigType.Broker -&gt; new BrokerConfigHandler(config, quotaManagers))// Create the config manager. start listening to notificationsdynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers)dynamicConfigManager.startup()&#125; 14.tell everyone we are alive12345678910111213val listeners = config.advertisedListeners.map &#123; endpoint =&gt; if (endpoint.port == 0) endpoint.copy(port = socketServer.boundPort(endpoint.listenerName)) else endpoint&#125;kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack, config.interBrokerProtocolVersion)kafkaHealthcheck.startup()// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint itcheckpointBrokerId(config.brokerId)&#125; kafkaHealthcheck.startup()12345678910111213141516171819202122232425def startup() &#123; zkUtils.zkClient.subscribeStateChanges(sessionExpireListener) register() &#125; /** * Register this broker as "alive" in zookeeper */ def register() &#123; val jmxPort = System.getProperty("com.sun.management.jmxremote.port", "-1").toInt val updatedEndpoints = advertisedEndpoints.map(endpoint =&gt; if (endpoint.host == null || endpoint.host.trim.isEmpty) endpoint.copy(host = InetAddress.getLocalHost.getCanonicalHostName) else endpoint ) // the default host and port are here for compatibility with older clients that only support PLAINTEXT // we choose the first plaintext port, if there is one // or we register an empty endpoint, which means that older clients will not be able to connect val plaintextEndpoint = updatedEndpoints.find(_.securityProtocol == SecurityProtocol.PLAINTEXT).getOrElse( new EndPoint(null, -1, null, null)) zkUtils.registerBrokerInZk(brokerId, plaintextEndpoint.host, plaintextEndpoint.port, updatedEndpoints, jmxPort, rack, interBrokerProtocolVersion) &#125; æ³¨å†Œæ–°çš„brokerid 15.register broker metrics.12 registerStats()&#125;]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafkaæºç é˜…è¯»]]></title>
    <url>%2F2017%2F05%2F02%2Fkafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[ä¸‹è½½kafkaçš„æºç åŒ…ï¼šhttp://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz å®‰è£…gradle 1.å®˜ç½‘ä¸‹è½½https://gradle.org/releasesé€‰æ‹©å½“æ—¶æœ€æ–°ç‰ˆæœ¬ï¼šhttps://downloads.gradle.org/distributions/gradle-3.5-all.zip 2.è§£å‹è§£å‹åˆ°D:\tool\gradle-3.5 3.é…ç½®ç¯å¢ƒå˜é‡GRADLE_HOMED:\tool\gradle-3.5 Pathè¿½åŠ ;%GRADLE_HOME%\BIN;4.æµ‹è¯•ï¼šgradle -v å®‰è£…å‚è€ƒ:http://blog.csdn.net/lizhitao/article/details/26875463 ç¼–è¯‘ï¼šï¼ˆæ­¤å¤„åšä¸»ç”¨æŸäº‘ä¸»æœºç¼–è¯‘çš„ï¼Œå¾ˆå¿«ï¼‰gradle idea ï¼ˆç¼–è¯‘å®ŒæŠŠ/root/.gradle/caches/modules-2ä¸‹ä¸‹è½½çš„æ–‡ä»¶æ”¾åˆ°æˆ‘ä»¬çš„ç¯å¢ƒä¸­C:\Users\Administrator.gradle\caches\modules-2 å†windowsä¸Šç¼–è¯‘ ï¼‰ ç”¨IDEAå·¥å…·æ‰“å¼€ ç›®å½•ä»‹ç»ï¼š æ¨¡å—å å«ä¹‰ admin ç®¡ç†å‘˜æ¨¡å—ï¼Œæ“ä½œå’Œç®¡ç†topicï¼Œparitionsç›¸å…³ï¼ŒåŒ…å«create/delete topic,æ‰©å±•patitions api ä¸»è¦è´Ÿè´£äº¤äº’æ•°æ®çš„ç»„è£…ï¼Œå®¢æˆ·ç«¯ä¸æœåŠ¡ç«¯äº¤äº’æ•°æ®ç¼–è§£ç  client Producerè¯»å–kafka brokerå…ƒæ•°æ®ä¿¡æ¯ï¼Œtopicå’Œpartitionsï¼Œä»¥åŠleader cluster åŒ…å«å‡ ä¸ªå®ä½“ç±»ï¼ŒBroker,Cluster,Partition,Replica,è§£é‡Šä»–ä»¬ä¹‹é—´å…³ç³»ï¼š Clusterç”±å¤šä¸ªbrokerç»„æˆï¼Œä¸€ä¸ªBrokeråŒ…å«å¤šä¸ªpartitionï¼Œä¸€ä¸ªtopicçš„æ‰€æœ‰partitionsåˆ†å¸ƒåœ¨ä¸åŒbrokerçš„ä¸­ï¼Œä¸€ä¸ªReplicaåŒ…å«å¤šä¸ªPartitionã€‚ common å¼‚å¸¸ç±»å’Œé”™è¯¯éªŒè¯ consumer è´Ÿè´£æ‰€æœ‰å®¢æˆ·ç«¯æ¶ˆè´¹è€…æ•°æ®å’Œé€»è¾‘å¤„ç† controller è´Ÿè´£ä¸­å¤®æ§åˆ¶å™¨é€‰ä¸¾ï¼Œpartitionçš„leaderé€‰ä¸¾ï¼Œå‰¯æœ¬åˆ†é…ï¼Œå‰¯æœ¬é‡æ–°åˆ†é…ï¼Œpartitionå’Œreplicaæ‰©å®¹ã€‚ coordinator partitionåˆ†é…æœºåˆ¶ javaapi æä¾›javaçš„producerå’Œconsumeræ¥å£api log Kafkaæ–‡ä»¶å­˜å‚¨æ¨¡å—ï¼Œè´Ÿè´£è¯»å†™æ‰€æœ‰kafkaçš„topicæ¶ˆæ¯æ•°æ®ã€‚ message å°è£…å¤šä¸ªæ¶ˆæ¯ç»„æˆä¸€ä¸ªâ€œæ¶ˆæ¯é›†â€æˆ–å‹ç¼©æ¶ˆæ¯é›†ã€‚ metrics å†…éƒ¨çŠ¶æ€çš„ç›‘æ§æ¨¡å— network ç½‘ç»œäº‹ä»¶å¤„ç†æ¨¡å—ï¼Œè´Ÿè´£å¤„ç†å’Œæ¥æ”¶å®¢æˆ·ç«¯è¿æ¥ producer producerå®ç°æ¨¡å—ï¼ŒåŒ…æ‹¬åŒæ­¥å’Œå¼‚æ­¥å‘é€æ¶ˆæ¯ã€‚ security å®‰å…¨ serializer åºåˆ—åŒ–æˆ–ååºåˆ—åŒ–å½“å‰æ¶ˆæ¯ server tools å·¥å…·æ¨¡å—ï¼Œ åŒ…å«a.å¯¼å‡ºå¯¹åº”consumerçš„offsetå€¼.b.å¯¼å‡ºLogSegmentsä¿¡æ¯ï¼Œå½“å‰topicçš„logå†™çš„ä½ç½®ä¿¡æ¯.c.å¯¼å‡ºzkä¸Šæ‰€æœ‰consumerçš„offsetå€¼.d.ä¿®æ”¹æ³¨å†Œåœ¨zkçš„consumerçš„offsetå€¼.f.producerå’Œconsumerçš„ä½¿ç”¨ä¾‹å­. utils Jsonå·¥å…·ç±»ï¼ŒZkutilså·¥å…·ç±»ï¼ŒUtilsåˆ›å»ºçº¿ç¨‹å·¥å…·ç±»ï¼ŒKafkaSchedulerå…¬å…±è°ƒåº¦å™¨ç±»ï¼Œå…¬å…±æ—¥å¿—ç±»ç­‰ç­‰ã€‚]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH oozie]]></title>
    <url>%2F2017%2F04%2F18%2FCDH%20oozie%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[æ³¨ï¼šæœ¬æ¬¡oozieæ˜¯å®‰è£…åˆ°è‡ªå®šä¹‰çš„é›†ç¾¤ä¸Šï¼Œå› ä¸ºå…¶ä»–èŠ‚ç‚¹éƒ½æ˜¯ç”¨CDHæ‰‹åŠ¨åŒ…å®‰è£…çš„ï¼Œä¸æ˜¯CDHè‡ªåŠ¨ä¸€é”®å¼å®‰è£…æ–¹å¼ ä¸‹è½½CDHçš„oozie ï¼Œå·²ç»ç¼–è¯‘å¥½ï¼Œæ— éœ€ç¼–è¯‘æœ¬æ¬¡å®‰è£…é€‰æ‹©äº†ä¸‹é¢ç½‘å€çš„CDH 5.7.0 ç‰ˆæœ¬CDHhttps://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball_57.html#concept_0vc_ddn_yk Linuxæœºå™¨ä¸Šæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ä¸‹è½½ï¼šwget https://archive.cloudera.com/cdh5/cdh/5/oozie-4.1.0-cdh5.7.0.tar.gz è§£å‹åå°†å¦‚ä¸‹å†…å®¹æ·»åŠ åˆ°core-site.xml12345678910vi $HADOOP_HOME/etc/hadoop/core-site.xml &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;hadoop000,hadoop001,hadoop002&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; æ³¨æ„proxyuseråé¢é‚£ä¸ªrootæ˜¯ä½¿ç”¨oozieçš„ç”¨æˆ·ï¼Œhadoop000,hadoop001,hadoop002æ˜¯é›†ç¾¤çš„ä¸»æœº å°†oozieæ·»åŠ åˆ°ç¯å¢ƒå˜é‡12export OOIZE_HOME=/home/app/oozie-4.1.0-cdh5.7.0export PATH=$PATH:$OOIZE_HOME/bin:$PATH æ·»åŠ ä»¥ä¸‹å†…å®¹åˆ°oozieé…ç½®æ–‡ä»¶ï¼Œ$OOIZE_HOME/conf/oozie-site.xml12345678910111213141516171819202122232425262728293031323334&lt;property&gt; &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt; &lt;value&gt;*=/root/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop&lt;/value&gt; &lt;description&gt; Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of the Hadoop service (JobTracker, HDFS). The wildcard &apos;*&apos; configuration is used when there is no exact match for an authority. The HADOOP_CONF_DIR contains the relevant Hadoop *-site.xml files. If the path is relative is looked within the Oozie configuration directory; though the path can be absolute (i.e. to point to Hadoop client conf/ directories in the local filesystem. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.driver&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.url&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/oozie?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.username&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.service.JPAService.jdbc.password&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;oozie.processing.timezone&lt;/name&gt; &lt;value&gt;GMT+0800&lt;/value&gt;&lt;/property&gt; åœ¨è§£å‹oozieäºŒè¿›åˆ¶å‘è¡ŒåŒ…çš„ç›®å½•ä¸­ï¼Œè§£å‹hadooplibså‘è¡ŒåŒ…ï¼Œä¹Ÿå°±æ˜¯oozie-hadooplibs-4.1.0-cdh5.7.0.tar.gz[hadoop@h71 oozie-4.1.0-cdh5.5.2]$ tar -zxvf oozie-hadooplibs-4.1.0-cdh5.5.2.tar.gz åœ¨oozieçš„è§£å‹ç›®å½•ä¸‹åˆ›å»ºlibextç›®å½•ã€‚å¹¶å°†hadooplibsä¸‹çš„jaråŒ…æ‹·è´åˆ°è¿™ä¸ªç›®å½•é‡Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯hadooplibsç›®å½•ä¸‹æœ‰ä¸ªæ–‡ä»¶å¤¹hadooplib-2.6.0-cdh5.7.0.oozie-4.1.0-cdh5.7.0 hadooplib-2.6.0-mr1-cdh5.7.0.oozie-4.1.0-cdh5.7.0ï¼›åè€…å¯¹åº”äºmapreduce1ï¼Œæ‰€ä»¥æˆ‘ä»¬æ‹·è´ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹çš„jaråŒ…å³å¯ã€‚ä¸Šä¼ extæ–‡ä»¶åˆ°hadooplibsä¸Šä¼ mysqlçš„é©±åŠ¨æ–‡ä»¶åˆ°hadooplibså…·ä½“ä¸Šä¼ ä»€ä¹ˆç‰ˆæœ¬çš„extå¯ä»¥çœ‹oozie-setup.shè„šæœ¬é‡ŒæŒ‡å®šçš„ext-åé¢çš„ç‰ˆæœ¬å· å°†hadooplibsé‡Œé¢çš„æ–‡ä»¶æ‹·è´åˆ°libext 1234567891011121314151617[root@hadoop000 libext]# ls /home/app/oozie-4.1.0-cdh5.7.0/libextactivation-1.1.jar commons-digester-1.8.jar hadoop-aws-2.6.0-cdh5.7.0.jar jackson-annotations-2.2.3.jar netty-all-4.0.23.Final.jarapacheds-i18n-2.0.0-M15.jar commons-httpclient-3.1.jar hadoop-client-2.6.0-cdh5.7.0.jar jackson-core-2.2.3.jar paranamer-2.3.jarapacheds-kerberos-codec-2.0.0-M15.jar commons-io-2.4.jar hadoop-common-2.6.0-cdh5.7.0.jar jackson-core-asl-1.8.8.jar protobuf-java-2.5.0.jarapi-asn1-api-1.0.0-M20.jar commons-lang-2.4.jar hadoop-hdfs-2.6.0-cdh5.7.0.jar jackson-databind-2.2.3.jar servlet-api-2.5.jarapi-util-1.0.0-M20.jar commons-logging-1.1.jar hadoop-mapreduce-client-app-2.6.0-cdh5.7.0.jar jackson-jaxrs-1.8.8.jar slf4j-api-1.7.5.jaravro-1.7.6-cdh5.7.0.jar commons-math3-3.1.1.jar hadoop-mapreduce-client-common-2.6.0-cdh5.7.0.jar jackson-mapper-asl-1.8.8.jar slf4j-log4j12-1.7.5.jaraws-java-sdk-core-1.10.6.jar commons-net-3.1.jar hadoop-mapreduce-client-core-2.6.0-cdh5.7.0.jar jackson-xc-1.8.8.jar snappy-java-1.0.4.1.jaraws-java-sdk-kms-1.10.6.jar curator-client-2.7.1.jar hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.0.jar jaxb-api-2.2.2.jar stax-api-1.0-2.jaraws-java-sdk-s3-1.10.6.jar curator-framework-2.7.1.jar hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.0.jar jersey-client-1.9.jar xercesImpl-2.10.0.jarcommons-beanutils-1.7.0.jar curator-recipes-2.7.1.jar hadoop-yarn-api-2.6.0-cdh5.7.0.jar jersey-core-1.9.jar xml-apis-1.4.01.jarcommons-beanutils-core-1.8.0.jar ext-2.2 hadoop-yarn-client-2.6.0-cdh5.7.0.jar jetty-util-6.1.26.cloudera.2.jar xmlenc-0.52.jarcommons-cli-1.2.jar ext-2.2.zip hadoop-yarn-common-2.6.0-cdh5.7.0.jar jsr305-3.0.0.jar xz-1.0.jarcommons-codec-1.4.jar gson-2.2.4.jar hadoop-yarn-server-common-2.6.0-cdh5.7.0.jar leveldbjni-all-1.8.jar zookeeper-3.4.5-cdh5.7.0.jarcommons-collections-3.2.2.jar guava-11.0.2.jar htrace-core4-4.0.1-incubating.jar log4j-1.2.17.jarcommons-compress-1.4.1.jar hadoop-annotations-2.6.0-cdh5.7.0.jar httpclient-4.2.5.jar mysql-connector-java-5.1.33-bin.jarcommons-configuration-1.6.jar hadoop-auth-2.6.0-cdh5.7.0.jar httpcore-4.2.5.jar netty-3.6.2.Final.jar ä½¿ç”¨å‘½ä»¤åˆ›å»ºæ•°æ®åº“bin/oozie-setup.sh db create -run oozie.sql ç¼–è¯‘waråŒ… bin/oozie-setup.sh prepare-war åœ¨$oozie_homeä¸‹ï¼Œæœ‰2ä¸ªsharelibå‹ç¼©åŒ…ï¼Œåˆ†åˆ«æ˜¯ oozie-sharelib-4.1.0-cdh5.7.0.tar.gz å’Œoozie-sharelib-4.1.0-cdh5.7.0-yarn.tar.gz å¾ˆæ˜æ˜¾ï¼Œæˆ‘ä»¬å¿…é¡»æ‹·è´ç¬¬äºŒä¸ªå¸¦yarnçš„å‹ç¼©åŒ…ï¼ˆå‰è¾¹çš„æ˜¯1.0ç‰ˆæœ¬çš„ï¼Œä¸å¸¦yarnçš„ï¼‰ bin/oozie-setup.sh sharelib create -fs hdfs://hadoop000:9000 -locallib oozie-sharelib-4.1.0-cdh5.7.0-yarn.tar.gz é‡å¯hadoop å¯åŠ¨oozie bin/oozied.sh startå¯åŠ¨åï¼Œè®¿é—®ç½‘å€http://hadoop000:11000/oozie/ æµ‹è¯•OOZIE è§£å‹tar -zxvf oozie-examples.tar.gzè¿›å…¥cd examples/apps ä»¥map-reduceç›®å½•ä¸ºä¾‹å­1234567891011121314151617181920212223242526[root@hadoop000 map-reduce]# vi job.properties ## Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements. See the NOTICE file# distributed with this work for additional information# regarding copyright ownership. The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#nameNode=hdfs://hadoop000:9000jobTracker=hadoop000:8032queueName=defaultexamplesRoot=examplesoozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/apps/map-reduce/workflow.xmloutputDir=map-reduce ä¿®æ”¹workflow.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@hadoop000 map-reduce]# vi workflow.xml &lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.2&quot; name=&quot;map-reduce-wf&quot;&gt; &lt;start to=&quot;mr-node&quot;/&gt; &lt;action name=&quot;mr-node&quot;&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&quot;/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to=&quot;end&quot;/&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt; &lt;end name=&quot;end&quot;/ ä¸Šä¼ æ–‡ä»¶hadoop fs -put examples/ /user/root å¯åŠ¨ [root@hadoop000 oozie-4.1.0-cdh5.7.0]# bin/oozie job -oozie http://hadoop000:11000/oozie -config examples/apps/map-reduce/job.properties -runjob: 0000000-180830181747338-oozie-root-W æ€æ‰jobbin/oozie job -oozie http://hadoop000:11000/oozie -kill 0000000-180830181747338-oozie-root-W å¦‚æœå¯åŠ¨åå‘ç°æ—¶é—´ä¸å¯¹ï¼Œéœ€è¦ä¿®æ”¹æ—¶é—´vi /home/app/oozie-4.1.0-cdh5.7.0/src/webapp/src/main/webapp/oozie-console.js function getTimeZone() {Ext.state.Manager.setProvider(new Ext.state.CookieProvider());return Ext.state.Manager.get(â€œTimezoneIdâ€,â€GMT+0800â€);} Oozieé‡å¯æ­¥éª¤ï¼š 1.è¿›å…¥oozieç›®å½• 2.å°†Oozieåœæ­¢è¿è¡Œ./bin/oozie-stop.sh 3.é‡æ–°è¿›è¡Œæ‰“åŒ…waråŒ… ./bin/oozie-setup.sh prepare-war 4.é‡æ–°æ‰“å¼€Oozieï¼š./bin/oozie-start.sh å¦å¤–åœ¨ä¿®æ”¹webæ˜¾ç¤ºçš„æ—¶é—´ oozieæ·»åŠ å‘Šè­¦é…ç½®oozie-site.xmlæ–‡ä»¶ oozie.email.smtp.host smtp.163.com oozie.email.from.address test@163.com oozie.email.smtp.auth true oozie.email.smtp.username test@163.com oozie.email.smtp.password 123456 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.2&quot; name=&quot;map-reduce-wf&quot;&gt; &lt;start to=&quot;mr-node&quot;/&gt; &lt;action name=&quot;mr-node&quot;&gt; &lt;map-reduce&gt; &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt; &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt; &lt;prepare&gt; &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&quot;/&gt; &lt;/prepare&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.queue.name&lt;/name&gt; &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.mapper.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.reducer.class&lt;/name&gt; &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.map.tasks&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.input.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.output.dir&lt;/name&gt; &lt;value&gt;/user/root/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; &lt;/map-reduce&gt; &lt;ok to=&quot;goemail&quot; /&gt; &lt;error to=&quot;fail&quot;/&gt; &lt;/action&gt; &lt;kill name=&quot;fail&quot;&gt; &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt; &lt;/kill&gt;&lt;action name=&quot;goemail&quot;&gt; &lt;email xmlns=&quot;uri:oozie:email-action:0.1&quot;&gt; &lt;to&gt;5lovezhm@163.com&lt;/to&gt; &lt;subject&gt;Email notifications for $&#123;wf:id()&#125;&lt;/subject&gt; &lt;body&gt;The wf $&#123;wf:id()&#125; successfully completed.&lt;/body&gt; &lt;/email&gt; &lt;ok to =&quot;end&quot;/&gt; &lt;error to =&quot;fail&quot;/&gt; &lt;/action&gt; &lt;end name=&quot;end&quot;/&gt;&lt;/workflow-app&gt; å¦‚æœæŠ¥é”™ï¼š JA006: Call From hadoop000/xxx.xx.xx.xx to hadoop000:10020 failed on connection exception: java.net.ConnectException: ????; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused å¯åŠ¨history$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH phoenixå®‰è£…]]></title>
    <url>%2F2017%2F04%2F18%2FCDH%20phoenix%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[é¦–å…ˆä¸‹è½½jdk ç•¥å®‰è£…maven ä¸‹è½½yumæºwget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo å®‰è£…mavenï¼šyum -y install apache-maven ç¼–è¯‘phoenixæ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬çš„phoenixhttps://github.com/chiastic-security/phoenix-for-cloudera/tree/4.8-HBase-1.2-cdh5.8 ä¸‹è½½ï¼ˆåšä¸»ä¸‹è½½åˆ°/softä¸‹ï¼‰å¹¶ç¼–è¯‘mvn clean package -DskipTests -Dcdh.flume.version=1.6.0 æ‰“å¼€è·¯å¾„ï¼š/soft/phoenix-for-cloudera-4.8-HBase-1.2-cdh5.8/phoenix-assembly/target æ‰¾åˆ°phoenix-4.8.0-cdh5.8.0.tar.gzå°†phoenix-4.8.0-cdh5.8.0ä¸­çš„phoenix-4.8.0-cdh5.8.0-server.jaræ‹·è´åˆ°æ¯ä¸€ä¸ªRegionServerä¸‹/opt/cloudera/parcels/CDH/lib/hbase/lib å¯åŠ¨ï¼š./sqlline.py slave1:2181/hbase å¦‚æœæŠ¥é”™ï¼šError: org.apache.hadoop.hbase.DoNotRetryIOException: Class org.apache.phoenix.coprocessor.MetaDataEndpointImpl cannot be loaded Set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks at org.apache.hadoop.hbase.master.HMaster.warnOrThrowExceptionForFailure(HMaster.java:1741) at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1602) at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1531) at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:469) at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55682) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:185) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:165) (state=08000,code=101) éœ€è¦ç‚¹å‡»CDH-&gt;hbase -&gt;é…ç½®-&gt;é«˜çº§hbase-site.xml çš„ HBase æœåŠ¡é«˜çº§é…ç½®ä»£ç æ®µï¼ˆå®‰å…¨é˜€ï¼‰æ·»åŠ hbase.table.sanity.checksfalse é‡å¯å³å¯å¦‚æœæƒ³è¦å®‰è£…phonenixå…ˆä¸‹è½½http://squirrel-sql.sourceforge.net/#installation å®‰è£…ï¼ˆå®‰è£…çš„æ—¶å€™å‹¾é€‰imort-dataå’Œmysqlï¼‰æ³¨ï¼šä¸‹è½½åç›´æ¥å®‰è£…jaråŒ…å³å¯ï¼Œä¸è¦è§£å‹ç¼©ç”±äºæ˜¯CDH æˆ‘æ‹·è´äº†å¦‚ä¸‹jaré˜²ç›—libä¸‹é¢phoenix-core-4.8.0-cdh5.8.0.jarphoenix-4.8.0-cdh5.8.0-client.jarphoenix-pherf-4.8.0-cdh5.8.0-minimal.jar é“¾æ¥å‚è€ƒ;http://www.cnblogs.com/raphael5200/p/5260198.htmlæœ‰å›¾ phoenix API http://phoenix.apache.org/language/functions.html http://phoenix.apache.org/language/index.html]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH-spark]]></title>
    <url>%2F2017%2F03%2F29%2FCDH-spark%2F</url>
    <content type="text"><![CDATA[æ·»åŠ æœåŠ¡é€‰æ‹©spark on yarnå®‰è£…çš„æ—¶å€™ä¼šé€‰æ‹©history serverï¼Œåšä¸»é€‰æ‹©çš„æ˜¯slave1ï¼Œåç»­å¯ä»¥é€šè¿‡è¿™ä¸ªåœ°å€çœ‹sparkçš„UI åšä¸»å®‰è£…çš„æ˜¯ï¼šscala version 2.10.5ï¼ˆåç»­å†UIä¸Šçœ‹åˆ°çš„ï¼‰spark version 1.6.0ï¼ˆæ‰§è¡Œspark-shellæ—¶çœ‹åˆ°çš„ï¼‰ å®‰è£…å®Œåæµ‹è¯•ï¼š su hdfsspark-submit â€“class org.apache.spark.examples.SparkPi â€“executor-memory 1G â€“total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100 æŸ¥çœ‹UIï¼šhttp://slave1:18088/ ç¼–å†™ä»£ç æµ‹è¯•ç¦»çº¿åŠŸèƒ½ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.zwjf.spark&lt;/groupId&gt; &lt;artifactId&gt;SpakrZwujf&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.10.5&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-make:transitive&lt;/arg&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;cn.zwjf.uidPhone&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 1234567891011121314151617181920212223package cn.zwjfimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/29. */object uidPhone &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName("uid_phone_etl") val sc = new SparkContext(conf) sc.textFile(args(0)).map(line =&gt; &#123; val arr = line.split(",") if(arr.length &gt; 1 &amp;&amp; !arr(1).equals("null"))&#123; arr(0)+","+arr(1) &#125;else&#123; "null" &#125; &#125;).filter(!_.equals("null")).saveAsTextFile(args(1)) sc.stop() &#125;&#125; æ‰“åŒ…æäº¤åˆ°æœåŠ¡å™¨ spark-submit \â€“class cn.zwjf.uidPhone â€“executor-memory 2G â€“total-executor-cores 4 \/var/lib/hadoop-hdfs/data/spark/SpakrZwujf-1.0-SNAPSHOT.jar \/bigdata/data/hdfs/Baidu/uid_phone/* \/bigdata/data/hdfs/Baidu/uid_phone3 /bigdata/data/hdfs/Baidu/uid_phone/* /bigdata/data/hdfs/Baidu/uid_phone2 æœ¬åœ°è°ƒè¯•æ–¹æ³•ï¼šã€http://www.jianshu.com/p/c801761ce088]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[strom ç›‘æ§]]></title>
    <url>%2F2017%2F03%2F26%2Fstrom-%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[çœ‹å½“å‰GCçš„æƒ…å†µjstat -gcutil ç«¯å£å· 1000 çœ‹waitçš„æ˜¯å¦è¿‡é«˜top -p ç«¯å£å· æŠŠæŸä¸ªç«¯å£å½“å‰çš„å †æ ˆä¿¡æ¯dumpåˆ°æŸä¸ªæ–‡ä»¶jstack ç«¯å£å· &gt; æ–‡ä»¶ çœ‹ç£ç›˜iostat -x 1 å¦‚æœå‘ç°åˆ†é…ä¸å‡åŒ€ï¼Œä¾‹å¦‚FiledGrouping userid=1ç‰¹åˆ«å¤šï¼Œå¯¼è‡´ä¸‹æ¸¸noltæˆ–executeææ­»äº†ï¼Œé‡æ–°å¯åŠ¨ä¹Ÿå¾ˆå¿«å°±æ­»æ‰ã€‚å¯ä»¥å¯¹userid åšhashæˆ–md5æˆ–è€…ç»„æˆæ›´å¤šæ¡ä»¶ï¼Œè®©ä¸‹æ¸¸Boltåˆ†å¸ƒæ›´å‡åŒ€ complete latency(è¿™é‡Œä¸»è¦é’ˆå¯¹spout)æ˜¯ä¸€ä¸ªtupleä»å‘å‡º, åˆ°ç»è¿‡boltå¤„ç†å®Œæˆ, æœ€ç»ˆè°ƒç”¨spoutçš„ackè¿™ä¸ªå®Œæ•´çš„è¿‡ç¨‹æ‰€èŠ±çš„æ—¶é—´.complete latencyå’Œååé‡å¯ä»¥è®¤ä¸ºæ˜¯äº’æ–¥çš„, äºŒè€…ä¸å¯å…¼å¾—, è¦æé«˜ååé‡, å¿…ç„¶ä¼šå¢åŠ complete latency, åä¹‹äº¦ç„¶. æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ ¹æ®å…·ä½“çš„åœºæ™¯, åœ¨äºŒè€…ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡.complete latencyå—åˆ°ä¸¤ä¸ªå› ç´ çš„å½±å“:boltçš„å¤„ç†æ—¶é—´spoutçš„parallelismæ•°é‡ å®˜ç½‘å¯¹å¹¶è¡Œåº¦ç­‰çš„è®¾ç½®æ–¹å¼ï¼šstorm.apache.org/releases/1.0.3/Understanding-the-parallelism-of-a-Storm-topology.html å¦‚æœexecutorsæ€»æ˜¯æŒ‚å°±éœ€è¦å…³æ³¨]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[strom-Metrics]]></title>
    <url>%2F2017%2F03%2F25%2Fstrom-Metrics%2F</url>
    <content type="text"><![CDATA[Storm æä¾›äº†ä¸€ä¸ªå¯ä»¥è·å–æ•´ä¸ªæ‹“æ‰‘ä¸­æ‰€æœ‰çš„ç»Ÿè®¡ä¿¡æ¯çš„metricsæ¥å£ã€‚Storm å†…éƒ¨é€šè¿‡è¯¥æ¥å£è·Ÿè¸ªå„ç±»ç»Ÿè®¡æ•°å­—ï¼šexecutor å’Œ acker çš„æ•°é‡ã€æ¯ä¸ª bolt çš„å¹³å‡å¤„ç†æ—¶å»¶ã€worker ä½¿ç”¨çš„æœ€å¤§å †å®¹é‡ç­‰ç­‰ï¼Œè¿™äº›ä¿¡æ¯éƒ½å¯ä»¥åœ¨ Nimbus çš„ UI ç•Œé¢ä¸­çœ‹åˆ°ã€‚ä½¿ç”¨ Metrics åªéœ€è¦å®ç°ä¸€ä¸ªæ¥å£æ–¹æ³•ï¼šgetValueAndResetï¼Œåœ¨æ–¹æ³•ä¸­å¯ä»¥æŸ¥æ‰¾æ±‡æ€»å€¼ã€å¹¶å°†è¯¥å€¼å¤ä½ä¸ºåˆå§‹å€¼ã€‚ä¾‹å¦‚ï¼Œåœ¨ MeanReducer ä¸­å°±å®ç°äº†é€šè¿‡è¿è¡Œæ€»æ•°é™¤ä»¥å¯¹åº”çš„è¿è¡Œè®¡æ•°çš„æ–¹å¼æ¥æ±‚å–å‡å€¼ï¼Œç„¶åå°†ä¸¤ä¸ªå€¼éƒ½é‡æ–°è®¾ç½®ä¸º 0ã€‚ Storm æä¾›äº†ä»¥ä¸‹å‡ ç§ metric ç±»å‹ï¼š AssignableMetric â€“ å°† metric è®¾ç½®ä¸ºæŒ‡å®šå€¼ã€‚æ­¤ç±»å‹åœ¨ä¸¤ç§æƒ…å†µä¸‹æœ‰ç”¨ï¼š1. metric æœ¬èº«ä¸ºå¤–éƒ¨è®¾ç½®çš„å€¼ï¼›2. ä½ å·²ç»å¦å¤–è®¡ç®—å‡ºäº†æ±‡æ€»çš„ç»Ÿè®¡å€¼ã€‚CombinedMetric â€“ å¯ä»¥å¯¹ metric è¿›è¡Œå…³è”æ›´æ–°çš„é€šç”¨æ¥å£ã€‚CountMetric â€“ è¿”å› metric çš„æ±‡æ€»ç»“æœã€‚å¯ä»¥è°ƒç”¨ incr() æ–¹æ³•æ¥å°†ç»“æœåŠ ä¸€ï¼›è°ƒç”¨ incrBy(n) æ–¹æ³•æ¥å°†ç»“æœåŠ ä¸Šç»™å®šå€¼ã€‚MultiCountMetric â€“ è¿”å›åŒ…å«ä¸€ç»„ CountMetric çš„ HashMapReducedMetricMeanReducer â€“ è·Ÿè¸ªç”±å®ƒçš„ reduce() æ–¹æ³•æä¾›çš„è¿è¡ŒçŠ¶æ€å‡å€¼ç»“æœï¼ˆå¯ä»¥æ¥å— Doubleã€Integerã€Long ç­‰ç±»å‹ï¼Œå†…ç½®çš„å‡å€¼ç»“æœæ˜¯ Double ç±»å‹ï¼‰ã€‚MeanReducer ç¡®å®æ˜¯ä¸€ä¸ªç›¸å½“æ£’çš„å®¶ä¼™ã€‚MultiReducedMetric â€“ è¿”å›åŒ…å«ä¸€ç»„ ReducedMetric çš„ HashMap è‡ªå®šä¹‰Metricï¼šä»£ç æ³¨å†Œï¼šconf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);æˆ–è€…ä¿®æ”¹é…ç½®æ–‡ä»¶topology.metrics.consumer.register: class: â€œorg.apache.storm.metric.LoggingMetricsConsumerâ€parallelism.hint: 1 class: â€œorg.apache.storm.metric.HttpForwardingMetricsConsumerâ€parallelism.hint: 1argument: â€œhttp://example.com:8080/metrics/my-topology/â€œ æ„å»ºè‡ªå·±çš„Metricå®šä¹‰ä¸å¯è¢«åºåˆ—å·ç±»å‹transientprivate transient CountMetric countMetric; é‡å†™prepare@Overridepublic void prepare(Map conf, TopologyContext context, OutputCollector collector) { // other intialization here. countMetric = new CountMetric(); context.registerMetric(â€œexecute_countâ€, countMetric, 60);} boltçš„execute public void execute(Tuple input) { countMetric.incr(); // handle tuple here.} builtin_metrics.clj ä¸ºå†…éƒ¨çš„ metrics è®¾ç½®äº†æ•°æ®ç»“æ„ï¼Œä»¥åŠå…¶ä»–æ¡†æ¶ç»„ä»¶å¯ä»¥ç”¨äºæ›´æ–°çš„è™šæ‹Ÿæ–¹æ³•ã€‚metrics æœ¬èº«æ˜¯åœ¨å›è°ƒä»£ç ä¸­å®ç°è®¡ç®—çš„ â€“ è¯·å‚è€ƒ clj/b/s/daemon/daemon/executor.clj ä¸­çš„ ack-spout-msg çš„ä¾‹å­ã€‚ LoggingMetricsConsumer,ç»Ÿè®¡æŒ‡æ ‡å€¼å°†è¾“å‡ºåˆ°metric.logæ—¥å¿—æ–‡ä»¶ä¸­ã€‚]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bashè„šæœ¬å®æˆ˜]]></title>
    <url>%2F2017%2F03%2F19%2Fbash%E8%84%9A%E6%9C%AC%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[ç»ƒä¹ é¢˜ç›®å†™ä¸€ä¸ªè„šæœ¬getinterface.shï¼Œè„šæœ¬å¯ä»¥æ¥å—å‚æ•°(i,I,a)ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š (1)ä½¿ç”¨ä»¥ä¸‹å½¢å¼ï¼šgetinterface.sh [-i interface|-I IP|-a] (2)å½“ç”¨æˆ·ä½¿ç”¨-ié€‰é¡¹æ—¶ï¼Œæ˜¾ç¤ºå…¶æŒ‡å®šç½‘å¡çš„IPåœ°å€ï¼› (3)å½“ç”¨æˆ·ä½¿ç”¨-Ié€‰é¡¹æ—¶ï¼Œæ˜¾ç¤ºå…¶åé¢çš„IPåœ°å€æ‰€å±çš„ç½‘ç»œæ¥å£ï¼›ï¼ˆå¦‚ 192.168.199.183ï¼šeth0ï¼‰ (4)å½“ç”¨æˆ·å•ç‹¬ä½¿ç”¨-aé€‰é¡¹æ—¶ï¼Œæ˜¾ç¤ºæ‰€æœ‰ç½‘ç»œæ¥å£åŠå…¶IPåœ°å€ï¼ˆloé™¤å¤–ï¼‰ åˆ†æå‚æ•°1ä¸ºparam1 å‚æ•°2ä½param2ï¼ˆ1ï¼‰åˆ©ç”¨CAT &lt;&lt; EOF * EOFæ‰“å°ä¿¡æ¯12345678cat &lt;&lt; EOF getinterface.sh [-i interface|-I IP|-a] -i interface) show ip of the interface -I IP) show interface of the IP and IP with :; -a) list all interfaces and their IPs except lo; *) quit=================================================================EOF ï¼ˆ2ï¼‰æ ¡éªŒæ¥å£æ˜¯å¦å­˜åœ¨ï¼ˆå‚æ•°2ï¼‰1234ifconfig $interface &gt;/dev/null $? -ne 1 #å¦‚æœå­˜åœ¨æ‰“å°ç½‘å¡çš„IPifconfig $parma2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' ï¼ˆ3ï¼‰å…ˆæ‰“å°æ‰€æœ‰æ¥å£1netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125; å¹¶å­˜åœ¨ä¸€ä¸ªå˜é‡ä¸­ 1list=`netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125;'` éå†é›†åˆï¼ŒæŸ¥çœ‹å’Œè¾“å…¥IPç›¸åŒçš„æ‰“å°IPå’Œæ¥å£12345678for inter in $list; do ip_temp=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' if [ $IP == $ip_temp ];then echo "$IP : $inter" exit 0 fi done ï¼ˆ4ï¼‰éå†é›†åˆï¼Œè¿‡æ»¤æ¥å£ä¸ºloçš„ï¼Œæ‰“å°IPå’Œæ¥å£12345678for inter in $list; do ip_temp=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;' if [ $inter != "lo" ];then echo "$IP : $inter" exit 0 fi done æœ€ç»ˆçš„è„šæœ¬å¦‚ä¸‹ï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344cat &lt;&lt; EOF getinterface.sh [-i interface|-I IP|-a] -i interface) show ip of the interface -I IP) show interface of the IP and IP with :; -a) list all interfaces and their IPs except lo; *) quit=================================================================EOFread -p "please choice " param1 param2list=`netstat -i | sed -n '3,65535p'|awk -F" " '&#123;print $1&#125;'`if [[ "$param1" == '-i' ]]; then ifconfig $interface &gt;/dev/null flag=$? if [ $flag -ne 1 ];then ip=`ifconfig $param2 | awk -F" " '/inet.*netmask/&#123;print $2&#125;'` echo "$param2 $ip" else echo "$parma2 is not exist" fielif [[ "$param1" == '-I' ]]; thenfor lt in $list; do ip =$(ifconfig $lt | awk -F" " '/inet.*netmask/&#123;print $2&#125;') #ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;') if [ $param2 == $ip ];then echo "$lt : $ip" fi doneelif [[ "$param1" == '-a' ]]; then for lt in $list; do ip=$(ifconfig $lt | awk -F" " '/inet.*netmask/&#123;print $2&#125;') #ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;') if [ $lt != "lo" ];then echo "$lt : $ip" exit 0 fi doneelse echo "quit" exit 0fi æ³¨æ„å¦‚ä¸Šå†…å®¹æ ¹æ®ä½ ä¸ªäººæœºå™¨æ˜¾ç¤ºæƒ…å†µè€Œå®šï¼Œå¦‚æœæ˜¯æ˜¾ç¤º inet addr:127.0.0.1 Mask:255.0.0.0 å°±è¦æŠŠæ­£åˆ™æ”¹æˆ ip=$(ifconfig $lt | awk -Fâ€ â€œ â€˜/inet.*Mask/{print $2}â€™ | awk -F: â€˜{print $2}â€™)]]></content>
      <categories>
        <category>bash</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH hadoopå®æˆ˜]]></title>
    <url>%2F2017%2F03%2F14%2FCDH%20hadoop%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[è§£å†³é—®é¢˜ï¼šå› ä¸ºæ•°æ®æ‰‹æœºå·ä¸€é¡¹åŒ…å«ä¸ºNULLçš„æ•°æ®ï¼Œéœ€è¦æ¸…æ´—ã€‚ åˆ›å»ºå·¥ç¨‹ï¼Œæ·»åŠ å¦‚ä¸‹ä¾èµ–123456789101112131415161718192021222324&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; ç¼–å†™mapperç±»1234567891011121314151617181920212223242526package map;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * Created by Administrator on 2017/3/14. */public class UidPhoneMapper extends Mapper&lt;Text, Text, Text, Text&gt; &#123; @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; String arr[] = key.toString().split(","); if (arr.length != 2) &#123; return; &#125; if(arr[1] == null)&#123; return; &#125; context.write(new Text(arr[0]), new Text(arr[1])); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package main;import map.UidPhoneMapper;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.GenericOptionsParser;import java.io.IOException;import org.apache.hadoop.fs.Path;/** * Created by Administrator on 2017/3/14. */public class UidPhoneDropNull &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); GenericOptionsParser parser = new GenericOptionsParser(conf, args); String[] otherArgs = parser.getRemainingArgs(); if (args.length != 2) &#123; System.err.println("Usage: NewlyJoin &lt;inpath&gt; &lt;output&gt;"); System.exit(2); &#125; Job job = new Job(conf, "UidPhoneDropNull"); // è®¾ç½®è¿è¡Œçš„job job.setJarByClass(UidPhoneDropNull.class); // è®¾ç½®Mapç›¸å…³å†…å®¹ job.setMapperClass(UidPhoneMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setInputFormatClass(KeyValueTextInputFormat.class); //è®¾ç½®æ–‡ä»¶è¾“å…¥æ ¼å¼ job.setNumReduceTasks(0); //è®¾ç½®Reduceä¸ªæ•°ä¸º0 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); // è®¾ç½®è¾“å…¥å’Œè¾“å‡ºçš„ç›®å½• FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); // æ‰§è¡Œï¼Œç›´åˆ°ç»“æŸå°±é€€å‡º System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; è¿è¡Œä¸Šé¢çš„ç¨‹åº12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[hdfs@slave3 jar]$ hadoop jar bigdataMR.jar main.UidPhoneDropNull /bigdata/data/hdfs/BD/UP/* /bigdata/data/hdfs/BD/UP2 17/03/14 17:37:54 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:803217/03/14 17:37:55 INFO input.FileInputFormat: Total input paths to process : 317/03/14 17:37:55 INFO mapreduce.JobSubmitter: number of splits:317/03/14 17:37:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_004417/03/14 17:37:55 INFO impl.YarnClientImpl: Submitted application application_1488970234114_004417/03/14 17:37:55 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0044/17/03/14 17:37:55 INFO mapreduce.Job: Running job: job_1488970234114_004417/03/14 17:38:00 INFO mapreduce.Job: Job job_1488970234114_0044 running in uber mode : false17/03/14 17:38:00 INFO mapreduce.Job: map 0% reduce 0%17/03/14 17:38:04 INFO mapreduce.Job: map 33% reduce 0%17/03/14 17:38:05 INFO mapreduce.Job: map 100% reduce 0%17/03/14 17:38:05 INFO mapreduce.Job: Job job_1488970234114_0044 completed successfully17/03/14 17:38:05 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=378748 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=13514456 HDFS: Number of bytes written=10824901 HDFS: Number of read operations=15 HDFS: Number of large read operations=0 HDFS: Number of write operations=6 Job Counters Launched map tasks=3 Data-local map tasks=3 Total time spent by all maps in occupied slots (ms)=8240 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=8240 Total vcore-seconds taken by all map tasks=8240 Total megabyte-seconds taken by all map tasks=8437760 Map-Reduce Framework Map input records=661590 Map output records=485478 Input split bytes=393 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=175 CPU time spent (ms)=3730 Physical memory (bytes) snapshot=712597504 Virtual memory (bytes) snapshot=8316772352 Total committed heap usage (bytes)=814743552 File Input Format Counters Bytes Read=13514063 File Output Format Counters Bytes Written=10824901[hdfs@slave3 jar]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP2Found 4 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7609438 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00000-rw-r--r-- 3 hdfs supergroup 3215441 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00001-rw-r--r-- 3 hdfs supergroup 22 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00002 æœ€åå°†æ•°æ®å¯¼å…¥åˆ°hivehive&gt; create EXTERNAL table IF NOT EXISTS UP2 (uid STRING,phone STRING) row format delimited fields terminated by â€˜,â€™ location â€˜/bigdata/data/hdfs/BD/UP2/â€˜;OKTime taken: 0.026 secondshive&gt; select count(1) from UP2;Query ID = hdfs_20170314174141_1b60d560-2036-44e6-ab65-19f17efc5b1bTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=In order to limit the maximum number of reducers: set hive.exec.reducers.max=In order to set a constant number of reducers: set mapreduce.job.reduces=Starting Job = job_1488970234114_0045, Tracking URL = http://master2:8088/proxy/application_1488970234114_0045/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0045Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 17:41:33,200 Stage-1 map = 0%, reduce = 0%2017-03-14 17:41:38,371 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.54 sec2017-03-14 17:41:43,486 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.38 secMapReduce Total cumulative CPU time: 4 seconds 380 msecEnded Job = job_1488970234114_0045MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 4.38 sec HDFS Read: 10831709 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 380 msecOK485478Time taken: 16.267 seconds, Fetched: 1 row(s) æ¯”æ¸…æ´—å‰æ•°æ®661590å°‘äº†176112æ¡]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH hive å®æˆ˜]]></title>
    <url>%2F2017%2F03%2F14%2FCDH%20hive%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[åˆ›å»ºæ•°æ®åº“123456hive&gt; create database BD;OKTime taken: 0.161 secondshive&gt; use BD;OKTime taken: 0.013 seconds åˆ›å»ºè¡¨12345678hive&gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by ',' ;OKTime taken: 0.211 secondshive&gt; load data inpath '/bigdata/data/hdfs/shuju/SO/part-m-00000' into table UP;Loading data to table BD.UPTable BD.UP stats: [numFiles=1, totalSize=898611236]OKTime taken: 0.317 seconds æŸ¥è¯¢è¡¨æ•°æ®1234567891011121314151617181920212223242526hive&gt; select count(1) from UP;Query ID = hdfs_20170314111111_d29152a3-56b3-42f9-b17b-1ddaf7451117Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0025, Tracking URL = http://master2:8088/proxy/application_1488970234114_0025/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0025Hadoop job information for Stage-1: number of mappers: 4; number of reducers: 12017-03-14 11:11:49,267 Stage-1 map = 0%, reduce = 0%2017-03-14 11:11:55,519 Stage-1 map = 25%, reduce = 0%, Cumulative CPU 3.31 sec2017-03-14 11:11:56,542 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 16.57 sec2017-03-14 11:12:01,655 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 18.41 secMapReduce Total cumulative CPU time: 18 seconds 410 msecEnded Job = job_1488970234114_0025MapReduce Jobs Launched: Stage-Stage-1: Map: 4 Reduce: 1 Cumulative CPU: 18.41 sec HDFS Read: 899021344 HDFS Write: 8 SUCCESSTotal MapReduce CPU Time Spent: 18 seconds 410 msecOK5674200Time taken: 19.429 seconds, Fetched: 1 row(s) åˆ é™¤è¡¨ï¼š123hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.088 seconds æŸ¥çœ‹hdfsä¸Šçš„æ•°æ®ï¼Œå‘ç°æ•°æ®è¢«åˆ é™¤123[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/SOFound 1 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-13 19:16 /bigdata/data/hdfs/shuju/SO/_SUCCESS æµ‹è¯•2ï¼šé¦–å…ˆæŸ¥è¯¢ï¼Œå‘ç°biaomingæ•°æ®å­˜åœ¨1234[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/biaomingFound 2 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS-rw-r--r-- 3 hdfs supergroup 13514063 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/part-m-00000 åˆ›å»ºå¤–éƒ¨è¡¨ï¼š123hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' ;OKTime taken: 0.033 seconds åŠ è½½æ•°æ®12345hive&gt; load data inpath '/bigdata/data/hdfs/shuju/biaoming/part-m-00000' into table UP;Loading data to table BD.UPTable BD.UP stats: [numFiles=1, totalSize=13514063]OKTime taken: 0.201 seconds æŸ¥è¯¢æ•°æ®ï¼š12345678910111213141516171819202122232425hive&gt; select count(1) from UP;Query ID = hdfs_20170314115959_6004ef81-913b-4d38-99dc-1199cc6e73f2Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0027, Tracking URL = http://master2:8088/proxy/application_1488970234114_0027/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0027Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 11:59:09,331 Stage-1 map = 0%, reduce = 0%2017-03-14 11:59:14,463 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.06 sec2017-03-14 11:59:19,587 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4.9 secMapReduce Total cumulative CPU time: 4 seconds 900 msecEnded Job = job_1488970234114_0027MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 4.9 sec HDFS Read: 13520683 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 4 seconds 900 msecOK661590Time taken: 17.707 seconds, Fetched: 1 row(s) åˆ é™¤æ•°æ®123hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.037 seconds å†æŸ¥çœ‹å…ƒæ•°æ®ï¼Œå‘ç°ä¾ç„¶è¢«åˆ é™¤äº†123[root@slave3 data]# hadoop fs -ls /bigdata/data/hdfs/shuju/biaomingFound 1 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS æµ‹è¯•3ï¼šé‡æ–°ä¸Šä¼ æ•°æ®ï¼š1sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver --connect 'jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=BD' --table=UP --target-dir /bigdata/data/hdfs/BD/UP --split-by uid -m 3 æŸ¥çœ‹æ•°æ®ï¼š1234567[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPFound 5 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000-rw-r--r-- 3 hdfs supergroup 2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001-rw-r--r-- 3 hdfs supergroup 3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002-rw-r--r-- 3 hdfs supergroup 22 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00003 åˆ›å»ºå¤–éƒ¨è¡¨æ•°æ®ï¼š123456789101112131415161718192021222324252627282930hive&gt; select count(1) from UP;FAILED: SemanticException [Error 10001]: Line 1:21 Table not found 'UP'hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' location '/bigdata/data/hdfs/BD/UP/';OKTime taken: 0.03 secondshive&gt; select count(1) from UP;Query ID = hdfs_20170314141111_5be2c701-ad6a-4717-8801-513f38d64928Total jobs = 1Launching Job 1 out of 1Number of reduce tasks determined at compile time: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1488970234114_0031, Tracking URL = http://master2:8088/proxy/application_1488970234114_0031/Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job -kill job_1488970234114_0031Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12017-03-14 14:11:48,608 Stage-1 map = 0%, reduce = 0%2017-03-14 14:11:54,743 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.26 sec2017-03-14 14:11:58,833 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 5.08 secMapReduce Total cumulative CPU time: 5 seconds 80 msecEnded Job = job_1488970234114_0031MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 Cumulative CPU: 5.08 sec HDFS Read: 13520952 HDFS Write: 7 SUCCESSTotal MapReduce CPU Time Spent: 5 seconds 80 msecOK661590Time taken: 16.253 seconds, Fetched: 1 row(s) æ­¤æ—¶å†åˆ é™¤è¡¨åæ•°æ®è¿˜å­˜åœ¨ã€‚123456789hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.037 seconds[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPFound 5 items-rw-r--r-- 3 hdfs supergroup 0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS-rw-r--r-- 3 hdfs supergroup 7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000-rw-r--r-- 3 hdfs supergroup 2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001-rw-r--r-- 3 hdfs supergroup 3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002 åˆ›å»ºæ™®é€šè¡¨ï¼š123hive&gt; create table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by ',' location '/bigdata/data/hdfs/BD/UP/';OKTime taken: 0.029 seconds åˆ é™¤è¡¨åæ•°æ®ä¸å­˜åœ¨12345hive&gt; DROP TABLE IF EXISTS UP;OKTime taken: 0.046 seconds[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UPls: `/bigdata/data/hdfs/BD/UP': No such file or directory æ€»ç»“ç”¨loadæ–¹å¼æ— è®ºå¤–éƒ¨è¡¨è¿˜æ˜¯å†…éƒ¨è¡¨æ•°æ®éƒ½ä¼šåˆ é™¤ï¼Œç”¨locationæ–¹å¼ï¼Œå¤–éƒ¨è¡¨ä¸ä¼šåˆ é™¤æ•°æ®ï¼Œå†…éƒ¨è¡¨ä¼šåˆ é™¤æ•°æ®]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH sqoop1 å®æˆ˜]]></title>
    <url>%2F2017%2F03%2F13%2FCDH%20sqoop1%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[ä¸‹è½½sqlserverçš„jdbcé©±åŠ¨åŒ…https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599è§£å‹å°†sqljdbc4.jaræ”¾åœ¨ï¼š/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/lib ä¸‹è½½SQL Server-Hadoop Connectorï¼šsqoop-sqlserver-1.0.tar.gz http://www.microsoft.com/en-us/download/details.aspx?id=27584 å¯¼å…¥æ•°æ®ï¼š[root@slave3 sqoop]# ./bin/sqoop import â€“connect â€˜jdbc: server://10.105.32.246 username=sa password=123456 database=databaseName â€“table=tableName â€“target-dir /bigdata/data/hdfsWarning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/bin/../../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.+======================================================================+| Error: JAVA_HOME is not set |+â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”-+| Please download the latest Sun JDK from the Sun Java web site || &gt; http://www.oracle.com/technetwork/java/javase/downloads || || HBase requires Java 1.7 or later. |+======================================================================+Error: JAVA_HOME is not set and could not be found. æŠ¥é”™è§£å†³æ–¹æ³•ï¼šä¿®æ”¹binä¸‹çš„configure-sqoop æ³¨é‡Šä»¥ä¸‹ä»£ç ï¼š123456789101112131415161718192021222324252627282930313233343536373839#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hive-hcatalog# elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hcatalog# else# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog# if [ ! -d $&#123;HCAT_HOME&#125; ]; then# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog# fi# fi#fi#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/accumulo&quot; ]; then# ACCUMULO_HOME=/usr/lib/accumulo# else# ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo# fi#fi#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hive-hcatalog# elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hcatalog# else# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog# if [ ! -d $&#123;HCAT_HOME&#125; ]; then# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog# fi# fi#fi#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/accumulo&quot; ]; then# ACCUMULO_HOME=/usr/lib/accumulo# else# ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo# fi#fi åœ¨æ‰§è¡Œå¦‚æœæç¤ºJAVA_HOMEä¸å­˜åœ¨ï¼Œæ‰‹åŠ¨æ‰§è¡Œä¸€ä¸‹export JAVA_HOME=/usr/java/jdk1.8.0_121 å†æ‰§è¡Œæç¤ºï¼šERROR tool.BaseSqoopTool: Error parsing arguments for import: ./bin/sqoop import â€“connect â€˜jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseNameâ€™ â€“table=tableName â€“target-dir /bigdata/data/hdfs â€“split-by order_id â€“fields-terminated-by â€˜\tâ€™ â€“m 3 éœ€è¦ä¸‹è½½sqoop-sqlserverä¸‹è½½åœ°å€å·²ç»å¤±æ•ˆï¼Œåœ¨CSDNä¸Šæ‰¾åˆ°ï¼šhttps://passport.csdn.net/account/login?from=http://download.csdn.net/download/nma_123456/9405343 è§£å‹è¿›å…¥ç›®å½•æ‰§è¡Œï¼š[root@slave3 sqoop-sqlserver-1.0]# export SQOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop[root@slave3 sqoop-sqlserver-1.0]# ./install.sh å†æ¬¡æ‰§è¡Œï¼šæŠ¥å¦‚ä¸‹é”™è¯¯Exception in thread â€œmainâ€ java.lang.NoClassDefFoundError: org/json/JSONObject at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:43) at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:767) at org.apache.sqoop.mapreduce.JobBase.putSqoopOptionsToConfiguration(JobBase.java:388) at org.apache.sqoop.mapreduce.JobBase.createJob(JobBase.java:374) at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:256) at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236)Caused by: java.lang.ClassNotFoundException: org.json.JSONObject at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) åˆ†æåŸå› ç¼ºå°‘org/json/JSONObjectï¼Œä¸Šç½‘ä¸‹è½½ä¸€ä¸ªjson.jaråœ°å€ï¼šhttp://download.csdn.net/download/haixia_12/8462933 æ‰”è¿›å»é‡æ–°æ‰§è¡Œå°±OKäº† æœ€ç»ˆå‘½ä»¤ï¼šsqoop import â€“driver com.microsoft.sqlserver.jdbc.SQLServerDriver â€“connect â€˜jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseNameâ€™ â€“table=tableName â€“target-dir /bigdata/data/hdfs/cards â€“split-by order_id -m 3 æŸ¥çœ‹sqlserverä¸Šæœ‰ä»€ä¹ˆæ•°æ®ï¼š sqoop list-tables â€“driver com.microsoft.sqlserver.jdbc.SQLServerDriver â€“connect â€˜jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseNameâ€™ sqlserverå¦‚æœè¿æ¥å¤±è´¥ï¼Œè¦çœ‹æ˜¯å¦å¼€å¯äº†è¿œç¨‹è®¿é—®å’ŒTCP/UDPç«¯å£æ˜ å°„ æŠ¥é”™17/03/13 17:20:01 ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.17/03/13 17:20:01 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf å¢åŠ ï¼šexport HADOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoopexport HIVE_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive export HIVE_CONF_DIR=/etc/hive/confexport HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib/*]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH sqoop1 å®æˆ˜-å¸¦æ¡ä»¶çš„å¯¼å…¥]]></title>
    <url>%2F2017%2F03%2F13%2FCDH%20sqoop1%20%E5%AE%9E%E6%88%98-%E5%B8%A6%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[å¯¼å…¥[hdfs@slave3 lib]$ sqoop import â€“connect jdbc:mysql://10.105.10.46:3306/shuju â€“username username â€“password password â€“query â€œSELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1â€ â€“fields-terminated-by â€œ,â€ â€“lines-terminated-by â€œ\nâ€ â€“split-by id â€“hive-import â€“create-hive-table â€“hive-table card_record â€“target-dir /bigdata/data/hdfs/shuju/biaoming17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.017/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain â€˜$CONDITIONSâ€™ in WHERE clause. at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332) at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858) at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658) at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107) at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488) at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615) at org.apache.sqoop.Sqoop.run(Sqoop.java:143) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227) at org.apache.sqoop.Sqoop.main(Sqoop.java:236) ä¿®æ”¹ä¸ºï¼šsqoop import â€“connect jdbc:mysql://10.105.10.46:3306/shuju â€“username username â€“password password â€“query â€˜SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONSâ€™ â€“fields-terminated-by â€œ,â€ â€“lines-terminated-by â€œ\nâ€ â€“split-by id â€“hive-import â€“create-hive-table â€“hive-table card_record â€“target-dir /bigdata/data/hdfs/shuju/biaoming åœ¨æ‰§è¡Œï¼Œå‘ç°æŠ¥é”™è¯´æˆ‘databaseä¸å­˜åœ¨ï¼Œå¹¶åœ¨ç›®å½•ä¸‹ç”Ÿæˆäº†ä¸€ä¸ªmetastore_dbï¼ŒåŸæ¥æ˜¯æ‰§è¡Œsqoopçš„æœºå™¨ä¸å¯¹ï¼Œæ¢äº†ä¸€å°æœºå™¨æ‰§è¡Œæ²¡æœ‰é—®é¢˜ï¼Œå¾ˆå¥‡æ€ªæ˜æ˜æ˜¯åˆ†å¸ƒå¼çš„ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªé—®é¢˜å‘¢ æœ€ç»ˆçš„å…·ä½“æ—¥å¿—ï¼š [hdfs@master1 root]$ sqoop import â€“connect jdbc:mysql://10.105.10.46:3306/shuju â€“username username â€“password password â€“query â€˜SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONSâ€™ â€“fields-terminated-by â€œ,â€ â€“lines-terminated-by â€œ\nâ€ â€“split-by id â€“hive-import â€“create-hive-table â€“hive-table card_record â€“target-dir /bigdata/data/hdfs/shuju/biaoming â€“hive-database shujuWarning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.17/03/15 14:32:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.017/03/15 14:32:53 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/15 14:32:53 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/15 14:32:53 INFO tool.CodeGenTool: Beginning code generation17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:32:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/15 14:33:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.jar17/03/15 14:33:00 INFO mapreduce.ImportJobBase: Beginning query import.17/03/15 14:33:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/15 14:33:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/15 14:33:01 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:803217/03/15 14:33:02 INFO db.DBInputFormat: Using read commited transaction isolation17/03/15 14:33:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM (SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 1) ) AS t117/03/15 14:33:03 INFO db.IntegerSplitter: Split size: 7508318; Num splits: 4 from: 20079 to: 3005335417/03/15 14:33:03 INFO mapreduce.JobSubmitter: number of splits:417/03/15 14:33:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_005717/03/15 14:33:03 INFO impl.YarnClientImpl: Submitted application application_1488970234114_005717/03/15 14:33:03 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0057/17/03/15 14:33:03 INFO mapreduce.Job: Running job: job_1488970234114_005717/03/15 14:33:08 INFO mapreduce.Job: Job job_1488970234114_0057 running in uber mode : false17/03/15 14:33:08 INFO mapreduce.Job: map 0% reduce 0%17/03/15 14:33:15 INFO mapreduce.Job: map 75% reduce 0%17/03/15 14:33:16 INFO mapreduce.Job: map 100% reduce 0%17/03/15 14:33:16 INFO mapreduce.Job: Job job_1488970234114_0057 completed successfully17/03/15 14:33:16 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=608160 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=428 HDFS: Number of bytes written=17645206 HDFS: Number of read operations=16 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 Job Counters Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=17968 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=17968 Total vcore-seconds taken by all map tasks=17968 Total megabyte-seconds taken by all map tasks=18399232 Map-Reduce Framework Map input records=283659 Map output records=283659 Input split bytes=428 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=268 CPU time spent (ms)=12960 Physical memory (bytes) snapshot=1151381504 Virtual memory (bytes) snapshot=11157348352 Total committed heap usage (bytes)=1045954560 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=1764520617/03/15 14:33:16 INFO mapreduce.ImportJobBase: Transferred 16.8278 MB in 15.7965 seconds (1.0653 MB/sec)17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Retrieved 283659 records.17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE (1 = 0)17/03/15 14:33:16 WARN hive.TableDefWriter: Column update_time had to be cast to a less precise type in Hive17/03/15 14:33:16 INFO hive.HiveImport: Loading uploaded data into Hive Logging initialized using configuration in jar:file:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.propertiesOKTime taken: 1.874 secondsLoading data to table shuju.card_recordTable shuju.card_record stats: [numFiles=4, totalSize=17645206]OKTime taken: 0.367 seconds]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH å®‰è£…]]></title>
    <url>%2F2017%2F03%2F08%2FCDH%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[è¿è¡Œç¯å¢ƒï¼š ä¸»æœºIP ä¸»æœºå å†…å­˜ 1.1.1.147 po-master1 16G 1.1.1.127 po-master2 8G 1.1.1.118 po-slave1 8G 1.1.1.92 po-slave2 8G 1.1.1.230 po-slave3 8G é…ç½®ä¸»æœºå(åˆ†åˆ«åœ¨äº”å°æœºå™¨ä¸Šæ‰§è¡Œ)vi /etc/sysconfig/networkhostname +ä¸»æœºåä¾‹å¦‚ï¼š hostname po-master1 é…ç½®æ˜ å°„å…³ç³»(æŠŠä»¥ä¸‹äº”æ¡å‘½ä»¤åœ¨äº”å°æœºå™¨ä¸Šæ‰§è¡Œ)echo â€œ1.1.1.147 po-master1â€&gt;&gt;/etc/hostsecho â€œ1.1.1.127 po-master2â€&gt;&gt;/etc/hostsecho â€œ1.1.1.118 po-slave1â€&gt;&gt;/etc/hostsecho â€œ1.1.1.92 po-slave2â€&gt;&gt;/etc/hostsecho â€œ1.1.1.230 po-slave3â€&gt;&gt;/etc/hosts å®‰è£…JDKï¼ˆåœ¨po-master1ä¸Šæ‰§è¡Œï¼‰1.ä¸‹è½½JDKå®‰è£…åŒ…ï¼šjdk-8u102-linux-x64.tar.gzæ³¨ï¼šä½œè€…æ”¾åˆ°/soft/javaå…·ä½“ä½ç½®å¯è‡ªè¡Œå®‰æ’ å®‰è£…ï¼š 123cd /soft/javamkdir jdk1.8.0_121rpm -ivh jdk-7u76-linux-x64.rpm --prefix=/soft/java åˆ›å»ºè¿æ¥ 1ln -s -f jdk1.8.0_121/ jdk å¼€æ”¾ç«¯å£ï¼ˆäº”å°æœºå™¨ä¸Šéƒ½éœ€è¦é…ç½®ï¼‰1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEP/sbin/iptables -A INPUT -s 0.0.0.0/0 -p tcp --dport 22 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 22 -j ACCEPT/sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 1004 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50075 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 1006 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50070 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50470 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 50495 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8485 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8480 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8032 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8030 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8031 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8033 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8088 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8040 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8042 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8041 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 10020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 13562 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 19888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60020 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 60030 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8080 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8085 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9095 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9090 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9083 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 10000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 16000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 2888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3888 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 3181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 4181 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 8019 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 9010 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 11000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 11001 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 14000 -j ACCEPT /sbin/iptables -A INPUT -s x.x.x.x -p tcp --dport 14001 -j ACCEPT /etc/rc.d/init.d/iptables save å…³é—­ç«¯å£è¯¦è§£-å‚è€ƒCDHå®˜ç½‘https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_ports_cdh5.html æµ‹è¯•ï¼ˆå¯å¿½ç•¥ï¼‰1/etc/init.d/iptables status äº”å°æœºå™¨é…ç½®äº’ç›¸å…ç§˜é’¥ç™»é™†1.åˆ›å»ºsshæ–‡ä»¶å¦‚æœå·²ç»åˆ›å»ºä¸è¦è¦†ç›–cat ~/.ssh/id_rsa.pub &gt;&gt;~/.ssh/authorized_keysåˆ†åˆ«æŠŠäº”å°æœºå™¨çš„å…¬é’¥åŠ è½½åˆ°authorized_keys 2.vi /etc/ssh/sshd_configæ‰“å¼€å¦‚ä¸‹å†…å®¹HostKey /etc/ssh/ssh_host_rsa_keyRSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys 3.é‡å¯/etc/init.d/sshd restart 4.æµ‹è¯•sshssh po-master1ssh po-master2ssh po-slave1ssh po-slave2ssh po-slave3 å‘å…¶ä»–æœºå™¨åˆ†å‘jdkscp -rp /soft/java/ root@po-master2:/soft/javascp -rp /soft/java/ root@po-salve1:/soft/javascp -rp /soft/java/ root@po-salve2:/soft/javascp -rp /soft/java/ root@po-salve3:/soft/java é…ç½®ç¯å¢ƒå˜é‡(åˆ†åˆ«åœ¨äº”å°æœºå™¨ä¸Šæ‰§è¡Œ)æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š1234echo "export JAVA_HOME=/soft/java/jdk" &gt;&gt; /etc/profileecho "export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/" &gt;&gt; /etc/profileecho "export CLASSPATH=.:$JAVA_HOME/lib" &gt;&gt; /etc/profile. /etc/profile æµ‹è¯•ï¼ˆå¯å¿½ç•¥ï¼‰1234[root@po-master1 java]# java -versionjava version "1.8.0_121"Java(TM) SE Runtime Environment (build 1.8.0_121-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) é…ç½®NTPæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯ï¼ˆå› ä¸ºä½¿ç”¨é˜¿é‡Œäº‘æ­¤å¤„çœç•¥ï¼‰é…ç½®mysql1.ä¸Šä¼ mysqlæ–‡ä»¶ï¼ˆåšä¸»æ”¾åˆ°/soft/mysqlç›®å½•ä¸‹ï¼‰2.è§£å‹cd /soft/mysqltar -zxvf mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz -C /usr/local3.å°†ç›®å½•é‡å‘½åcd /usr/localmv mysql-5.7.17-linux-glibc2.5-x86_64/ mysql4.åˆ›å»ºdataç›®å½• mkdir /usr/local/mysql/data 5.å®‰è£…æ’ä»¶ï¼ˆç½‘ä¸Šæœ‰äººè¯´ä¸å®‰è£…æç¤ºlibiaoé”™è¯¯ï¼Œåšä¸»ç”¨é˜¿é‡Œäº‘libaioå·²ç»æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ‰€ä»¥ä¸ç”¨å®‰è£…ï¼Œä¹Ÿä¸çŸ¥é“æ²¡å®‰è£…æœ‰ä»€ä¹ˆåå¤„ï¼‰ yum install libaio 6.å®‰è£…mysqlcd /usr/local/mysql/bin./mysql_install_db â€“user=root â€“basedir=/usr/local/mysql â€“datadir=//usr/local/mysql/data 1.å®˜ç½‘ä¸‹è½½yumæºhttps://dev.mysql.com/downloads/repo/yum/2.å®‰è£…yumæºyum localinstall mysql57-community-release-el6-9.noarch.rpm3.å®‰è£…mysqlyum install mysql-community-server 4.åˆ›å»ºç»„å’Œç”¨æˆ·groupadd mysqluseradd mysql -g mysql 5.ä¿®æ”¹é…ç½®æ–‡ä»¶å¼€å¯äºŒè¿›åˆ¶æ—¥å¿—vi /etc/my.cnf ï¼ˆåœ¨[mysqld]ä¸‹é¢æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼‰server-id=1log-bin=/home/mysql/log/logbin.log 6.å¼€å¯æœåŠ¡service mysqld start 7.æŸ¥çœ‹mysqlé»˜è®¤çš„å¯†ç grep â€˜temporary passwordâ€™ /var/log/mysqld.log 8.æ ¹æ®å¯†ç è¿›å…¥mysqlmysql -u root -pALTER USER â€˜rootâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜MyNewPass4!â€™; ä¾‹å¦‚ï¼šALTER USER â€˜rootâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜passwordâ€™;Query OK, 0 rows affected (0.01 sec) æ³¨ï¼šMySQLâ€™s validate_password plugin is installed by default. This will require that passwords contain at least one upper case letter, one lower case letter, one digit, and one special character, and that the total password length is at least 8 characters. 9.æˆæƒï¼ˆç»™å…¶ä»–å››å°æœºå™¨æˆæƒï¼‰grant all privileges on oozie. to â€˜oozieâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on oozie. to â€˜oozieâ€™@â€™10.28.92.19â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on oozie. to â€˜oozieâ€™@â€™10.28.100.108â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on oozie. to â€˜oozieâ€™@â€™10.28.100.212â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on oozie.* to â€˜oozieâ€™@â€™10.28.100.254â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION; GRANT all privileges on . to â€˜rootâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO â€˜rootâ€™@â€™10.28.92.19â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO â€˜rootâ€™@â€™10.28.100.108â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO â€˜rootâ€™@â€™10.28.100.212â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;GRANT ALL PRIVILEGES ON . TO â€˜rootâ€™@â€™10.28.100.254â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION; GRANT ALL PRIVILEGES ON . TO â€˜rootâ€™@â€™182.48.105.23â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION; grant all privileges on hive. to â€˜hiveâ€™@â€™localhostâ€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on hive. to â€˜hiveâ€™@â€™10.28.92.19â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on hive. to â€˜hiveâ€™@â€™10.28.100.108â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on hive. to â€˜hiveâ€™@â€™10.28.100.212â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION;grant all privileges on hive.* to â€˜hiveâ€™@â€™10.28.100.254â€™ IDENTIFIED BY â€˜passwordâ€™ WITH GRANT OPTION; flush privileges; å…³äºæ–°ç‰ˆæœ¬çš„è´¦æˆ·è¯´æ˜ï¼šhttps://dev.mysql.com/doc/refman/5.7/en/adding-users.html 10.åˆ›å»ºæ•°æ®åº“create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci; ##å®‰è£…cloudera manager1.ä¸‹è½½åœ°å€ï¼šhttp://archive-primary.cloudera.com/cm5/cm/5/ï¼ˆåšä¸»ä¸‹è½½çš„æ˜¯cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz æ”¾åœ¨/soft/bigdata/clouderamanagerä¸‹ï¼‰cd /soft/bigdata/clouderamanagertar -xvf cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz æµ‹è¯•ï¼šï¼ˆå¯é€‰ï¼‰cat /etc/passwd åˆ›å»ºç”¨æˆ·ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰useradd â€“system â€“home=/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server â€“no-create-home â€“shell=/bin/false â€“comment â€œCloudera SCM Userâ€ cloudera-scm æµ‹è¯•ï¼ˆå¯é€‰ï¼‰[root@master1 cloudera-scm-server]# cat /etc/passwdâ€¦.cloudera-scm:x:498:498:Cloudera SCM User:/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server:/bin/false ä¿®æ”¹ä¸»æœºåå’Œç«¯å£å·cd /soft/bigdata/clouderamanager/cm-5.10.0/etc/cloudera-scm-agentvi config.ini Hostname of the CM server.server_host=po-master1 Port that the CM server is listening on.server_port=7182 ä¸‹è½½é©±åŠ¨åŒ…ä¸‹è½½mysql-connector-java-*.jarï¼ˆåšä¸»ä¸‹è½½çš„mysql-connector-java-5.1.7-bin.jarï¼‰æ”¾åˆ°/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib ç›®å½•ä¸‹ ä¸ºCloudera Manager 5å»ºç«‹æ•°æ®åº“ï¼š/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/schema/scm_prepare_database.sh mysql scm -hlocalhost -uroot -ppassword â€“scm-host localhost scm password scmæ ¼å¼æ˜¯:scm_prepare_database.sh æ•°æ®åº“ç±»å‹ æ•°æ®åº“ æœåŠ¡å™¨ ç”¨æˆ·å å¯†ç  â€“scm-host Cloudera_Manager_Serveræ‰€åœ¨çš„æœºå™¨ï¼Œåé¢é‚£ä¸‰ä¸ªä¸çŸ¥é“ä»£è¡¨ä»€ä¹ˆï¼Œç›´æ¥ç…§æŠ„å®˜ç½‘çš„äº†ã€‚ å¼€å¯Cloudera Manager 5 Serverç«¯ï¼š æ³¨æ„scmè§é¢æ˜¯ä¸¤ä¸ª-ï¼Œå› ä¸ºåšå®¢å…³ç³»ä¸èƒ½å…¨éƒ¨æ˜¾ç¤º å‘å…¶ä»–æœºå™¨åˆ†å‘CDHscp -rp /soft/bigdata/clouderamanager root@po-master2:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave1:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave2:/soft/bigdatascp -rp /soft/bigdata/clouderamanager root@po-slave3:/soft/bigdata å‡†å¤‡Parcelsï¼Œç”¨ä»¥å®‰è£…CDH5 ï¼ˆåšä¸»æ”¾åœ¨:/soft/bigdata/clouderamanager/cloudera/parcel-repoï¼Œè·¯å¾„å¿…é¡»åŒ…å«cloudera/parcel-repoï¼‰å®˜æ–¹åœ°å€ï¼šhttp://archive.cloudera.com/cdh5/parcelsåšä¸»é€‰æ‹©çš„http://archive.cloudera.com/cdh5/parcels/latest/ éœ€è¦ä¸‹è½½ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶â€¢ CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcelâ€¢ manifest.json æ‰“å¼€ manifest.jsonæ‰¾åˆ°CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcelçš„hashå€¼é‡Œçš„å†…å®¹â€œhashâ€: â€œ52f95da433f203a05c2fd33eb0f144e6a5c9d558â€echo â€˜52f95da433f203a05c2fd33eb0f144e6a5c9d558â€™ &gt;&gt; CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha æµ‹è¯•ï¼ˆå¯é€‰ï¼‰[root@master1 parcel-repo]# lltotal 1466572-rw-râ€“râ€“ 1 root root 1501694035 Mar 6 14:24 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel-rw-râ€“râ€“ 1 root root 41 Mar 20 15:26 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha-rw-râ€“râ€“ 1 root root 64807 Mar 17 17:07 manifest.json å¯åŠ¨/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server startï¼ˆä¸»èŠ‚ç‚¹å¯åŠ¨ï¼‰/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent startï¼ˆæ‰€æœ‰èŠ‚ç‚¹ä¸Šå¯åŠ¨ï¼‰ æµ‹è¯•netstat -an | grep 7182netstat -an | grep 7180 ç™»é™†http://po-master1:7180é»˜è®¤ç”¨æˆ·å¯†ç éƒ½æ˜¯admin ç‚¹å‡»ç»§ç»­é€‰æ‹©å…è´¹çš„ç‚¹å‡»ç»§ç»­ å‹¾é€‰æœºå™¨ ç‚¹å‡»æ›´å¤šé€‰é¡¹ä¿®æ”¹parcelè·¯å¾„/soft/bigdata/clouderamanager/cloudera/parcel-repoæ’å…¥å›¾5 éœ€è¦é‡å¯æ‰€æœ‰èŠ‚ç‚¹çš„æœåŠ¡/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server restartï¼ˆä¸»èŠ‚ç‚¹å¯åŠ¨ï¼‰/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent restartï¼ˆæ‰€æœ‰èŠ‚ç‚¹ä¸Šå¯åŠ¨ï¼‰ é€‰æ‹©å¦‚ä¸‹å†…å®¹ç‚¹å‡»ç»§ç»­ ç­‰å¾…å®‰è£…â€¦ å®‰è£…å®Œæˆï¼Œç‚¹å‡»ç»§ç»­ å®‰è£…è¿‡ç¨‹æœ‰ä¸ªå°æç¤ºï¼šå·²å¯ç”¨é€æ˜å¤§é¡µé¢å‹ç¼©ï¼Œå¯èƒ½ä¼šå¯¼è‡´é‡å¤§æ€§èƒ½é—®é¢˜ã€‚è¯·è¿è¡Œâ€œecho never &gt; /sys/kernel/mm/transparent_hugepage/defragâ€ä»¥ç¦ç”¨æ­¤è®¾ç½®ï¼Œç„¶åå°†åŒä¸€å‘½ä»¤æ·»åŠ åˆ° /etc/rc.local ç­‰åˆå§‹è„šæœ¬ä¸­ï¼Œä»¥ä¾¿åœ¨ç³»ç»Ÿé‡å¯æ—¶äºˆä»¥è®¾ç½®ã€‚ä»¥ä¸‹ä¸»æœºå°†å—åˆ°å½±å“ï¼š é€‰æ‹©è‡ªå®šä¹‰æœåŠ¡ï¼Œé€‰æ‹©è‡ªå·±éœ€è¦çš„æœåŠ¡ ç­‰å¾…å®‰è£… å®‰è£…è¿‡ç¨‹ä¸­ä¼šé‡åˆ°é”™è¯¯ï¼š æ˜¯ç¼ºå°‘jdbcé©±åŠ¨æŠŠæ–‡ä»¶è€ƒå…¥åˆ°libä¸‹å³å¯ é…ç½®NameNode HAè¿›å…¥HDFSç•Œé¢ï¼Œç‚¹å‡»â€œå¯ç”¨High Availabilityâ€è¾“å…¥NameServiceåç§°ï¼Œè¿™é‡Œè®¾ç½®ä¸ºï¼šnameservice1ï¼Œç‚¹å‡»ç»§ç»­æŒ‰é’®ã€‚é…ç½®JourNodeçš„è·¯å¾„ï¼Œ(åšä¸»ä¿®æ”¹ä¸º/opt/dfs/jn) é”™è¯¯æ•´ç†;Fatal error during KafkaServer startup. Prepare to shutdownkafka.common.InconsistentBrokerIdException: Configured broker.id 52 doesnâ€™t match stored broker.id 102 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs). at kafka.server.KafkaServer.getBrokerId(KafkaServer.scala:648) at kafka.server.KafkaServer.startup(KafkaServer.scala:187) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:37) at kafka.Kafka$.main(Kafka.scala:67) at com.cloudera.kafka.wrap.Kafka$.main(Kafka.scala:76) at com.cloudera.kafka.wrap.Kafka.main(Kafka.scala) è¿›å…¥åˆ°/var/local/kafka/dataç›®å½•æŸ¥çœ‹meta.propertieé‡Œé¢çš„kakfa çš„broker idæ˜¯ä»€ä¹ˆ [main]: Metastore Thrift Server threw an exceptionâ€¦javax.jdo.JDOFatalInternalException: Error creating transactional connection factory at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)NestedThrowablesStackTrace:java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325) at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282) at org.datanucleus.store.AbstractStoreManager.(AbstractStoreManager.java:240) at org.datanucleus.store.rdbms.RDBMSStoreManager.(RDBMSStoreManager.java:286) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187) at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the â€œBONECPâ€ plugin to create a ConnectionPool gave an error : The specified datastore driver (â€œcom.mysql.jdbc.Driverâ€) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.(ConnectionFactoryImpl.java:85) â€¦ 54 moreCaused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (â€œcom.mysql.jdbc.Driverâ€) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver. at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58) at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238) â€¦ 56 more æŠŠé©±åŠ¨ç¨‹åºæ”¾åœ¨/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib SERVER[po-master1] E0103: Could not load service classes, Cannot load JDBC driver class â€˜com.mysql.jdbc.Driverâ€™org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot load JDBC driver class â€˜com.mysql.jdbc.Driverâ€™ at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)Caused by: org.apache.openjpa.persistence.PersistenceException: Cannot load JDBC driver class â€˜com.mysql.jdbc.Driverâ€™ at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:102) at org.apache.openjpa.jdbc.conf.JDBCConfigurationImpl.getDBDictionaryInstance(JDBCConfigurationImpl.java:603) at org.apache.openjpa.jdbc.meta.MappingRepository.endConfiguration(MappingRepository.java:1518) at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:531) at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:456) at org.apache.openjpa.lib.conf.PluginValue.instantiate(PluginValue.java:120) at org.apache.openjpa.conf.MetaDataRepositoryValue.instantiate(MetaDataRepositoryValue.java:68) at org.apache.openjpa.lib.conf.ObjectValue.instantiate(ObjectValue.java:83) at org.apache.openjpa.conf.OpenJPAConfigurationImpl.newMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:967) at org.apache.openjpa.conf.OpenJPAConfigurationImpl.getMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:958) at org.apache.openjpa.kernel.AbstractBrokerFactory.makeReadOnly(AbstractBrokerFactory.java:644) at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:203) at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:156) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:227) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:154) at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:60) at org.apache.oozie.service.JPAService.getEntityManager(JPAService.java:514) at org.apache.oozie.service.JPAService.init(JPAService.java:215) at org.apache.oozie.service.Services.setServiceInternal(Services.java:386) at org.apache.oozie.service.Services.setService(Services.java:372) at org.apache.oozie.service.Services.loadServices(Services.java:305) â€¦ 26 moreCaused by: org.apache.commons.dbcp.SQLNestedException: Cannot load JDBC driver class â€˜com.mysql.jdbc.Driverâ€™ at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1429) at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1371) at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044) at org.apache.openjpa.lib.jdbc.DelegatingDataSource.getConnection(DelegatingDataSource.java:110) at org.apache.openjpa.lib.jdbc.DecoratingDataSource.getConnection(DecoratingDataSource.java:87) at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:91) â€¦ 46 moreCaused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680) at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526) at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1420) â€¦ 51 more æŠŠmysql-connector-java.jarï¼Œmysql-connector-java-5.1.39.jaré©±åŠ¨ç¨‹åºæ”¾åœ¨ï¼š/var/lib/oozie [main]: Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidatesRequired table missing : â€œDBSâ€œ in Catalog â€œâ€ Schema â€œâ€. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable â€œdatanucleus.autoCreateTablesâ€org.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : â€œDBSâ€œ in Catalog â€œâ€ Schema â€œâ€. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable â€œdatanucleus.autoCreateTablesâ€ at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:485) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3380) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190) at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:185) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.(MetaStoreDirectSql.java:136) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:340) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.(RawStoreProxy.java:57) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.(RetryingHMSHandler.java:78) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984) at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236) at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136) SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table &apos;oozie.VALIDATE_CONN&apos; doesn&apos;t exist) org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table â€˜oozie.VALIDATE_CONNâ€™ doesnâ€™t exist) at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414) æŠ¥è¿™ä¸ªé”™è¯¯éœ€è¦ä¿®æ”¹hiveçš„é…ç½®ã€‚æœç´¢autoCreateSchema æ”¹ä¸ºtrue SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table â€˜oozie.VALIDATE_CONNâ€™ doesnâ€™t exist)org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table â€˜oozie.VALIDATE_CONNâ€™ doesnâ€™t exist) at org.apache.oozie.service.Services.loadServices(Services.java:309) at org.apache.oozie.service.Services.init(Services.java:213) at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944) at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:759) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414) ç‚¹å‡»ç•Œé¢ä¸Šçš„Oozie ç‚¹å‡»æ“ä½œï¼Œåˆ›å»ºOozieæ•°æ®åº“è¡¨ æœ€åå¯¼å…¥ç¯å¢ƒå˜é‡å°±å¯ä»¥æµ‹è¯•äº†export ZK_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/export HBASE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hbase/export HADOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/export HIVE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/export SQOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/export OOZIE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/oozie/export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$OOZIE_HOME/bin:$ZK_HOME/bin:$HBASE_HOME/bin æœ€åæµ‹è¯•é˜¶æ®µï¼Œå¯å¿½ç•¥ï¼Œæœ¬æ–‡å®Œã€‚æµ‹è¯•zookeeperï¼šåœ¨po-slave1ï¼Œpo-slave2ï¼Œpo-slave3ä¸Šæ‰§è¡Œï¼ˆï¼‰ï¼š1234[root@po-slave1 data]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: leader 1234[root@po-slave2 data]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower 1234[root@po-slave3 dfs]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower 12345678910111213141516171819202122232425262728293031323334[root@po-slave3 dfs]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfgMode: follower[root@po-slave3 dfs]# zkCli.sh Connecting to localhost:21812017-03-21 16:05:56,829 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.10.0--1, built on 01/20/2017 20:10 GMT2017-03-21 16:05:56,832 [myid:] - INFO [main:Environment@100] - Client environment:host.name=po-slave32017-03-21 16:05:56,832 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_1212017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation2017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/soft/java/jdk/jre2017-03-21 16:05:56,834 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/classes:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../src/java/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf:.:/soft/java/jdk/lib2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd642017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:os.version=2.6.32-642.13.1.el6.x86_642017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.name=root2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/root2017-03-21 16:05:56,835 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/opt/dfs2017-03-21 16:05:56,836 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@506c589eWelcome to ZooKeeper!2017-03-21 16:05:56,873 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@975] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)JLine support is enabled2017-03-21 16:05:56,935 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@852] - Socket connection established, initiating session, client: /127.0.0.1:42694, server: localhost/127.0.0.1:21812017-03-21 16:05:56,941 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1235] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15aeb7f0edb054c, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0] ls /[controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, config, hbase][zk: localhost:2181(CONNECTED) 1] æµ‹è¯•hdfs1234567891011121314[root@po-master1 ~]# hadoop dfs -ls /DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 3 itemsdrwxr-xr-x - hbase hbase 0 2017-03-21 10:30 /hbasedrwxrwxrwt - hdfs supergroup 0 2017-03-20 17:06 /tmpdrwxr-xr-x - hdfs supergroup 0 2017-03-20 17:06 /user[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -mkdir /data2[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -put hdfs-audit.log /data2/hdfs-audit.log[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -ls /data2Found 1 items-rw-r--r-- 3 hdfs supergroup 2908825 2017-03-21 17:28 /data2/hdfs-audit.log æµ‹è¯•ç½‘é¡µ æµ‹è¯•hadoopé¡µé¢http://po-master1:50030/jobtracker.jsp æµ‹è¯•hive1234567891011121314151617[root@po-master1 ~]# hiveJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future releaseJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.propertiesWARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive&gt; show databases;OKdefaultTime taken: 1.836 seconds, Fetched: 1 row(s)hive&gt; create database test;OKTime taken: 0.06 secondshive&gt; drop database test;OKTime taken: 0.184 seconds æµ‹è¯•hbase12345678910111213141516[root@po-master1 ~]# hbase shellJava HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/21 16:20:23 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.availableHBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.Type "exit&lt;RETURN&gt;" to leave the HBase ShellVersion 1.2.0-cdh5.10.0, rUnknown, Fri Jan 20 12:13:18 PST 2017hbase(main):001:0&gt; listTABLE 0 row(s) in 0.2020 seconds=&gt; []hbase(main):002:0&gt; create 't1','id','name'0 row(s) in 2.3540 seconds=&gt; Hbase::Table - t1hbase(main):003:0&gt; listTABLE t1 1 row(s) in 0.0100 seconds=&gt; ["t1"]hbase(main):004:0&gt; å¸è½½å®‰è£…ï¼š1umount /soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-agent/process é—®é¢˜æ±‡æ€»ï¼šå¦‚æœå¯åŠ¨CDHåæ— æ³•ç‚¹å‡»HDFS çš„WEB UIï¼ŒæŸ¥çœ‹ç«¯å£åˆæ˜¯è¢«ç›‘å¬æ˜¯å› ä¸ºéœ€è¦ä¿®æ”¹é…ç½®ï¼š![æ­¤å¤„è¾“å…¥å›¾ç‰‡çš„æè¿°][19] åŒç†job çš„web UIä¹Ÿéœ€è¦ä¿®æ”¹![æ­¤å¤„è¾“å…¥å›¾ç‰‡çš„æè¿°][20]]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH Kuduå®‰è£…åŠä»‹ç»]]></title>
    <url>%2F2017%2F03%2F01%2FCDH%20Kudu%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[å®‰è£…Kuduçš„è¦æ±‚1.æ“ä½œç³»ç»Ÿå’Œç‰ˆæœ¬æ”¯æŒClouderaã€‚2.é€šè¿‡Cloudera Managerç®¡ç†Kuduï¼Œè¦æ±‚Cloudera Manager5.4.3æˆ–æ›´æ”¹çš„ç‰ˆæœ¬ã€‚CDH 5.4æˆ–æ›´é«˜ç‰ˆæœ¬çš„è¦æ±‚ã€‚æ¨èCloudera Manager5.4.7ï¼Œå› ä¸ºå®ƒå¢åŠ äº†Kudué‡‡é›†çš„æŒ‡æ ‡æ”¯æŒã€‚3.å¦‚æœå›ºæ€å­˜å‚¨æ˜¯å¯ç”¨çš„ï¼Œåœ¨è¿™ç§é«˜æ€§èƒ½çš„åª’ä½“å­˜å‚¨Kudu WALså¯ä»¥æ˜¾è‘—æ”¹å–„æ—¶çš„Kudué…ç½®é«˜è€ç”¨æ€§ã€‚ é€šè¿‡Cloudera Managerå®‰è£…Kuduæƒ³è¦é€šè¿‡Cloudera Managerå®‰è£…Kuduï¼Œé¦–å…ˆä¸‹è½½Kuduçš„å®šåˆ¶æœåŠ¡æè¿°ç¬¦ï¼ˆCSDï¼‰æ–‡ä»¶http://archive.cloudera.com/kudu/csd/KUDU-5.10.0.jar?_ga=1.154711740.507225699.1488251539å¹¶ä¸Šä¼ åˆ°/opt/cloudera/csd/ç”¨ä»¥ä¸‹æ“ä½œç³»ç»Ÿå‘½ä»¤é‡å¯Clouderaç®¡ç†æœåŠ¡å™¨ã€‚ 1$ sudo service cloudera-scm-server restart ä»http://archive.cloudera.com/kudu/parcels/ç½‘ç«™æ‰¾åˆ°ç›¸åº”çš„Kudu parcelï¼Œ å¹¶æ·»åŠ åˆ° Parcel Settings &gt; è¿œç¨‹ Parcel å­˜å‚¨åº“ URL æ¥ä¸‹æ¥å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼å®‰è£…Using Parcels1.Hosts &gt; Parcels &gt; Kudu &gt; Download2.Locations &gt; Distribute &gt; Activate3.é‡å¯é›†ç¾¤4.Actions &gt; Add a Service. &gt; Kudu (Beta) &gt; Continue5.é€‰æ‹©ä¸€ä¸ªä¸»æœºä½œä¸ºmasterï¼Œä¸€äº›ä¸»æœºä½œä¸ºtablet æœåŠ¡è§’è‰²ã€‚ä¸€ä¸ªä¸»æœºå³å¯ä»¥æ˜¯masteråˆå¯ä»¥æ˜¯tabletï¼Œä½†æ˜¯å¯¹äºå¤§é›†ç¾¤æ¥è¯´ä¼šé€ æˆæ€§èƒ½é—®é¢˜ã€‚Kuduçš„masterä¸æ˜¯ä¸€ä¸ªèµ„æºå¯†é›†å‹çš„ï¼Œè¢«å…¶ä»–ç›¸ä¼¼çš„å¤„ç†ä¾‹å¦‚ï¼šHDFSçš„namenodeï¼ŒYARNçš„ResourceManageræ”¶é›†ã€‚é€‰æ‹©å®Œä¸»æœºç‚¹å‡»Continue6.åœ¨masters å’Œ tableté…ç½®Kuduçš„å­˜å‚¨ä½ç½®å’Œé¢„å†™æ—¥å¿—ï¼ˆWALï¼‰æ–‡ä»¶ï¼ŒCloudera Managerå°†ä¼šåˆ›å»ºæ–‡ä»¶å¤¹ã€‚6.1 ä½ å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç›®å½•å­˜å‚¨æ•°æ®å’ŒWALs6.2 ä½ ä¸å¯ä»¥å°†WALs ç›®å½•è®¾ç½®ä¸ºæ•°æ®çš„å­ç›®å½•6.3 å¦‚æœä½ çš„ä¸»æœºåŠæ˜¯masteråˆæ˜¯tabletï¼Œé…ç½®ä¸åŒçš„masterå’ŒtabletæœåŠ¡ï¼Œä¾‹å¦‚ /data/kudu/master and /data/kudu/tserver. 6.4 å¦‚æœä½ é€‰æ‹©çš„æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒæ‰“æ´æŠ€æœ¯;æœåŠ¡å¯åŠ¨å¤±è´¥6.4.1 é€€å‡ºé…ç½®å‘å¯¼ï¼Œç‚¹å‡»Cloudera Manager æ¥å£ä¸Šçš„Cloudera å›¾æ ‡6.4.2 åˆ° Kudu (Beta) æœåŠ¡6.4.3 Configuration &gt; Kudu (Beta) Service Advanced Configuration Snippet (Safety Valve) &gt; gflagfile5.4.4 æ·»åŠ å¦‚ä¸‹å†…å®¹å¹¶ä¿å­˜æ”¹å˜1--block_manager=file Note: The file block manager does not perform well at scale and should only be used for small-scale development and testing. 7.å¦‚æœä½ ä¸éœ€è¦é€€å‡ºï¼Œç‚¹å‡»Continueã€‚Kuduçš„masterå’ŒtabletæœåŠ¡å·²ç»å¯åŠ¨ã€‚å¦åˆ™ç‚¹å‡» Kudu (Beta) &gt; Actions &gt; Start.8.ä½¿ç”¨å…¶ä¸­ä¸€ç§æ–¹æ³•éªŒè¯æœåŠ¡ï¼š8.1 é€šè¿‡pså‘½ä»¤éªŒè¯ä¸€ä¸ªæˆ–å…¨éƒ¨kudu-masteræˆ–kudu-tserverç¨‹åºæ˜¯å¦è¿è¡Œ8.2 é€šè¿‡æ‰“å¼€Webæµè§ˆå™¨ä¸­çš„URLè®¿é—®masteræˆ–è€…tabletã€‚masterçš„URLï¼š http://:8051/tabletçš„URLï¼š http://:8050/9.é‡å¯ç›‘æ§æœåŠ¡å¹¶æ£€æŸ¥Kuduçš„å›¾è¡¨ï¼Œåˆ°Cloudera ManageræœåŠ¡ç‚¹å‡»Service Monitor &gt; Actions &gt; Restart10.ç®¡ç†è§’è‰²ã€‚Kudu (Beta) æœåŠ¡ ä½¿ç”¨ Actions æ¥åœæ­¢ï¼Œå¯åŠ¨ï¼Œé‡å¯æˆ–è€…å…¶ä»–ç®¡ç†æœåŠ¡ Using PackagesKudu ä»“åº“ å’Œ Package é“¾æ¥ Operating System Repository Package Individual Packages RHEL RHEL 6 RHEL 6 Ubuntu Trusty Trusty 1.1 ä¸‹è½½Kuduçš„yumæºæ–‡ä»¶åˆ°ï¼šRHELï¼ˆ/etc/yum.repos.d/ï¼‰ æˆ–è€… Ubuntuï¼ˆ/etc/apt/sources.list.d/ï¼‰1.2 å¦‚æœä½ éœ€è¦C++å®¢æˆ·ç«¯å¼€å‘åº“æˆ–Kuduçš„SDKï¼ŒRHEL kudu-client-devel åŒ… æˆ– Ubuntuçš„ libkuduclient0 and libkuduclient-dev åŒ…1.3 å¦‚æœä½ ä½¿ç”¨Cloudera Managerï¼Œä¸å®‰è£… kudu-master å’Œ kudu-tserveråŒ…ï¼ŒCloudera Managerä½¿ç”¨Kudu æä¾›æ“ä½œç³»ç»Ÿå¯åŠ¨è„šæœ¬ã€‚ 2.ç¾¤é›†ä¸Šå®‰è£…KuduæœåŠ¡ã€‚å»ä½ æƒ³å®‰è£…Kuduæ‰€åœ¨çš„é›†ç¾¤ã€‚å•å‡» Actions &gt; Add a Service &gt; Kudu &gt; Continueã€‚ 3.é€‰æ‹©ä¸€ä¸ªä¸»æœºä½œä¸ºmasterï¼Œä¸€äº›ä¸»æœºä½œä¸ºtablet æœåŠ¡è§’è‰²ã€‚ä¸€ä¸ªä¸»æœºå³å¯ä»¥æ˜¯masteråˆå¯ä»¥æ˜¯tabletï¼Œä½†æ˜¯å¯¹äºå¤§é›†ç¾¤æ¥è¯´ä¼šé€ æˆæ€§èƒ½é—®é¢˜ã€‚Kuduçš„masterä¸æ˜¯ä¸€ä¸ªèµ„æºå¯†é›†å‹çš„ï¼Œè¢«å…¶ä»–ç›¸ä¼¼çš„å¤„ç†ä¾‹å¦‚ï¼šHDFSçš„namenodeï¼ŒYARNçš„ResourceManageræ”¶é›†ã€‚é€‰æ‹©å®Œä¸»æœºç‚¹å‡»Continue4.åœ¨masters å’Œ tableté…ç½®Kuduçš„å­˜å‚¨ä½ç½®å’Œé¢„å†™æ—¥å¿—ï¼ˆWALï¼‰æ–‡ä»¶ï¼ŒCloudera Managerå°†ä¼šåˆ›å»ºæ–‡ä»¶å¤¹ã€‚4.1 ä½ å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç›®å½•å­˜å‚¨æ•°æ®å’ŒWALs4.2 ä½ ä¸å¯ä»¥å°†WALs ç›®å½•è®¾ç½®ä¸ºæ•°æ®çš„å­ç›®å½•5.3 å¦‚æœä½ çš„ä¸»æœºåŠæ˜¯masteråˆæ˜¯tabletï¼Œé…ç½®ä¸åŒçš„masterå’ŒtabletæœåŠ¡ï¼Œä¾‹å¦‚ /data/kudu/master and /data/kudu/tserver. 5 å¦‚æœä½ é€‰æ‹©çš„æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒæ‰“æ´æŠ€æœ¯;æœåŠ¡å¯åŠ¨å¤±è´¥ç‚¹å‡» Continue &gt; Kudu masters å’Œ tablet å¯åŠ¨ã€‚å¦åˆ™åˆ°Kuduçš„æœåŠ¡ä¸Šç‚¹å‡» Actions &gt; Start. 6.ä½¿ç”¨å…¶ä¸­ä¸€ç§æ–¹æ³•éªŒè¯æœåŠ¡ï¼š6.1 é€šè¿‡pså‘½ä»¤éªŒè¯ä¸€ä¸ªæˆ–å…¨éƒ¨kudu-masteræˆ–kudu-tserverç¨‹åºæ˜¯å¦è¿è¡Œ6.2 é€šè¿‡æ‰“å¼€Webæµè§ˆå™¨ä¸­çš„URLè®¿é—®masteræˆ–è€…tabletã€‚masterçš„URLï¼š http://:8051/tabletçš„URLï¼š http://:8050/ 7.é‡å¯ç›‘æ§æœåŠ¡å¹¶æ£€æŸ¥Kuduçš„å›¾è¡¨ï¼Œåˆ°Cloudera ManageræœåŠ¡ç‚¹å‡»Service Monitor &gt; Actions &gt; Restart8.ç®¡ç†è§’è‰²ã€‚Kudu æœåŠ¡ ä½¿ç”¨ Actions æ¥åœæ­¢ï¼Œå¯åŠ¨ï¼Œé‡å¯æˆ–è€…å…¶ä»–ç®¡ç†æœåŠ¡ æ­¤ä¹ƒå®˜æ–¹æ¨èï¼Œæ–¹æ³•1ï¼Œåšä¸»äº²è¯•æ²¡æœ‰æˆåŠŸï¼Œå®‰è£…å®Œä¸æ˜¾ç¤ºæœåŠ¡ æœ¬æ–‡å‚è€ƒé¡µé¢ï¼šhttps://www.cloudera.com/documentation/enterprise/latest/topics/kudu_install_cm.html]]></content>
      <categories>
        <category>cloudera</category>
      </categories>
      <tags>
        <tag>cloudera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canalåˆ†æbinlog]]></title>
    <url>%2F2017%2F02%2F23%2Fcanal%E5%88%86%E6%9E%90binlog%2F</url>
    <content type="text"><![CDATA[ç¯å¢ƒä»‹ç»linuxæœåŠ¡å™¨ï¼šcentos7.1mysql : 5.7.10canal : 1.0.23 ä¸€.centos7ä¸‹å®‰è£…mysql1.ä¸‹è½½mysqlçš„repoæº1wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 2.å®‰è£…mysql-community-release-el7-5.noarch.rpmåŒ…1rpm -ivh mysql-community-release-el7-5.noarch.rpm 3.å®‰è£…mysql1yum install mysql-server 4.è®¾ç½®ç”¨æˆ·12345CREATE USER &apos;canal&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;localhost&apos; WITH GRANT OPTION;CREATE USER &apos;canal&apos;@&apos;%&apos; IDENTIFIED BY &apos;canal&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos; WITH GRANT OPTION;flush privileges; æ³¨ï¼šcanalçš„åŸç†æ˜¯æ¨¡æ‹Ÿè‡ªå·±ä¸ºmysql slaveï¼Œæ‰€ä»¥è¿™é‡Œä¸€å®šéœ€è¦åšä¸ºmysql slaveçš„ç›¸å…³æƒé™.CREATE USER canal IDENTIFIED BY â€˜canalâ€™;GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON . TO â€˜canalâ€™@â€™%â€™;â€“ GRANT ALL PRIVILEGES ON . TO â€˜canalâ€™@â€™%â€™ ;FLUSH PRIVILEGES; 5.ä¿®æ”¹é…ç½®æ–‡ä»¶1234[mysqld]log-bin=mysql-bin #æ·»åŠ è¿™ä¸€è¡Œå°±okbinlog-format=ROW #é€‰æ‹©rowæ¨¡å¼server_id=1 #é…ç½®mysql replactionéœ€è¦å®šä¹‰ï¼Œä¸èƒ½å’Œcanalçš„slaveIdé‡å¤ äºŒ.å®‰è£…canal1.ä¸‹è½½canal1wget https://github.com/alibaba/canal/releases/download/v1.0.23/canal.deployer-1.0.23.tar.gz 2.è§£å‹ç¼©12mkdir /root/canaltar zxvf canal.deployer-1.0.23.tar.gz -C /root/canal 3.ä¿®æ”¹é…ç½®æ–‡ä»¶(å¦‚æœæ˜¯è®¿é—®æœ¬æœºï¼Œå¹¶ä¸”ç”¨æˆ·å¯†ç éƒ½ä¸ºcanalåˆ™ä¸éœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶)1vi /root/canal/conf/example/instance.properties 4.å¯åŠ¨1sh /root/canal/bin/startup.sh 5.æŸ¥çœ‹æ—¥å¿—1234567[root@zhm1 ~]# cat /root/canal/logs/canal/canal.logOpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release.2017-02-22 17:40:08.901 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.2017-02-22 17:40:09.069 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.118.128:11111]2017-02-22 17:40:09.758 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... å…·ä½“instanceçš„æ—¥å¿—ï¼š1[root@zhm1 ~]# cat /root/canal/logs/example/example.log 6.å…³é—­sh /root/canal/bin/stop.sh ä¸‰.å†™å®¢æˆ·ç«¯ä»£ç 1.åœ¨mavençš„setting.xmlåŠ å…¥é˜¿é‡Œçš„é•œåƒ123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;&lt;/mirror&gt; 2.åˆ›å»ºåˆå§‹é¡¹ç›®1mvn archetype:generate -DgroupId=com.alibaba.otter -DartifactId=canal.sample 3.pomæ–‡ä»¶å¢åŠ 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.0.12&lt;/version&gt;&lt;/dependency&gt; 4. ClientSampleä»£ç 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package com.alibaba.otter;/** * Created by Administrator on 2017/2/23. */import java.net.InetSocketAddress;import java.util.List;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.common.utils.AddressUtils;import com.alibaba.otter.canal.protocol.Message;import com.alibaba.otter.canal.protocol.CanalEntry.Column;import com.alibaba.otter.canal.protocol.CanalEntry.Entry;import com.alibaba.otter.canal.protocol.CanalEntry.EntryType;import com.alibaba.otter.canal.protocol.CanalEntry.EventType;import com.alibaba.otter.canal.protocol.CanalEntry.RowChange;import com.alibaba.otter.canal.protocol.CanalEntry.RowData;public class ClientSample &#123; public static void main(String args[]) &#123; // åˆ›å»ºé“¾æ¥ CanalConnector connector = CanalConnectors.newSingleConnector( new InetSocketAddress("192.168.118.128",//AddressUtils.getHostIp(), 11111), "example", "", ""); int batchSize = 1000; int emptyCount = 0; try &#123; connector.connect(); connector.subscribe(".*\\..*"); connector.rollback(); int totalEmptyCount = 120; while (emptyCount &lt; totalEmptyCount) &#123; Message message = connector.getWithoutAck(batchSize); // è·å–æŒ‡å®šæ•°é‡çš„æ•°æ® long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) &#123; emptyCount++; System.out.println("empty count : " + emptyCount); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; &#125; else &#123; emptyCount = 0; // System.out.printf("message[batchId=%s,size=%s] \n", batchId, size); printEntry(message.getEntries()); &#125; connector.ack(batchId); // æäº¤ç¡®è®¤ // connector.rollback(batchId); // å¤„ç†å¤±è´¥, å›æ»šæ•°æ® &#125; System.out.println("empty too many times, exit"); &#125; finally &#123; connector.disconnect(); &#125; &#125; private static void printEntry(List&lt;Entry&gt; entrys) &#123; for (Entry entry : entrys) &#123; if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) &#123; continue; &#125; RowChange rowChage = null; try &#123; rowChage = RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException("ERROR ## parser of eromanga-event has an error, data:" + entry.toString(),e); &#125; EventType eventType = rowChage.getEventType(); System.out.println(String.format("================&gt; binlog[%s:%s] , name[%s,%s] , eventType : %s", entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); for (RowData rowData : rowChage.getRowDatasList()) &#123; if (eventType == EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); &#125; else if (eventType == EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); &#125; else &#123; System.out.println("-------&gt; before"); printColumn(rowData.getBeforeColumnsList()); System.out.println("-------&gt; after"); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;Column&gt; columns) &#123; for (Column column : columns) &#123; System.out.println(column.getName() + " : " + column.getValue() + " update=" + column.getUpdated()); &#125; &#125;&#125; 5. mysqlä¸‹æ‰§è¡Œæ“ä½œ1234567891011mysql&gt; use test;Database changedmysql&gt; CREATE TABLE `xdual` ( -&gt; `ID` int(11) NOT NULL AUTO_INCREMENT, -&gt; `X` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, -&gt; PRIMARY KEY (`ID`) -&gt; ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ;Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into xdual(id,x) values(4,now());Query OK, 1 row affected (0.06 sec) å¯ä»¥ä»æ§åˆ¶å°ä¸­çœ‹åˆ°ï¼š1234567empty count : 1empty count : 2empty count : 3empty count : 4================&gt; binlog[mysql-bin.001946:313661577] , name[test,xdual] , eventType : INSERTID : 4 update=trueX : 2017-02-23 14:20:00 update=true å››.canalé›†ç¾¤æ­å»º1.å®‰è£…zookeeperç•¥ 2.ä¿®æ”¹é…ç½®æ–‡ä»¶123456vi /root/canal/conf/canal.propertiescanal.zkServers=1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181canal.instance.global.spring.xml = classpath:spring/default-instance.xmlvi /root/canal/conf/example/instance.propertiescanal.instance.mysql.slaveId = 1234 å¦å¤–ä¸€ä¸ªæœºå™¨æ”¹æˆ1235ä¸1234ä¸åŒå³å¯ åˆ†åˆ«åœ¨ä¸¤å°æœºå™¨ä¸Šå¯åŠ¨ï¼Œå‘ç°åªæœ‰ä¸€å°æœºå™¨logsä¸‹é¢æœ‰exampleç›®å½•ï¼Œå¹¶ä¸”æ˜¾ç¤ºå¯åŠ¨æˆåŠŸ 3.è¿›å…¥åˆ°zkClientæŸ¥çœ‹çŠ¶æ€è·å–æ­£åœ¨è¿è¡Œçš„canal server1get /otter/canal/destinations/example/running è·å–æ­£åœ¨è¿æ¥çš„canal client1get /otter/canal/destinations/example/1001/running è·å–å½“å‰æœ€åä¸€æ¬¡æ¶ˆè´¹è½¦æˆåŠŸçš„binlog1get /otter/canal/destinations/example/1001/cursor 4.å®¢æˆ·ç«¯ä»£ç ä¿®æ”¹å¦‚ä¸‹ï¼š1CanalConnector connector = CanalConnectors.newClusterConnector("1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181", "example", "", ""); ä»£ç å¯åŠ¨ åœæ­¢ä¸€ä¸ªcanal å…¨éƒ¨åœæ­¢canal æŸ¥çœ‹å½“å‰canalæ¶ˆè´¹åˆ°å“ªä¸ªposition[zk: localhost:2181(CONNECTED) 15] get /otter/canal/destinations/example/1001/cursor{â€œ@typeâ€:â€com.alibaba.otter.canal.protocol.position.LogPositionâ€,â€identityâ€:{â€œslaveIdâ€:-1,â€sourceAddressâ€:{â€œaddressâ€:â€po-master1â€,â€portâ€:3306}},â€postionâ€:{â€œincludedâ€:false,â€journalNameâ€:â€logbin.000004â€,â€positionâ€:6897322,â€serverIdâ€:1,â€timestampâ€:1492065268000}} æŸ¥çœ‹mysqlçš„æ¨¡å¼show global variables like â€˜%binlog_format%â€™;æŸ¥çœ‹binlogä½ç½®show master status é”™è¯¯æ•´ç†ï¼šå¯åŠ¨å‘ç°é”™è¯¯ï¼š[root@slave1 canal]# cat logs/borrow/borrow.log2017-06-12 03:57:25.931 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]2017-06-12 03:57:25.938 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [borrow/instance.properties]2017-06-12 03:57:25.946 [main] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration, e.g. on the BeanWrapper/BeanFactory!2017-06-12 03:57:25.951 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-borrow2017-06-12 03:57:25.953 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - subscribe filter change to test_ydxsb..*2017-06-12 03:57:25.953 [main] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successfulâ€¦.2017-06-12 03:57:25.971 [destination = borrow , address = /10.105.0.191:3306 , EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position by switch ::14969673010002017-06-12 03:57:26.047 [destination = borrow , address = /10.105.0.191:3306 , EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - Didnâ€™t find the corresponding binlog files from mysql-bin.000001 to mysql-bin.0000012017-06-12 03:57:26.050 [destination = borrow , address = /10.105.0.191:3306 , EventParser] ERROR c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - dump address /10.105.0.191:3306 has an error, retrying. caused bycom.alibaba.otter.canal.parse.exception.CanalParseException: canâ€™t find start position for borrow2017-06-12 03:57:26.052 [destination = borrow , address = /10.105.0.191:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:borrow[com.alibaba.otter.canal.parse.exception.CanalParseException: canâ€™t find start position for borrow éœ€è¦åˆ æ‰borrowç›®å½•ä¸‹çš„meta.dat å¦‚æœæç¤ºï¼šæ‰¾ä¸åˆ°æ—¥å¿—çš„ä½ç½®åœ¨mysqlä¸‹æ‰§è¡Œ(å¯èƒ½æ˜¯è®¾ç½®å®Œæ ¼å¼ï¼Œæ ¼å¼ä¸å¯¹)set global binlog_checksum=â€™NONEâ€™]]></content>
      <categories>
        <category>canal</category>
      </categories>
      <tags>
        <tag>canal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm1.0.3é›†ç¾¤éƒ¨ç½²]]></title>
    <url>%2F2017%2F02%2F15%2Fstorm1.0.3%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[æ˜¨æ—¥å·²ç»å‘å¸ƒstorm1.0.3ç‰ˆæœ¬ï¼Œæ¥ç©ä¸€ç© ç¯å¢ƒï¼šzookeeper3.4.9èŠ‚ç‚¹ï¼šstorm01ï¼Œstorm02ï¼Œstorm03storm1.0.3 nimbusèŠ‚ç‚¹storm01ï¼ŒsupervisorèŠ‚ç‚¹storm02ï¼Œstorm03 1234567[storm@localhost app]$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz[storm@localhost app]$ tar zxvf zookeeper-3.4.9.tar.gz[storm@localhost app]$ ln -s -f zookeeper-3.4.9 zookeeper[storm@localhost app]$ mkdir zookeeper/data[storm@localhost app]$ cd zookeeper/conf/[storm@localhost conf]$ cp zoo_sample.cfg zoo.cfg[storm@localhost conf]$ vi zoo.cfg ä¿®æ”¹dataDir1234dataDir=/home/storm/app/zookeeper/dataserver.1=storm01:2888:3888server.2=storm02:2888:3888server.3=storm03:2888:3888 åœ¨storm01ä¸‹æ‰§è¡Œ12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 1 &gt; myid åœ¨storm02ä¸‹æ‰§è¡Œ12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 2 &gt; myid åœ¨storm03ä¸‹æ‰§è¡Œ12[storm@storm01 conf]# cd /home/storm/app/zookeeper/data[storm@storm01 data]# echo 3 &gt; myid éƒ¨ç½²storm123[storm@localhost app]$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/storm/apache-storm-1.0.3/apache-storm-1.0.3.tar.gz[storm@localhost app]$ tar zxvf apache-storm-1.0.3.tar.gz[storm@localhost app]$ ln -s -f apache-storm-1.0.3 storm ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œjdkå®‰è£…æ­¤å¤„ç•¥12345678[storm@localhost app]$ su[root@localhost app]# echo &quot;export JAVA_HOME=/home/storm/app/jdk&quot;&gt;&gt; /etc/profile[root@localhost app]# echo &quot;export ZOOKEEPER_HOME=/home/storm/app/zookeeper&quot;&gt;&gt; /etc/profile[root@localhost app]# echo &quot;export STORM_HOME=/home/storm/app/storm&quot;&gt;&gt; /etc/profile[root@localhost app]# source /etc/profile[root@localhost app]# echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;ZOOKEEPER_HOME&#125;/bin:$&#123;STORM_HOME&#125;/bin:$PATH&quot;&gt;&gt; /etc/profile[root@localhost app]# source /etc/profile[root@localhost app]# su storm ä¿®æ”¹stomé…ç½®æ–‡ä»¶12[storm@localhost app]$ cd storm/conf[storm@localhost conf]$ vi storm.yaml 123456789101112storm.zookeeper.servers: - &quot;storm01&quot; - &quot;storm02&quot; - &quot;storm03&quot;storm.local.dir: &quot;/home/storm/app/storm/data&quot;storm.zookeeper.root: &quot;/storm&quot; nimbus.seeds: [&quot;storm01&quot;]supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 å¯åŠ¨zookeeperï¼Œåœ¨ä¸‰ä¸ªèŠ‚ç‚¹åˆ†åˆ«æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤1[storm@storm01 zookeeper]$ zkServer.sh start 1[storm@storm02 zookeeper]$ zkServer.sh start 1[storm@storm03 zookeeper]$ zkServer.sh start å¯åŠ¨stormé›†ç¾¤åœ¨storm01èŠ‚ç‚¹ä¸Šå¯åŠ¨nimbusï¼Œuiï¼Œlogviewer ï¼š123[storm@storm01 zookeeper]$ nohup storm nimbus &amp;[storm@storm01 zookeeper]$ nohup storm ui &amp;[storm@storm01 zookeeper]$ nohup storm logviewer &amp; åœ¨stomr02ï¼Œstorm03èŠ‚ç‚¹ä¸Šå¯åŠ¨supervisor12[storm@storm02 zookeeper]$ nohup storm supervisor &amp;[storm@storm02 zookeeper]$ nohup storm logviewer &amp; 12[storm@storm03 zookeeper]$ nohup storm supervisor &amp;[storm@storm03 zookeeper]$ nohup storm logviewer &amp;]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm1.0.2å®‰è£…]]></title>
    <url>%2F2017%2F01%2F28%2Fstorm1.0.2%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[å®‰è£…é…ç½®zookeeperç•¥ ä¸‹è½½ï¼šwget http://mirrors.cnnic.cn/apache/storm/apache-storm-1.0.2/apache-storm-1.0.2.tar.gz è§£å‹ï¼Œå¹¶åˆ›å»ºç¬¦å·é“¾æ¥ä¿®æ”¹profileé…ç½®æ–‡ä»¶ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼švi storm.yaml æ·»åŠ å¦‚ä¸‹å†…å®¹storm.zookeeper.servers: - &quot;www.hadoop01.com&quot; storm.zookeeper.root: â€œ/storm_1.0.2â€nimbus.seeds: [â€œwww.hadoop01.comâ€]supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 å¯åŠ¨å¯åŠ¨zookeeperå¯åŠ¨nimbus./bin/storm nimbus &amp;å¯åŠ¨ui ./bin/storm ui &amp; ç™»å½•UIï¼šhttp://www.hadoop01.com:8080/index.htmlå¯åŠ¨supervisor./bin/storm supervisor &amp;å¯åŠ¨logviewer./bin/storm logviewer &amp; æ‰§è¡Œdemocd /home/hadoop/apps/storm/storm/examples/storm-starter storm jar storm-starter-topologies-1.0.2.jar org.apache.storm.starter.WordCountTopology first-topology æŸ¥çœ‹UIç‚¹å‡»spout:]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stormåŸºç¡€çŸ¥è¯†]]></title>
    <url>%2F2017%2F01%2F27%2Fstorm%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[StormåŸºæœ¬æ¦‚å¿µStormæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œä¸»è¦ç”±Clojureç¼–ç¨‹è¯­è¨€ç¼–å†™ã€‚æœ€åˆæ˜¯ç”±Nathan MarzåŠå…¶å›¢é˜Ÿåˆ›å»ºäºBackTypeï¼Œè¯¥é¡¹ç›®åœ¨è¢«Twitterå–å¾—åå¼€æºã€‚å®ƒä½¿ç”¨ç”¨æˆ·åˆ›å»ºçš„â€œç®¡ï¼ˆspoutsï¼‰â€å’Œâ€œèºæ “ï¼ˆboltsï¼‰â€æ¥å®šä¹‰ä¿¡æ¯æºå’Œæ“ä½œæ¥å…è®¸æ‰¹é‡ã€åˆ†å¸ƒå¼å¤„ç†æµå¼æ•°æ®ã€‚æœ€åˆçš„ç‰ˆæœ¬å‘å¸ƒäº2011å¹´9æœˆ17æ—¥ã€‚Stormåº”ç”¨è¢«è®¾è®¡æˆä¸ºä¸€ä¸ªæ‹“æ‰‘ç»“æ„ï¼Œå…¶æ¥å£åˆ›å»ºä¸€ä¸ªè½¬æ¢â€œæµâ€ã€‚å®ƒæä¾›ä¸MapReduceä½œä¸šç±»ä¼¼çš„åŠŸèƒ½ï¼Œå½“é‡åˆ°å¼‚å¸¸æ—¶è¯¥æ‹“æ‰‘ç»“æ„ç†è®ºä¸Šå°†ä¸ç¡®å®šåœ°è¿è¡Œï¼Œç›´åˆ°å®ƒè¢«æ‰‹åŠ¨ç»ˆæ­¢ ä¸»è¦ç‰ˆæœ¬ ç‰ˆæœ¬ å‘å¸ƒæ—¥æœŸ 0.9.0.1 2013å¹´12æœˆ8æ—¥ 0.9.0 2013å¹´12æœˆ8æ—¥ 0.8.0 2012å¹´8æœˆ2æ—¥ 0.7.0 2012å¹´2æœˆ28æ—¥ 0.6.0 2011å¹´12æœˆ15æ—¥ 0.5.0 2011å¹´9æœˆ19æ—¥ ç¼–ç¨‹æ¨¡å‹Topologyä¸€ä¸ªå®æ—¶è®¡ç®—åº”ç”¨ç¨‹åºé€»è¾‘è¢«å°è£…åœ¨Topologyå¯¹è±¡ä¸­ï¼Œ ç±»ä¼¼Hadoopä¸­çš„jobï¼Œ Topologyä¼šä¸€ç›´è¿è¡Œç›´åˆ°ä½ æ˜¾å¼æ€æ­»å®ƒ ###DataSourceå¤–éƒ¨æ•°æ®æº Spoutæ¥å—å¤–éƒ¨æ•°æ®æºçš„ç»„ä»¶ï¼Œå°†å¤–éƒ¨æ•°æ®æºè½¬åŒ–æˆStormå†…éƒ¨çš„æ•°æ®ï¼Œä»¥Tupleä¸ºåŸºæœ¬çš„ä¼ è¾“å•å…ƒä¸‹å‘ç»™Bolt Boltæ¥å—Spoutå‘é€çš„æ•°æ®ï¼Œæˆ–ä¸Šæ¸¸çš„boltçš„å‘é€çš„æ•°æ®ã€‚æ ¹æ®ä¸šåŠ¡é€»è¾‘è¿›è¡Œå¤„ç†ã€‚å‘é€ç»™ä¸‹ä¸€ä¸ªBoltæˆ–è€…æ˜¯å­˜å‚¨åˆ°æŸç§ä»‹è´¨ä¸Šã€‚ä»‹è´¨å¯ä»¥æ˜¯Rediså¯ä»¥æ˜¯mysqlï¼Œæˆ–è€…å…¶ä»–ã€‚ TupleStormå†…éƒ¨ä¸­æ•°æ®ä¼ è¾“çš„åŸºæœ¬å•å…ƒï¼Œé‡Œé¢å°è£…äº†ä¸€ä¸ªListå¯¹è±¡ï¼Œç”¨æ¥ä¿å­˜æ•°æ®ã€‚ StreamGrouping:æ•°æ®åˆ†ç»„ç­–ç•¥7ç§ï¼šshuffleGrouping(Randomå‡½æ•°),NonGrouping(Randomå‡½æ•°,ç›®å‰å’ŒshuffleGroupingä¸€æ ·),FieldGrouping(Hashå–æ¨¡),Local or ShuffleGrouping ï¼ˆæœ¬åœ°æˆ–éšæœºï¼Œä¼˜å…ˆæœ¬åœ°ï¼‰,Fields groupingï¼ˆæ ¹æ®Tupleä¸­çš„æŸä¸€ä¸ªFiledæˆ–è€…å¤šä¸ªFiledçš„æ˜¯å€¼æ¥åˆ’åˆ†ã€‚ æ¯”å¦‚Streamæ ¹æ®Fieldä¸ºuser-idæ¥groupingï¼Œ ç›¸åŒuser-idå€¼çš„Tupleä¼šè¢«åˆ†å‘åˆ°ç›¸åŒçš„Taskä¸­ï¼‰,Global groupingï¼ˆæ•´ä¸ªStreamä¼šé€‰æ‹©ä¸€ä¸ªTaskä½œä¸ºåˆ†å‘çš„ç›®çš„åœ°ï¼Œ é€šå¸¸æ˜¯æœ€æ–°çš„é‚£ä¸ªidçš„Taskï¼‰Direct groupingï¼ˆäº§ç”Ÿæ•°æ®çš„Spout/Boltè‡ªå·±æ˜ç¡®å†³å®šè¿™ä¸ªTupleè¢«Boltçš„é‚£äº›Taskæ‰€æ¶ˆè´¹ï¼‰ Stormä¼˜ç‚¹å¥å£®æ€§å½“Workerå¤±æ•ˆæˆ–æœºå™¨å‡ºç°æ•…éšœæ—¶ï¼Œ è‡ªåŠ¨åˆ†é…æ–°çš„Workeræ›¿æ¢å¤±æ•ˆWorker å‡†ç¡®æ€§é‡‡ç”¨Ackeræœºåˆ¶ï¼Œä¿è¯æ•°æ®ä¸ä¸¢å¤±é‡‡ç”¨äº‹åŠ¡æœºåˆ¶ï¼Œä¿è¯æ•°æ®å‡†ç¡®æ€§ stormæ¶æ„ Stormçš„ä¸»çº¿ä¸»è¦åŒ…æ‹¬4æ¡ï¼šnimbus, supervisor, workerå’Œtaskã€‚ å¯¹äºstorm0.9.6çš„é…ç½®æ–‡ä»¶nimbusé…ç½®å‚æ•°è¿˜æ˜¯nimbus.hosts ç°åœ¨æ–°ç‰ˆæœ¬1.0.2å·²ç»ä¿®æ”¹ä¸ºnimbus.seeds,å·²ç»å¯ä»¥æ”¯æŒHAæ­¤å¤„ä¹‹ä¸€é…ç½®nimbus.seeds: XXã€‚æ­¤å¤„nimbus.seeds:åé¢è¦æœ‰ä¸€ä¸ªç©ºæ ¼ï¼Œå¦åˆ™å¯åŠ¨æŠ¥é”™stormå¯åŠ¨nimbusçš„æ—¶å€™jpsä¼šæœ‰ä¸€ä¸ªè¿›ç¨‹config_valueç„¶åå˜æˆnimbusã€‚å…¶ä»–å¯åŠ¨åŒä¸Š]]></content>
      <categories>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonæ˜¥èŠ‚ç¥ç¦è¯­è‡ªåŠ¨å›å¤]]></title>
    <url>%2F2017%2F01%2F27%2FPython%E6%98%A5%E8%8A%82%E7%A5%9D%E7%A6%8F%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%9B%9E%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[é¦–å…ˆå®‰è£…ä¸¤ä¸ªåº“pip install itchat pillow 1234567891011121314151617ç¯å¢ƒwin7ï¼Œpyython3ç¼–å†™pyæ–‡ä»¶è¾“å…¥ä»¥ä¸‹å†…å®¹import itchat, time, refrom itchat.content import *@itchat.msg_register([TEXT])def text_reply(msg): match = re.search('å¹´', msg['Text']).span() if match: itchat.send(('é¸¡å¹´å¤§å‰'), msg['FromUserName'])@itchat.msg_register([PICTURE, RECORDING, VIDEO, SHARING])def other_reply(msg): itchat.send(('é¸¡å¹´å¤§å‰'), msg['FromUserName'])itchat.auto_login(enableCmdQR=True,hotReload=True)itchat.run() è¿è¡Œåï¼Œæ‰«æç”Ÿæˆçš„äºŒç»´ç å³å¯ ç™»å½•æˆåŠŸæœ‰ä»¥ä¸‹æç¤º]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonæœºå™¨å­¦ä¹ å‡†å¤‡]]></title>
    <url>%2F2017%2F01%2F27%2FPython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[ä¸‹è½½NumPyhttp://download.csdn.net/download/z1137730824/8384347ï¼ˆnumpy 64ä½ï¼‰http://download.csdn.net/detail/u010156024/9302649ï¼ˆnumpy 32ä½ï¼‰ ä¸‹è½½matplotlibhttps://pypi.python.org/pypi/matplotlib/ åšä¸»é€‰æ‹©matplotlib-2.0.0-cp27-cp27m-win_amd64.whl (md5)ä¸‹è½½å®ŒæŠŠmatplotlib-2.0.0-cp27-cp27m-win_amd64.whlæ”¹æˆmatplotlib-2.0.0-cp27-cp27m-win_amd64.zipè§£å‹åˆ°Pythonç›®å½•ä¸‹çš„Libæ–‡ä»¶å¤¹ä¸‹çš„site-packagesç›®å½• å®‰è£…dateutilç›´æ¥ç”¨pip install python-dateutilæˆ–è€…å»ç½‘ä¸Šä¸‹è½½https://pypi.python.org/pypi/python-dateutil å®‰è£…pyparsingpip install pyparsingæˆ–è€…å»ç½‘ä¸Šä¸‹è½½https://pypi.python.org/pypi/pyparsing/2.0.2http://pyparsing.wikispaces.com/Download+and+Installation æœ€ç»ˆå®‰è£…å¦‚ä¸‹å®‰è£…åŒ…ï¼šnumpy, setuptools, python-dateutil, pytz, pyparsing, and cyclerï¼Œfunctools32]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonç½‘ç»œçˆ¬è™«]]></title>
    <url>%2F2017%2F01%2F26%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[pythonå®˜æ–¹æä¾›çš„ç½‘é¡µä¸‹è½½å™¨æ˜¯urllib2ç¬¬ä¸‰æ–¹æœ‰æ›´å¼ºå¤§çš„ä¸‹è½½å™¨æ˜¯requests åœ¨python2.xé‡Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨urllib2.urlopen(â€œhttp://www.baidu.com&quot;)æ‰“å¼€ç½‘é¡µä½†æ˜¯åœ¨python3.xé‡Œ urllib2 éœ€è¦æ”¹æˆurllib.request åœ¨python2.xé‡Œå¯ä»¥ä½¿ç”¨urllib.urlencodeï¼šä¾‹å¦‚ï¼švalues = {â€œusernameâ€:â€Pythonçˆ¬è™«â€,â€passwordâ€:â€123456789â€}data = urllib.urlencode(values)ä½†æ˜¯åœ¨python3.xé‡Œ urllib2 éœ€è¦æ”¹æˆurllib.requestdata = urllib.parse.urlencode(values)è€Œä¸”è¿˜éœ€è¦æŠŠdataæ ¼å¼è½¬æ¢data = data.encode(â€˜utf-8â€™)æˆ–data = data.encode(encoding=â€™UTF8â€™)å¦åˆ™ä¼šæç¤ºTypeError: POST data should be bytes or an iterable of bytes. It cannot be of type str.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Kylin]]></title>
    <url>%2F2017%2F01%2F18%2FApache%20Kylin%2F</url>
    <content type="text"><![CDATA[kylinApache Kylinâ„¢ is an open source Distributed Analytics Engine designed to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets, original contributed from eBay Inc. Apache Kylinâ„¢ lets you query massive data set at sub-second latency in 3 steps. Identify a Star Schema on Hadoop. Build Cube from the identified tables. Query with ANSI-SQL and get results in sub-second, via ODBC, JDBC or RESTful API.]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Beam]]></title>
    <url>%2F2017%2F01%2F16%2FApache%20Beam%2F</url>
    <content type="text"><![CDATA[Apache BeamApache Beam provides an advanced unified programming model, allowing you to implement batch and streaming data processing jobs that can run on any execution engine. Apache Beamæä¾›äº†ä¸€ä¸ªå…ˆè¿›çš„ç»Ÿä¸€ç¼–ç¨‹æ¨¡å‹ï¼Œå¯ä»¥å®ç°æ‰¹é‡å’Œæµæ•°æ®å¤„ç†å·¥ä½œï¼Œå¯ä»¥è¿è¡Œåœ¨ä»»ä½•æ‰§è¡Œå¼•æ“ã€‚Apache Beam is: UNIFIED - Use a single programming model for both batch and streaming use cases. PORTABLE - Execute pipelines on multiple execution environments, including Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow. EXTENSIBLE - Write and share new SDKs, IO connectors, and transformation libraries. ç»Ÿä¸€çš„ - å¯¹æ‰¹å¤„ç†å’Œæµåª’ä½“ç”¨ä¾‹ä½¿ç”¨å•ä¸€ç¼–ç¨‹æ¨¡å‹ã€‚ è½»ä¾¿çš„(å¯ç§»æ¤) - ä¾¿æºå¼ç®¡é“ä¸Šæ‰§è¡Œå¤šä¸ªæ‰§è¡Œç¯å¢ƒï¼ŒåŒ…æ‹¬Apacheçš„å…ˆç«¯ï¼ŒApacheFlinkï¼ŒApache Sparkï¼Œå’Œè°·æ­Œäº‘æ•°æ®æµã€‚ å¯æ‰©å±•çš„ - å†™å’Œåˆ†äº«æ–°çš„è½¯ä»¶å¼€å‘å·¥å…·åŒ…ï¼ŒIOè¿æ¥å™¨ï¼Œå’ŒåŠ¨æ€åº“ã€‚ Get Started To use Beam for your data processing tasks, start by reading the Beam Overview and performing the steps in the Quickstart. Then dive into the Documentation section for in-depth concepts and reference materials for the Beam model, SDKs, and runners.ä½¿ç”¨Beamä¸ºä½ çš„æ•°æ®å¤„ç†ä»»åŠ¡ï¼Œå¼€å§‹é€šè¿‡é˜…è¯»Beamæ¦‚è¿°å’Œç¤ºä¾‹æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚ç„¶åæ½œå…¥æ–‡æ¡£éƒ¨åˆ†è¿›è¡Œæ·±å…¥çš„æ¦‚å¿µå’ŒBeamæ¨¡å‹ï¼ŒSDKsï¼Œå’Œæ‰§è¡Œã€‚ Contribute Beam is an Apache Software Foundation project, available under the Apache v2 license. Beam is an open source community and contributions are greatly appreciated! If youâ€™d like to contribute, please see the Contribute section.Beam æ˜¯Apacheè½¯ä»¶åŸºé‡‘ä¼šçš„é¡¹ç›®ï¼ŒApache v2è®¸å¯ä¸‹å¯ç”¨ã€‚Beamæ˜¯ä¸€ä¸ªå¼€æºç¤¾åŒºå’Œè´¡çŒ®éå¸¸æ„Ÿè°¢ï¼å¦‚æœä½ æƒ³è´¡çŒ®ï¼Œè¯·å‚é˜…æŠ•ç¨¿éƒ¨åˆ†ã€‚ Apache Beam Overview Apache Beam is an open source, unified programming model that you can use to create a data processing pipeline. You start by building a program that defines the pipeline using one of the open source Beam SDKs. The pipeline is then executed by one of Beamâ€™s supported distributed processing back-ends, which include Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow.Apacheçš„Beamæ˜¯ä¸€ä¸ªå¼€æºçš„ã€ç»Ÿä¸€çš„ç¼–ç¨‹æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥åˆ›å»ºä¸€ä¸ªæ•°æ®å¤„ç†ç®¡é“ã€‚ä½ å¼€å§‹å»ºç«‹ä¸€ä¸ªç¨‹åºï¼Œå®šä¹‰äº†ç®¡é“ä½¿ç”¨ä¸€ä¸ªå¼€æºçš„Beamçš„SDKã€‚ç®¡é“æ˜¯é€šè¿‡ä¸€Beamçš„æ”¯æŒåˆ†å¸ƒå¼å¤„ç†åç«¯æ‰§è¡Œï¼ŒåŒ…æ‹¬Apacheçš„å…ˆç«¯ï¼ŒApache Flinkï¼ŒApacheçš„Sparkï¼Œå’Œè°·æ­Œäº‘æ•°æ®æµã€‚ Beam is particularly useful for Embarrassingly Parallel data processing tasks, in which the problem can be decomposed into many smaller bundles of data that can be processed independently and in parallel. You can also use Beam for Extract, Transform, and Load (ETL) tasks and pure data integration. These tasks are useful for moving data between different storage media and data sources, transforming data into a more desirable format, or loading data onto a new system.Beamçš„é«˜åº¦å¹¶è¡Œçš„æ•°æ®å¤„ç†ä»»åŠ¡æ˜¯ç‰¹åˆ«æœ‰ç”¨çš„ï¼Œå…¶ä¸­çš„é—®é¢˜å¯ä»¥è¢«åˆ†è§£æˆè®¸å¤šè¾ƒå°çš„è®¸å¤šæ•°æ®ï¼Œå¯ä»¥ç‹¬ç«‹å’Œå¹¶è¡Œå¤„ç†ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨Beamå˜æ¢ï¼Œæå–ï¼Œå’ŒåŠ è½½ï¼ˆETLï¼‰ä»»åŠ¡å’Œçº¯æ•°æ®é›†æˆã€‚è¿™äº›ä»»åŠ¡ç”¨äºåœ¨ä¸åŒçš„å­˜å‚¨ä»‹è´¨å’Œæ•°æ®æºä¹‹é—´ç§»åŠ¨æ•°æ®ï¼Œå°†æ•°æ®è½¬æ¢æˆæ›´ç†æƒ³çš„æ ¼å¼ï¼Œæˆ–è€…å°†æ•°æ®åŠ è½½åˆ°æ–°ç³»ç»Ÿä¸Šã€‚ Apache Beam SDKs The Beam SDKs provide a unified programming model that can represent and transform data sets of any size, whether the input is a finite data set from a batch data source, or an infinite data set from a streaming data source. The Beam SDKs use the same classes to represent both bounded and unbounded data, and the same transforms to operate on that data. You use the Beam SDK of your choice to build a program that defines your data processing pipeline.Beamçš„SDKæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ç¼–ç¨‹æ¨¡å‹ï¼Œå¯ä»¥è¡¨ç¤ºå’Œå˜æ¢ä»»æ„å¤§å°çš„æ•°æ®é›†ï¼Œè¾“å…¥æ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰é™çš„æ•°æ®é›†ä»ä¸€ä¸ªæ‰¹å¤„ç†çš„æ•°æ®æºï¼Œæˆ–æ— é™çš„æ•°æ®é›†ä»ä¸€ä¸ªæµçš„æ•°æ®æºã€‚Beamçš„SDKä½¿ç”¨åŒä¸€ç±»çš„ä»£è¡¨æœ‰ç•Œå’Œæ— ç•Œçš„æ•°æ®ï¼Œå’Œç›¸åŒçš„å˜æ¢ï¼Œå¯¹è¿™äº›æ•°æ®çš„æ“ä½œã€‚ä½ ç”¨ä½ é€‰æ‹©çš„Beam SDKæ„å»ºçš„ç¨‹åºå®šä¹‰æ•°æ®å¤„ç†ç®¡é“ã€‚ Beam currently supports the following language-specific SDKs:Language SDK StatusJava Active DevelopmentPython Coming SoonOther TBD Apache Beam Pipeline Runners The Beam Pipeline Runners translate the data processing pipeline you define with your Beam program into the API compatible with the distributed processing back-end of your choice. When you run your Beam program, youâ€™ll need to specify the appropriate runner for the back-end where you want to execute your pipeline.Beamæµç®¡é“ç¿»è¯‘æ‚¨å®šä¹‰çš„æ•°æ®å¤„ç†ç®¡é“ä¸æ‚¨çš„Beamç¨‹åºåˆ°APIä¸æ‚¨é€‰æ‹©çš„åˆ†å¸ƒå¼å¤„ç†åç«¯å…¼å®¹ã€‚å½“ä½ è¿è¡Œä½ çš„BeamæŸè®¡åˆ’æ—¶ï¼Œä½ éœ€è¦æŒ‡å®šä½ æƒ³è¦æ‰§è¡Œä½ çš„ç®¡é“çš„åç«¯çš„åˆé€‚çš„æ‰§è¡Œ.ã€‚ Beam currently supports Runners that work with the following distributed processing back-ends:Beamç›®å‰æ”¯æŒè¿è¡Œä¸ä¸‹åˆ—åˆ†å¸ƒå¼å¤„ç†åç«¯Runner StatusApache Apex In DevelopmentApache Flink In DevelopmentApache Spark In DevelopmentGoogle Cloud Dataflow In Development Quickstart æ¥ä¸€æ®µhello Worldå§ç¯å¢ƒå‡†å¤‡ï¼šJDK1.7+ã€‚Maven mavenå‘½ä»¤ï¼š12345678910$ mvn archetype:generate \ -DarchetypeRepository=https://repository.apache.org/content/groups/snapshots \ -DarchetypeGroupId=org.apache.beam \ -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples \ -DarchetypeVersion=LATEST \ -DgroupId=org.example \ -DartifactId=word-count-beam \ -Dversion="0.1" \ -Dpackage=org.apache.beam.examples \ -DinteractiveMode=false æŸ¥çœ‹æ–‡ä»¶12345[root@zhm1 Beam]# lsword-count-beam[root@zhm1 Beam]# cd word-count-beam/[root@zhm1 word-count-beam]# ls src/main/java/org/apache/beam/examples/common DebuggingWordCount.java MinimalWordCount.java WindowedWordCount.java WordCount.java è¿è¡Œï¼š12mvn compile exec:java -Dexec.mainClass=org.apache.beam.examples.WordCount \&gt; -Dexec.args="--inputFile=pom.xml --output=counts" -Pdirect-runner ç»“æœï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150[root@zhm1 word-count-beam]# ll -lh counts*-rw-r--r--. 1 root root 618 1æœˆ 19 14:02 counts-00000-of-00005-rw-r--r--. 1 root root 596 1æœˆ 19 14:02 counts-00001-of-00005-rw-r--r--. 1 root root 585 1æœˆ 19 14:02 counts-00002-of-00005-rw-r--r--. 1 root root 581 1æœˆ 19 14:02 counts-00003-of-00005-rw-r--r--. 1 root root 593 1æœˆ 19 14:02 counts-00004-of-00005[root@zhm1 word-count-beam]# cat counts-00000-of-00005work: 1IS: 1versions: 1direct: 3specified: 1incubating: 1more: 1snapshots: 4submission: 1... Minimal WordCount demonstrates the basic principles involved in building a pipeline. WordCount introduces some of the more common best practices in creating re-usable and maintainable pipelines. Debugging WordCount introduces logging and debugging practices. Windowed WordCount demonstrates how you can use Beamâ€™s programming model to handle both bounded and unbounded datasets.```javaä»Minimal WordCountåˆ†æä»£ç ï¼š/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * "License"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.apache.beam.examples;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Count;import org.apache.beam.sdk.transforms.DoFn;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.ParDo;import org.apache.beam.sdk.transforms.SimpleFunction;import org.apache.beam.sdk.values.KV;/** * An example that counts words in Shakespeare. * * &lt;p&gt;This class, &#123;@link MinimalWordCount&#125;, is the first in a series of four successively more * detailed 'word count' examples. Here, for simplicity, we don't show any error-checking or * argument processing, and focus on construction of the pipeline, which chains together the * application of core transforms. * * &lt;p&gt;Next, see the &#123;@link WordCount&#125; pipeline, then the &#123;@link DebuggingWordCount&#125;, and finally the * &#123;@link WindowedWordCount&#125; pipeline, for more detailed examples that introduce additional * concepts. * * &lt;p&gt;Concepts: * * &lt;pre&gt; * 1. Reading data from text files * 2. Specifying 'inline' transforms * 3. Counting items in a PCollection * 4. Writing data to text files * &lt;/pre&gt; * * &lt;p&gt;No arguments are required to run this pipeline. It will be executed with the DirectRunner. You * can see the results in the output files in your current working directory, with names like * "wordcounts-00001-of-00005. When running on a distributed service, you would use an appropriate * file service. */public class MinimalWordCount &#123; public static void main(String[] args) &#123; // Create a PipelineOptions object. This object lets us set various execution // options for our pipeline, such as the runner you wish to use. This example // will run with the DirectRunner by default, based on the class path configured // in its dependencies. PipelineOptions options = PipelineOptionsFactory.create(); //é»˜è®¤æ˜¯options.setRunner(DirectRunner.class); // Create the Pipeline object with the options we defined above. Pipeline p = Pipeline.create(options); // Apply the pipeline's transforms. // Concept #1: Apply a root transform to the pipeline; in this case, TextIO.Read to read a set // of input text files. TextIO.Read returns a PCollection where each element is one line from // the input text (a set of Shakespeare's texts). // This example reads a public data set consisting of the complete works of Shakespeare. p.apply(TextIO.Read.from("gs://apache-beam-samples/shakespeare/*")) // è¯»å–æœ¬åœ°æ–‡ä»¶ï¼Œæ„å»ºç¬¬ä¸€ä¸ªPTransfor // Concept #2: Apply a ParDo transform to our PCollection of text lines. This ParDo invokes a // DoFn (defined in-line) on each element that tokenizes the text line into individual words. // The ParDo returns a PCollection&lt;String&gt;, where each element is an individual word in // Shakespeare's collected texts. .apply("ExtractWords", ParDo.of(new DoFn&lt;String, String&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; for (String word : c.element().split("[^a-zA-Z']+")) &#123; if (!word.isEmpty()) &#123; c.output(word); &#125; &#125; &#125; &#125;)) // Concept #3: Apply the Count transform to our PCollection of individual words. The Count // transform returns a new PCollection of key/value pairs, where each key represents a unique // word in the text. The associated value is the occurrence count for that word. .apply(Count.&lt;String&gt;perElement()) // Apply a MapElements transform that formats our PCollection of word counts into a printable // string, suitable for writing to an output file. .apply("FormatResults", MapElements.via(new SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Long&gt; input) &#123; return input.getKey() + ": " + input.getValue(); &#125; &#125;)) // Concept #4: Apply a write transform, TextIO.Write, at the end of the pipeline. // TextIO.Write writes the contents of a PCollection (in this case, our PCollection of // formatted strings) to a series of text files. // // By default, it will write to a set of files with names like wordcount-00001-of-00005 .apply(TextIO.Write.to("wordcounts")); // Run the pipeline. p.run().waitUntilFinish(); &#125;&#125; åˆ›å»ºç®¡é“1PipelineOptions options = PipelineOptionsFactory.create(); ä¸‹ä¸€æ­¥æ˜¯ä½¿ç”¨æˆ‘ä»¬åˆšæ‰æ„å»ºçš„é€‰é¡¹åˆ›å»ºä¸€ä¸ªPipelineå¯¹è±¡ã€‚Pipelineå¯¹è±¡æ„å»ºè¦æ‰§è¡Œçš„å˜æ¢å›¾ï¼Œä¸ç‰¹å®šæµæ°´çº¿ç›¸å…³è”ã€‚1Pipeline p = Pipeline.create(options); åº”ç”¨ç®¡é“å˜æ¢ Minimalçš„WordCountæµæ°´çº¿åŒ…å«å‡ ä¸ªå˜æ¢ï¼Œä»¥å°†æ•°æ®è¯»å…¥æµæ°´çº¿ï¼Œæ“çºµæˆ–ä»¥å…¶ä»–æ–¹å¼transformæ•°æ®ï¼Œå¹¶å†™å‡ºç»“æœã€‚æ¯ä¸ªtransformè¡¨ç¤ºç®¡é“ä¸­çš„æ“ä½œã€‚ æ¯ä¸ªtransformé‡‡ç”¨æŸç§è¾“å…¥ï¼ˆæ•°æ®æˆ–å…¶ä»–ï¼‰ï¼Œå¹¶äº§ç”Ÿä¸€äº›è¾“å‡ºæ•°æ®ã€‚è¾“å…¥å’Œè¾“å‡ºæ•°æ®æ˜¯ç”±SDKç±»è¡¨ç¤ºPCollectionã€‚PCollectionæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ç±»ï¼Œç”±Beam SDKæä¾›ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥è¡¨ç¤ºå‡ ä¹ä»»ä½•å¤§å°çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ— é™æ•°æ®é›†ã€‚ Minimal WordCountæµæ°´çº¿åŒ…å«äº”ä¸ªtransform 1.ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶Read transformåº”ç”¨äºæµæ°´çº¿å¯¹è±¡æœ¬èº«ï¼Œå¹¶äº§ç”ŸPCollectionä½œä¸ºè¾“å‡ºã€‚è¾“å‡ºPCollectionä¸­çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºè¾“å…¥æ–‡ä»¶ä¸­çš„ä¸€è¡Œæ–‡æœ¬ã€‚æ­¤ç¤ºä¾‹æ°å¥½ä½¿ç”¨å­˜å‚¨åœ¨å¯å…¬å¼€è®¿é—®çš„Google Cloud Storageå­˜å‚¨æ¡¶ï¼ˆâ€œgsï¼š//â€ï¼‰ä¸­çš„è¾“å…¥æ•°æ®ã€‚1p.apply(TextIO.Read.from("gs://apache-beam-samples/shakespeare/*")) 2.ä¸€ä¸ªParDo transformå®ƒè°ƒç”¨DoFnäº†tokenizesæ–‡æœ¬è¡Œæˆå•ä¸ªçš„å•è¯æ¯ä¸ªå…ƒç´ ï¼ˆåœ¨çº¿ä½œä¸ºä¸€ä¸ªåŒ¿åç±»ä¸­å®šä¹‰ï¼‰ã€‚å¯¹äºæ­¤transformçš„è¾“å…¥æ˜¯PCollectionç”±å…ˆå‰ç”Ÿæˆçš„æ–‡æœ¬çš„è¡ŒTextIO.Readå˜æ¢ã€‚çš„ParDoå˜æ¢è¾“å‡ºä¸€ä¸ªæ–°çš„PCollectionï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºçš„æ–‡æœ¬çš„å•è¯ã€‚12345678910.apply("ExtractWords", ParDo.of(new DoFn&lt;String, String&gt;() &#123; @ProcessElement public void processElement(ProcessContext c) &#123; for (String word : c.element().split("[^a-zA-Z']+")) &#123; if (!word.isEmpty()) &#123; c.output(word); &#125; &#125; &#125; &#125;)) 3.è¯¥SDKæä¾›çš„Count transformæ˜¯ä¸€ä¸ªé€šç”¨çš„transformï¼Œå®ƒæ¥å—ä¸€ä¸ªPCollectionä»»ä½•ç±»å‹çš„ï¼Œå’Œè¿”å›PCollectioné”®/å€¼å¯¹ã€‚æ¯ä¸ªé”®è¡¨ç¤ºè¾“å…¥é›†åˆä¸­çš„å”¯ä¸€å…ƒç´ ï¼Œæ¯ä¸ªå€¼è¡¨ç¤ºé”®åœ¨è¾“å…¥é›†åˆä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ è¿™æ¡ç®¡çº¿ï¼Œç”¨äºå°†è¾“å…¥Countçš„æ˜¯PCollectionç”±ä»¥å‰äº§ç”Ÿä¸ªåˆ«å•è¯ParDoï¼Œå¹¶è¾“å‡ºä¸ºä¸€ä¸ªPCollectionï¼Œå…¶ä¸­æ¯ä¸ªé”®ä»£è¡¨ä¸€ä¸ªå”¯ä¸€å­—ä¸­çš„æ–‡æœ¬å’Œç›¸å…³å€¼æ˜¯å‡ºç°è®¡æ•°æ¯ä¸ªé”®/å€¼å¯¹ã€‚1.apply(Count.&lt;String&gt;perElement()) 4.ä¸‹ä¸€ä¸ªtransformå°†å”¯ä¸€å­—å’Œå‡ºç°è®¡æ•°çš„æ¯ä¸ªé”®/å€¼å¯¹æ ¼å¼åŒ–ä¸ºé€‚äºå†™å…¥è¾“å‡ºæ–‡ä»¶çš„å¯æ‰“å°å­—ç¬¦ä¸²ã€‚ MapElementsæ˜¯ä¸€ä¸ªæ›´é«˜å±‚æ¬¡çš„å¤åˆè½¬æ¢ï¼Œå®ƒå°è£…äº†ä¸€ä¸ªç®€å•çš„ParDo; ä¸ºè¾“å…¥ä¸­çš„æ¯ä¸ªå…ƒä»¶PCollectionï¼ŒMapElementsé€‚ç”¨äºäº§ç”Ÿæ°å¥½ä¸€ä¸ªè¾“å‡ºå…ƒä»¶çš„åŠŸèƒ½ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒMapElementsè°ƒç”¨SimpleFunctionï¼ˆåœ¨çº¿ä½œä¸ºåŒ¿åç±»ä¸­å®šä¹‰ï¼‰ï¼Œè¯¥ç¡®å®çš„æ ¼å¼ã€‚ä½œä¸ºè¾“å…¥ï¼ŒMapElementså–PCollectionæ‰€äº§ç”Ÿçš„é”®/å€¼å¯¹çš„Countï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªæ–°PCollectionçš„å¯æ‰“å°å­—ç¬¦ä¸²ã€‚123456.apply("FormatResults", MapElements.via(new SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() &#123; @Override public String apply(KV&lt;String, Long&gt; input) &#123; return input.getKey() + ": " + input.getValue(); &#125; &#125;)) 5.ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶Writeã€‚æ­¤transforméœ€è¦çš„æœ€ç»ˆPCollectionæ ¼å¼å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥å’Œæ¯ä¸ªå…ƒç´ å†™å…¥åˆ°è¾“å‡ºæ–‡æœ¬æ–‡ä»¶ã€‚è¾“å…¥ä¸­çš„æ¯ä¸ªå…ƒç´ PCollectionè¡¨ç¤ºåœ¨æ‰€äº§ç”Ÿçš„è¾“å‡ºæ–‡ä»¶çš„æ–‡æœ¬çš„ä¸€ä¸ªè¡Œã€‚1.apply(TextIO.Write.to("wordcounts")); æ³¨æ„ï¼Œè¯¥Writeå˜æ¢äº§ç”Ÿç±»å‹çš„çç¢ç»“æœå€¼PDoneï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹è¢«å¿½ç•¥ è¿è¡Œç®¡é“é€šè¿‡è°ƒç”¨è¿è¡Œç®¡çº¿runçš„æ–¹æ³•ï¼Œå®ƒä¼šå°†æ‚¨çš„ç®¡é“ç”±æ‚¨åœ¨åˆ›å»ºæ‚¨çš„ç®¡é“ä¸­æŒ‡å®šçš„ç®¡é“è¿è¡Œç¨‹åºæ‰§è¡Œã€‚1p.run().waitUntilFinish(); æ³¨æ„ï¼Œè¯¥runæ–¹æ³•æ˜¯å¼‚æ­¥çš„ã€‚å¯¹äºé˜»æ­¢æ‰§è¡Œï¼Œè€Œä¸æ˜¯ï¼Œè¿è¡Œç®¡çº¿è¿½åŠ waitUntilFinishæ–¹æ³•ã€‚]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>Beam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç®—æ³•æ¨æ¼”]]></title>
    <url>%2F2017%2F01%2F10%2F%E7%AE%97%E6%B3%95%E6%8E%A8%E6%BC%94%2F</url>
    <content type="text"><![CDATA[æ¡ä»¶æ¦‚ç‡äº‹ä»¶Aåœ¨å¦å¤–ä¸€ä¸ªäº‹ä»¶Bå·²ç»å‘ç”Ÿæ¡ä»¶ä¸‹çš„å‘ç”Ÿæ¦‚ç‡ã€‚æ¡ä»¶æ¦‚ç‡è¡¨ç¤ºä¸ºPï¼ˆA|Bï¼‰ï¼Œè¯»ä½œâ€œåœ¨Bæ¡ä»¶ä¸‹Açš„æ¦‚ç‡â€ã€‚ è®¾Aï¼ŒB æ˜¯ä¸¤ä¸ªäº‹ä»¶ï¼Œä¸”Aä¸æ˜¯ä¸å¯èƒ½äº‹ä»¶ï¼Œåˆ™ç§°ä¸ºåœ¨äº‹ä»¶Aå‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œäº‹ä»¶Bå‘ç”Ÿçš„æ¡ä»¶æ¦‚ç‡ã€‚ä¸€èˆ¬åœ°ï¼Œï¼Œä¸”å®ƒæ»¡è¶³ä»¥ä¸‹ä¸‰æ¡ä»¶ï¼šï¼ˆ1ï¼‰éè´Ÿæ€§ï¼›ï¼ˆ2ï¼‰è§„èŒƒæ€§ï¼›ï¼ˆ3ï¼‰å¯åˆ—å¯åŠ æ€§ã€‚ å¦‚ä¸Šå›¾æ±‚ï¼šçº¢çƒæ¥è‡ªAçš„æ¦‚ç‡Pï¼ˆA|çº¢ï¼‰ = Pï¼ˆAçº¢ï¼‰/Pï¼ˆçº¢ï¼‰ å…¨æ¦‚ç‡å…¬å¼å…¨æ¦‚ç‡å…¬å¼ä¸ºæ¦‚ç‡è®ºä¸­çš„é‡è¦å…¬å¼ï¼Œå®ƒå°†å¯¹ä¸€å¤æ‚äº‹ä»¶Açš„æ¦‚ç‡æ±‚è§£é—®é¢˜è½¬åŒ–ä¸ºäº†åœ¨ä¸åŒæƒ…å†µä¸‹å‘ç”Ÿçš„ç®€å•äº‹ä»¶çš„æ¦‚ç‡çš„æ±‚å’Œé—®é¢˜ã€‚ å…¬å¼è¡¨ç¤ºè‹¥äº‹ä»¶A1ï¼ŒA2ï¼Œâ€¦ï¼ŒAnæ„æˆä¸€ä¸ªå®Œå¤‡äº‹ä»¶ç»„ä¸”éƒ½æœ‰æ­£æ¦‚ç‡ï¼Œåˆ™å¯¹ä»»æ„ä¸€ä¸ªäº‹ä»¶Béƒ½æœ‰å…¬å¼æˆç«‹ã€‚ å¦‚ä¸Šå›¾æ±‚ï¼šçº¢çƒçš„æ¦‚ç‡Pï¼ˆçº¢ï¼‰ = Pï¼ˆAçº¢ï¼‰+Pï¼ˆBçº¢ï¼‰ è´å¶æ–¯å®šç†è´å¶æ–¯å®šç†æ˜¯å…³äºéšæœºäº‹ä»¶Aå’ŒBçš„æ¡ä»¶æ¦‚ç‡ï¼ˆæˆ–è¾¹ç¼˜æ¦‚ç‡ï¼‰çš„ä¸€åˆ™å®šç†ã€‚å…¶ä¸­P(A|B)æ˜¯åœ¨Bå‘ç”Ÿçš„æƒ…å†µä¸‹Aå‘ç”Ÿçš„å¯èƒ½æ€§ã€‚ æ¬§å‡ é‡Œå¾—å®šç†äºŒç»´ç©ºé—´çš„å…¬å¼0Ï = sqrt( (x1-x2)^2+(y1-y2)^2 ) |x| = âˆš( x2 + y2 )ä¸‰ç»´ç©ºé—´çš„å…¬å¼0Ï = âˆš( (x1-x2)^2+(y1-y2)^2+(z1-z2)^2 ) |x| = âˆš( x2 + y2 + z2 )nç»´ç©ºé—´çš„å…¬å¼nç»´æ¬§æ°ç©ºé—´æ˜¯ä¸€ä¸ªç‚¹é›†,å®ƒçš„æ¯ä¸ªç‚¹ X æˆ–å‘é‡ x å¯ä»¥è¡¨ç¤ºä¸º (x1ï¼Œx2ï¼Œâ€¦ï¼Œx[n]) ï¼Œå…¶ä¸­ xi æ˜¯å®æ•°ï¼Œç§°ä¸º X çš„ç¬¬iä¸ªåæ ‡ã€‚ä¸¤ä¸ªç‚¹ A = (a1ï¼Œa2ï¼Œâ€¦ï¼Œa[n]) å’Œ B = (b1ï¼Œb2ï¼Œâ€¦ï¼Œb[n]) ä¹‹é—´çš„è·ç¦» Ï(Aï¼ŒB) å®šä¹‰ä¸ºä¸‹é¢çš„å…¬å¼ï¼šÏ(Aï¼ŒB) =âˆš [ âˆ‘( a[i] - b[i] )^2 ] (i = 1ï¼Œ2ï¼Œâ€¦ï¼Œn)å‘é‡ x = (x1ï¼Œx2ï¼Œâ€¦ï¼Œx[n]) çš„è‡ªç„¶é•¿åº¦ |x| å®šä¹‰ä¸ºä¸‹é¢çš„å…¬å¼ï¼š|x| = âˆš( x1^2 + x2^2 + â€¦ + x[n]^2 ) çº¿æ€§æ–¹ç¨‹çº¿æ€§æ–¹ç¨‹ä¹Ÿç§°ä¸€æ¬¡æ–¹ç¨‹ã€‚æŒ‡æœªçŸ¥æ•°éƒ½æ˜¯ä¸€æ¬¡çš„æ–¹ç¨‹ã€‚å…¶ä¸€èˆ¬çš„å½¢å¼æ˜¯ax+by+â€¦+cz+d=0ã€‚çº¿æ€§æ–¹ç¨‹çš„æœ¬è´¨æ˜¯ç­‰å¼ä¸¤è¾¹ä¹˜ä»¥ä»»ä½•ç›¸åŒçš„éé›¶æ•°ï¼Œæ–¹ç¨‹çš„æœ¬è´¨éƒ½ä¸å—å½±å“ã€‚ax + by = cæœºå™¨å­¦ä¹ é‡Œé¢ç”¨é€¼è¿‘è®ºæ¥ç®—]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>ç®—æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python åˆæ¢]]></title>
    <url>%2F2017%2F01%2F10%2Fpython%20%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[åˆæ¢Pythonæ˜¯ä¸€ç§è§£é‡Šå‹è¯­è¨€ï¼Œä¸ç”¨ç¼–è¯‘Pythonæ˜¯äº¤äº’å¼è¯­è¨€Pythonæ˜¯é¢å‘å¯¹è±¡çš„è¯­è¨€ ä»ç®€å•çš„æ–‡å­—å¤„ç†åˆ°çˆ¬è™«æ¸¸æˆ pythonæœ‰2ä¸ªåˆ†æ”¯ï¼Œä¸€ä¸ªæ˜¯2.x ä¸€ä¸ªæ˜¯3.xè¯­æ³•ä¼šä¸åŒä¾‹å¦‚print 1 åœ¨3.xç‰ˆæœ¬æ˜¯ä¸€ä¸ªå‡½æ•°è¦å†™æˆprint(1) å’Œjavaä¸åŒçš„æ˜¯æœ‰ä»¥ä¸‹ä¸¤ä¸ªç±»å‹å®šä¹‰å…ƒç¥–ç±»å‹() å…ƒç»„æ˜¯å¯è¯»çš„å®šä¹‰å­—å…¸ç±»å‹{} åˆå§‹åŒ–a={2,3} typeä»¥ä¸‹açš„ç±»å‹ä¸ºset å¼•ç”¨æ¨¡å—import åŒ…å.ç±»åfrom åŒ…å.ç±»å import *from åŒ…å.ç±»å import æ–¹æ³•å as åˆ«å å¦‚æœæƒ³åˆ«çš„æ¨¡å—ä¸èƒ½å¼•ç”¨è‡ªå·±æ¨¡å—çš„å†…å®¹å¯ä»¥å¢åŠ if name==â€™mainâ€˜ æ¯ä¸ªç›®å½•ä¸‹éƒ½æœ‰init.pyæ–‡ä»¶ï¼Œè¿™ä¸ªæ˜¯åˆå§‹åŒ–æ¨¡å— pythonå¼€å¯å¤šçº¿ç¨‹ï¼Œå¯ä»¥ä½¿ç”¨å†…ç½®å…³é”®å­—threadingç§æœ‰å˜é‡å‰é¢åŠ __å®ä¾‹ä¸èƒ½è®¿é—®ç§æœ‰å˜é‡ pythonä½¿ç”¨å¼•ç”¨è®¡æ•°æ³•è¿½è¸ªå†…å­˜å¹¶å›æ”¶ ç»§æ‰¿child(parent)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ç®—æ³•æ€ç»´]]></title>
    <url>%2F2017%2F01%2F04%2F%E7%AE%97%E6%B3%95%E6%80%9D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[ç«æŸ´æ£æ¸¸æˆ ç§»åŠ¨ä¸¤æ ¹ç«æŸ´å¾—åˆ°çš„æœ€å¤§å€¼æ˜¯å¤šå°‘ï¼Ÿ ä¸€èˆ¬äººçš„ç¬¬ä¸€ååº”æ˜¯ï¼š å¦‚æœæˆ‘ä»¬æƒ³åˆ°äº†ä½æ•°è¶Šå¤šï¼Œæ•°å­—è¶Šå¤§ï¼Œå°±ä¼šæƒ³åˆ°ï¼š æ­¤æ—¶å¦‚æœæƒ³åˆ°äº†å¹³æ–¹è®¡ç®— é‚£ä¹ˆä½ éœ€è¦çš„æ˜¯ç»§ç»­å‘æ•£å››ä½ï¼Œæƒ³åˆ°ç§‘å­¦è®¡æ•°æ³• ä½†æ˜¯æ­¤æ—¶å¹¶ä¸æ˜¯æœ€å¤§çš„ åªæœ‰å½“ä½ æƒ³åˆ°äº†ä»¥ä¸‹çš„æ•°æ®ï¼Œé‚£ä¹ˆæ­å–œä½ æˆåŠŸäº† æ€è·¯å†³å®šå‡ºè·¯]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>ç®—æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot solr]]></title>
    <url>%2F2016%2F12%2F28%2Fspring%20boot%20solr%2F</url>
    <content type="text"><![CDATA[pomå¼•å…¥solrçš„jaråŒ…1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-solr&lt;/artifactId&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[dockerç‰ˆKafkaé›†ç¾¤]]></title>
    <url>%2F2016%2F12%2F19%2Fdocker%E7%8E%A9kafka%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[å…¥æ‰‹centos7é¦–å…ˆæ‰“å¼€centos å®˜ç½‘ä¸‹è½½https://www.centos.org/download/ é€‰æ‹©DVD ISO æ‰¾åˆ°åœ°å€å¹¶ä¸‹è½½http://101.96.8.151/isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso å®‰è£…å‚è€ƒï¼šhttp://blog.csdn.net/alex_my/article/details/38142229 å®‰è£…dockerå®‰è£…è¯¦æƒ…å¯å‚è€ƒå®˜ç½‘https://docs.docker.com/engine/installation/linux/centos/åšä¸»ä½¿ç”¨å¦å¤–ä¸€ç§ç®€å•æ–¹å¼å®‰è£…å®‰è£…docker1yum install docker å¯åŠ¨æœåŠ¡ï¼ˆCENTOS7ä¹‹å‰çš„ç‰ˆæœ¬ï¼‰12service docker startchkconfig docker on Centosä¹‹åçš„ï¼š12systemctl start docker.servicesystemctl enable docker.service 123456789101112131415161718docker versionClient: Version: 1.10.3 API version: 1.22 Package version: docker-common-1.10.3-59.el7.centos.x86_64 Go version: go1.6.3 Git commit: 3999ccb-unsupported Built: Thu Dec 15 17:24:43 2016 OS/Arch: linux/amd64Server: Version: 1.10.3 API version: 1.22 Package version: docker-common-1.10.3-59.el7.centos.x86_64 Go version: go1.6.3 Git commit: 3999ccb-unsupported Built: Thu Dec 15 17:24:43 2016 OS/Arch: linux/amd64 å®‰è£…docker-composeåšä¸»å½“æ—¶æœ‰1.7.1çš„ç‰ˆæœ¬åœ¨å®‰è£…å®Œä¹‹åå¯åŠ¨æŠ¥é”™Cannot open self /usr/bin/docker-compose or archive /usr/bin/docker-compose.pkg åæ”¹ç”¨æ—©ä¸€ç‚¹çš„ç‰ˆæœ¬1.7.0æµ‹è¯•æ²¡æœ‰é—®é¢˜ä¸‹è½½å’Œå®‰è£…åœ°å€å¯å‚è€ƒï¼šhttps://github.com/docker/compose/releases?after=docs-v1.7.1-2016-05-3112345678curl -L https://github.com/docker/compose/releases/download/1.7.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composedocker-compose versiondocker-compose version 1.7.0, build 0d7bf73docker-py version: 1.8.0CPython version: 2.7.9OpenSSL version: OpenSSL 1.0.1e 11 Feb 2013 ç¼–å†™dockeræ–‡ä»¶ï¼Œæ­¤å¤„å‚è€ƒJasonå¤§ç¥çš„ï¼Œå°Šé‡åŸåˆ›ï¼Œè½¬è½½è¯·æ ‡æ³¨kafka.Dockerfile12345678910111213141516171819202122232425262728293031323334353637383940414243444546FROM centos:6.6RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\ mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoRUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pipRUN mkdir /opt/java &amp;&amp;\ wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/javaENV KAFKA_VERSION "0.8.2.2"RUN mkdir /opt/kafka &amp;&amp;\ wget http://apache.fayea.com/kafka/$KAFKA_VERSION/kafka_2.11-$KAFKA_VERSION.tgz -P /opt/kafkaRUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\ JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\ sed -i "/^PATH/i export JAVA_HOME=$JAVA_HOME" /root/.bash_profile &amp;&amp;\ sed -i "s%^PATH.*$%&amp;:$JAVA_HOME/bin%g" /root/.bash_profile &amp;&amp;\ source /root/.bash_profileRUN tar zxvf /opt/kafka/kafka*.tgz -C /opt/kafka &amp;&amp;\ sed -i 's/num.partitions.*$/num.partitions=3/g' /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.propertiesRUN echo "source /root/.bash_profile" &gt; /opt/kafka/start.sh &amp;&amp;\ echo "cd /opt/kafka/kafka_2.11-"$KAFKA_VERSION &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ #echo "sed -i 's%zookeeper.connect=.*$%zookeeper.connect=zookeeper:2181%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""ZOOKEEPER_CONNECT"" ] &amp;&amp; sed -i 's%.*zookeeper.connect=.*$%zookeeper.connect='$""ZOOKEEPER_CONNECT'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""BROKER_ID"" ] &amp;&amp; sed -i 's%broker.id=.*$%broker.id='$""BROKER_ID'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""BROKER_PORT"" ] &amp;&amp; sed -i 's%port=.*$%port='$""BROKER_PORT'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "sed -i 's%#advertised.host.name=.*$%advertised.host.name='$""(hostname -i)'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""ADVERTISED_HOST_NAME"" ] &amp;&amp; sed -i 's%.*advertised.host.name=.*$%advertised.host.name='$""ADVERTISED_HOST_NAME'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "sed -i 's%#host.name=.*$%host.name='$""(hostname -i)'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "[ ! -z $""HOST_NAME"" ] &amp;&amp; sed -i 's%.*host.name=.*$%host.name='$""HOST_NAME'""%g' /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ echo "delete.topic.enable=true" &gt;&gt; /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.properties &amp;&amp;\ echo "bin/kafka-server-start.sh config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\ chmod a+x /opt/kafka/start.shRUN yum install -y ncEXPOSE 9092WORKDIR /opt/kafka/kafka_2.11-$KAFKA_VERSIONENTRYPOINT ["sh", "/opt/kafka/start.sh"] zookeeper.Dockerfile1234567891011121314151617181920212223242526272829303132333435363738FROM centos:6.6RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\ mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoRUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pipRUN mkdir /opt/java &amp;&amp;\ wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/javaRUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\ JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\ sed -i &quot;/^PATH/i export JAVA_HOME=$JAVA_HOME&quot; /root/.bash_profile &amp;&amp;\ sed -i &quot;s%^PATH.*$%&amp;:$JAVA_HOME/bin%g&quot; /root/.bash_profile &amp;&amp;\ source /root/.bash_profileENV ZOOKEEPER_VERSION &quot;3.4.6&quot;RUN mkdir /opt/zookeeper &amp;&amp;\ wget http://mirror.olnevhost.net/pub/apache/zookeeper/zookeeper-$ZOOKEEPER_VERSION/zookeeper-$ZOOKEEPER_VERSION.tar.gz -P /opt/zookeeperRUN tar zxvf /opt/zookeeper/zookeeper*.tar.gz -C /opt/zookeeperRUN echo &quot;source /root/.bash_profile&quot; &gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;cp /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo_sample.cfg /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_PORT&quot;&quot; ] &amp;&amp; sed -i &apos;s%.*clientPort=.*$%clientPort=&apos;$&quot;&quot;ZOOKEEPER_PORT&apos;&quot;&quot;%g&apos; /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_ID&quot;&quot; ] &amp;&amp; mkdir -p /tmp/zookeeper &amp;&amp; echo $&quot;&quot;ZOOKEEPER_ID &gt; /tmp/zookeeper/myid&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;[[ ! -z $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot; ]] &amp;&amp; for server in $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot;; do echo $&quot;&quot;server&quot;&quot; &gt;&gt; /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg; done&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\ echo &quot;/opt/zookeeper/zookeeper-$&quot;ZOOKEEPER_VERSION&quot;/bin/zkServer.sh start-foreground&quot; &gt;&gt; /opt/zookeeper/start.shRUN yum install -y ncEXPOSE 2181WORKDIR /opt/zookeeper/zookeeper-$ZOOKEEPER_VERSIONENTRYPOINT [&quot;sh&quot;, &quot;/opt/zookeeper/start.sh&quot;] docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139version: &apos;2.0&apos;services: zookeeper0: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper0 hostname: zookeeper0 ports: - &quot;2181:2181&quot; - &quot;2888:2888&quot; - &quot;3888:3888&quot; expose: - 2181 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_ID: 0 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882 zookeeper1: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper1 hostname: zookeeper1 ports: - &quot;2182:2182&quot; - &quot;28881:28881&quot; - &quot;38881:38881&quot; expose: - 2182 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2182 ZOOKEEPER_ID: 1 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882# depends_on:# - zookeeper0 zookeeper2: build: context: . dockerfile: zookeeper.Dockerfile image: jason/zookeeper:3.4.6 container_name: zookeeper2 hostname: zookeeper2 ports: - &quot;2183:2183&quot; - &quot;28882:28882&quot; - &quot;38882:38882&quot; expose: - 2183 - 2888 - 3888 environment: ZOOKEEPER_PORT: 2183 ZOOKEEPER_ID: 2 ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882# depends_on:# - zookeeper1 kafka0: build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka0 hostname: kafka0 ports: - &quot;9092:9092&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 0 BROKER_PORT: 9092 ADVERTISED_HOST_NAME: kafka0 HOST_NAME: kafka0 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9092# links:# - zookeeper kafka1: build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka1 hostname: kafka1 ports: - &quot;9093:9093&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 1 BROKER_PORT: 9093 ADVERTISED_HOST_NAME: kafka1 HOST_NAME: kafka1 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9093# links:# - zookeeper kafka2: build: . build: context: . dockerfile: kafka.Dockerfile image: jason/kafka:0.8.2.2 container_name: kafka2 hostname: kafka2 ports: - &quot;9094:9094&quot; environment: ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka BROKER_ID: 2 BROKER_PORT: 9094 ADVERTISED_HOST_NAME: kafka2 HOST_NAME: kafka2 volumes: - /var/run/docker.sock:/var/run/docker.sock depends_on: - zookeeper0 - zookeeper1 - zookeeper2 expose: - 9094# links:# - zookeeper å¯åŠ¨docker-compose1[root@zhm1 docker-file]# docker-compose up -d å¦‚æœæ›¾ç»æœ‰å¯åŠ¨çš„éœ€è¦å…ˆåœæ­¢ç§»é™¤åœ¨å¯åŠ¨1[root@zhm1 docker-file]# docker-compose stop;docker-compose rm -f 12345678[root@zhm1 docker-file]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES891585b4767a jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 9092/tcp, 0.0.0.0:9093-&gt;9093/tcp kafka181f90ccb917a jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 9092/tcp, 0.0.0.0:9094-&gt;9094/tcp kafka22edb39ed0e97 jason/kafka:0.8.2.2 "sh /opt/kafka/start." About an hour ago Up About an hour 0.0.0.0:9092-&gt;9092/tcp kafka0b21ac8fb0f72 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 2181/tcp, 2888/tcp, 0.0.0.0:2183-&gt;2183/tcp, 0.0.0.0:28882-&gt;28882/tcp, 3888/tcp, 0.0.0.0:38882-&gt;38882/tcp zookeeper248f5ada24ff7 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2888-&gt;2888/tcp, 0.0.0.0:3888-&gt;3888/tcp zookeeper0d247a8ac6a30 jason/zookeeper:3.4.6 "sh /opt/zookeeper/st" About an hour ago Up About an hour 2181/tcp, 2888/tcp, 0.0.0.0:2182-&gt;2182/tcp, 0.0.0.0:28881-&gt;28881/tcp, 3888/tcp, 0.0.0.0:38881-&gt;38881/tcp zookeeper1 æ­¤æ—¶å¯ä»¥è¿›å…¥å®¹å™¨å†…æŸ¥çœ‹ï¼š123docker exec -it zookeeper0 bashsource /root/.bash_profilebin/zkCli.sh -server zookeeper0:2181 1234567[zk: zookeeper0:2182(CONNECTED) 0] ls /[zookeeper, kafka][zk: zookeeper0:2182(CONNECTED) 1] ls /kafka[admin, consumers, controller, controller_epoch, brokers, config][zk: zookeeper0:2182(CONNECTED) 2] ls /kafka/brokers/ids[0, 1, 2][zk: zookeeper0:2182(CONNECTED) 3] æ­¤å¤„æ³¨æ„ï¼Œå› ä¸ºdocker-compose.ymlæ–‡ä»¶é‡Œé…ç½®çš„kafkaå¼•ç”¨zookeeperæ ¹ç›®å½•ä¸‹çš„kafkaç›®å½•ï¼Œæ•…åœ¨å†™zookeeperçš„æ—¶å€™è¦å¢åŠ /kafka123456789[root@zhm1 docker-file]# docker exec -it kafka0 bash[root@kafka0 kafka_2.11-0.8.2.2]# source /root/.bash_profile[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --create --partitions 3 --replication-factor 1Created topic "topic1".[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --describe Topic:topic1 PartitionCount:3 ReplicationFactor:1 Configs: Topic: topic1 Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: topic1 Partition: 1 Leader: 2 Replicas: 2 Isr: 2 Topic: topic1 Partition: 2 Leader: 0 Replicas: 0 Isr: 0 kafkaæœ¯è¯­TopicTopic,æ˜¯KAFKAå¯¹æ¶ˆæ¯åˆ†ç±»çš„ä¾æ®;ä¸€æ¡æ¶ˆæ¯,å¿…é¡»æœ‰ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„Topic;æ¯”å¦‚ç°åœ¨åˆä¸¤ä¸ªTopic,åˆ†åˆ«æ˜¯TopicAå’ŒTopicB,Producerå‘TopicAå‘é€ä¸€ä¸ªæ¶ˆæ¯messageA,ç„¶åå‘TopicBå‘é€ä¸€ä¸ªæ¶ˆæ¯messaeB;é‚£ä¹ˆ,è®¢é˜…TopicAçš„Consumerå°±ä¼šæ”¶åˆ°æ¶ˆæ¯messageA,è®¢é˜…TopicBçš„Consumerå°±ä¼šæ”¶åˆ°æ¶ˆæ¯messaeB;(æ¯ä¸ªConsumerå¯ä»¥åŒæ—¶è®¢é˜…å¤šä¸ªTopic,ä¹Ÿå³æ˜¯è¯´,åŒæ—¶è®¢é˜…TopicAå’ŒTopicBçš„Consumerå¯ä»¥æ”¶åˆ°messageAå’ŒmessaeB)ã€‚åŒä¸€ä¸ªGroup idçš„consumersåœ¨åŒä¸€ä¸ªTopicçš„åŒä¸€æ¡æ¶ˆæ¯åªèƒ½è¢«ä¸€ä¸ªconsumeræ¶ˆè´¹ï¼Œå®ç°äº†ç‚¹å¯¹ç‚¹æ¨¡å¼ï¼Œä¸åŒGroup idçš„Consumersåœ¨åŒä¸€ä¸ªTopicä¸Šçš„åŒä¸€æ¡æ¶ˆæ¯å¯ä»¥åŒæ—¶æ¶ˆè´¹åˆ°ï¼Œåˆ™å®ç°äº†å‘å¸ƒè®¢é˜…æ¨¡å¼ã€‚é€šè¿‡Consumerçš„Group idå®ç°äº†JMSçš„æ¶ˆæ¯æ¨¡å¼ Partitionæ¯ä¸€ä¸ªTopicå¯ä»¥æœ‰å¤šä¸ªPartition,è¿™æ ·åšæ˜¯ä¸ºäº†æé«˜KAFKAç³»ç»Ÿçš„å¹¶å‘èƒ½åŠ›ï¼Œæ¯ä¸ªPartitionä¸­æŒ‰ç…§æ¶ˆæ¯å‘é€çš„é¡ºåºä¿å­˜ç€Producerå‘æ¥çš„æ¶ˆæ¯,æ¯ä¸ªæ¶ˆæ¯ç”¨IDæ ‡è¯†,ä»£è¡¨è¿™ä¸ªæ¶ˆæ¯åœ¨æ”¹Partitionä¸­çš„åç§»é‡,è¿™æ ·,çŸ¥é“äº†ID,å°±å¯ä»¥æ–¹ä¾¿çš„å®šä½ä¸€ä¸ªæ¶ˆæ¯äº†;æ¯ä¸ªæ–°æäº¤è¿‡æ¥çš„æ¶ˆæ¯,è¢«è¿½åŠ åˆ°Partitionçš„å°¾éƒ¨;å¦‚æœä¸€ä¸ªPartitionè¢«å†™æ»¡äº†,å°±ä¸å†è¿½åŠ ;(æ³¨æ„,KAFKAä¸ä¿è¯ä¸åŒPartitionä¹‹é—´çš„æ¶ˆæ¯æœ‰åºä¿å­˜) LeaderPartitionä¸­è´Ÿè´£æ¶ˆæ¯è¯»å†™çš„èŠ‚ç‚¹;Leaderæ˜¯ä»Partitionçš„èŠ‚ç‚¹ä¸­éšæœºé€‰å–çš„ã€‚æ¯ä¸ªPartitionéƒ½ä¼šåœ¨é›†ä¸­çš„å…¶ä¸­ä¸€å°æœåŠ¡å™¨å­˜åœ¨Leaderã€‚ä¸€ä¸ªTopicå¦‚æœæœ‰å¤šä¸ªPartitionï¼Œåˆ™ä¼šæœ‰å¤šä¸ªLeaderã€‚ ReplicationFactorä¸€ä¸ªPartitionä¸­å¤åˆ¶æ•°æ®çš„æ‰€æœ‰èŠ‚ç‚¹,åŒ…æ‹¬å·²ç»æŒ‚äº†çš„;æ•°é‡ä¸ä¼šè¶…è¿‡é›†ç¾¤ä¸­brokerçš„æ•°é‡ isrReplicationFactorçš„å­é›†,å­˜æ´»çš„ä¸”å’ŒLeaderä¿æŒåŒæ­¥çš„èŠ‚ç‚¹;leadeä¼šç»´æŠ¤ä¸€ä¸ªä¸å…¶åŸºæœ¬ä¿æŒåŒæ­¥çš„Replicaåˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨æˆä¸ºISRï¼ˆin-sync Replicaï¼‰.å¦‚æœä¸€ä¸ªFolloweræ¯”leaderè½åå¤ªå¤šï¼Œæˆ–è€…è¶…è¿‡ä¸€æ®µæ—¶é—´æœªå‘èµ·æ•°æ®å¤åˆ¶è¯·æ±‚ï¼ˆkafkaçš„æ•°æ®å¤åˆ¶æ˜¯followerçš„pullå½¢å¼ï¼‰ï¼Œåˆ™leaderå°†å…¶ä»ISRä¸­ç§»é™¤ balabala..é‚£ä¹ˆå¤šï¼Œæ¥ä¸ªå®ä¾‹ç©ç©å§ æ‰“å¼€å¦å¤–ä¸€ä¸ªç»ˆç«¯ï¼Œè¿›å…¥kafka1å¯åŠ¨ä¸ªæ¶ˆè´¹è€…ï¼Œæ­¤å¤„è¦æ³¨æ„æ¶ˆè´¹è€…è·¯å¾„ä¹Ÿè¦æŒ‡å®škafka1[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginning 1[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-console-producer.sh --broker-list kafka0:9092 --topic topic3 è¾“å…¥hello world å¦å¤–ä¸€ä¸ªç»ˆç«¯æ˜¾ç¤º12[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginninghello world å¦‚æœè®¾ç½®topicçš„æ—¶å€™è®¾ç½®log.cleanup.policyä¸ºcompactï¼Œåˆ™æ¶ˆè´¹çš„æ¶ˆæ¯ä¼šè¢«å‹ç¼©]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ç”¨gitä¸Šä¼ ä»£ç åˆ°ç äº‘]]></title>
    <url>%2F2016%2F12%2F13%2F%E7%A0%81%E4%BA%91%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[ç”¨gitä¸Šä¼ ä»£ç åˆ°ç äº‘ ä¸‹è½½gitå®¢æˆ·ç«¯Window ä¸‹çš„å®‰è£…ä» http://git-scm.com/download ä¸Šä¸‹è½½windowç‰ˆçš„å®¢æˆ·ç«¯ï¼Œç„¶åä¸€ç›´ä¸‹ä¸€æ­¥ä¸‹ä¸€æ­¥å®‰è£…gitå³å¯ï¼Œè¯·æ³¨æ„ï¼Œå¦‚æœä½ ä¸ç†Ÿæ‚‰æ¯ä¸ªé€‰é¡¹çš„æ„æ€ï¼Œè¯·ä¿æŒé»˜è®¤çš„é€‰é¡¹ git config â€“global user.name Jenickgit config â€“global user.email 258409707@qq.com ä»¥ä¸‹å†…å®¹Wieè½¬è½½ï¼š1.å…ˆåœ¨è¿œç¨‹åº“åˆ›å»ºé¡¹ç›®ï¼Œç„¶åå¤åˆ¶è¿œç¨‹åº“é‡Œé¢çš„é¡¹ç›®çš„åœ°å€ï¼› 2.æ‰“å¼€å‘½ä»¤è¡Œï¼ˆç”µè„‘ä¸Šç”¨windows+Ré”®ï¼‰ï¼Œåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ç”¨äºä»è¿œç¨‹åº“é‡Œå…‹ éš†æ‰€è¦æ›´æ”¹çš„æ–‡ä»¶ï¼šå‘½ä»¤ä¸ºï¼šmkdir + â€œç›®å½•åâ€ï¼›3.ç”¨cd + â€œç›®å½•åâ€è·³è½¬åˆ°æ‰€åˆ›å»ºçš„ç›®å½•åä¸‹ï¼Œç”¨å‘½ä»¤git clone +è¿œç¨‹åº“ æ–‡ä»¶åœ°å€ï¼›æ­¤æ—¶åˆ·æ–°æ‰€åˆ›å»ºçš„ç›®å½•ï¼Œä¼šå‘ç°æ–°å‡ºç°äº†ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œé‚£å°±æ˜¯ ä»è¿œç¨‹åº“é‡Œé¢å¤åˆ¶çš„æ–‡ä»¶ç›®å½•ï¼ˆé‡Œé¢åŒ…å«æœ‰åç¼€ä¸º.gitçš„æ–‡ä»¶ï¼‰ 4.ç”¨cd å‘½ä»¤è·³è½¬åˆ°æ–°çš„æ–‡ä»¶å¤¹é‡Œï¼ŒæŠŠæ‰€éœ€è¦æäº¤çš„æ–‡ä»¶å¤åˆ¶åˆ°è¿™ä¸ªæ–‡ä»¶å¤¹ é‡Œï¼› 5.ç”¨git statuså‘½ä»¤èƒ½çœ‹åˆ°æ‰€æ”¹å˜äº†çš„æ–‡ä»¶ç›®å½•åˆ—è¡¨ï¼Œç”¨git diffå‘½ä»¤èƒ½çœ‹ åˆ°å…·ä½“æ”¹å˜äº†çš„å“ªä¸€è¡Œä»£ç ï¼› 6.ç”¨å‘½ä»¤git branch + â€œåˆ†æ”¯åâ€åˆ›å»ºæœ¬åœ°åˆ†æ”¯ï¼Œç”¨å‘½ä»¤git branchå¯æŸ¥çœ‹ ç›®å‰æ‰€åœ¨åˆ†æ”¯ï¼Œç”¨ git checkout â€œä½ æ‰€åˆ›å»ºçš„åˆ†æ”¯åâ€ï¼›è·³è½¬åˆ°ä½ æ‰€åˆ›å»ºçš„åˆ† æ”¯ï¼› 7.ç”¨git add+(ç©ºæ ¼)+ â€œ.â€å°†æ‰€æœ‰çš„ä¿®æ”¹è¿½åŠ åˆ°æ–‡ä»¶ä¸­å»ï¼› 8.ç”¨git commit -mâ€œæ›´æ”¹çš„æ–‡ä»¶å¤‡æ³¨â€å‘½ä»¤å°†ä½ çš„æ–‡ä»¶æäº¤åˆ°æœ¬åœ°åº“ï¼› 9.ç”¨git checkout master åˆ‡æ¢å›åˆ°ä¸»åˆ†æ”¯ï¼› 10.ç”¨git pull origin master å‘½ä»¤å°†ä¸»åˆ†æ”¯ä»è¿œç¨‹ä»“åº“é‡Œé¢æ‹‰è¿‡æ¥ï¼› 11.ç”¨git merge+â€ä½ æ‰€åˆ›å»ºçš„åˆ†æ”¯åâ€åˆå¹¶åˆ†æ”¯ï¼› 12.ç”¨git push origin master å‘½ä»¤å°†åˆå¹¶åˆ†æ”¯æäº¤åˆ°è¿œç¨‹åº“ï¼› 13.åˆ·æ–°ä¸€ä¸‹è¿œç¨‹åº“ï¼Œå°±èƒ½å‘ç°æ‰€è¦æäº¤çš„æ–‡ä»¶äº† æ–‡ï¼ç„è–›çƒ¨ï¼ˆç®€ä¹¦ä½œè€…ï¼‰åŸæ–‡é“¾æ¥ï¼šhttp://www.jianshu.com/p/edf037f921c7è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ï¼Œè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œå¹¶æ ‡æ³¨â€œç®€ä¹¦ä½œè€…â€ã€‚]]></content>
      <categories>
        <category>å…¶ä»–</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka stream å®æˆ˜3]]></title>
    <url>%2F2016%2F12%2F07%2Fkafka%20stream%20%E5%AE%9E%E6%88%983%2F</url>
    <content type="text"><![CDATA[åœºæ™¯ï¼š æ¯5ç§’è¾“å‡ºè¿‡å»1å°æ—¶18å²åˆ°35å²ç”¨æˆ·æ‰€è´­ä¹°çš„å•†å“ä¸­ï¼Œæ¯ç§å“ç±»é”€å”®é¢æ’åå‰åçš„è®¢å•æ±‡æ€»ä¿¡æ¯ã€‚ ä½¿ç”¨æ•°æ®å†…çš„æ—¶é—´(Event Time)ä½œä¸ºtimestamp æ¯5ç§’è¾“å‡ºä¸€æ¬¡ æ¯æ¬¡è®¡ç®—åˆ°è¾“å‡ºä¸ºæ­¢è¿‡å»1å°æ—¶çš„æ•°æ® æ”¯æŒè®¢å•è¯¦æƒ…å’Œç”¨æˆ·è¯¦æƒ…çš„æ›´æ–°å’Œå¢åŠ  è¾“å‡ºå­—æ®µåŒ…å«æ—¶é—´çª—å£ï¼ˆèµ·å§‹æ—¶é—´ï¼Œç»“æŸæ—¶é—´ï¼‰ï¼Œå“ç±»ï¼ˆcategoryï¼‰ï¼Œå•†å“åï¼ˆitem_nameï¼‰ï¼Œé”€é‡ï¼ˆquantityï¼‰ï¼Œå•ä»·ï¼ˆpriceï¼‰ï¼Œæ€»é”€å”®é¢ï¼Œè¯¥å•†å“åœ¨è¯¥å“ç±»å†…çš„é”€å”®é¢æ’å éšæœºåˆ›å»ºæ•°æ®çš„ç±»1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.io.*;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;import java.util.Random;/** * Created by Administrator on 2016/12/30. */public class createFile &#123; //cåˆ›å»ºè®¢å•æ–‡ä»¶ public static void main(String[] args) throws IOException, ParseException &#123; FileOutputStream outSTr = null; FileOutputStream out = null; BufferedOutputStream Buff=null; outSTr = new FileOutputStream(new File("C:/add2.csv")); Buff=new BufferedOutputStream(outSTr); String[] a1 = &#123;"Jack", "Lily", "Mike", "Lucy", "LiLei", "HanMeimei"&#125;; Random rand = new Random(); String[] a2 = &#123;"iphone", "ipad", "iwatch", "ipod"&#125;; Calendar calendar = Calendar.getInstance(); String str="2016-11-11 00:00:00"; SimpleDateFormat sdf= new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); Date date =sdf.parse(str); calendar.setTime(date); System.out.println (calendar.getTime ()); SimpleDateFormat sdf2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); String dateStr = ""; int count = 0; while(! (count == 60*60*24))&#123;//86400 System.out.println(count); count++; System.out.println(dateStr); int num = rand.nextInt(a1.length); // System.out.println(a1[num]); int num2 = rand.nextInt(a2.length); // System.out.println(a2[num2]); calendar.add (Calendar.SECOND, 1); sdf2.format(calendar.getTime()); dateStr = sdf2.format(calendar.getTime()); // System.out.println (dateStr); Buff.write((a1[num]+","+a2[num2]+", "+dateStr+","+rand.nextInt(5)+"\r\n").getBytes()); Buff.flush(); &#125; &#125;&#125; å…¶ä»–æ–‡ä»¶åœ¨å®è·µ1é‡Œæœ‰ ç¼–å†™DSLæ–‡ä»¶ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426import java.io.IOException;import java.util.ArrayList;import java.util.Collection;import java.util.Collections;import java.util.Comparator;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import org.apache.kafka.streams.kstream.TimeWindows;import org.apache.kafka.streams.kstream.Windowed;import com.jasongj.kafka.stream.model.Item;import com.jasongj.kafka.stream.model.Order;import com.jasongj.kafka.stream.model.User;import com.jasongj.kafka.stream.serdes.SerdesFactory;import com.jasongj.kafka.stream.timeextractor.OrderTimestampExtractor;/** * Created by root on 17-1-5. */public class topN &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items", "items-state-store"); KStream&lt;Windowed&lt;String&gt;, GroupInfo&gt; kStream = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; (orderUser.userAddress != null &amp;&amp; (orderUser.getAge() &gt;= 18 &amp;&amp; orderUser.getAge() &lt;= 35 ))) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;String, GroupInfo&gt;pair((orderUserItem.itemType), new GroupInfo(orderUserItem.itemType, new ArrayList&lt;GroupItemInfo&gt;()&#123; private static final long serialVersionUID = 1L; &#123; add(new GroupItemInfo(orderUserItem.transactionDate, orderUserItem.itemName, orderUserItem.quantity, orderUserItem.itemPrice, (orderUserItem.quantity * orderUserItem.itemPrice))); &#125;&#125;) )) .groupByKey(Serdes.String(), SerdesFactory.serdFrom(GroupInfo.class)) .reduce((GroupInfo v1, GroupInfo v2) -&gt; &#123; GroupInfo v3 = new GroupInfo(v1.getItemType()); List&lt;GroupItemInfo&gt; newItemlist = new ArrayList&lt;GroupItemInfo&gt;(); newItemlist.addAll(v1.getItemList()); newItemlist.addAll(v2.getItemList()); v3.setItemList(newItemlist); return v3; &#125; , TimeWindows.of(1000 * 60 * 60).advanceBy(1000 * 5) , "gender-amount-state-store").toStream(); kStream.map((Windowed&lt;String&gt; window, GroupInfo groupInfo) -&gt; &#123; return new KeyValue&lt;String, String&gt;(window.key(), groupInfo.printTop10(window.window().start(), window.window().end())); &#125;).to(Serdes.String(), Serdes.String(), "gender-amount"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class GroupInfo&#123; private String itemType; private List&lt;GroupItemInfo&gt; itemList; public GroupInfo()&#123; &#125; public GroupInfo(String itemType)&#123; this.itemType = itemType; &#125; public GroupInfo(String itemType, List&lt;GroupItemInfo&gt; itemList)&#123; this.itemType = itemType; this.itemList = itemList; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public List&lt;GroupItemInfo&gt; getItemList() &#123; return itemList; &#125; public void setItemList(List&lt;GroupItemInfo&gt; itemList) &#123; this.itemList = itemList; &#125; /** * æ ¹æ®é‡‘é¢æ±‡æ€»å€’åº * @param allItems * @return */ private List&lt;GroupItemInfo&gt; sortBySumDesc(Collection&lt;GroupItemInfo&gt; allItems)&#123; List&lt;GroupItemInfo&gt; result = new ArrayList&lt;GroupItemInfo&gt;(); result.addAll(allItems); Collections.sort(result, new Comparator&lt;GroupItemInfo&gt;()&#123; @Override public int compare(GroupItemInfo o1, GroupItemInfo o2) &#123; if(o1.getSum() == o2.getSum())&#123; return 0; &#125;else if(o1.getSum() &gt; o2.getSum())&#123; return -1; &#125;else&#123; return 1; &#125; &#125; &#125;); return result; &#125; /** * æ‰¾å›å‰10å * @param startDate * @param endDate * @return */ public String printTop10(long startDate, long endDate)&#123; double allAmount = 0.0; Map&lt;String, GroupItemInfo&gt; groupMap = new HashMap&lt;String, GroupItemInfo&gt;(); for(GroupItemInfo item : itemList)&#123; String key = item.getItemName(); allAmount += item.getSum(); if(groupMap.containsKey(key))&#123; GroupItemInfo oldItem = groupMap.get(key); oldItem.setCount(oldItem.getCount() + item.getCount()); oldItem.setSum(oldItem.getSum() + item.getSum()); &#125;else&#123; groupMap.put(key, item); &#125; &#125; List&lt;GroupItemInfo&gt; sortedResult = sortBySumDesc(groupMap.values()); StringBuffer sb = new StringBuffer(); for(int i = 1; i &lt;= 10 ; i++)&#123; if(sortedResult.size() &gt;= i &amp;&amp; sortedResult.get(i-1) != null)&#123; GroupItemInfo oneItem = sortedResult.get(i-1); sb.append(startDate).append(",").append(endDate).append(",").append(itemType).append(",").append(oneItem.getItemName()).append(",") .append(oneItem.getCount()).append(",").append(oneItem.getPrice()).append(",").append(oneItem.getSum()).append(",").append(allAmount) .append(",").append(i).append("\n"); &#125;else&#123; break; &#125; &#125; return sb.toString(); &#125; &#125; private static class GroupItemInfo&#123; private long transactionDate; private String itemName; private int count; private double price; private double sum; public GroupItemInfo()&#123; &#125; public GroupItemInfo(long transactionDate, String itemName, int count, double price, double sum) &#123; this.transactionDate = transactionDate; this.itemName = itemName; this.count = count; this.price = price; this.sum = sum; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; public double getSum() &#123; return sum; &#125; public void setSum(double sum) &#123; this.sum = sum; &#125; &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if(order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if(user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if(orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if(item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125;&#125;]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka stream å®æˆ˜2]]></title>
    <url>%2F2016%2F12%2F06%2Fkafka%20stream%20%E5%AE%9E%E6%88%982%2F</url>
    <content type="text"><![CDATA[åœºæ™¯ï¼š åœ¨ä¸Šä¸€ä¸ªç¤ºä¾‹ï¼ˆç®—å‡ºç”¨æˆ·ä¸å•†å“åŒåœ°å€çš„è®¢å•ä¸­ï¼Œç”·å¥³åˆ†åˆ«æ€»å…±èŠ±äº†å¤šå°‘é’±ï¼‰çš„åŸºç¡€ä¸Šï¼Œç®—å‡ºä¸åŒåœ°åŒºï¼ˆç”¨æˆ·åœ°å€ï¼‰ï¼Œä¸åŒæ€§åˆ«çš„è®¢å•æ•°åŠå•†å“æ€»æ•°å’Œæ€»é‡‘é¢ã€‚è¾“å‡ºç»“æœschemaå¦‚ä¸‹åœ°åŒºï¼ˆç”¨æˆ·åœ°åŒºï¼Œå¦‚SHï¼‰ï¼Œæ€§åˆ«ï¼Œè®¢å•æ€»æ•°ï¼Œå•†å“æ€»æ•°ï¼Œæ€»é‡‘é¢ ç¤ºä¾‹è¾“å‡º SH, male, 3, 4, 188888.88 BJ, femail, 5, 8, 288888.88 æ¨æ¼”è¿‡ç¨‹ï¼š å®ç°ä»£ç 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741package com.jasongj.kafka.stream;import java.io.IOException;import java.util.Properties;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import com.jasongj.kafka.stream.model.Item;import com.jasongj.kafka.stream.model.Order;import com.jasongj.kafka.stream.model.User;import com.jasongj.kafka.stream.serdes.SerdesFactory;import com.jasongj.kafka.stream.timeextractor.OrderTimestampExtractor;/** * Created by root on 17-1-5. */public class Di9KeZuoYe &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items", "items-state-store"); KTable&lt;AddrSex, OrderMoney&gt; kTable = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != null) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .filter((String item, OrderUserItem orderUserItem) -&gt; StringUtils.compare(orderUserItem.userAddress, orderUserItem.itemAddress) == 0) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;AddrSex, OrderMoney&gt;pair(new AddrSex(orderUserItem.userAddress, orderUserItem.gender), OrderMoney.fromItem(orderUserItem.quantity, orderUserItem.itemPrice))) .groupByKey(SerdesFactory.serdFrom(AddrSex.class), SerdesFactory.serdFrom(OrderMoney.class)) .reduce((OrderMoney v1, OrderMoney v2) -&gt; new OrderMoney(v1.orderNum + v2.orderNum, v1.itemNum + v2.itemNum, v1.TotalMoney + v2.TotalMoney), "gender-amount-state-store"); kTable .toStream() .map((AddrSex addrSex, OrderMoney orderMoney) -&gt; new KeyValue&lt;String, String&gt;(addrSex.toString(), orderMoney.toString())) .to("gender-amount"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if (order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if (user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if (orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if (item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125; public static class AddrSex &#123; private String addr; private String gender; public AddrSex() &#123; &#125; public AddrSex(String addr, String gender) &#123; this.addr = addr; this.gender = gender; &#125; public String getAddr() &#123; return addr; &#125; public void setAddr(String addr) &#123; this.addr = addr; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; AddrSex addrSex = (AddrSex) o; if (!addr.equals(addrSex.addr)) return false; return gender.equals(addrSex.gender); &#125; @Override public int hashCode() &#123; int result = addr.hashCode(); result = 31 * result + gender.hashCode(); return result; &#125; @Override public String toString() &#123; return addr + " " + gender; &#125; &#125; public static class OrderMoney &#123; private int orderNum; private int itemNum; private Double TotalMoney; public OrderMoney() &#123; &#125; public OrderMoney(int orderNum, int itemNum, Double totalMoney) &#123; this.orderNum = orderNum; this.itemNum = itemNum; TotalMoney = totalMoney; &#125; public int getOrderNum() &#123; return orderNum; &#125; public void setOrderNum(int orderNum) &#123; this.orderNum = orderNum; &#125; public int getItemNum() &#123; return itemNum; &#125; public void setItemNum(int itemNum) &#123; this.itemNum = itemNum; &#125; public Double getTotalMoney() &#123; return TotalMoney; &#125; public void setTotalMoney(Double totalMoney) &#123; TotalMoney = totalMoney; &#125; public static OrderMoney fromItem(int quantity, Double itemPrice) &#123; OrderMoney orderMoney = new OrderMoney(); orderMoney.setOrderNum(1); orderMoney.setItemNum(quantity); orderMoney.setTotalMoney((double) quantity * itemPrice); return orderMoney; &#125; @Override public String toString() &#123; return orderNum + " " + itemNum + " " + TotalMoney; &#125; &#125;&#125; è¾“å‡ºç»“æœ]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka stream å®æˆ˜]]></title>
    <url>%2F2016%2F12%2F05%2Fkafka%20stream%20%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[åœºæ™¯ï¼šç›®å‰æœ‰ä¸‰ä¸ªè¡¨ï¼šäº§å“è¡¨ï¼Œç”¨æˆ·è¡¨ï¼Œè®¢å•è¡¨ã€‚æ±‚æ€§åˆ«å¹³å‡æ¶ˆè´¹é‡‘é¢ æ•°æ®ï¼šäº§å“è¡¨1234iphone, BJ, phone, 5388.88ipad, SH, pad, 4888.88iwatch, SZ, watch, 2668.88ipod, GZ, pod, 1888.88 ç”¨æˆ·è¡¨1234Jack, BJ, male, 23Lily, SH, female, 21Mike, SZ, male, 22Lucy, GZ, female, 20 è®¢å•è¡¨12345678910111213141516171819Jack, iphone, 2016-11-11 00:00:01, 3Jack, ipad, 2016-11-11 00:00:02, 4Jack, iwatch, 2016-11-11 00:00:03, 5Jack, ipod, 2016-11-11 00:00:04, 4Lily, ipad, 2016-11-11 00:00:06, 3Lily, iwatch, 2016-11-11 00:00:07, 4Lily, iphone, 2016-11-11 00:00:08, 2Lily, ipod, 2016-11-11 00:00:09, 3Mike, ipad, 2016-11-11 00:00:11, 2Mike, iwatch, 2016-11-11 00:00:12, 3Mike, iphone, 2016-11-11 00:00:13, 4Mike, ipod, 2016-11-11 00:00:14, 3Lucy, ipod, 2016-11-11 00:00:16, 3Lucy, ipad, 2016-11-11 00:00:17, 4Lucy, iwatch, 2016-11-11 00:00:18, 3Lucy, iphone, 2016-11-11 00:00:19, 5 åˆ›å»ºæ‰“å°ç±»123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.Arrays;import java.util.Properties;import java.util.concurrent.atomic.AtomicLong;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.DoubleDeserializer;import org.apache.kafka.common.serialization.StringDeserializer;public class DemoConsumerManualCommit &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; "jenick.com:9092", "gender-amount2", "group4", "consumer2" &#125;; if (args == null || args.length != 4) &#123; System.err.println( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"); System.exit(1); &#125; String bootstrap = args[0]; String topic = args[1]; String groupid = args[2]; String clientid = args[3]; Properties props = new Properties(); props.put("bootstrap.servers", bootstrap); props.put("group.id", groupid); props.put("enable.auto.commit", "false"); props.put("key.deserializer", StringDeserializer.class.getName()); props.put("value.deserializer", DoubleDeserializer.class.getName()); props.put("max.poll.interval.ms", "300000"); props.put("max.poll.records", "500"); props.put("auto.offset.reset", "earliest"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(topic)); AtomicLong atomicLong = new AtomicLong(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); records.forEach(record -&gt; &#123; System.out.printf("client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); if (atomicLong.get() % 10 == 0) &#123;// consumer.commitSync(); &#125; &#125;); &#125; &#125;&#125; TimestampExtractor12345678910111213141516171819202122232425262728293031323334import java.time.LocalDateTime;import java.time.ZoneOffset;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.streams.processor.TimestampExtractor;import com.fasterxml.jackson.databind.JsonNode;import com.kafka.stream.model.Item;import com.kafka.stream.model.Order;import com.kafka.stream.model.User;public class OrderTimestampExtractor implements TimestampExtractor &#123; @Override public long extract(ConsumerRecord&lt;Object, Object&gt; record) &#123; Object value = record.value(); if (record.value() instanceof Order) &#123; Order order = (Order) value; return order.getTransactionDate(); &#125; if (value instanceof JsonNode) &#123; return ((JsonNode) record.value()).get("transactionDate").longValue(); &#125; if (value instanceof Item) &#123; return LocalDateTime.of(2015, 12,11,1,0,10).toEpochSecond(ZoneOffset.UTC) * 1000; &#125; if (value instanceof User) &#123; return LocalDateTime.of(2015, 12,11,0,0,10).toEpochSecond(ZoneOffset.UTC) * 1000; &#125; return LocalDateTime.of(2015, 11,10,0,0,10).toEpochSecond(ZoneOffset.UTC) * 1000;// throw new IllegalArgumentException("OrderTimestampExtractor cannot recognize the record value " + record.value()); &#125;&#125; åˆ›å»ºstreamå¤„ç†ç±»123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271import java.io.IOException;import java.util.Properties;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.KeyValue;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.kstream.KStream;import org.apache.kafka.streams.kstream.KStreamBuilder;import org.apache.kafka.streams.kstream.KTable;import com.kafka.stream.model.Item;import com.kafka.stream.model.Order;import com.kafka.stream.model.User;import com.kafka.stream.serdes.SerdesFactory;import com.kafka.stream.timeextractor.OrderTimestampExtractor;public class PurchaseAnalysis &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-purchase-analysis2"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "jenick.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "jenick.com:2181"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, OrderTimestampExtractor.class); KStreamBuilder streamBuilder = new KStreamBuilder(); KStream&lt;String, Order&gt; orderStream = streamBuilder.stream(Serdes.String(), SerdesFactory.serdFrom(Order.class), "orders"); KTable&lt;String, User&gt; userTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(User.class), "users", "users-state-store"); KTable&lt;String, Item&gt; itemTable = streamBuilder.table(Serdes.String(), SerdesFactory.serdFrom(Item.class), "items2", "items-state-store");// itemTable.toStream().foreach((String itemName, Item item) -&gt; System.out.printf("Item info %s-%s-%s-%s\n", item.getItemName(), item.getAddress(), item.getType(), item.getPrice())); KTable&lt;String, Double&gt; kTable = orderStream .leftJoin(userTable, (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), Serdes.String(), SerdesFactory.serdFrom(Order.class)) .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != null) .map((String userName, OrderUser orderUser) -&gt; new KeyValue&lt;String, OrderUser&gt;(orderUser.itemName, orderUser)) .through(Serdes.String(), SerdesFactory.serdFrom(OrderUser.class), (String key, OrderUser orderUser, int numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; 0x7FFFFFFF) % numPartitions, "orderuser-repartition-by-item2") .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class)) .filter((String item, OrderUserItem orderUserItem) -&gt; StringUtils.compare(orderUserItem.userAddress, orderUserItem.itemAddress) == 0)// .foreach((String itemName, OrderUserItem orderUserItem) -&gt; System.out.printf("%s-%s-%s-%s\n", itemName, orderUserItem.itemAddress, orderUserItem.userName, orderUserItem.userAddress)) .map((String item, OrderUserItem orderUserItem) -&gt; KeyValue.&lt;String, Double&gt;pair(orderUserItem.gender, (Double)(orderUserItem.quantity * orderUserItem.itemPrice))) .groupByKey(Serdes.String(), Serdes.Double()) .reduce((Double v1, Double v2) -&gt; v1 + v2, "gender-amount-state-store");// kTable.foreach((str, dou) -&gt; System.out.printf("%s-%s\n", str, dou)); kTable .toStream() .map((String gender, Double total) -&gt; new KeyValue&lt;String, String&gt;(gender, String.valueOf(total))) .to("gender-amount2"); KafkaStreams kafkaStreams = new KafkaStreams(streamBuilder, props); kafkaStreams.cleanUp(); kafkaStreams.start(); System.in.read(); kafkaStreams.close(); kafkaStreams.cleanUp(); &#125; public static class OrderUser &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static OrderUser fromOrder(Order order) &#123; OrderUser orderUser = new OrderUser(); if(order == null) &#123; return orderUser; &#125; orderUser.userName = order.getUserName(); orderUser.itemName = order.getItemName(); orderUser.transactionDate = order.getTransactionDate(); orderUser.quantity = order.getQuantity(); return orderUser; &#125; public static OrderUser fromOrderUser(Order order, User user) &#123; OrderUser orderUser = fromOrder(order); if(user == null) &#123; return orderUser; &#125; orderUser.gender = user.getGender(); orderUser.age = user.getAge(); orderUser.userAddress = user.getAddress(); return orderUser; &#125; &#125; public static class OrderUserItem &#123; private String userName; private String itemName; private long transactionDate; private int quantity; private String userAddress; private String gender; private int age; private String itemAddress; private String itemType; private double itemPrice; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; public String getUserAddress() &#123; return userAddress; &#125; public void setUserAddress(String userAddress) &#123; this.userAddress = userAddress; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getItemAddress() &#123; return itemAddress; &#125; public void setItemAddress(String itemAddress) &#123; this.itemAddress = itemAddress; &#125; public String getItemType() &#123; return itemType; &#125; public void setItemType(String itemType) &#123; this.itemType = itemType; &#125; public double getItemPrice() &#123; return itemPrice; &#125; public void setItemPrice(double itemPrice) &#123; this.itemPrice = itemPrice; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser) &#123; OrderUserItem orderUserItem = new OrderUserItem(); if(orderUser == null) &#123; return orderUserItem; &#125; orderUserItem.userName = orderUser.userName; orderUserItem.itemName = orderUser.itemName; orderUserItem.transactionDate = orderUser.transactionDate; orderUserItem.quantity = orderUser.quantity; orderUserItem.userAddress = orderUser.userAddress; orderUserItem.gender = orderUser.gender; orderUserItem.age = orderUser.age; return orderUserItem; &#125; public static OrderUserItem fromOrderUser(OrderUser orderUser, Item item) &#123; OrderUserItem orderUserItem = fromOrderUser(orderUser); if(item == null) &#123; return orderUserItem; &#125; orderUserItem.itemAddress = item.getAddress(); orderUserItem.itemType = item.getType(); orderUserItem.itemPrice = item.getPrice(); return orderUserItem; &#125; &#125;&#125; Hashåˆ†åŒº123456789101112131415161718192021222324252627282930313233343536import java.util.List;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import org.apache.kafka.common.PartitionInfo;public class HashPartitioner implements Partitioner &#123; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes != null) &#123; int hashCode = 0; if (key instanceof Integer || key instanceof Long) &#123; hashCode = (int) key; &#125; else &#123; hashCode = key.hashCode(); &#125; hashCode = hashCode &amp; 0x7fffffff; return hashCode % numPartitions; &#125; else &#123; return 0; &#125; &#125; @Override public void close() &#123; &#125;&#125; åºåˆ—åŒ–1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.io.IOException;import java.util.Map;import org.apache.kafka.common.errors.SerializationException;import org.apache.kafka.common.serialization.Deserializer;import com.fasterxml.jackson.databind.ObjectMapper;public class GenericDeserializer&lt;T&gt; implements Deserializer&lt;T&gt; &#123; private Class&lt;T&gt; type; private ObjectMapper objectMapper = new ObjectMapper(); public GenericDeserializer() &#123;&#125; public GenericDeserializer(Class&lt;T&gt; type) &#123; this.type = type; &#125; @SuppressWarnings("unchecked") @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; if(type != null) &#123; return; &#125; String typeProp = isKey ? "key.deserializer.type" : "value.deserializer.type"; String typeName = (String)configs.get(typeProp); try &#123; type = (Class&lt;T&gt;)Class.forName(typeName); &#125; catch (Exception ex) &#123; throw new SerializationException("Failed to initialize GenericDeserializer for " + typeName, ex); &#125; &#125; @Override public T deserialize(String topic, byte[] data) &#123; if (data == null) &#123; return null; &#125; try &#123; return this.objectMapper.readValue(data, type); &#125; catch (IOException ex) &#123; throw new SerializationException(ex); &#125; &#125; @Override public void close() &#123; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.io.IOException;import java.util.Map;import org.apache.kafka.common.errors.SerializationException;import org.apache.kafka.common.serialization.Serde;import org.apache.kafka.common.serialization.Serializer;import com.fasterxml.jackson.databind.ObjectMapper;import com.kafka.stream.model.User;public class GenericSerializer&lt;T&gt; implements Serializer&lt;T&gt; &#123; private Class&lt;T&gt; type; private ObjectMapper objectMapper = new ObjectMapper(); public GenericSerializer() &#123;&#125; public GenericSerializer(Class&lt;T&gt; type) &#123; this.type = type; &#125; @SuppressWarnings("unchecked") @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; if(type != null) &#123; return; &#125; String typeProp = isKey ? "key.serializer.type" : "value.serializer.type"; String typeName = (String)configs.get(typeProp); try &#123; type = (Class&lt;T&gt;)Class.forName(typeName); &#125; catch (Exception ex) &#123; throw new SerializationException("Failed to initialize GenericSerializer for " + typeName, ex); &#125; &#125; @Override public byte[] serialize(String topic, T object) &#123; if (object == null) &#123; return null; &#125; try &#123; return this.objectMapper.writerFor(type).writeValueAsBytes(object); &#125; catch (IOException ex) &#123; throw new SerializationException(ex); &#125; &#125; @Override public void close() &#123; &#125;&#125; 1234567891011121314151617import org.apache.kafka.common.serialization.Serde;import org.apache.kafka.common.serialization.Serdes;public class SerdesFactory &#123; /** * @param &lt;T&gt; The class should have a constructor without any * arguments and have setter and getter for every member variable * @param pojoClass POJO class. * @return Instance of &#123;@link Serde&#125; */ public static &lt;T&gt; Serde&lt;T&gt; serdFrom(Class&lt;T&gt; pojoClass) &#123; return Serdes.serdeFrom(new GenericSerializer&lt;T&gt;(pojoClass), new GenericDeserializer&lt;T&gt;(pojoClass)); &#125;&#125; beanå¯¹è±¡ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Item &#123; private String itemName; private String address; private String type; private double price; public Item() &#123;&#125; public Item(String itemName, String address, String type, double price) &#123; this.itemName = itemName; this.address = address; this.type = type; this.price = price; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getType() &#123; return type; &#125; public void setType(String type) &#123; this.type = type; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Order &#123; private String userName; private String itemName; private long transactionDate; private int quantity; public Order() &#123;&#125; public Order(String userName, String itemName, long transactionDate, int quantity) &#123; this.userName = userName; this.itemName = itemName; this.transactionDate = transactionDate; this.quantity = quantity; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getItemName() &#123; return itemName; &#125; public void setItemName(String itemName) &#123; this.itemName = itemName; &#125; public long getTransactionDate() &#123; return transactionDate; &#125; public void setTransactionDate(long transactionDate) &#123; this.transactionDate = transactionDate; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class User &#123; private String name; private String address; private String gender; private int age; public User() &#123;&#125; public User(String name, String address, String gender, int age) &#123; this.name = name; this.address = address; this.gender = gender; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getGender() &#123; return gender; &#125; public void setGender(String gender) &#123; this.gender = gender; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; ç”¨æˆ·ç”Ÿäº§è€…1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.IOException;import java.nio.charset.Charset;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.User;import com.kafka.stream.serdes.GenericSerializer;public class UserProducer &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", User.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, User&gt; producer = new KafkaProducer&lt;String, User&gt;(props); List&lt;User&gt; users = readUser(); users.forEach((User user) -&gt; producer.send(new ProducerRecord&lt;String, User&gt;("users", user.getName(), user))); producer.close(); &#125; public static List&lt;User&gt; readUser() throws IOException &#123; List&lt;String&gt; lines = IOUtils.readLines(OrderProducer.class.getResourceAsStream("/users.csv"), Charset.forName("UTF-8")); List&lt;User&gt; users = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new User(values[0], values[1], values[2], Integer.parseInt(values[3]))) .collect(Collectors.toList()); return users; &#125;&#125; äº§å“ç”Ÿäº§è€…1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.IOException;import java.nio.charset.Charset;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.Item;import com.kafka.stream.serdes.GenericSerializer;public class ItemProducer &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", Item.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, Item&gt; producer = new KafkaProducer&lt;String, Item&gt;(props); List&lt;Item&gt; items = readItem(); items.forEach((Item item) -&gt; producer.send(new ProducerRecord&lt;String, Item&gt;("items2", item.getItemName(), item))); producer.close(); &#125; public static List&lt;Item&gt; readItem() throws IOException &#123; List&lt;String&gt; lines = IOUtils.readLines(OrderProducer.class.getResourceAsStream("/items.csv"), Charset.forName("UTF-8")); List&lt;Item&gt; items = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new Item(values[0], values[1], values[2], Double.parseDouble(values[3]))) .collect(Collectors.toList()); return items; &#125;&#125; è®¢å•ç”Ÿäº§è€…123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.io.IOException;import java.io.InputStream;import java.nio.charset.Charset;import java.time.LocalDateTime;import java.time.ZoneOffset;import java.time.format.DateTimeFormatter;import java.util.List;import java.util.Properties;import java.util.stream.Collectors;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.serialization.StringSerializer;import com.kafka.producer.HashPartitioner;import com.kafka.stream.model.Order;import com.kafka.stream.serdes.GenericSerializer;public class OrderProducer &#123; private static DateTimeFormatter dataTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put("bootstrap.servers", "jenick.com:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", StringSerializer.class.getName()); props.put("value.serializer", GenericSerializer.class.getName()); props.put("value.serializer.type", Order.class.getName()); props.put("partitioner.class", HashPartitioner.class.getName()); Producer&lt;String, Order&gt; producer = new KafkaProducer&lt;String, Order&gt;(props); List&lt;Order&gt; orders = readOrder(); orders.forEach((Order order) -&gt; producer.send(new ProducerRecord&lt;String, Order&gt;("orders", order.getUserName(), order))); producer.close(); &#125; public static List&lt;Order&gt; readOrder() throws IOException &#123; InputStream inputStream = OrderProducer.class.getResourceAsStream("/orders.csv"); List&lt;String&gt; lines = IOUtils.readLines(inputStream, Charset.forName("UTF-8")); List&lt;Order&gt; orders = lines.stream() .filter(StringUtils::isNoneBlank) .map((String line) -&gt; line.split("\\s*,\\s*")) .filter((String[] values) -&gt; values.length == 4) .map((String[] values) -&gt; new Order(values[0], values[1], LocalDateTime.parse(values[2], dataTimeFormatter).toEpochSecond(ZoneOffset.UTC) * 1000, Integer.parseInt(values[3]))) .collect(Collectors.toList()); return orders; &#125;&#125; éœ€è¦åˆ›å»ºtopicå¦‚ä¸‹ï¼š12345/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic orders/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic items/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic users/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic gender-amount/root/kafka/kafka1/kafka_2.11-0.10.1.1/bin/kafka-topics.sh --create --zookeeper jenick.com:2181 --replication-factor 1 --partitions 1 --topic orderuser-repartition-by-item é¦–å…ˆå¯åŠ¨DemoConsumerManualCommitï¼Œç„¶åå¯åŠ¨PurchaseAnalysiså…ˆæŠŠtableçš„æ•°æ®åˆ›å»ºè¿›å»UserProducerã€ItemProduceræœ€åå¯åŠ¨streamçš„OrderProducer è¿è¡Œç»“æœï¼š 12client : consumer2 , topic: gender-amount2 , partition: 0 , offset = 0, key = male, value = 7.489721245848924E-67client : consumer2 , topic: gender-amount2 , partition: 2 , offset = 0, key = female, value = 6.008913963681483E-67]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[codis ha]]></title>
    <url>%2F2016%2F12%2F03%2Fcodis%20ha%2F</url>
    <content type="text"><![CDATA[å¯åŠ¨å¤šä¸ªå®ä¾‹ï¼šcd /usr/local/codis/src/github.com/CodisLabs/codis[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf â€“protected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf â€“protected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6381.conf â€“protected-mode no[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6382.conf â€“protected-mode no å¯åŠ¨zookeeper[root@zhm1 codis]# zkServer.sh start å¯åŠ¨dashboard[root@zhm1 codis]# zkServer.sh startnohup bin/codis-dashboard â€“ncpu=2 â€“config=dashboard.conf â€“log=dashboard.log â€“log-level=WARN &amp; å¯åŠ¨codis-proxy[root@zhm1 codis]# nohup bin/codis-proxy â€“ncpu=2 â€“config=proxy.conf â€“log =proxy.log â€“log-level=WARN &amp; å¯åŠ¨codis-fe[root@zhm1 codis]# ./bin/codis-fe â€“ncpu=2 â€“log=fe.log â€“log-level=WARN â€“dashboard-list=conf/codis.json â€“listen=192.168.110.129:18090 &amp; å°†å®ä¾‹æ·»åŠ åˆ°group åˆ†é…slot å¯åŠ¨codis-hanohup ./bin/codis-ha â€“log=ha.log â€“log-level=WARN â€“dashboard=127.0.0.1:18080 &amp; [root@zhm1 codis]# ps -ef | grep codisroot 3096 2884 0 09:51 pts/0 00:00:32 bin/codis-dashboard â€“ncpu=2 â€“config=dashboard.conf â€“log=dashboard.log â€“log-level=WARNroot 3114 2884 0 09:52 pts/0 00:00:04 bin/codis-proxy â€“ncpu=2 â€“config=proxy.conf â€“log =proxy.log â€“log-level=WARNroot 3149 2884 0 09:53 pts/0 00:00:02 ./bin/codis-fe â€“ncpu=2 â€“log=fe.log â€“log-level=WARN â€“dashboard-list=conf/codis.json â€“listen=192.168.110.129:18090root 3175 1 0 09:54 ? 00:00:08 ./bin/codis-server :6379root 3179 1 0 09:54 ? 00:00:08 ./bin/codis-server :6380root 3888 1 0 10:46 ? 00:00:02 ./bin/codis-server :6382root 3892 1 0 10:46 ? 00:00:02 ./bin/codis-server :6381root 4680 2884 0 11:12 pts/0 00:00:00 ./bin/codis-ha â€“log=ha.log â€“log-level=WARN â€“dashboard=127.0.0.1:18080root 4712 2884 0 11:15 pts/0 00:00:00 grep â€“color=auto codis kill6379 çš„ç«¯å£kill -9 3175æŸ¥çœ‹6380 çš„æ—¥å¿—3179:M 23 Dec 11:19:07.739 * MASTER MODE enabled (user request from â€˜id=7 addr=192.168.110.129:51438 fd=8 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveofâ€™)çœ‹åˆ°6380æˆä¸ºä¸» å†å¯åŠ¨6379 å¹¶æ²¡æœ‰æˆä¸º6380çš„ä¸»ï¼Œéœ€è¦æ‰‹åŠ¨ç‚¹å‡»webä¸Šç»¿è‰²çš„å·¥å…·æ‰³æ‰‹æ€€ç–‘å¯èƒ½æ˜¯æˆ‘çš„é…ç½®æ–‡ä»¶æ²¡æœ‰å†™slave ofçš„åŸå›  å†6380è¾“å…¥å†…å®¹[root@zhm1 codis]# ./bin/redis-cli -p 6380127.0.0.1:6380&gt; set aaa 123OK127.0.0.1:6380&gt; å¯åŠ¨6379çœ‹[root@zhm1 codis]# ./bin/redis-cli -p 6379127.0.0.1:6379&gt; get aaaâ€œ123â€ æ€»ç»“codiså’Œredisé›†ç¾¤çš„åŒºåˆ« a. Redis Cluster çš„é›†ç¾¤ä¿¡æ¯å­˜å‚¨åœ¨æ¯ä¸ªé›†ç¾¤èŠ‚ç‚¹ä¸Šï¼Œè€ŒCodis é›†ç¾¤çš„ä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ªç‹¬ç«‹çš„å­˜å‚¨ç³»ç»Ÿï¼ˆZookeeperï¼‰é‡Œã€‚b. å¤–éƒ¨è®¿é—®é›†ç¾¤ï¼šå¯¹Redis Clusterï¼Œç›´æ¥é€šè¿‡ä»¥é›†ç¾¤æ¨¡å¼å¯åŠ¨çš„redis å®¢æˆ·ç«¯ â€œredis-cli -câ€æ¥è®¿é—®ã€‚å¯¹ Codis é›†ç¾¤é€šè¿‡ç‹¬ç«‹çš„ codis-proxy èŠ‚ç‚¹æ¥è®¿é—®ã€‚c. Redis Cluster æœ‰ 16384 ä¸ªslot å¯ä»¥åˆ†é…ï¼Œè€Œ Codis é›†ç¾¤åªæœ‰1024 ä¸ªslot å¯ä»¥åˆ†é…ã€‚d. ç›‘æ§å’Œæ“ä½œï¼šCodis é›†ç¾¤æœ‰å›¾å½¢åŒ–çš„ Code FE ç®¡ç†å·¥å…·ï¼ˆå¯ä»¥å®ŒæˆCodis Proxyï¼ŒCodis Groupã€Codis Server çš„æ·»åŠ å’Œåˆ é™¤ï¼Œåˆ†é… Slotã€æå‡Slave ä¸ºMaster ç­‰æ“ä½œï¼‰ï¼Œè€Œ Redis Cluster å¥½åƒæ²¡æœ‰è¿™ç±»å·¥å…·ã€‚]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[codis é›†ç¾¤]]></title>
    <url>%2F2016%2F12%2F02%2Fcodis%20%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[å®‰è£…goè¯­è¨€http://www.golangtc.com/download é€‰æ‹©1.7.3ä¸‹è½½1tar -zxvf go1.7.3.linux-amd64.tar.gz -C /usr/local/ ä¸‹è½½jdkå’Œzookeeperå¹¶è§£å‹ é…ç½®zookeeper åˆ›å»ºæ–‡ä»¶å¤¹ zkDataå’ŒzkLog123456cp zoo_sample.cfg zoo.cfgvi zoo.cfg dataDir=/root/zookeeper/zookeeper-3.4.6/zkDatadataLogDir=/usr/bigdata/zookeeper-3.4.6/zkLogserver.1=master:2888:3888 è¿›å…¥zkData1echo 1 &gt; myid â€“ é…ç½®ç¯å¢ƒå˜é‡12345export JAVA_HOME=/root/java/jdk1.8.0_111export GOROOT=/usr/local/goexport GOPATH=/usr/local/codisexport ZOOKEEPER_HOME=/root/zookeeper/zookeeper-3.4.6export PATH=$PATH:$JAVA_HOME/bin:$GOROOT/bin:$GOPATH/bin:$ZOOKEEPER_HOME/bin ä¸‹è½½codishttps://github.com/CodisLabs/codis/releasesé€‰æ‹©ä¸goç‰ˆæœ¬ä¸€æ ·çš„codisç‰ˆæœ¬ 123456mkdir -p $GOPATH/src/github.com/CodisLabstar -xzf codis-3.1.0.tar.gz -C /usr/local/codis/src/github.com/CodisLabs/cd /usr/local/codis/src/github.com/CodisLabs/mv codis-3.1.0/ codiscd codis/make 1make gotest æ‰§è¡ŒæˆåŠŸåè¿›å…¥bin ç›®å½•1cd /usr/local/codis/src/github.com/CodisLabs/codis/bin 123mkdir /etc/codiscd /usr/local/codis/src/github.com/CodisLabs/codis/externcp redis-2.8.21/redis.conf /etc/codis/redis6379.conf åˆ›å»ºredis é…ç½®æ–‡ä»¶ä¸­æ‰€æŒ‡å®šçš„ç›®å½•123456789101112131415161718192021222324[root@zhm1 codis]# cd /opt[root@zhm1 opt]# mkdir -p codisapp/run[root@zhm1 opt]# mkdir -p codisapp/logs[root@zhm1 opt]# mkdir -p codisapp/data/6379 codisapp/data/6380[root@zhm1 opt]# vi /etc/codis/redis6379.confdaemonize yespidfile /opt/codisapp/run/redis6379.pidport 6379logfile "/opt/codisapp/logs/redis6379.log"dbfilename dump.rdbdir /opt/codisapp/data/6379cp /etc/codis/redis6379.conf /etc/codis/redis6380.confvi /etc/codis/redis6380.confdaemonize yespidfile /opt/codisapp/run/redis6380.pidport 6380logfile "/opt/codisapp/logs/redis6380.log"dbfilename dump.rdbdir /opt/codisapp/data/6380 å¯åŠ¨redis123[root@zhm1 opt]# cd /usr/local/codis/src/github.com/CodisLabs/codis/[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf --protected-mode no [root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf --protected-mode no æ­¤å¤„å¯åŠ¨å¢åŠ â€“protected-mode no å‚æ•°å¯ä»¥é˜²æ­¢åé¢æ·»åŠ serverçš„æ—¶å€™æç¤º1Cause": "DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside. å¯åŠ¨dashboardè‡ªå®šä¹‰dashboard é…ç½®æ–‡ä»¶1./bin/codis-dashboard --default-config | tee dashboard.conf ä¿®æ”¹IP1234567891011121314151617181920[root@zhm1 codis]# vi dashboard.conf ################################################### ## Codis-Dashboard ## #################################################### Set Coordinator, only accept "zookeeper" &amp; "etcd".coordinator_name = "zookeeper"coordinator_addr = "192.168.110.129:2181"# Set Codis Product Name/Auth.product_name = "codis-demo"product_auth = ""# Set bind address for admin(rpc), tcp only.admin_addr = "192.168.110.129:18080"# Set quorum value for sentinel, default is 2.sentinel_quorum = 2 å¯åŠ¨zookeeper1234[root@zhm1 codis]# zkServer.sh startJMX enabled by defaultUsing config: /root/zookeeper/zookeeper-3.4.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTED å¯åŠ¨dashboard1[root@zhm1 codis]# nohup bin/codis-dashboard --ncpu=2 --config=dashboard.conf --log=dashboard.log --log-level=WARN &amp; æŸ¥çœ‹æ—¥å¿—è¾“å‡ºï¼Œè¿æ¥zookeeper æˆåŠŸ1234567891011121314151617181920[root@zhm1 codis]# cat dashboard.log.2016-12-212016/12/21 11:18:34 main.go:77: [WARN] set ncpu = 22016/12/21 11:18:34 topom.go:110: [WARN] create new topom:&#123; "token": "6047b5ce9ef2826c1eb40692023b9bf4", "start_time": "2016-12-21 11:18:34.481106135 +0800 CST", "admin_addr": "192.168.110.129:18080", "product_name": "codis-demo", "pid": 66994, "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis", "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux"&#125;2016/12/21 11:18:35 main.go:124: [WARN] create topom with configcoordinator_name = "zookeeper"coordinator_addr = "192.168.110.129:2181"admin_addr = "192.168.110.129:18080"product_name = "codis-demo"product_auth = ""sentinel_quorum = 22016/12/21 11:18:36 topom.go:381: [WARN] admin start service on 192.168.110.129:18080 å¯åŠ¨codis-proxyç¼–è¯‘codis-proxy é…ç½®æ–‡ä»¶123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687[root@zhm1 codis]# ./bin/codis-proxy --default-config | tee proxy.conf################################################### ## Codis-Proxy ## #################################################### Set Codis Product Name/Auth.product_name = "codis-demo"product_auth = ""# Set bind address for admin(rpc), tcp only.admin_addr = "0.0.0.0:11080"# Set bind address for proxy, proto_type can be "tcp", "tcp4", "tcp6", "unix" or "unixpacket".proto_type = "tcp4"proxy_addr = "0.0.0.0:19000"# Set jodis address &amp; session timeout, only accept "zookeeper" &amp; "etcd".jodis_name = ""jodis_addr = ""jodis_timeout = "20s"jodis_compatible = false# Set datacenter of proxy.proxy_datacenter = ""# Set max number of alive sessions.proxy_max_clients = 1000# Set max offheap memory size. (0 to disable)proxy_max_offheap_size = "1024mb"# Set heap placeholder to reduce GC frequency.proxy_heap_placeholder = "256mb"# Proxy will ping backend redis in a predefined interval. (0 to disable)backend_ping_period = "5s"# Set backend recv buffer size &amp; timeout.backend_recv_bufsize = "128kb"backend_recv_timeout = "30s"# Set backend send buffer &amp; timeout.backend_send_bufsize = "128kb"backend_send_timeout = "30s"# Set backend pipeline buffer size.backend_max_pipeline = 1024# Set backend never read replica groups, default is falsebackend_primary_only = false# Set backend parallel connections per serverbackend_primary_parallel = 1backend_replica_parallel = 1# Set backend tcp keepalive period. (0 to disable)backend_keepalive_period = "75s"# If there is no request from client for a long time, the connection will be closed. (0 to disable)# Set session recv buffer size &amp; timeout.session_recv_bufsize = "128kb"session_recv_timeout = "30m"# Set session send buffer size &amp; timeout.session_send_bufsize = "64kb"session_send_timeout = "30s"# Make sure this is higher than the max number of requests for each pipeline request, or your client may be blocked.# Set session pipeline buffer size.session_max_pipeline = 512# Set session tcp keepalive period. (0 to disable)session_keepalive_period = "75s"# Set metrics server (such as http://localhost:28000), proxy will report json formatted metrics to specified server in a predefined period.metrics_report_server = ""metrics_report_period = "1s"# Set influxdb server (such as http://localhost:8086), proxy will report metrics to influxdb.metrics_report_influxdb_server = ""metrics_report_influxdb_period = "1s"metrics_report_influxdb_username = ""metrics_report_influxdb_password = ""metrics_report_influxdb_database = "" product_name é›†ç¾¤åç§°ï¼Œå‚è€ƒdashboard å‚æ•°è¯´æ˜jodis_addr Jodis æ³¨å†Œzookeeper åœ°å€æ­¤å¤„æŠŠIPä¿®æ”¹ä¸€ä¸‹jodis_addr = â€œ192.168.110.129:2181â€ å¯åŠ¨codis-proxy 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859nohup bin/codis-proxy --ncpu=2 --config=proxy.conf --log =proxy.log --log-level=WARN &amp;[root@zhm1 codis]# cat =proxy.log.2016-12-212016/12/21 11:25:01 main.go:100: [WARN] set ncpu = 2, max-ncpu = 02016/12/21 11:25:01 proxy.go:89: [WARN] [0xc42010e840] create new proxy:&#123; "token": "673cc48d563a3614c13a3a294067f0b9", "start_time": "2016-12-21 11:25:01.70765896 +0800 CST", "admin_addr": "192.168.110.129:11080", "proto_type": "tcp4", "proxy_addr": "192.168.110.129:19000", "product_name": "codis-demo", "pid": 67097, "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis", "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux", "hostname": "zhm1.cn", "datacenter": ""&#125;2016/12/21 11:25:01 proxy.go:353: [WARN] [0xc42010e840] admin start service on [::]:110802016/12/21 11:25:01 main.go:159: [WARN] create proxy with configproto_type = "tcp4"proxy_addr = "0.0.0.0:19000"admin_addr = "0.0.0.0:11080"jodis_name = ""jodis_addr = "192.168.110.129:2181"jodis_timeout = "20s"jodis_compatible = falseproduct_name = "codis-demo"product_auth = ""proxy_datacenter = ""proxy_max_clients = 1000proxy_max_offheap_size = "1gb"proxy_heap_placeholder = "256mb"backend_ping_period = "5s"backend_recv_bufsize = "128kb"backend_recv_timeout = "30s"backend_send_bufsize = "128kb"backend_send_timeout = "30s"backend_max_pipeline = 1024backend_primary_only = falsebackend_primary_parallel = 1backend_replica_parallel = 1backend_keepalive_period = "75s"session_recv_bufsize = "128kb"session_recv_timeout = "30m"session_send_bufsize = "64kb"session_send_timeout = "30s"session_max_pipeline = 512session_keepalive_period = "75s"metrics_report_server = ""metrics_report_period = "1s"metrics_report_influxdb_server = ""metrics_report_influxdb_period = "1s"metrics_report_influxdb_username = ""metrics_report_influxdb_password = ""metrics_report_influxdb_database = ""2016/12/21 11:25:01 main.go:180: [WARN] [0xc42010e840] proxy waiting online ...2016/12/21 11:25:01 proxy.go:377: [WARN] [0xc42010e840] proxy start service on 0.0.0.0:190002016/12/21 11:25:02 main.go:180: [WARN] [0xc42010e840] proxy waiting online ... codis-proxy å¯åŠ¨åï¼Œå¤„äºwaiting çŠ¶æ€ï¼Œç›‘å¬proxy_addr åœ°å€ï¼Œä½†æ˜¯ä¸ä¼šaccept è¿æ¥ï¼Œæ·»åŠ åˆ°é›†ç¾¤å¹¶å®Œæˆé›†ç¾¤çŠ¶æ€çš„åŒæ­¥ï¼Œæ‰èƒ½æ”¹å˜çŠ¶æ€ä¸ºonlineã€‚ æ·»åŠ codis-proxy åˆ°é›†ç¾¤æ·»åŠ çš„æ–¹æ³•æœ‰ä»¥ä¸‹ä¸¤ç§ï¼šé€šè¿‡codis-fe æ·»åŠ ï¼Œé€šè¿‡Add Proxy æŒ‰é’®ï¼Œå°†admin_addr åŠ å…¥åˆ°é›†ç¾¤ä¸­ï¼›é€šè¿‡codis-admin å‘½ä»¤è¡Œå·¥å…·æ·»åŠ ï¼š./bin/codis-admin â€“dashboard=192.168.110.129:18080 â€“create-proxy -x 192.168.110.129:11080 æŸ¥çœ‹zookeeper å­˜å‚¨çš„æ•°æ®1234567891011zkCli.sh[zk: localhost:2181(CONNECTED) 0] ls /[codis3, zookeeper, kafka][zk: localhost:2181(CONNECTED) 1] ls /codis3[codis-demo][zk: localhost:2181(CONNECTED) 2] ls /codis3/codis-demo[proxy, topom][zk: localhost:2181(CONNECTED) 3] ls /codis3/codis-demo/proxy[proxy-673cc48d563a3614c13a3a294067f0b9][zk: localhost:2181(CONNECTED) 4] ls /codis3/codis-demo/topom[] é…ç½®å¯åŠ¨Cdis FE é›†ç¾¤ç®¡ç†ç•Œé¢åˆ›å»ºconf ç›®å½•ï¼Œæ”¾ä¸‹é¢ç”Ÿæˆçš„codis.json æ–‡ä»¶12345678910111213mkdir conf./bin/codis-admin --dashboard-list --zookeeper=192.168.110.129 | tee conf/codis.json2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - zkclient setup new connection to 192.168.110.1292016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Connected to 192.168.110.129:21812016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Authenticated: id=25085718467510274, timeout=40000[ &#123; "name": "codis-demo", "dashboard": "192.168.110.129:18080" &#125;]2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Recv loop terminated: err=EOF2016/12/21 11:32:15 zkclient.go:23: [INFO] zookeeper - Send loop terminated: err=&lt;nil&gt; å¯åŠ¨codis-fe1234567./bin/codis-fe --ncpu=2 --log=fe.log --log-level=WARN --dashboard-list=conf/codis.json --listen=192.168.110.129:18090 &amp;[3] 67271[root@zhm1 codis]# cat fe.log.2016-12-21 2016/12/21 11:33:34 main.go:102: [WARN] set ncpu = 22016/12/21 11:33:34 main.go:105: [WARN] set listen = 192.168.110.129:180902016/12/21 11:33:34 main.go:117: [WARN] set assets = /usr/local/codis/src/github.com/CodisLabs/codis/bin/assets2016/12/21 11:33:34 main.go:132: [WARN] set --dashboard-list = conf/codis.json è‡³æ­¤codis çš„å›¾å½¢ç•Œé¢å·²ç»èƒ½å¤Ÿæ˜¾ç¤ºã€‚æ‰“å¼€æµè§ˆå™¨è®¿é—®http://192.168.110.129:18090/ï¼Œé€šè¿‡ç®¡ç†ç•Œé¢æ“ä½œcodis å¦‚æœæ˜¯æœºå™¨å¤–è®¿é—®éœ€è¦å…³é—­é˜²ç«å¢™centos7çš„å‘½ä»¤æ˜¯1234[root@zhm1 codis]# systemctl stop firewalld.service[root@zhm1 codis]# systemctl disable firewalld.serviceRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service. åˆ›å»ºç»„å’Œå®ä¾‹æ“ä½œæ­¥éª¤ï¼šåœ¨New Group åé¢è¾“å…¥1ï¼ˆè¡¨ç¤ºå¢åŠ ç¼–å·ä¸º1 çš„ç»„ï¼‰ï¼Œç‚¹å‡»New Group å®Œæˆç»„çš„åˆ›å»ºï¼›ç„¶åï¼ŒAdd Serverï¼Œåé¢è®¾ç½®ip å’Œç«¯å£ï¼Œå¹¶æŒ‡å‘ç»„ç¼–å·1ï¼Œç‚¹å‡»Add Server å®Œæˆredis å®ä¾‹çš„åˆ›å»ºï¼› æ·»åŠ ç¬¬2 ä¸ªç»„ï¼Œä»¥åŠç¬¬2 ä¸ªç»„çš„å®ä¾‹ï¼Œæ“ä½œæ­¥éª¤åŒä¸Šã€‚ å¯¹slot è¿›è¡Œåˆ†ç»„è¾“å…¥slotèŒƒå›´å’Œç»„ å¦å¤–åˆ†äº«ä¸€ä¸ªgithubä¸Šçš„å¥½æ–‡dockerç©codisé›†ç¾¤https://github.com/ruo91/docker-codis]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka Stream]]></title>
    <url>%2F2016%2F12%2F02%2FKafkaStream%2F</url>
    <content type="text"><![CDATA[ä»¥ä¸‹å†…å®¹æ‘˜è‡ªå®˜ç½‘ï¼Œç†Ÿæ‚‰çš„åŒå­¦å¯è·³è¿‡ Use Kafka Streams to process dataKafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the WordCountDemo example code (converted to use Java 8 lambda expressions for easy reading).Kafka Streamsæ˜¯Kafkaä¸­ç”¨äºå®¢æˆ·ç«¯çš„åº“ï¼Œä¸»è¦ç”¨äºè·å–å®æ—¶æµå¤„ç†ä»¥åŠåˆ†æKafka brokersä¸­å­˜å‚¨çš„æ•°æ®ã€‚è¿™ä¸ªä¾‹å­å°†ä¼šå±•ç¤ºå¦‚ä½•ä½¿ç”¨è¿™ä¸ªåº“æ¥è¿è¡Œä¸€ä¸ªæµå¼å¤„ç†åº”ç”¨ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªWordCountDemoçš„ä¸»è¦ä»£ç ï¼ˆè½¬æ¢æˆJava8 lambdaè¡¨è¾¾å¼æ›´æ˜“è¯»ï¼‰ï¼š 123456789KTable wordCounts = textLines // Split each text line, by whitespace, into words. .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split("\\W+"))) // Ensure the words are available as record keys for the next aggregate operation. .map((key, value) -&gt; new KeyValue&lt;&gt;(value, value)) // Count the occurrences of each word (record key) and store the results into a table named "Counts". .countByKey("Counts") It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on an infinite, unbounded stream of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed â€œallâ€ the input data. å®ƒå®ç°äº†WordCountç®—æ³•ï¼Œè®¡ç®—äº†è¾“å…¥æ–‡æœ¬ä¸­çš„è¯é¢‘ã€‚ç„¶è€Œï¼Œå¹¶ä¸åƒå…¶ä»–çš„WordCountçš„ä¾‹å­ï¼Œéƒ½æ˜¯è®¡ç®—å›ºå®šå¤§å°çš„æ•°æ®ï¼Œè¿™ä¸ªWordCount demoåº”ç”¨ç¨å¾®æœ‰ç‚¹ä¸åŒï¼Œå®ƒæ˜¯åŸºäºä¸ä¼šç»ˆæ­¢çš„æ•°æ®æµè®¡ç®—çš„ã€‚å’Œè®¡ç®—å›ºå®šæ•°æ®çš„æ¨¡å‹æ¯”è¾ƒå½¢ä¼¼çš„æ˜¯ï¼Œå®ƒä¹Ÿä¼šä¸åœçš„æ›´æ–°è¯é¢‘è®¡ç®—ç»“æœã€‚ç„¶è€Œï¼Œç”±äºå®ƒæ˜¯åŸºäºæ°¸ä¸åœæ­¢çš„æ•°æ®æµï¼Œæ‰€ä»¥ä¼šå‘¨æœŸæ€§çš„è¾“å‡ºå½“å‰çš„è®¡ç®—ç»“æœï¼Œä»–ä¼šä¸åœçš„å¤„ç†æ›´å¤šçš„æ•°æ®ï¼Œå› ä¸ºå®ƒä¹Ÿä¸çŸ¥é“ä½•æ—¶å®ƒå¤„ç†è¿‡â€œæ‰€æœ‰â€çš„è¾“å…¥æ•°æ®ã€‚ We will now prepare input data to a Kafka topic, which will subsequently be processed by a Kafka Streams application.ç°åœ¨æˆ‘ä»¬å°†è¾“å…¥æ•°æ®å¯¼å…¥Kafka topicï¼Œè¿™äº›æ•°æ®å°†ä¼šè¢«Kafka Streamsåº”ç”¨å¤„ç† 1&gt; echo -e "all streams lead to kafka\nhello kafka streams\njoin kafka summit" &gt; file-input.txt Or on Windows:123&gt; echo all streams lead to kafka&gt; file-input.txt&gt; echo hello kafka streams&gt;&gt; file-input.txt&gt; echo|set /p=join kafka summit&gt;&gt; file-input.txt Next, we send this input data to the input topic named streams-file-input using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»ˆç«¯produceræ¥å°†è¿™äº›è¾“å…¥æ•°æ®å‘é€åˆ°åä¸ºstreams-file-inputçš„topicï¼ˆåœ¨å®è·µä¸­ï¼Œæµæ•°æ®ä¼šæŒç»­ä¸æ–­çš„æµå…¥kafkaï¼Œå½“åº”ç”¨å°†ä¼šå¯åŠ¨å¹¶è¿è¡Œæ—¶ï¼‰ï¼š12345&gt; bin/kafka-topics.sh --create \ --zookeeper localhost:2181 \ --replication-factor 1 \ --partitions 1 \ --topic streams-file-input 1&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input &lt; file-input.txt We can now run the WordCount demo application to process the input data:æˆ‘ä»¬å¯ä»¥è¿è¡ŒWordCount demoåº”ç”¨æ¥å¤„ç†è¾“å…¥æ•°æ®1&gt; bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo There wonâ€™t be any STDOUT output except log entries as the results are continuously written back into another topic named streams-wordcount-output in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.ä¸ä¼šæœ‰ä»»ä½•çš„stdoutè¾“å‡ºé™¤äº†æ—¥å¿—æ¡ç›®ï¼Œç»“æœä¼šæŒç»­ä¸æ–­çš„å†™å›kafkaä¸­å¦ä¸€ä¸ªåä¸ºstreams-wordcount-outputçš„topicã€‚è¿™ä¸ªdemoå°†ä¼šè¿è¡Œæ•°ç§’ï¼Œä¸ä¼šåƒå…¸å‹çš„æµå¤„ç†åº”ç”¨ï¼Œè‡ªåŠ¨ç»ˆæ­¢ã€‚ We can now inspect the output of the WordCount demo application by reading from its output topic:æˆ‘ä»¬ç°åœ¨é€šè¿‡é˜…è¯»ä¸»é¢˜çš„æ•°æ¥æ¥æ£€æŸ¥WordCount demoåº”ç”¨ç¨‹åºçš„è¾“å‡ºï¼š 12345678&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \ --topic streams-wordcount-output \ --from-beginning \ --formatter kafka.tools.DefaultMessageFormatter \ --property print.key=true \ --property print.value=true \ --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \ --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer with the following output data being printed to the console:ç»ˆç«¯ä¼šæ‰“å°å‡ºä»¥ä¸‹æ•°æ®ï¼š12345678all 1lead 1to 1hello 1streams 2join 1kafka 3summit 1 Here, the first column is the Kafka message key, and the second column is the message value, both in in java.lang.String format. Note that the output is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word, aka record key such as â€œkafkaâ€. For multiple records with the same key, each later record is an update of the previous one.ç¬¬ä¸€åˆ—æ˜¯Kafkaæ¶ˆæ¯çš„keyï¼Œç¬¬äºŒåˆ—æ˜¯æ¶ˆæ¯valueï¼Œä¸¤è€…éƒ½æ˜¯java.lang.Stringæ ¼å¼ã€‚æ³¨æ„ï¼Œè¾“å‡ºå®é™…ä¸Šåº”è¯¥æ˜¯æŒç»­çš„æ›´æ–°æ•°æ®æµï¼Œæ•°æ®æµä¸­çš„æ¯ä¸€ä¸ªè®°å½•ï¼ˆä¾‹å¦‚ï¼Œä¸Šé¢è¾“å‡ºçš„æ¯ä¸€è¡Œï¼‰éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬è¯æ±‡çš„æ•°é‡ï¼Œæˆ–è€…æ˜¯è®°å½•äº†keyçš„æ•°é‡ï¼Œä¾‹å¦‚ä¸Šé¢çš„â€œkafkaâ€ã€‚å¯¹äºå¤šæ¡è®°å½•çš„keyä¸€è‡´è¿™ç§æƒ…å†µï¼Œæ¯ä¸€æ¡åé¢çš„è®°å½•éƒ½æ˜¯å¯¹å‰ä¸€æ¡è®°å½•çš„æ›´æ–°ã€‚ Now you can write more input messages to the streams-file-input topic and observe additional messages added to streams-wordcount-output topic, reflecting updated word counts (e.g., using the console producer and the console consumer, as described above).ç°åœ¨ä½ å¯ä»¥å†™å…¥æ›´å¤šçš„æ¶ˆæ¯åˆ°streams-file-inputè¿™ä¸ªtopicï¼Œå¯ä»¥è§‚å¯Ÿåˆ°æ›´å¤šçš„æ¶ˆæ¯ä¼šå‘é€åˆ°streams-wordcount-outputè¿™ä¸ªtopicï¼Œåæ˜ äº†æ›´æ–°ä¹‹åçš„è¯æ±‡æ•°é‡ã€‚ You can stop the console consumer via Ctrl-C.ä½ å¯ä»¥ä½¿ç”¨Ctrl+Cç»“æŸæ§åˆ¶å°çš„æ¶ˆè´¹è€… Stream ProcessingMany users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an â€œarticlesâ€ topic; further processing might normalize or deduplicate this content and published the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza.å¾ˆå¤šç”¨æˆ·å°†kafkaç”¨ä½œå¤šçº§æ•°æ®å¤„ç†ä¹‹é—´çš„æ¶ˆæ¯ç®¡é“ï¼šåŸå§‹æ•°æ®å­˜æ”¾äºKafkaä¸åŒçš„topicsä¸­ï¼Œç„¶åç»è¿‡èšåˆã€å¢å¼ºã€æˆ–è€…å…¶ä»–çš„è½¬æ¢ä¹‹åï¼Œå¯¼å…¥Kafkaæ–°çš„topicsä¸­ï¼Œä»¥ä¾›åé¢çš„æ¶ˆè´¹ã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ–°é—»æ¨èçš„å¤„ç†æµç¨‹æ¥æ‰€ï¼šé¦–å…ˆä»RSSä¿¡æ¯æµä¸­è·å–æ–‡ç« å†…å®¹ï¼Œç„¶åå¯¼å…¥åä¸ºâ€œarticlesâ€çš„topic; å…¶æ¬¡ï¼Œåé¢çš„å¤„ç†å¯èƒ½æ˜¯å¯¹è¿™äº›å†…å®¹è¿›è¡Œè§„èŒƒåŒ–æˆ–è€…ç²¾ç®€æ“ä½œï¼Œç„¶åå°†ç»è¿‡ä¸Šè¿°å¤„ç†çš„å†…å®¹å¯¼å…¥æ–°çš„topic;æœ€åçš„å¤„ç†å¯èƒ½æ˜¯è¯•å›¾å°†è¿™äº›å†…å®¹æ¨èç»™ç”¨æˆ·ã€‚è¿™æ ·çš„å¤„ç†æµç¨‹å®é™…å±•ç°äº†å®æ—¶æµåœ¨ç‹¬ç«‹çš„topicsä¹‹é—´æµåŠ¨çš„æµç¨‹å›¾ã€‚ä»0.10.0.0å¼€å§‹ï¼ŒApache Kafkaæ¨å‡ºäº†ä¸€æ¬¾ç§°ä¸ºKafka Streamsçš„æµå¼å¤„ç†åº“ï¼Œä¼˜ç‚¹æ˜¯è½»é‡çº§åŒæ—¶æ€§èƒ½å¾ˆå¥½ï¼Œå®ƒå¯ä»¥å®Œæˆä¸Šé¢æ‰€æè¿°çš„å¤šçº§å¤„ç†ã€‚é™¤äº†Kafka streamsä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€äº›å¼€æºæµå¼å¤„ç†å·¥å…·å¯ä»¥é€‰ç”¨ï¼ŒåŒ…æ‹¬Apache Stormå’ŒSamzaã€‚ OverviewKafka Streams is a client library for processing and analyzing data stored in Kafka and either write the resulting data back to Kafka or send the final output to an external system. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management of application state. Kafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafkaâ€™s parallelism model.Kafka Streamsæ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯ç¨‹åºåº“ç”¨äºå¤„ç†å’Œåˆ†æå­˜å‚¨åœ¨Kafkaä¸­çš„æ•°æ®ï¼Œå¹¶å°†å¾—åˆ°çš„æ•°æ®å†™å…¥Kafkaæˆ–å‘é€æœ€ç»ˆè¾“å‡ºåˆ°å¤–éƒ¨ç³»ç»Ÿã€‚å®ƒå»ºç«‹åœ¨å¦‚é€‚å½“åŒºåˆ†äº‹ä»¶çš„æ—¶é—´å’ŒåŠ å·¥æ—¶é—´ï¼Œçª—å£å‡½æ•°çš„æ”¯æŒï¼Œå’Œç®€å•è€Œé«˜æ•ˆçš„åº”ç”¨ç¨‹åºçŠ¶æ€ç®¡ç†ã€‚Kafka Streamsæœ‰ä¸€ä¸ªä½é—¨æ§›è¿›å…¥ï¼šä½ å¯ä»¥å¿«é€Ÿç¼–å†™å’Œè¿è¡Œä¸€ä¸ªå°è§„æ¨¡çš„æ¦‚å¿µè¯æ˜åœ¨ä¸€å°æœºå™¨ä¸Šï¼Œä½ åªéœ€è¦è¿è¡Œåœ¨å¤šå°æœºå™¨ä¸Šçš„åº”ç”¨ç¨‹åºçš„é¢å¤–çš„å®ä¾‹æ‰©å±•åˆ°é«˜å®¹é‡çš„ç”Ÿäº§å·¥ä½œè´Ÿè½½ã€‚Kafka Streams é€æ˜åœ°å¤„ç†ç›¸åŒçš„åº”ç”¨ç¨‹åºé€šè¿‡åˆ©ç”¨Kafka çš„å¹¶è¡Œæ¨¡å‹çš„å¤šä¸ªå®ä¾‹çš„è´Ÿè½½å¹³è¡¡ã€‚ Some highlights of Kafka Streams:Kafka Streamsçš„ä¸€äº›äº®ç‚¹ Designed as a simple and lightweight client library, which can beeasily embedded in any Java application and integrated with anyexisting packaging, deployment and operational tools that users havefor their streaming applications. ä½œä¸ºä¸€ä¸ªç®€å•è€Œè½»é‡çº§çš„å®¢æˆ·ç«¯åº“ï¼Œå®ƒå¯ä»¥æ–¹ä¾¿çš„åµŒå…¥ä»»ä½•javaåº”ç”¨å’Œé›†æˆä»»ä½•ç°æœ‰çš„åŒ…ï¼Œéƒ¨ç½²å’Œè¿è¥å·¥å…·ï¼Œç”¨æˆ·æœ‰å¯¹äºä»–ä»¬çš„æµåº”ç”¨ã€‚ Has no external dependencies on systems other than Apache Kafkaitself as the internal messaging layer; notably, it uses Kafkaâ€™spartitioning model to horizontally scale processing while maintainingstrong ordering guarantees. å¯¹å…¶ä»–æ¯”Apache Kafka æ²¡æœ‰å¤–éƒ¨ä¾èµ–å®ƒæœ¬èº«ä½œä¸ºå†…éƒ¨æ¶ˆæ¯å±‚ï¼Œç‰¹åˆ«æ˜¯ï¼Œå®ƒä½¿ç”¨Kafkaçš„ åˆ†å‰²æ¨¡å‹åœ¨ä¿æŒåŒæ—¶è¿›è¡Œæ°´å¹³ç¼©æ”¾å¤„ç†çš„åˆ†åŒºæ¨¡å‹å¼ºæ’åºä¿è¯ã€‚ Supports fault-tolerant local state, which enables very fast andefficient stateful operations like joins and windowed aggregations. æ”¯æŒå®¹é”™çš„æœ¬åœ°çŠ¶æ€ï¼Œä½¿éå¸¸å¿«é€Ÿå’Œé«˜æ•ˆçš„çŠ¶æ€æ“ä½œçš„åŠ å…¥å’Œçª—å£èšé›†ã€‚ Employs one-record-at-a-time processing to achieve low processinglatency, and supports event-time based windowing operations. é‡‡ç”¨åŒä¸€æ—¶åˆ»åªæœ‰ä¸€æ¡è®°å½•å¤„ç†å®ç°ä½çš„å¤„ç†å»¶è¿Ÿï¼Œå¹¶æ”¯æŒåŸºäºæ—¶é—´äº‹ä»¶çš„çª—å£æ“ä½œã€‚ Offers necessary stream processing primitives, along with ahigh-level Streams DSL and a low-level Processor API. æä¾›å¿…è¦çš„æµå¤„ç†åŸºå…ƒï¼Œä»¥åŠ high-level Streams DSLå’Œ low-level Processor APIã€‚ Developer Guide å¼€å‘è€…æŒ‡å—There is a quickstart example that provides how to run a stream processing program coded in the Kafka Streams library. This section focuses on how to write, configure, and execute a Kafka Streams application. æœ‰ä¸€ä¸ªå¿«é€Ÿå…¥é—¨ç¤ºä¾‹æä¾›äº†å¦‚ä½•è¿è¡Œä¸€ä¸ªæµå¤„ç†ç¨‹åºåœ¨å¡å¤«å¡æµçš„åº“ä»£ç ã€‚æœ¬èŠ‚é‡ç‚¹ä»‹ç»å¦‚ä½•ç¼–å†™ã€é…ç½®å’Œæ‰§è¡Œå¡å¤«å¡æµåº”ç”¨ç¨‹åºã€‚ Core Concepts æ ¸å¿ƒæ¦‚å¿µWe first summarize the key concepts of Kafka Streams.æˆ‘ä»¬é¦–å…ˆæ€»ç»“äº†Kafka Streamsçš„å…³é”®æ¦‚å¿µã€‚ Stream Processing Topology æµå¤„ç†Topology A stream is the most important abstraction provided by Kafka Streams:it represents an unbounded, continuously updating data set. A streamis an ordered, replayable, and fault-tolerant sequence of immutabledata records, where a data record is defined as a key-value pair. æµæ˜¯Kafka Streamsæä¾›çš„æœ€é‡è¦çš„æŠ½è±¡ï¼šå®ƒè¡¨ç¤ºä¸€ä¸ªæ— ç•Œçš„ï¼Œä¸æ–­æ›´æ–°çš„æ•°æ®é›†ã€‚ä¸€ä¸ªæµæ˜¯ä¸€ä¸ªæœ‰åºçš„ã€å¯é‡å¤çš„ï¼Œå’Œä¸å˜çš„å®¹é”™åºåˆ—æ•°æ®è®°å½•ï¼Œå…¶ä¸­ä¸€ä¸ªæ•°æ®è®°å½•è¢«å®šä¹‰ä¸ºä¸€ä¸ªé”®å€¼å¯¹ã€‚ A stream processing application written in Kafka Streams defines itscomputational logic through one or more processor topologies, where aprocessor topology is a graph of stream processors (nodes) that areconnected by streams (edges).åœ¨Kafka Streamsä¸­å†™çš„æµå¤„ç†åº”ç”¨ç¨‹åºå®šä¹‰äº†å®ƒçš„è®¡ç®—é€»è¾‘é€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ªå¤„ç†å™¨çš„topologiesï¼Œå…¶ä¸­å¤„ç†å™¨çš„topologyæ˜¯ä¸€ä¸ªæµå¤„ç†å™¨ï¼ˆèŠ‚ç‚¹ï¼‰çš„å›¾å½¢ç”±æµè¿æ¥ï¼ˆè¾¹ç¼˜ï¼‰ã€‚ A stream processor is a node in the processor topology; it representsa processing step to transform data in streams by receiving one inputrecord at a time from its upstream processors in the topology,applying its operation to it, and may subsequently producing one ormore output records to its downstream processors. æµå¤„ç†å™¨æ˜¯å¤„ç†å™¨topologyä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ï¼›å®ƒè¡¨ç¤ºé€šè¿‡æ¥æ”¶ä¸€ä¸ªè¾“å…¥æ¥å˜æ¢æµä¸­çš„æ•°æ®çš„å¤„ç†æ­¥éª¤åœ¨topologyä¸­çš„ä¸Šæ¸¸å¤„ç†å™¨ä¸Šè®°å½•çš„æ—¶é—´ï¼Œåº”ç”¨å®ƒçš„æ“ä½œï¼Œå¹¶å¯èƒ½éšåäº§ç”Ÿä¸€ä¸ªæˆ–å‘ä¸‹æ¸¸å¤„ç†å™¨çš„æ›´å¤šè¾“å‡ºè®°å½•ã€‚ Kafka Streams offers two ways to define the stream processing topology: the Kafka Streams DSL provides the most common data transformation operations such as map and filter; the lower-level Processor API allows developers define and connect custom processors as well as to interact with state stores.Kafka Streams æä¾›äº†ä¸¤ç§æ–¹å¼æ¥å®šä¹‰æµå¤„ç†topologyï¼šKafka Streams DSLæä¾›äº†æœ€å¸¸ç”¨çš„æ•°æ®è½¬æ¢æ“ä½œï¼Œå¦‚mapå’Œfilterï¼›lower-level Processor APIå…è®¸å¼€å‘è€…å®šä¹‰å’Œè¿æ¥å®šåˆ¶å¤„ç†å™¨ä»¥åŠå­˜å‚¨äº¤äº’çš„çŠ¶æ€ã€‚ Time æ—¶é—´A critical aspect in stream processing is the notion of time, and how it is modeled and integrated. For example, some operations such as windowing are defined based on time boundaries.æµå¤„ç†ä¸­çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯æ—¶é—´çš„æ¦‚å¿µï¼Œä»¥åŠå®ƒæ˜¯å¦‚ä½•å»ºæ¨¡å’Œé›†æˆã€‚ä¾‹å¦‚ï¼Œä¸€äº›æ“ä½œå¦‚çª—å£æ˜¯åŸºäºæ—¶é—´è¾¹ç•Œçš„å®šä¹‰ã€‚ Common notions of time in streams are:æµä¸­çš„æ—¶é—´çš„å…±åŒæ¦‚å¿µæ˜¯ï¼š Event time - The point in time when an event or data record occurred,i.e. was originally created â€œat the sourceâ€. äº‹ä»¶æ—¶é—´ - å½“å‘ç”Ÿäº‹ä»¶æˆ–æ•°æ®è®°å½•æ—¶çš„æ—¶é—´ç‚¹ï¼Œå³æœ€åˆåˆ›å»ºçš„â€œåœ¨æºå¤´ä¸Šâ€ã€‚ Processing time - The point in time when the event or data recordhappens to be processed by the stream processing application, i.e.when the record is being consumed. The processing time may bemilliseconds, hours, or days etc. later than the original event time. å¤„ç†æ—¶é—´ - äº‹ä»¶æˆ–æ•°æ®è®°å½•çš„æ—¶é—´ç‚¹ç¢°å·§è¢«æµå¤„ç†åº”ç”¨ç¨‹åºå¤„ç†ï¼Œå³å½“è®°å½•è¢«æ¶ˆè€—ã€‚å¤„ç†æ—¶é—´å¯èƒ½æ˜¯æ¯”åŸå§‹äº‹ä»¶æ—¶é—´æ™šçš„æ¯«ç§’æ•°ã€å°æ—¶æˆ–æ•°å¤©ç­‰ã€‚ Ingestion time - The point in time when an event or data record isstored in a topic partition by a Kafka broker. The difference toevent time is that this ingestion timestamp is generated when therecord is appended to the target topic by the Kafka broker, not whenthe record is created â€œat the sourceâ€. The difference to processingtime is that processing time is when the stream processingapplication processes the record. For example, if a record is neverprocessed, there is no notion of processing time for it, but it stillhas an ingestion time. æ‘„å–æ—¶é—´ - å½“ä¸€ä¸ªäº‹ä»¶æˆ–æ•°æ®è®°å½•çš„æ—¶é—´ç‚¹å­˜å‚¨åœ¨Kafka brokerçš„ä¸»é¢˜åˆ†åŒºä¸­ã€‚äº‹ä»¶æ—¶é—´ä¸åŒçš„æ˜¯ï¼Œè¿™ç§æ‘„å–æ—¶é—´æˆ³æ—¶äº§ç”Ÿçš„è®°å½•è¿½åŠ åˆ°ç›®æ ‡ä¸»é¢˜ç”±Kafka broker ï¼Œè€Œä¸æ˜¯å½“è®°å½•æ˜¯åœ¨â€œæºâ€åˆ›å»ºçš„ã€‚å¤„ç†å·®å¼‚æ—¶é—´æ˜¯å¤„ç†æ—¶é—´çš„æ—¶å€™æ˜¯æµå¤„ç†çš„åº”ç”¨ç¨‹åºå¤„ç†è®°å½•ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªè®°å½•æ˜¯ä»æ¥æ²¡æœ‰å¤„ç†ï¼Œæ²¡æœ‰å¤„ç†æ—¶é—´çš„æ¦‚å¿µï¼Œä½†å®ƒä»ç„¶æœ‰ä¸€ä¸ªæ‘„å–æ—¶é—´ The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafkaâ€™s configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps.äº‹ä»¶çš„æ—¶é—´å’Œæ‘„å–æ—¶é—´ä¹‹é—´çš„é€‰æ‹©å®é™…ä¸Šæ˜¯é€šè¿‡Kafkaçš„é…ç½®ï¼ˆä¸æ˜¯Kafka Streamsï¼‰ï¼šä»Kafka0.10.xèµ·ï¼Œæ—¶é—´æˆ³è¢«è‡ªåŠ¨åµŒå…¥åˆ°Kafkaçš„æ¶ˆæ¯ä¸­ã€‚æ ¹æ®Kafkaçš„é…ç½®è¿™äº›æ—¶é—´æˆ³è¡¨ç¤ºäº‹ä»¶æ—¶é—´æˆ–æ‘„å–æ—¶é—´ã€‚å„è‡ªçš„Kafkaé…ç½®è®¾ç½®å¯ä»¥åœ¨brokerçº§åˆ«æˆ–æ¯ä¸ªä¸»é¢˜ä¸ŠæŒ‡å®šã€‚åœ¨Kafka Streamsçš„é»˜è®¤æ—¶é—´æˆ³æå–å™¨å°†æ£€ç´¢è¿™äº›åµŒå…¥æ—¶é—´æˆ³as-isã€‚å› æ­¤ï¼Œæ‚¨çš„åº”ç”¨ç¨‹åºçš„æœ‰æ•ˆæ—¶é—´è¯­ä¹‰ä¾èµ–äºæœ‰æ•ˆçš„Kafkaé…ç½®è¿™äº›åµŒå…¥æ—¶é—´æˆ³ã€‚ Kafka Streams assigns a timestamp to every data record via the TimestampExtractor interface. Concrete implementations of this interface may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce different notions of time depending on their business needs. For example, per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins.Kafka Streamsåˆ†é…ä¸€ä¸ªæ—¶é—´æˆ³çš„æ¯ä¸€ä¸ªæ•°æ®è®°å½•é€šè¿‡TimestampExtractoræ¥å£ã€‚è¿™ä¸ªæ¥å£çš„å…·ä½“å®ç°å¯ä»¥æ£€ç´¢æˆ–è®¡ç®—åŸºäºæ•°æ®è®°å½•å¦‚åµŒå…¥æ—¶é—´æˆ³å­—æ®µæä¾›äº‹ä»¶æ—¶é—´è¯­ä¹‰å†…å®¹çš„æ—¶é—´æˆ³ï¼Œæˆ–ä½¿ç”¨ä»»ä½•å…¶ä»–çš„æ–¹æ³•ï¼Œå¦‚åŠ å·¥æ—¶è¿”å›å½“å‰æ—¶é’Ÿæ—¶é—´ï¼Œä»è€Œäº§ç”Ÿè¯­ä¹‰æµå¤„ç†åº”ç”¨ç¨‹åºçš„å¤„ç†æ—¶é—´ã€‚å› æ­¤ï¼Œå¼€å‘äººå‘˜å¯ä»¥æ‰§è¡Œä¸åŒçš„æ—¶é—´æ¦‚å¿µï¼Œè¿™å–å†³äºä»–ä»¬çš„ä¸šåŠ¡éœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œæ¯ä¸ªè®°å½•æ—¶é—´æˆ³çš„æè¿°å…³äºæµæ—¶é—´çš„è¿›å±•ï¼ˆè™½ç„¶è®°å½•å¯èƒ½ä¼šè¶…å‡ºæµï¼‰å’Œä¿ƒä½¿æ—¶é—´ä¾èµ–æ“ä½œä¾‹å¦‚ joinsã€‚ Finally, whenever a Kafka Streams application writes records to Kafka, then it will also assign timestamps to these new records. The way the timestamps are assigned depends on the context:æœ€åï¼Œå½“Kafkaè®°å½•å†™å…¥åˆ°Kafka Streamsåº”ç”¨ï¼Œé‚£ä¹ˆå®ƒä¹Ÿä¼šå¯¹æ–°çºªå½•æŒ‡å®šæ—¶é—´æˆ³ã€‚æ—¶é—´æˆ³æ˜¯åˆ†é…æ–¹å¼å–å†³äºcontextï¼š When new output records are generated via processing some inputrecord, for example, context.forward() triggered in the process()function call, output record timestamps are inherited from inputrecord timestamps directly. å½“é€šè¿‡å¤„ç†ä¸€äº›è¾“å…¥è€Œäº§ç”Ÿæ–°çš„è¾“å‡ºè®°å½•æ—¶è®°å½•ï¼Œä¾‹å¦‚ï¼Œcontext.forward()å¼•å‘çš„process()å‡½æ•°è°ƒç”¨ï¼Œè¾“å‡ºè®°å½•çš„æ—¶é—´æˆ³æ˜¯ç»§æ‰¿è‡ªè¾“å…¥ç›´æ¥è®°å½•æ—¶é—´æˆ³ã€‚ When new output records are generated via periodic functions such aspunctuate(), the output record timestamp is defined as the currentinternal time (obtained through context.timestamp()) of the streamtask. å½“æ–°çš„è¾“å‡ºè®°å½•é€šè¿‡è¯¸å¦‚punctuate()å‘¨æœŸå‡½æ•°ç”Ÿæˆçš„è¾“å‡ºè®°å½•æ—¶é—´æˆ³å®šä¹‰ä¸ºå½“å‰çš„å†…éƒ¨æ—¶é—´ï¼ˆcontext.timestamp()è·å¾—ï¼‰çš„æµä»»åŠ¡ã€‚ For aggregations, the timestamp of a resulting aggregate updaterecord will be that of the latest arrived input record that triggeredthe update. å¯¹äºä¸€ä¸ªèšåˆï¼Œå½¢æˆçš„èšåˆæ›´æ–°è®°å½•çš„æ—¶é—´æˆ³å°†æœ€æ–°åˆ°è¾¾çš„è¾“å…¥è®°å½•è§¦å‘æ›´æ–°ã€‚ States çŠ¶æ€Some stream processing applications donâ€™t require state, which means the processing of a message is independent from the processing of all other messages. However, being able to maintain state opens up many possibilities for sophisticated stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.ä¸€äº›æµå¤„ç†åº”ç”¨ç¨‹åºä¸éœ€è¦çŠ¶æ€ï¼Œè¿™æ„å‘³ç€æ¶ˆæ¯çš„å¤„ç†æ˜¯ç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–æ¶ˆæ¯çš„å¤„ç†ã€‚ç„¶è€Œï¼Œèƒ½å¤Ÿä¿æŒçŠ¶æ€ä¸ºå¤æ‚çš„æµå¤„ç†åº”ç”¨ç¨‹åºæ‰“å¼€äº†è®¸å¤šå¯èƒ½æ€§ï¼šæ‚¨å¯ä»¥åŠ å…¥è¾“å…¥æµï¼Œæˆ–ç»„å’Œæ±‡æ€»æ•°æ®è®°å½•ã€‚è®¸å¤šè¿™æ ·çš„çŠ¶æ€çš„æ“ä½œï¼Œç”±Kafka Streams DSLæä¾›ã€‚ Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data. This is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a persistent key-value store, an in-memory hashmap, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.Kafka Streamsæä¾›äº†æ‰€è°“çš„çŠ¶æ€å­˜å‚¨ï¼Œå®ƒå¯ä»¥ç”¨äºæµå¤„ç†åº”ç”¨ç¨‹åºæ¥å­˜å‚¨å’ŒæŸ¥è¯¢æ•°æ®ã€‚å½“å®æ–½çŠ¶æ€æ“ä½œæ—¶è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„èƒ½åŠ›ã€‚åœ¨Kafka Streamsçš„æ¯ä¸€é¡¹ä»»åŠ¡çš„ä¸€ä¸ªæˆ–å¤šä¸ªçŠ¶æ€å­˜å‚¨å°†å¯ä»¥é€šè¿‡APIæ¥å­˜å‚¨å’ŒæŸ¥è¯¢å¤„ç†æ‰€éœ€çš„æ•°æ®è®¿é—®ã€‚è¿™äº›çŠ¶æ€å­˜å‚¨å¯ä»¥æ˜¯ä¸€ä¸ªæŒç»­çš„é”®å€¼å­˜å‚¨ï¼Œå†…å­˜ä¸­çš„HashMapï¼Œæˆ–å¦ä¸€ä¸ªæ–¹ä¾¿çš„æ•°æ®ç»“æ„ã€‚Kafka Streamsæä¾›äº†å®¹é”™å’Œæœ¬åœ°çŠ¶æ€å­˜å‚¨çš„è‡ªåŠ¨æ¢å¤ã€‚ Kafka Streams allows direct read-only queries of the state stores by methods, threads, processes or applications external to the stream processing application that created the state stores. This is provided through a feature called Interactive Queries. All stores are named and Interactive Queries exposes only the read operations of the underlying implementation.Kafka Streamså…è®¸é€šè¿‡æ–¹æ³•ï¼Œçº¿ç¨‹ï¼Œè¿›ç¨‹æˆ–åº”ç”¨ç¨‹åºçš„å¤–éƒ¨çš„æµå¤„ç†åº”ç”¨ç¨‹åºåˆ›å»ºçš„çŠ¶æ€å­˜å‚¨çš„çŠ¶æ€å­˜å‚¨çš„ç›´æ¥åªè¯»æŸ¥è¯¢ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªè¢«ç§°ä¸ºäº¤äº’å¼æŸ¥è¯¢çš„åŠŸèƒ½ã€‚æ‰€æœ‰çš„å­˜å‚¨éƒ½è¢«å‘½åå’Œäº¤äº’æŸ¥è¯¢åªå…¬å¼€åº•å±‚å®ç°çš„è¯»æ“ä½œã€‚ As we have mentioned above, the computational logic of a Kafka Streams application is defined as a processor topology. Currently Kafka Streams provides two sets of APIs to define the processor topology, which will be described in the subsequent sections.æ­£å¦‚æˆ‘ä»¬ä¸Šæ–‡æ‰€æåˆ°çš„ï¼ŒKafka Streamsåº”ç”¨ç¨‹åºçš„è®¡ç®—é€»è¾‘è¢«å®šä¹‰ä¸ºä¸€ä¸ªå¤„ç†topologyã€‚ç›®å‰ï¼ŒKafka Streamsæä¾›äº†ä¸¤ç»„çš„æ¥å£æ¥å®šä¹‰å¤„ç†çš„topologyï¼Œè¿™å°†åœ¨éšåçš„ç« èŠ‚ä¸­æè¿°ã€‚ Low-Level Processor APIProcessorDevelopers can define their customized processing logic by implementing the Processor interface, which provides process and punctuate methods. The process method is performed on each of the received record; and the punctuate method is performed periodically based on elapsed time. In addition, the processor can maintain the current ProcessorContext instance variable initialized in the init method, and use the context to schedule the punctuation period (context().schedule), to forward the modified / new key-value pair to downstream processors (context().forward), to commit the current processing progress (context().commit), etc.å¼€å‘è€…å¯ä»¥é€šè¿‡å¤„ç†å™¨æ¥å£å®šä¹‰è‡ªå·±çš„å®šåˆ¶çš„å¤„ç†é€»è¾‘ï¼Œå®ƒæä¾›äº†æ–¹æ³•å’Œæ ‡ç‚¹çš„æ–¹æ³•ã€‚processæ–¹æ³•æ˜¯å¯¹æ¯ä¸ªæ¥æ”¶çš„è®°å½•æ‰§è¡Œï¼›å’Œæ ‡ç‚¹æ³•æ˜¯åŸºäºæ—¶é—´è¿›è¡Œå®šæœŸã€‚æ­¤å¤–ï¼Œè¯¥å¤„ç†å™¨å¯ä»¥ç»´æŒç›®å‰çš„ProcessorContextå®ä¾‹å˜é‡åœ¨initæ–¹æ³•åˆå§‹åŒ–ï¼Œå¹¶ä½¿ç”¨contextå®‰æ’æ ‡ç‚¹ç¬¦å·å‘¨æœŸï¼ˆcontext().scheduleï¼‰ï¼Œæå‡ºä¿®æ”¹/æ–°çš„é”®å€¼å¯¹ä¸‹æ¸¸å¤„ç†å™¨ï¼ˆcontext().forwardï¼‰ï¼ŒæŠŠå½“å‰çš„å¤„ç†è¿›åº¦ï¼ˆcontext().commitï¼‰ï¼Œç­‰ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738public class MyProcessor extends Processor &#123; private ProcessorContext context; private KeyValueStore kvStore; @Override @SuppressWarnings("unchecked") public void init(ProcessorContext context) &#123; this.context = context; this.context.schedule(1000); this.kvStore = (KeyValueStore) context.getStateStore("Counts"); &#125; @Override public void process(String dummy, String line) &#123; String[] words = line.toLowerCase().split(" "); for (String word : words) &#123; Integer oldValue = this.kvStore.get(word); if (oldValue == null) &#123; this.kvStore.put(word, 1); &#125; else &#123; this.kvStore.put(word, oldValue + 1); &#125; &#125; &#125; @Override public void punctuate(long timestamp) &#123; KeyValueIterator iter = this.kvStore.all(); while (iter.hasNext()) &#123; KeyValue entry = iter.next(); context.forward(entry.key, entry.value.toString()); &#125; iter.close(); context.commit(); &#125; @Override public void close() &#123; this.kvStore.close(); &#125; &#125;; In the above implementation, the following actions are performed:åœ¨ä¸Šé¢çš„å®ç°ä¸­ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š In the init method, schedule the punctuation every 1 second andretrieve the local state store by its name â€œCountsâ€. åœ¨initæ–¹æ³•ï¼Œscheduleæ¯1ç§’å’Œæ ‡ç‚¹ç¬¦å·æ£€ç´¢æœ¬åœ°çŠ¶æ€å­˜å‚¨ç”±å®ƒçš„åç§°â€œè®¡æ•°â€ã€‚ In the process method, upon each received record, split the valuestring into words, and update their counts into the state store (wewill talk about this feature later in the section). åœ¨å¤„ç†æ–¹æ³•ä¸­ï¼Œåœ¨æ¯ä¸ªæ¥æ”¶åˆ°çš„è®°å½•ä¸­ï¼Œå°†å€¼å­—ç¬¦ä¸²åˆ†å‰²æˆå•è¯ï¼Œå¹¶æ›´æ–°ä»–ä»¬çš„è®¡æ•°åˆ°çŠ¶æ€å­˜å‚¨åŒºï¼ˆæˆ‘ä»¬å°†åœ¨æœ¬èŠ‚ä¸­è®¨è®ºè¿™ä¸ªåŠŸèƒ½ï¼‰ã€‚ In the punctuate method, iterate the local state store and send theaggregated counts to the downstream processor, and commit the currentstream state. åœ¨æ ‡ç‚¹æ³•ï¼Œè¿­ä»£å±€éƒ¨çŠ¶æ€å­˜å‚¨å’Œå‘é€æ±‡æ€»è®¡æ•°åˆ°ä¸‹æ¸¸çš„å¤„ç†å™¨ï¼Œå¹¶æ‰¿è¯ºç›®å‰æµçŠ¶æ€ã€‚ Processor TopologyWith the customized processors defined in the Processor API, developers can use the TopologyBuilder to build a processor topology by connecting these processors together:ä¸å®šåˆ¶çš„å¤„ç†å™¨åœ¨Processor APIï¼Œå¼€å‘è€…å¯ä»¥ä½¿ç”¨TopologyBuilderæ¥è¿æ¥è¿™äº›å¤„ç†å™¨ä¸€èµ·å»ºç«‹ä¸€ä¸ªtopologyï¼š12345678TopologyBuilder builder = new TopologyBuilder();builder.addSource("SOURCE", "src-topic") .addProcessor("PROCESS1", MyProcessor1::new /* the ProcessorSupplier that can generate MyProcessor1 */, "SOURCE") .addProcessor("PROCESS2", MyProcessor2::new /* the ProcessorSupplier that can generate MyProcessor2 */, "PROCESS1") .addProcessor("PROCESS3", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") .addSink("SINK1", "sink-topic1", "PROCESS1") .addSink("SINK2", "sink-topic2", "PROCESS2") .addSink("SINK3", "sink-topic3", "PROCESS3"); There are several steps in the above code to build the topology, and here is a quick walk through:åœ¨ä¸Šé¢çš„ä»£ç ä¸­æœ‰å‡ ä¸ªæ­¥éª¤æ¥æ„å»ºtopologyï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªå¿«é€Ÿçš„æ­¥è¡Œé€šè¿‡ï¼š First of all a source node named â€œSOURCEâ€ is added to the topologyusing the addSource method, with one Kafka topic â€œsrc-topicâ€ fed toit. é¦–å…ˆï¼ŒæºèŠ‚ç‚¹å‘½åä¸ºâ€œæºâ€æ·»åŠ åˆ°topologyä½¿ç”¨addSourceæ–¹æ³•ï¼Œä½¿ç”¨â€œsrc-topicâ€è¿™ä¸ªKafka ä¸»é¢˜ã€‚ Three processor nodes are then added using the addProcessor method;here the first processor is a child of the â€œSOURCEâ€ node, but is theparent of the other two processors. ä¸‰ä¸ªprocessorèŠ‚ç‚¹ï¼Œç„¶åä½¿ç”¨addProcessoræ–¹æ³•æ·»åŠ ï¼›è¿™é‡Œç¬¬ä¸€ä¸ªprocessoræ˜¯ä¸€ä¸ªâ€œæºâ€èŠ‚ç‚¹çš„å­èŠ‚ç‚¹ï¼Œä½†æ˜¯å…¶ä»–ä¸¤å¤„processors Finally three sink nodes are added to complete the topology using theaddSink method, each piping from a different parent processor nodeand writing to a separate topic. æœ€åï¼Œæ·»åŠ ä¸‰ä¸ªæ±‡èšèŠ‚ç‚¹æ¥å®Œæˆä½¿ç”¨çš„topology addSinkæ–¹æ³•ï¼Œæ¯ä¸ªç®¡é“ä»ä¸åŒçš„çˆ¶èŠ‚ç‚¹çš„processorå’Œå†™åˆ°ä¸€ä¸ªå•ç‹¬çš„ä¸»é¢˜ã€‚ Local State Storeæœ¬åœ°çŠ¶æ€å­˜å‚¨Note that the Processor API is not limited to only accessing the current records as they arrive, but can also maintain local state stores that keep recently arrived records to use in stateful processing operations such as aggregation or windowed joins. To take advantage of this local states, developers can use the TopologyBuilder.addStateStore method when building the processor topology to create the local state and associate it with the processor nodes that needs to access it; or they can connect a created local state store with the existing processor nodes through TopologyBuilder.connectProcessorAndStateStores.æ³¨æ„Processor APIä¸ä»…é™äºè®¿é—®å½“å‰è®°å½•ï¼Œè¿˜å¯ä»¥ç»´æŒæœ¬åœ°çŠ¶æ€æ‘ç²—ï¼Œä½¿è®°å½•ä½¿ç”¨çŠ¶æ€çš„å¤„ç†æ“ä½œï¼Œå¦‚èšé›†æˆ–çª—å£çš„åŠ å…¥ã€‚åˆ©ç”¨å±€éƒ¨çŠ¶æ€ï¼Œå¼€å‘äººå‘˜å¯ä»¥ä½¿ç”¨TopologyBuilder.addStateStoreæ–¹æ³•å½“æ­å»ºprocessor topologyåˆ›å»ºæœ¬åœ°çŠ¶æ€ï¼Œå®ƒä¸processorèŠ‚ç‚¹éœ€è¦è®¿é—®å®ƒçš„è”æƒ³ï¼›æˆ–è€…ä»–ä»¬å¯ä»¥è¿æ¥åˆ›å»ºçš„å±€éƒ¨çŠ¶æ€å­˜å‚¨ä¸ç°æœ‰çš„å¤„ç†å™¨èŠ‚ç‚¹é€šè¿‡TopologyBuilder.connectProcessorAndStateStores.123456789101112TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", "src-topic") .addProcessor("PROCESS1", MyProcessor1::new, "SOURCE") // create the in-memory state store "COUNTS" associated with processor "PROCESS1" .addStateStore(Stores.create("COUNTS").withStringKeys().withStringValues().inMemory().build(), "PROCESS1") .addProcessor("PROCESS2", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") .addProcessor("PROCESS3", MyProcessor3::new /* the ProcessorSupplier that can generate MyProcessor3 */, "PROCESS1") // connect the state store "COUNTS" with processor "PROCESS2" .connectProcessorAndStateStores("PROCESS2", "COUNTS"); .addSink("SINK1", "sink-topic1", "PROCESS1") .addSink("SINK2", "sink-topic2", "PROCESS2") .addSink("SINK3", "sink-topic3", "PROCESS3"); ä¾‹å­ï¼šæ­å»ºkafka_2.11-0.10.1.0 é›†ç¾¤ï¼šè®¾ç½®server.properties1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#brokerçš„å…¨å±€å”¯ä¸€ç¼–å·ï¼Œä¸èƒ½é‡å¤broker.id=0#ç”¨æ¥ç›‘å¬é“¾æ¥çš„ç«¯å£ï¼Œproduceræˆ–consumerå°†åœ¨æ­¤ç«¯å£å»ºç«‹è¿æ¥port=9092#å¤„ç†ç½‘ç»œè¯·æ±‚çš„çº¿ç¨‹æ•°é‡num.network.threads=3#ç”¨æ¥å¤„ç†ç£ç›˜IOçš„çº¿ç¨‹æ•°é‡num.io.threads=8#å‘é€å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°socket.send.buffer.bytes=102400#æ¥å—å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°socket.receive.buffer.bytes=102400#è¯·æ±‚å¥—æ¥å­—çš„ç¼“å†²åŒºå¤§å°socket.request.max.bytes=104857600#kafkaè¿è¡Œæ—¥å¿—å­˜æ”¾çš„è·¯å¾„log.dirs=/home/hadoop/apps/kafka_2.11-0.10.1.0/logs/kafka#topicåœ¨å½“å‰brokerä¸Šçš„åˆ†ç‰‡ä¸ªæ•°num.partitions=2#ç”¨æ¥æ¢å¤å’Œæ¸…ç†dataä¸‹æ•°æ®çš„çº¿ç¨‹æ•°é‡num.recovery.threads.per.data.dir=1#segmentæ–‡ä»¶ä¿ç•™çš„æœ€é•¿æ—¶é—´ï¼Œè¶…æ—¶å°†è¢«åˆ é™¤log.retention.hours=168#æ»šåŠ¨ç”Ÿæˆæ–°çš„segmentæ–‡ä»¶çš„æœ€å¤§æ—¶é—´log.roll.hours=168#æ—¥å¿—æ–‡ä»¶ä¸­æ¯ä¸ªsegmentçš„å¤§å°ï¼Œé»˜è®¤ä¸º1Glog.segment.bytes=1073741824#å‘¨æœŸæ€§æ£€æŸ¥æ–‡ä»¶å¤§å°çš„æ—¶é—´log.retention.check.interval.ms=300000#æ—¥å¿—æ¸…ç†æ˜¯å¦æ‰“å¼€log.cleaner.enable=true#brokeréœ€è¦ä½¿ç”¨zookeeperä¿å­˜metaæ•°æ®zookeeper.connect=www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka0.10.1.0#zookeeperé“¾æ¥è¶…æ—¶æ—¶é—´zookeeper.connection.timeout.ms=6000#partion bufferä¸­ï¼Œæ¶ˆæ¯çš„æ¡æ•°è¾¾åˆ°é˜ˆå€¼ï¼Œå°†è§¦å‘flushåˆ°ç£ç›˜log.flush.interval.messages=10000#æ¶ˆæ¯bufferçš„æ—¶é—´ï¼Œè¾¾åˆ°é˜ˆå€¼ï¼Œå°†è§¦å‘flushåˆ°ç£ç›˜log.flush.interval.ms=3000#åˆ é™¤topicéœ€è¦server.propertiesä¸­è®¾ç½®delete.topic.enable=trueå¦åˆ™åªæ˜¯æ ‡è®°åˆ é™¤delete.topic.enable=true#æ­¤å¤„çš„host.nameä¸ºæœ¬æœºIP(é‡è¦),å¦‚æœä¸æ”¹,åˆ™å®¢æˆ·ç«¯ä¼šæŠ›å‡º:Producer connection to localhost:9092 unsuccessful é”™è¯¯!host.name=www.hadoop01.com åˆ†å‘åˆ°å…¶ä»–æœºå™¨12scp -rp kafka_2.11-0.10.1.0/ hadoop@www.hadoop02.com:/home/hadoop/apps/scp -rp kafka_2.11-0.10.1.0/ hadoop@www.hadoop03.com:/home/hadoop/apps/ ä¿®æ”¹server.properties ä»¥ä¸‹ä¸¤ä¸ªå‚æ•°12broker.id=0host.name=www.hadoop01.com zookeeperè®¾ç½®ï¼Œå¯åŠ¨ç•¥ã€‚ åˆ›å»ºä¸»é¢˜ 12345678910[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --replication-factor 1 --partitions 1 --topic wordsCreated topic "words".[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --replication-factor 1 --partitions 1 --topic countsCreated topic "counts".[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --describe --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --topic words Topic:words PartitionCount:1 ReplicationFactor:1 Configs: Topic: words Partition: 0 Leader: 2 Replicas: 2 Isr: 2[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --describe --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --topic counts Topic:counts PartitionCount:1 ReplicationFactor:1 Configs: Topic: counts Partition: 0 Leader: 2 Replicas: 2 Isr: 2 ä¾æ¬¡å¯åŠ¨ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.Arrays;import java.util.Properties;import java.util.concurrent.atomic.AtomicLong;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.DoubleDeserializer;import org.apache.kafka.common.serialization.IntegerDeserializer;import org.apache.kafka.common.serialization.StringDeserializer;public class DemoConsumerManualCommit &#123; public static void main(String[] args) throws Exception &#123; args = new String[] &#123; "www.hadoop01.com:9092", "gender-amount", "group4", "consumer2" &#125;; if (args == null || args.length != 4) &#123; System.err.println( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"); System.exit(1); &#125; String bootstrap = args[0]; String topic = args[1]; String groupid = args[2]; String clientid = args[3]; Properties props = new Properties(); props.put("bootstrap.servers", bootstrap); props.put("group.id", groupid); props.put("enable.auto.commit", "false"); props.put("key.deserializer", StringDeserializer.class.getName()); //props.put("value.deserializer", DoubleDeserializer.class.getName()); props.put("value.deserializer", IntegerDeserializer.class.getName()); props.put("max.poll.interval.ms", "300000"); props.put("max.poll.records", "500"); props.put("auto.offset.reset", "earliest"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(topic)); AtomicLong atomicLong = new AtomicLong(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); records.forEach(record -&gt; &#123; System.out.printf("client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n", clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value()); if (atomicLong.get() % 10 == 0) &#123;// consumer.commitSync(); &#125; &#125;); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839import java.io.IOException;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.serialization.IntegerSerializer;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.kafka.common.serialization.StringSerializer;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.processor.TopologyBuilder;import org.apache.kafka.streams.state.Stores;public class WordCountTopology &#123; public static void main(String[] args) throws IOException &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount-processor"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "www.hadoop01.com:9092"); props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, "www.hadoop01.com:2181/kafka0.10.1.0"); props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass()); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); TopologyBuilder builder = new TopologyBuilder(); builder.addSource("SOURCE", new StringDeserializer(), new StringDeserializer(), "words") .addProcessor("WordCountProcessor", WordCountProcessor::new, "SOURCE") .addStateStore(Stores.create("Counts").withStringKeys().withIntegerValues().inMemory().build(), "WordCountProcessor")// .connectProcessorAndStateStores("WordCountProcessor", "Counts") .addSink("SINK", "count", new StringSerializer(), new IntegerSerializer(), "WordCountProcessor"); KafkaStreams stream = new KafkaStreams(builder, props); stream.start(); System.in.read(); stream.close(); stream.cleanUp(); &#125;&#125; å¯åŠ¨producer12kafka-console-producer.sh --broker-list www.hadoop01.com:9092 --topic wordshello apache hello kafka æŠ¥é”™ï¼š123456Exception in thread "StreamThread-1" org.apache.kafka.streams.errors.StreamsException: Extracted timestamp value is negative, which is not allowed. at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:111) at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117) at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:144) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:415) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:242) ä¸ºæ—¶é—´æˆ³çš„åŸå› kafka 18Mayçš„æ—¶å€™ä½•å¦‚äº†æ—¶é—´æˆ³çš„ä¸œä¸œåæ¥æ›´æ”¹äº†ä¸»é¢˜ï¼š12bin/kafka-topics.sh --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --create --topic word --partitions 1 --replication-factor 1 --config message.timestamp.type=LogAppendTimebin/kafka-topics.sh --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 --create --topic count --partitions 1 --replication-factor 1 --config message.timestamp.type=LogAppendTime å…ˆå¯åŠ¨DemoConsumerManualCommitå†å¯åŠ¨WordCountTopology å‘½ä»¤è¡Œè¾“å…¥12kafka-console-producer.sh --broker-list www.hadoop01.com:9092 --topic word_testhello apache kafka hello apache spark hello storm æœ€åæ§åˆ¶å°è¾“å‡º åº”è¯¥æ˜¯12345client : consumer2 , topic: count , partition: 0 , offset = 0, key = apache, value = 2client : consumer2 , topic: count , partition: 0 , offset = 1, key = hello, value = 3client : consumer2 , topic: count , partition: 0 , offset = 2, key = kafka, value = 1client : consumer2 , topic: count , partition: 0 , offset = 3, key = spark, value = 1client : consumer2 , topic: count , partition: 0 , offset = 4, key = storm, value = 1 æŸ¥çœ‹æ‰€æœ‰ä¸»é¢˜1bin/kafka-topics.sh --list --zookeeper www.hadoop01.com:2181/kafka0.10.1.0 å‘ç°å¤šäº†ä¸€ä¸ªstreams-wordcount-processor-Counts-changelog In the next section we present another way to build the processor topology: the Kafka Streams DSL. High-Level Streams DSLTo build a processor topology using the Streams DSL, developers can apply the KStreamBuilder class, which is extended from the TopologyBuilder. A simple example is included with the source code for Kafka in the streams/examples package. The rest of this section will walk through some code to demonstrate the key steps in creating a topology using the Streams DSL, but we recommend developers to read the full example source codes for details.ä½¿ç”¨DSL Streamsåˆ›å»ºtopologyï¼Œå¼€å‘äººå‘˜å¯ä»¥åº”ç”¨KStreamBuilderç±»ï¼Œè¿™æ˜¯ä»TopologyBuilderå»¶ä¼¸ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯åŒ…å«äº†streams/examples packageçš„æºä»£ç ã€‚æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†å°†é€šè¿‡ä¸€äº›ä»£ç æ¥æ¼”ç¤ºä½¿ç”¨æµDSLåˆ›å»ºtopologyçš„å…³é”®æ­¥éª¤ï¼Œä½†æˆ‘ä»¬å»ºè®®å¼€å‘è€…é˜…è¯»å®Œæ•´çš„ç¤ºä¾‹æºä»£ç çš„ç»†èŠ‚ã€‚ KStream and KTable The DSL uses two main abstractions. A KStream is an abstraction of a record stream, where each data record represents a self-contained datum in the unbounded data set. A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesnâ€™t exist yet, the update will be considered a create). To illustrate the difference between KStreams and KTables, letâ€™s imagine the following two data records are being sent to the stream: (â€œaliceâ€, 1) â€“&gt; (â€œaliceâ€, 3). If these records a KStream and the stream processing application were to sum the values it would return 4. If these records were a KTable, the return would be 3, since the last record would be considered as an update. DSLä½¿ç”¨çš„ä¸¤ç§ä¸»è¦çš„æŠ½è±¡ã€‚ä¸€ä¸ªKStreamæ˜¯è®°å½•æµçš„ä¸€ä¸ªæŠ½è±¡çš„æ¦‚å¿µï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®è®°å½•ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„æ•°æ®çš„æ•°æ®é›†ã€‚ä¸€ä¸ªKTableæ˜¯ä¸€ä¸ªå˜æ›´çš„æµçš„ä¸€ä¸ªæŠ½è±¡çš„æ¦‚å¿µï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®è®°å½•çš„æ›´æ–°ã€‚æ›´ç²¾ç¡®åœ°è¯´ï¼Œæ•°æ®è®°å½•ä¸­çš„å€¼è¢«è®¤ä¸ºæ˜¯åŒä¸€ä¸ªè®°å½•é”®çš„æœ€åä¸€ä¸ªå€¼çš„æ›´æ–°ï¼Œå¦‚æœæœ‰ï¼ˆå¦‚æœä¸€ä¸ªç›¸åº”çš„keyä¸å­˜åœ¨ï¼Œåˆ™è¯¥æ›´æ–°å°†è¢«è§†ä¸ºä¸€ä¸ªåˆ›å»ºï¼‰ã€‚è¯´æ˜kstreamså’ŒKTablesä¹‹é—´çš„å·®å¼‚ï¼Œè®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹ä»¥ä¸‹ä¸¤ä¸ªæ•°æ®è®°å½•è¢«å‘é€åˆ°æµï¼š(â€œaliceâ€, 1)â€”â€”&gt;(â€œaliceâ€, 3)ã€‚å¦‚æœè¿™äº›è®°å½•KStreamå’Œæµå¤„ç†åº”ç”¨è¿›è¡Œæ€»ç»“çš„å€¼å°†è¿”å›4ã€‚å¦‚æœè¿™äº›è®°å½•æ˜¯ä¸€ä¸ªKTableï¼Œè¿”å›çš„è¿”å›çš„å°†æ˜¯3ï¼Œå› ä¸ºè¿‡å»çš„è®°å½•å°†è¢«è§†ä¸ºä¸€ç§æ›´æ–°ã€‚ Create Source Streams from Kafkaåˆ›å»ºKafka Streams Either a record stream (defined as KStream) or a changelog stream (defined as KTable) can be created as a source stream from one or more Kafka topics (for KTable you can only create the source stream from a single topic).ä¸€ä¸ªè®°å½•æµï¼ˆå®šä¹‰ä¸ºKStreamï¼‰æˆ–æ›´æ–°æµï¼ˆå®šä¹‰ä¸ºKTableï¼‰å¯ä»¥åˆ›å»ºä»ä¸€ä¸ªæˆ–å¤šä¸ªKafkaä¸»é¢˜çš„æºæµï¼ˆä¸ºKTableä½ åªèƒ½ä»ä¸€ä¸ªå•ä¸€çš„ä¸»é¢˜åˆ›å»ºæºæµï¼‰ã€‚ 123KStreamBuilder builder = new KStreamBuilder();KStream source1 = builder.stream("topic1", "topic2");KTable source2 = builder.table("topic3", "stateStoreName"); Windowing a stream çª—å£æµA stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for join and aggregation operations, etc. Kafka Streams currently defines the following types of windows: æµå¤„ç†å™¨å¯èƒ½éœ€è¦å°†æ•°æ®è®°å½•åˆ’åˆ†æˆæ—¶é—´æ¡¶ï¼Œå³é€šè¿‡æ—¶é—´çª—å£çš„æµã€‚è¿™é€šå¸¸æ˜¯éœ€è¦çš„è¿æ¥å’Œèšåˆæ“ä½œç­‰ã€‚Kafkaæµç›®å‰å®šä¹‰äº†ä»¥ä¸‹ç±»å‹çš„çª—å£ï¼š Hopping time windows are windows based on time intervals. They modelfixed-sized, (possibly) overlapping windows. A hopping window isdefined by two properties: the windowâ€™s size and its advance interval(aka â€œhopâ€). The advance interval specifies by how much a windowmoves forward relative to the previous one. For example, you canconfigure a hopping window with a size 5 minutes and an advanceinterval of 1 minute. Since hopping windows can overlap a data recordmay belong to more than one such windows.è·³è·ƒçš„æ—¶é—´ï¼ˆè·³æ—¶?ï¼‰çª—å£æ˜¯åŸºäºæ—¶é—´é—´éš”çš„çª—å£ã€‚ä»–ä»¬æ¨¡å‹å›ºå®šå¤§å°çš„ï¼Œï¼ˆå¯èƒ½ï¼‰é‡å çš„çª—å£ã€‚è·³è·ƒçª—å£æ˜¯ç”±ä¸¤ä¸ªå±æ€§å®šä¹‰çš„ï¼šçª—å£çš„å¤§å°å’Œå®ƒçš„æå‰é—´éš”ï¼ˆåˆåâ€hopâ€ï¼‰ã€‚æå‰é—´éš”ç”±ä¸€ä¸ªçª—å£ç›¸å¯¹äºå‰ä¸€ä¸ªçª—å£ç§»åŠ¨çš„å¤šå°‘æ¥æŒ‡å®šã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥é…ç½®ä¸€ä¸ªå…·æœ‰å¤§å°5åˆ†é’Ÿå’Œä¸€ä¸ªæå‰é—´éš”1åˆ†é’Ÿçš„è·³è·ƒçª—å£ã€‚ç”±äºè·³è·ƒçª—å£å¯ä»¥é‡å æ•°æ®è®°å½•ï¼Œå¯èƒ½å±äºå¤šä¸ªè¿™æ ·çš„çª—å£ã€‚ Tumbling time windows are a special case of hopping time windows and,like the latter, are windows based on time intervals. They modelfixed-size, non-overlapping, gap-less windows. A tumbling window isdefined by a single property: the windowâ€™s size. A tumbling window isa hopping window whose window size is equal to its advance interval.Since tumbling windows never overlap, a data record will belong toone and only one window.ç¿»æ»šæ—¶é—´çª—å£æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æƒ…å†µä¸‹çš„è·³è·ƒæ—¶é—´çª—å£ï¼Œå’Œåè€…ä¸€æ ·ï¼Œæ˜¯åŸºäºæ—¶é—´é—´éš”çš„çª—å£ã€‚ä»–ä»¬æ¨¡å‹å›ºå®šçš„å¤§å°ï¼Œä¸é‡å ï¼Œæ— ç¼éš™çš„çª—å£ã€‚ä¸€ä¸ªç¿»æ»šçª—å£æ˜¯ç”±ä¸€ä¸ªå•ä¸€å±æ€§å®šä¹‰çš„ï¼šçª—å£çš„å¤§å°ã€‚ä¸€ä¸ªç¿»æ»šçª—å£æ˜¯ä¸€ä¸ªè·³è·ƒçš„çª—å£ï¼Œå®ƒçš„çª—å£å¤§å°ç­‰äºå®ƒçš„é¢„å…ˆé—´éš”ã€‚ç”±äºç¿»æ»šçš„çª—å£æ°¸è¿œä¸ä¼šé‡å ï¼Œæ•°æ®è®°å½•å°†å±äºä¸€ä¸ªå¹¶ä¸”åªæœ‰ä¸€ä¸ªçª—å£ã€‚ Sliding windows model a fixed-size window that slides continuouslyover the time axis; here, two data records are said to be included inthe same window if the difference of their timestamps is within thewindow size. Thus, sliding windows are not aligned to the epoch, buton the data record timestamps. In Kafka Streams, sliding windows areused only for join operations, and can be specified through theJoinWindows class.æ»‘åŠ¨çª—å£æ¨¡å‹ä¸€ä¸ªå›ºå®šå¤§å°çš„çª—å£ï¼Œå¹»ç¯ç‰‡ä¸æ–­åœ¨æ—¶é—´è½´ä¸Šï¼›åœ¨è¿™é‡Œï¼Œä¸¤ä¸ªæ•°æ®è®°å½•ï¼Œè¯´æ˜¯å¦‚æœä»–ä»¬çš„æ—¶é—´å·®å¼‚æ˜¯åœ¨çª—å£çš„å¤§å°ï¼ŒåŒ…æ‹¬åœ¨åŒä¸€ä¸ªçª—å£ã€‚å› æ­¤ï¼Œæ»‘åŠ¨çª—å£ä¸ä¸€è‡´çš„æ—¶ä»£ï¼Œä½†åœ¨æ•°æ®è®°å½•çš„æ—¶é—´æˆ³ã€‚Kafkaæµï¼Œæ»‘åŠ¨çª—å£åªç”¨äºè¿æ¥æ“ä½œï¼Œå¹¶å¯ä»¥é€šè¿‡JoinWindowsç±»æŒ‡å®šã€‚ JoinsA join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely. In Kafka Streams, you may perform the following join operations:ä¸€ä¸ªè¿æ¥æ“ä½œåŸºäºå®ƒä»¬çš„æ•°æ®è®°å½•çš„é”®åˆå¹¶ä¸¤ä¸ªæµï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªæ–°çš„æµã€‚åœ¨è®°å½•æµçš„åŠ å…¥é€šå¸¸éœ€è¦æ‰§è¡Œåœ¨è§†çª—åŸºç¡€å¦åˆ™è®°å½•å¿…é¡»ä¿æŒå±¥è¡ŒåŠ å…¥çš„æ•°é‡å¯ä»¥æ— é™å¢é•¿ã€‚åœ¨Kafka Streamsä¸­ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹è¿æ¥æ“ä½œï¼š KStream-to-KStream Joins are always windowed joins, since otherwisethe memory and state required to compute the join would growinfinitely in size. Here, a newly received record from one of thestreams is joined with the other streamâ€™s records within thespecified window interval to produce one result for each matchingpair based on user-provided ValueJoiner. A new KStream instancerepresenting the result stream of the join is returned from thisoperator.KStream-to-KStream Joinsæ€»æ˜¯çª—å£è¿æ¥ï¼Œå¦åˆ™å†…å­˜å’Œè®¡ç®—æ‰€éœ€çš„joinä¼šæ— é™å¢é•¿çš„å¤§å°ã€‚åœ¨è¿™é‡Œï¼Œæ–°æ¥æ”¶çš„è®°å½•ä»ä¸€æ¡æ•°æ®æµä¸å…¶ä»–æµçš„è®°å½•åœ¨æŒ‡å®šçš„çª—å£é—´éš”ä¸ºå¯¹æ¯ä¸€ä¸ªåŒ¹é…çš„åŸºäºç”¨æˆ·æä¾›çš„ValueJoineräº§ç”Ÿçš„ä¸€ä¸ªç»“æœã€‚ä¸€ä¸ªæ–°çš„KStreamå®ä¾‹è¡¨ç¤ºçš„åŠ å…¥å¯¼è‡´æµä»è¿™ä¸ªæ“ä½œç¬¦è¿”å›ã€‚ KTable-to-KTable Joins are join operations designed to be consistentwith the ones in relational databases. Here, both changelog streamsare materialized into local state stores first. When a new record isreceived from one of the streams, it is joined with the otherstreamâ€™s materialized state stores to produce one result for eachmatching pair based on user-provided ValueJoiner. A new KTableinstance representing the result stream of the join, which is also achangelog stream of the represented table, is returned from thisoperator.KTable-to-KTable Joins æ“ä½œè®¾è®¡ä¸å…³ç³»æ•°æ®åº“ä¸­çš„ä¸€è‡´è¡ŒåŠ¨ã€‚åœ¨è¿™é‡Œï¼Œæ— è®ºæ˜¯ä¿®æ”¹æµç‰©åŒ–åœ¨å½“åœ°å•†åº—ã€‚å½“ä¸€ä¸ªæ–°çš„è®°å½•ä»ä¸€ä¸ªæµçš„æ¥æ”¶ï¼Œå®ƒä¸å…¶ä»–æµçš„ç‰©åŒ–çŠ¶æ€å­˜å‚¨ä¸ºå¯¹æ¯ä¸€ä¸ªåŒ¹é…çš„åŸºäºç”¨æˆ·æä¾›çš„ValueJoineräº§ç”Ÿçš„ä¸€ä¸ªç»“æœã€‚ä¸€ä¸ªæ–°çš„KTableå®ä¾‹ä»£è¡¨äº†è¿æ¥æµï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªä»£è¡¨è¡¨æ›´æ–°æµï¼Œæ˜¯ä»è¿™ä¸ªæ“ä½œç¬¦è¿”å›ã€‚ KStream-to-KTable Joins allow you to perform table lookups against achangelog stream (KTable) upon receiving a new record from anotherrecord stream (KStream). An example use case would be to enrich astream of user activities (KStream) with the latest user profileinformation (KTable). Only records received from the record streamwill trigger the join and produce results via ValueJoiner, not viceversa (i.e., records received from the changelog stream will be usedonly to update the materialized state store). A new KStream instancerepresenting the result stream of the join is returned from thisoperator.KStream-to-KTable Joinså…è®¸ä½ æ‰§è¡Œè¡¨æŸ¥æ‰¾å’Œä¿®æ”¹æµï¼ˆktableï¼‰åœ¨ä»å¦ä¸€ä¸ªè®°å½•æµæ¥æ”¶ä¸€ä¸ªæ–°çš„è®°å½•ï¼ˆKStreamï¼‰ã€‚ä¸€ä¸ªä¾‹å­ä½¿ç”¨æ¡ˆä¾‹å°†ä¸°å¯Œç”¨æˆ·æ´»åŠ¨æµï¼ˆKStreamï¼‰æœ€æ–°çš„ç”¨æˆ·é…ç½®æ–‡ä»¶ä¿¡æ¯ï¼ˆKTableï¼‰ã€‚åªè®°å½•æ”¶åˆ°çš„è®°å½•æµä¼šè§¦å‘è¿æ¥å¹¶é€šè¿‡ValueJoineräº§ç”Ÿç»“æœï¼Œåä¹‹äº¦ç„¶ï¼ˆå³è®°å½•æ”¶åˆ°æ›´æ–°æµåªä¼šè¢«ç”¨æ¥æ›´æ–°ç‰©åŒ–çŠ¶æ€å­˜å‚¨ï¼‰ã€‚ä¸€ä¸ªæ–°çš„KStreamå®ä¾‹è¡¨ç¤ºçš„åŠ å…¥å¯¼è‡´æµä»è¿™ä¸ªæ“ä½œç¬¦è¿”å›ã€‚ Depending on the operands the following join operations are supported: inner joins, outer joins and left joins. Their semantics are similar to the corresponding operators in relational databases. aTransform a stream There is a list of transformation operations provided for KStream and KTable respectively. Each of these operations may generate either one or more KStream and KTable objects and can be translated into one or more connected processors into the underlying processor topology. All these transformation methods can be chained together to compose a complex processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types. Among these transformations, filter, map, mapValues, etc, are stateless transformation operations and can be applied to both KStream and KTable, where users can usually pass a customized function to these functions as a parameter, such as Predicate for filter, KeyValueMapper for map, etc:12// written in Java 8+, using lambda expressionsKStream mapped = source1.mapValue(record -&gt; record.get("category")); Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor; Stateful transformations, on the other hand, require accessing an associated state for processing and producing outputs. For example, in join and aggregate operations, a windowing state is usually used to store all the received records within the defined window boundary so far. The operators can then access these accumulated records in the store and compute based on them.1234567891011// written in Java 8+, using lambda expressionsKTable, Long&gt; counts = source1.groupByKey().aggregate( () -&gt; 0L, // initial value (aggKey, value, aggregate) -&gt; aggregate + 1L, // aggregating value TimeWindows.of("counts", 5000L).advanceBy(1000L), // intervals in milliseconds Serdes.Long() // serde for aggregated value);KStream joined = source1.leftJoin(source2, (record1, record2) -&gt; record1.get("user") + "-" + record2.get("region");); Write streams back to Kafka At the end of the processing, users can choose to (continuously) write the final resulted streams back to a Kafka topic through KStream.to and KTable.to.1joined.to("topic4"); If your application needs to continue reading and processing the records after they have been materialized to a topic via to above, one option is to construct a new stream that reads from the output topic; Kafka Streams provides a convenience method called through:12345// equivalent to//// joined.to("topic4");// materialized = builder.stream("topic4");KStream materialized = joined.through("topic4");]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis é›†ç¾¤]]></title>
    <url>%2F2016%2F12%2F02%2Fredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[åŒæ—¶åœ¨æ¯å°æœºå™¨é…ç½®å¦‚ä¸‹å†…å®¹ï¼šåˆ›å»ºä¸€ä¸ªç©ºçš„é›†ç¾¤æ–‡ä»¶ vi nodes-6379.conf :wq ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œå¼€å¯é›†ç¾¤æ¨¡å¼ vi redis.conf cluster-enabled yes cluster-config-file nodes-6379.conf å¯åŠ¨ï¼š ./src/redis-server redis.conf ç„¶åè¾“å…¥æŸ¥çœ‹å‘½ä»¤1234567891011[hadoop@www.hadoop01.com ~]$ps -ef | grep redishadoop 2657 2506 0 21:59 pts/0 00:00:00 ./src/redis-server *:6379 [cluster]hadoop 2695 2670 0 22:00 pts/1 00:00:00 grep redis [hadoop@www.hadoop02.com ~]$ps -ef | grep redishadoop 2991 2662 0 10:12 pts/0 00:00:01 ./src/redis-server *:6379 [cluster]hadoop 3039 2999 0 10:21 pts/1 00:00:00 grep redis [hadoop@www.hadoop03.com ~]$ps -ef | grep redishadoop 2482 2417 0 13:41 pts/0 00:00:01 ./src/redis-server *:6379 [cluster]hadoop 2530 2490 0 13:51 pts/1 00:00:00 grep redis æ¯ä¸ªåé¢éƒ½æœ‰ä¸€ä¸ªclusteræ­¤æ—¶å¹¶æ²¡æœ‰æŠŠèŠ‚ç‚¹æ·»åŠ åˆ°ä¸€ä¸ªé›†ç¾¤ï¼Œç”¨å‘½ä»¤æŸ¥çœ‹nodesä¸º112345678910111213[hadoop@www.hadoop01.com redis-3.0.7]$./src/redis-cli127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:5cluster_slots_ok:5cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:1cluster_size:1cluster_current_epoch:0cluster_my_epoch:0cluster_stats_messages_sent:0cluster_stats_messages_received:0 è¿™æ—¶éœ€è¦æ·»åŠ é›†ç¾¤èŠ‚ç‚¹ï¼š123456789101112131415161718127.0.0.1:6379&gt; Cluster meet www.hadoop02.com 6379(error) ERR Invalid node address specified: www.hadoop02.com:6379127.0.0.1:6379&gt; Cluster meet 192.168.247.152 6379OK127.0.0.1:6379&gt; Cluster meet 192.168.247.154 6379OK127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:5cluster_slots_ok:5cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:27cluster_stats_messages_received:26 å¯ä»¥çœ‹åˆ°èŠ‚ç‚¹æ•°ä¸º3-&gt;cluster_known_nodes:3ï¼Œä½†æ˜¯é›†ç¾¤çŠ¶æ€æ˜¯å¤±è´¥-&gt;cluster_state:fail åˆ†é…æ§½ä½ä¸€å…±16384ä¸ªæ§½ä½cluster addslots 0 1 2 â€¦ å†™ä¸ªè„šæœ¬ï¼š1234567vi fenpeicaowei.sh#bashfor i in &#123;10000..16384&#125;; do ./src/redis-cli cluster addslots $i; donechmod a+x fenpeicaowei.sh./fenpeicaowei.sh æŸ¥çœ‹æœ‰å“ªäº›èŠ‚ç‚¹(å› ä¸ºä¹‹å‰é…ç½®äº†ä¸»ä»ï¼Œè¯»è€…å¿½ç•¥slaveèŠ‚ç‚¹ï¼Œå¦å¤–ä»èŠ‚ç‚¹åˆ†é…æ§½ä½æ˜¯ä¸ä¼šè¢«åŒæ­¥çš„)12345./src/redis-cli -c127.0.0.1:6379&gt; cluster nodes276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481166569926 1 connectedda0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481166571948 1 connected64a2d824459edc01c7b6b205021d01da96513121 192.168.247.152:6379 myself,master - 0 0 1 connected 1584 2171 5649 9842 10000-16383 æŸ¥çœ‹æ§½ä½åˆ†é…åˆ°å“ªäº›èŠ‚ç‚¹1234567891011121314151617181920212223242526272829303132333435363738394041127.0.0.1:6379&gt; cluster slots1) 1) (integer) 1584 2) (integer) 1584 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63792) 1) (integer) 2171 2) (integer) 2171 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63793) 1) (integer) 5649 2) (integer) 5649 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63794) 1) (integer) 9842 2) (integer) 9842 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 63795) 1) (integer) 10000 2) (integer) 16383 3) 1) "192.168.247.152" 2) (integer) 6379 4) 1) "192.168.247.150" 2) (integer) 6379 5) 1) "192.168.247.154" 2) (integer) 6379 123456789101112127.0.0.1:6379&gt; cluster infocluster_state:failcluster_slots_assigned:6388cluster_slots_ok:6388cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:3002cluster_stats_messages_received:2999 å…¨éƒ¨åˆ†é…å®Œæˆï¼Œå°±å¯ä»¥çœ‹åˆ°123456789101112127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:3cluster_size:1cluster_current_epoch:1cluster_my_epoch:1cluster_stats_messages_sent:792cluster_stats_messages_received:798 æŸ¥çœ‹æŸä¸ªkeyå¯¹åº”çš„æ§½ä½12127.0.0.1:6379&gt; cluster keyslot 3(integer) 1584 é‡æ–°åˆ†ç‰‡redisæä¾›ä¸€ä¸ªå·¥å…·ï¼šredis-trib.rb ä½†æ˜¯è¦ç”¨è¿™ä¸ªå·¥å…·éœ€è¦å®‰è£…rubyç¯å¢ƒ ä¸‹è½½repoæ–‡ä»¶wget http://mirrors.163.com/.help/CentOS6-Base-163.repo å¤‡ä»½å¹¶æ›¿æ¢ç³»ç»Ÿçš„repoæ–‡ä»¶cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo.bakmv CentOS6-Base-163.repo CentOS-Base.repo æ‰§è¡Œyumæºæ›´æ–°å‘½ä»¤123456789yum clean allyum makecacheyum updateyum install -y ruby* --skip-broken yum install rubygemswget https://rubygems.global.ssl.fastly.net/gems/redis-3.2.2.gemgem install redisgem install -l ./redis-3.2.2.gem ç”¨redis-trib.rbåˆ›å»ºé›†ç¾¤./src/redis-trib.rb create â€“replicas 1 192.168.1.150:6379 192.168.1.152:6379 192.168.1.154:6379å¯èƒ½ä¼šæŠ¥é”™ï¼Œæ˜¯ç‰ˆæœ¬å¤ªä½çš„åŸå› è§£å†³æ–¹æ¡ˆï¼šftp://ftp.ruby-lang.org/pub/ruby/ ä¸‹è½½æœ€æ–°çš„rubyå®‰è£…åŒ… ruby-2.3.3.tar.gzgem update â€“system å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬2.5.1gem install redis å®‰è£…redisæ¨¡å— é‡æ–°æ‰§è¡Œ123[hadoop@www.hadoop02.com redis-3.0.7]$./src/redis-trib.rb create --replicas 1 192.168.247.150:6379 192.168.247.152:6379 192.168.247.154:6379&gt;&gt;&gt; Creating cluster[ERR] Node 192.168.247.150:6379 is not empty. Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0. å› ä¸ºåˆšæ‰å·²ç»åˆ†é…äº†èŠ‚ç‚¹ ç°åœ¨åšé‡æ–°åˆ†åŒºæŸ¥çœ‹åˆ†åŒºå‰ï¼š1234127.0.0.1:6379&gt; cluster nodes276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slave 64a2d824459edc01c7b6b205021d01da96513121 0 1481242407249 1 connected64a2d824459edc01c7b6b205021d01da96513121 192.168.247.152:6379 master - 0 1481242405165 1 connected 0-16383da0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 myself,slave 64a2d824459edc01c7b6b205021d01da96513121 0 0 0 connected åˆ†åŒºï¼š12345678910111213141516171819202122232425262728293031323334[hadoop@www.hadoop02.com redis-3.0.7]$./src/redis-trib.rb reshard 127.0.0.1:6379&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6379)M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379 slots:0-16383 (16384 slots) master 2 additional replica(s)S: 276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379 slots: (0 slots) slave replicates 64a2d824459edc01c7b6b205021d01da96513121S: da0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379 slots: (0 slots) slave replicates 64a2d824459edc01c7b6b205021d01da96513121[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 5000What is the receiving node ID? 276a3675a706d4626a25c54892c368ca6c37002c*** The specified node is not known or not a master, please retry.What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da9651312*** The specified node is not known or not a master, please retry.What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da96513121Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs.Source node #1:allReady to move 5000 slots. Source nodes: Destination node: M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379 slots:0-16383 (16384 slots) master 2 additional replica(s) Resharding plan:Do you want to proceed with the proposed reshard plan (yes/no)? yes rehashåªèƒ½åœ¨ä¸»èŠ‚ç‚¹è¿›è¡Œï¼Œæ•…247.152åˆ†é…ç»™äº†247.152ï¼Œæ²¡æœ‰å˜åŒ–]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hbase regionå¤„ç†]]></title>
    <url>%2F2016%2F12%2F01%2FRegionServer%2F</url>
    <content type="text"><![CDATA[å‡å°‘RegionServerèŠ‚ç‚¹é€€å½¹ï¼šåœ¨åˆ¶å®šèŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œåœæ­¢çš„èŠ‚ç‚¹ä¸Šçš„RegionServer1./bin/hbase-daemon.sh stop regionserver å¦‚æœåœ¨å…³é—­æ—¶Hbaseå­˜åœ¨æ­£åœ¨è¿è¡Œçš„è´Ÿè½½å‡è¡¡ï¼Œé‚£ä¹ˆMaseré’ˆå¯¹Regionçš„æ¢å¤æ“ä½œå’Œè´Ÿè½½å‡è¡¡ä¹‹å‰å¯èƒ½ä¼šäº§ç”Ÿç«äº‰ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç¢°åˆ°çš„å‡ ç‡æ›´å¤§ï¼Œæ‰€ä»¥å»ºè®®ç°é‡‘ç”¨è´Ÿè½½å‡è¡¡ 12./hbase shell./hbase(main):001:0&gt;blance_switch false æ»šåŠ¨é‡å¯1./bin/graceful_stop slave2 å¦‚æœåŒæ—¶é€€å½¹å¤šä¸ªèŠ‚ç‚¹ï¼Œä¸ºäº†é˜²æ­¢æ­£åœ¨é€€å½¹çš„èŠ‚ç‚¹æŠŠregion moveåˆ°å³å°†é€€å½¹çš„èŠ‚ç‚¹ä¸Šå¯ä»¥åˆ©ç”¨zookeeperï¼Œåœ¨Hbase_root/training znodeä¸­åˆ›å»ºè¦é€€å½¹çš„RegionServerçš„entryæ»šåŠ¨é‡å¯ä¸‰ç§æ–¹å¼ï¼šä½¿ç”¨rolling-restart.shè„šæœ¬æ‰‹å·¥è¿›è¡Œæ»šåŠ¨é‡å¯ç¼–å†™è‡ªå®šä¹‰çš„æ»šåŠ¨é‡å¯è„šæœ¬ å¤‡Masterä¸ºé¿å…Masteræ•…éšœï¼Œæ·»åŠ å¤‡ç”¨Masteråœ¨æ‰€æœ‰èŠ‚ç‚¹çš„confç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶backup-mastersåœ¨é‡Œé¢æ·»åŠ å¤‡ä»½ä¸»æœºå å¯åŠ¨ï¼š1./bin/hbase-daemon.sh start master å¦‚æœå‘ç°ç«¯å£å†²çªï¼Œéœ€è¦ä¿®æ”¹hbase-site.xml1234&lt;property&gt;&lt;name&gt;hbase.regionserver.port&lt;/name&gt;&lt;value&gt;16012&lt;/value&gt;&lt;/property&gt; æŸ¥çœ‹è¿›ç¨‹å ç”¨ç«¯å£å·ï¼š1netstat -an -p | grep -i PID å¢åŠ èŠ‚ç‚¹åœ¨conf/regionserverså¢åŠ èŠ‚ç‚¹ï¼Œç›´æ¥startä¼šè·³è¿‡å·²ç»å¯åŠ¨çš„regionserver Hbaseå†·å¤‡éœ€è¦åœæ­¢hbasedistcpè¯¥å‘½ä»¤ä½äºhdoopçš„toolsåŒ…ä¸­hadoop distcp /hbase /hbasebackup hbase-site.xmlé‡Œé¢æœ‰hbase.rootdirå‚æ•°ï¼Œå¯ä»¥ä¿®æ”¹ä¸ºå¤‡ä»½åçš„è·¯å¾„æ¥æ¢å¤æ•°æ® æ“ä½œæµç¨‹ï¼š123456789101112131415161718192021222324./bin/hbase shellcreate 'testbackup' 'columnfamily'put 'testbackup','row1','columnfamily:column1','value1'scan 'testbackup'./bin/stop-hbase.shhadoop distcp /hbae /hbasebackup./bin/start-hbase.sh./bin/hbase shelldisable 'testbackup'drop 'testbackup'list./bin/stop-hbase.shhdfs dfs -mv /hbase /hbase_tmphadoop distcp -overwrite /hbasebackupd /hbase./bin/start-hbase.sh./bin/hbase shelllistscan 'testbackup' Hbaseçƒ­å¤‡ä¸éœ€è¦åœæ­¢hbaseæ–¹å¼ä¸€ï¼šCopyTbleæ–¹å¼äºŒï¼šExportæ–¹å¼ä¸‰ï¼šé›†ç¾¤å¤åˆ¶ æ–¹å¼äºŒ12345678910111213./bin/hbase org.apache.hadoop.hbase.mapreduce.Export testbackup /tmp/test./bin/hbase org.apache.hadoop.hbase.mapreduce.Export testbackup hdfs://www.hadoop01.com/test./bin/hbase shelldisbale testbackupdrop testbackupcreate 'testbackup' 'columnfamily'scan testbackup.Import testbackup /tmp/test./bin/hbase shellscan testbackup æ–¹å¼ä¸€ï¼š12345create 'mubiao' 'columnfamily'./bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=mubiao testbackup./hbase shellscan 'mubiao']]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker_install]]></title>
    <url>%2F2016%2F11%2F27%2Fdocker-install%2F</url>
    <content type="text"><![CDATA[1.å¢åŠ yumæºåœ¨å‘½ä»¤è¡Œè¾“å…¥ï¼š12345678cat &gt;/etc/yum.repos.d/docker.repo &lt;&lt;-EOF[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/6enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF é€šè¿‡yumåŠ è½½docker1yum install docker-engine 2.ç¦ç”¨selinux#####2.1.æŸ¥çœ‹selinuxçŠ¶æ€ [root@localhost ~]# cat /etc/selinux/config12345678910111213141516171819202122232425# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=enforcing# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted``` 2.2.ä¿®æ”¹è¯¥é…ç½®æ–‡ä»¶ä¸­å°†enforcingæ›¿æ¢ä¸ºdisabled [root@localhost ~]# cat /etc/selinux/config```js# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted 2.3.Dockerä½¿ç”¨érootç”¨æˆ·å°†å½“å‰ç”¨æˆ·åŠ å…¥dockerç»„sudo gpasswd -a ${USER} docker 2.4.ç„¶åé‡å¯restart 3.å¯åŠ¨docker123456789101112131415161718/etc/init.d/docker start[hadoop@www.hadoop02.com ~]$ps -ef | grep docker root 2398 1 0 19:26 ? 00:00:00 /usr/bin/docker -dhadoop 2817 2761 0 19:27 pts/0 00:00:00 grep docker[hadoop@www.hadoop02.com ~]$docker versionClient version: 1.7.1Client API version: 1.19Go version (client): go1.4.2Git commit (client): 786b29dOS/Arch (client): linux/amd64Get http:///var/run/docker.sock/v1.19/version: dial unix /var/run/docker.sock: permission denied. Are you trying to connect to a TLS-enabled daemon without TLS?[hadoop@www.hadoop02.com ~]$docker -d FATA[0000] Error starting daemon: open /var/run/docker.pid: permission denied æ·»åŠ hadoopç”¨æˆ·åˆ°dockerç»„12[hadoop@www.hadoop02.com run]$sudo gpasswd -a hadoop dockerAdding user hadoop to group docker æŸ¥çœ‹dockerä¿¡æ¯12345678910111213141516171819202122232425262728[hadoop@www.hadoop02.com ~]$docker infoContainers: 0Images: 0Storage Driver: devicemapper Pool Name: docker-253:0-798129-pool Pool Blocksize: 65.54 kB Backing Filesystem: extfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 305.7 MB Data Space Total: 107.4 GB Data Space Available: 3.967 GB Metadata Space Used: 729.1 kB Metadata Space Total: 2.147 GB Metadata Space Available: 2.147 GB Udev Sync Supported: true Deferred Removal Enabled: false Data loop file: /var/lib/docker/devicemapper/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata Library Version: 1.02.95-RHEL6 (2015-04-15)Execution Driver: native-0.2Logging Driver: json-fileKernel Version: 2.6.32-573.el6.x86_64Operating System: &lt;unknown&gt;CPUs: 1Total Memory: 1.82 GiBName: www.hadoop02.comID: XL2K:2DEZ:GF27:JL4P:AHWJ:HEGF:QNQ7:HVT3:3SLS:75GJ:LRQM:E7OJ å¯ä»¥ç”¨ip addrçœ‹docker0ç½‘æ¡¥åˆ†é…äº†ä¸€ä¸ªç§æœ‰ç½‘æ®µ12345docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN link/ether 26:75:34:2e:06:26 brd ff:ff:ff:ff:ff:ff inet 172.17.42.1/16 scope global docker0 inet6 fe80::2475:34ff:fe2e:626/64 scope link valid_lft forever preferred_lft forever ä¸Šä¼ dockeræ–‡ä»¶buildæ–‡ä»¶ï¼Œæ³¨æ„æœ€åæœ‰ä¸ªç‚¹docker build -t hadoop/zookeeper:3.4.6 -f zookeeper.Dockerfile .å‘ç°æŠ¥é”™ï¼ŒåŸå› æ˜¯æ²¡æœ‰å®‰è£…REPL 12345wget http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpmrpm -ivh epel-release-6-8.noarch.rpmyum repolistyum makecache ä¸æ‰§è¡Œæ­¤å¥ä¹Ÿå¯ æ‰§è¡Œdocker search centoså¯ä»¥çœ‹åˆ°ä¿¡æ¯123456789101112131415161718192021222324252627docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 2757 [OK] jdeathe/centos-ssh CentOS-6 6.8 x86_64 / CentOS-7 7.2.1511 x8... 42 [OK]jdeathe/centos-ssh-apache-php CentOS-6 6.8 x86_64 / Apache / PHP / PHP M... 21 [OK]nimmis/java-centos This is docker images of CentOS 7 with dif... 16 [OK]gluster/gluster-centos Official GlusterFS Image [ CentOS7 + Glus... 12 [OK]million12/centos-supervisor Base CentOS-7 with supervisord launcher, h... 12 [OK]torusware/speedus-centos Always updated official CentOS docker imag... 8 [OK]nickistre/centos-lamp LAMP on centos setup 7 [OK]kinogmt/centos-ssh CentOS with SSH 6 [OK]nathonfowlie/centos-jre Latest CentOS image with the JRE pre-insta... 4 [OK]centos/mariadb55-centos7 3 [OK]consol/sakuli-centos-xfce Sakuli JavaScript based end-2-end testing ... 2 [OK]timhughes/centos Centos with systemd installed and running 1 [OK]blacklabelops/centos CentOS Base Image! Built and Updates Daily! 1 [OK]darksheer/centos Base Centos Image -- Updated hourly 1 [OK]harisekhon/centos-scala Scala + CentOS (OpenJDK tags 2.10-jre7 - 2... 1 [OK]harisekhon/centos-java Java on CentOS (OpenJDK, tags jre/jdk7-8) 1 [OK]repositoryjp/centos Docker Image for CentOS. 0 [OK]labengine/centos Centos image base 0 [OK]smartentry/centos centos with smartentry 0 [OK]januswel/centos yum update-ed CentOS image 0 [OK]vcatechnology/centos A CentOS Image which is updated daily 0 [OK]grayzone/centos auto build for centos. 0 [OK]ustclug/centos USTC centos 0 [OK]kz8s/centos Official CentOS plus epel-release 0 [OK] é‡æ–°æ‰§è¡Œdocker build 12345678[hadoop@www.hadoop02.com ~]$docker run -itd --name zookeeper -h zookeeper -p 2181:2181 jason/zookeeper:3.4.6 bashb023a24424772462cc8d63e79dc3c3fe9e06dd0825ad617c65006e1ef331656d[hadoop@www.hadoop02.com ~]$docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb023a2442477 jason/zookeeper:3.4.6 "sh /opt/zookeeper/s 11 seconds ago Up 8 seconds 0.0.0.0:2181-&gt;2181/tcp zookeeper docker build -t hadoop/kafka:0.8.2.2 -f zookeeper.Dockerfile .]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka_high_level_apiæµ‹è¯•]]></title>
    <url>%2F2016%2F11%2F25%2Fkafka_high_level_api%2F</url>
    <content type="text"><![CDATA[1.æŸ¥çœ‹å®˜ç½‘APIKafka API Automatic Offset Committing1234567891011121314Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("foo", "bar")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; Manual Offset Control123456789101112131415161718192021Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "false"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("foo", "bar")); final int minBatchSize = 200; List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = new ArrayList&lt;&gt;(); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; buffer.add(record); &#125; if (buffer.size() &gt;= minBatchSize) &#123; insertIntoDb(buffer); consumer.commitSync(); buffer.clear(); &#125; &#125; å¦å¤–auto.offset.resetè¿™ä¸ªå‚æ•°å¯ä»¥è®¾ç½®ä¸ºsmallestå’Œlargestlargestè¡¨ç¤ºæ¥å—æ¥æ”¶æœ€å¤§çš„offset(å³æœ€æ–°æ¶ˆæ¯),smallestè¡¨ç¤ºæœ€å°offset,å³ä»topicçš„å¼€å§‹ä½ç½®æ¶ˆè´¹æ‰€æœ‰æ¶ˆæ¯. Name Description Type Default Valid Values Importance auto.offset.reset What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted) earliest: automatically reset the offset to the earliest offset latest: automatically reset the offset to the latest offset. none: throw exception to the consumer if no previous offset is found for the consumerâ€™s group anything else: throw exception to the consumer. string latest [latest, earliest, none] medium kafka high level APIæäº¤offseté¦–å…ˆåˆ¤æ–­config.offsetsStorage == â€œzookeeperâ€å¦‚æœæ˜¯zkï¼Œé€šè¿‡foreachæäº¤æ‰€æ¶ˆè´¹çš„æ‰€æœ‰çš„topicçš„æ‰€æœ‰çš„partitionçš„offsetã€‚æäº¤çš„æ–¹å¼æ˜¯commitOffsetToZooKeeper-&gt;updatePersistentPathå¦‚æœæ˜¯kafkaä¼šé€šè¿‡NIOçš„offsetchannelæäº¤å½“å‰çš„offsetåˆ°broker kafka high level APIæµ‹è¯•å¯åŠ¨kafka1bin/kafka-server-start.sh config/server.properties &amp; nohup kafka-server-start.sh /home/hadoop/apps/kafka_2.11-0.9.0.1/config/server.properties &amp; åˆ›å»ºtopic_high_level_api_test1bin/kafka-topics.sh --create --zookeeper www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka --replication-factor 1 --partitions 3 --topic topic_high_level_api_test å¯åŠ¨kafka high level demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package kafka.javaapi.consumer;import kafka.consumer.Consumer;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.message.MessageAndMetadata;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;/** * Created by zhanghongming on 2016/11/26. */public class DemoHighLevelConsumer &#123; public static void main(String[] args) &#123; args = new String[]&#123;"www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka", "topic_high_level_api_test", "group1", "consumer1"&#125;; if (args == null || args.length != 4) &#123; System.err.print( "Usage:\n\tjava -jar kafka_consumer.jar $&#123;zookeeper_list&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;consumer_id&#125;"); System.exit(1); &#125; String zk = args[0]; String topic = args[1]; String groupid = args[2]; String consumerid = args[3]; Properties props = new Properties(); props.put("zookeeper.connect", zk); props.put("zookeeper.session.timeout.ms", "3600000"); props.put("group.id", groupid); props.put("client.id", "test"); props.put("consumer.id", consumerid); props.put("auto.offset.reset","smallest");// "largest"); props.put("auto.commit.enable", "true"); props.put("auto.commit.interval.ms", "60000"); ConsumerConfig consumerConfig = new ConsumerConfig(props); ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(consumerConfig); Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(topic, 1); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumerConnector.createMessageStreams(topicCountMap); KafkaStream&lt;byte[], byte[]&gt; stream1 = consumerMap.get(topic).get(0); ConsumerIterator&lt;byte[], byte[]&gt; it1 = stream1.iterator(); while (it1.hasNext()) &#123; MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it1.next(); String message = String.format("Consumer ID:%s, Topic:%s, GroupID:%s, PartitionID:%s, Offset:%s, Message Key:%s, Message Payload: %s", consumerid, messageAndMetadata.topic(), groupid, messageAndMetadata.partition(), messageAndMetadata.offset(), new String(messageAndMetadata.key()),new String(messageAndMetadata.message())); System.out.println(message); &#125; &#125;&#125; å¯åŠ¨ç”Ÿäº§è€…ç”¨éšæœºç®—æ³•ç”Ÿäº§æ•°æ®12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package kafka;import java.util.ArrayList;import java.util.List;import java.util.Properties;import java.util.Scanner;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import kafka.serializer.StringEncoder;public class ProducerDemo &#123; static private final String TOPIC = "topic_high_level_api_test"; static private final String ZOOKEEPER = "www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka"; static private final String BROKER_LIST = "www.hadoop01.com:9092,www.hadoop02.com:9092,www.hadoop03.com:9092";// static private final int PARTITIONS = TopicAdmin.partitionNum(ZOOKEEPER, TOPIC); static private final int PARTITIONS = 3; public static void main(String[] args) throws Exception &#123; Producer&lt;String, String&gt; producer = initProducer(); System.out.print("1111"); sendOne(producer, TOPIC); &#125; private static Producer&lt;String, String&gt; initProducer() &#123; Properties props = new Properties(); props.put("metadata.broker.list", BROKER_LIST); // props.put("serializer.class", "kafka.serializer.StringEncoder"); props.put("serializer.class", StringEncoder.class.getName()); props.put("partitioner.class", RoundRobinPartitioner.class.getName()); // props.put("partitioner.class", "kafka.producer.DefaultPartitioner");// props.put("compression.codec", "0"); props.put("producer.type", "async"); props.put("batch.num.messages", "3"); props.put("queue.buffer.max.ms", "10000000"); props.put("queue.buffering.max.messages", "1000000"); props.put("queue.enqueue.timeout.ms", "20000000"); ProducerConfig config = new ProducerConfig(props); Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config); return producer; &#125; public static void sendOne(Producer&lt;String, String&gt; producer, String topic) throws InterruptedException &#123; KeyedMessage&lt;String, String&gt; message1 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 31"); producer.send(message1); KeyedMessage&lt;String, String&gt; message2 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 32"); producer.send(message2); KeyedMessage&lt;String, String&gt; message3 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 33"); producer.send(message3); KeyedMessage&lt;String, String&gt; message4 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 34"); producer.send(message4); KeyedMessage&lt;String, String&gt; message5 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 35"); producer.send(message5); KeyedMessage&lt;String, String&gt; message6 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 36"); producer.send(message6); KeyedMessage&lt;String, String&gt; message7 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 37"); producer.send(message7); KeyedMessage&lt;String, String&gt; message8 = new KeyedMessage&lt;String, String&gt;(topic, "31", "test 38"); producer.send(message8); producer.close(); &#125;&#125; æŸ¥çœ‹kafka high level demoæ‰“å°çš„ä¿¡æ¯1234567Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:0, Message Key:31, Message Payload: test 32Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:0, Message Key:31, Message Payload: test 31Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:0, Offset:0, Message Key:31, Message Payload: test 33Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:1, Message Key:31, Message Payload: test 35Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:2, Offset:2, Message Key:31, Message Payload: test 38Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:1, Message Key:31, Message Payload: test 34Consumer ID:consumer1, Topic:topic_high_level_api_test, GroupID:group1, PartitionID:1, Offset:2, Message Key:31, Message Payload: test 37 æŸ¥çœ‹zookeeperé‡Œçš„ä¿¡æ¯ 123456789101112131415161718192021222324252627282930313233343536373839[zk: localhost:2181(CONNECTED) 92] get /kafka/consumers/group1/offsets/topic_high_level_api_test/13cZxid = 0x36000000f4ctime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f7mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000f4cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0[zk: localhost:2181(CONNECTED) 93] get /kafka/consumers/group1/offsets/topic_high_level_api_test/23cZxid = 0x36000000f1ctime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f6mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000f1cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0[zk: localhost:2181(CONNECTED) 94] get /kafka/consumers/group1/offsets/topic_high_level_api_test/01cZxid = 0x36000000eectime = Sat Nov 26 16:29:21 CST 2016mZxid = 0x36000000f5mtime = Sat Nov 26 16:31:21 CST 2016pZxid = 0x36000000eecversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 1numChildren = 0 å¯ä»¥å¯¹åº”çš„ä¸Šï¼Œåˆ†åŒº0æ¶ˆè´¹äº†1æ¡æ•°æ®ï¼Œåˆ†åŒº2å’Œ3æ¶ˆè´¹äº†3æ¡æ•°æ® å¦‚æœæˆ‘ä»¬é˜Ÿä¸Šè¿°æ•°æ®åšå¤„ç†ï¼Œkafka high level demoé‡Œçš„auto.commit.enableè®¾ç½®ä¸ºfalse å¯åŠ¨kafka high level demoï¼Œå‘é€æ•°æ®ï¼Œå†é‡æ–°å¯åŠ¨kafka high level demo è¿˜ä¼šæ¶ˆè´¹ä¹‹å‰å‘é€çš„æ•°æ®ï¼Œæ˜¯å› ä¸ºoffsetæ²¡æœ‰æ›´æ”¹ æ­¤æ—¶æˆ‘ä»¬å¯ä»¥åœ¨ä»£ç é‡Œå¢åŠ æ‰‹å·¥æäº¤çš„ä»£ç 1consumerConnector.commitOffsets();]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sparkç®—å­]]></title>
    <url>%2F2016%2F04%2F05%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[sparkçš„ç®—å­åˆ†ä¸ºä¸¤å¤§ç±»Transformationså’ŒActionsTransformationsï¼š map(func) :è¿”å›ä¸€ä¸ªæ–°çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç”±æ¯ä¸ªåŸå…ƒç´ ç»è¿‡funcå‡½æ•°è½¬æ¢åç»„æˆ filter(func) : è¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”±ç»è¿‡funcå‡½æ•°åè¿”å›å€¼ä¸ºtrueçš„åŸå…ƒç´ ç»„æˆ flatMap(func) : ç±»ä¼¼äºmapï¼Œä½†æ˜¯æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ ï¼Œä¼šè¢«æ˜ å°„ä¸º0åˆ°å¤šä¸ªè¾“å‡ºå…ƒç´ ï¼ˆå› æ­¤ï¼Œfuncå‡½æ•°çš„è¿”å›å€¼æ˜¯ä¸€ä¸ªSeqï¼Œè€Œä¸æ˜¯å•ä¸€å…ƒç´ ï¼‰ mapPartitions(func) ç±»ä¼¼äºmapï¼Œä½†æ˜¯è¿è¡Œåœ¨ä¸åŒçš„RDDåˆ†åŒºï¼Œfuncå‡½æ•°è¿è¡Œåœ¨ä¸€ä¸ªRDDä¸Šå¿…é¡»æ˜¯ Iterator =&gt; Iterator ç±» mapPartitionsWithIndex(func) ç±»ä¼¼äºmapPartitions, è€Œä¸”è¿˜æä¾›äº†ä¸€ä¸ªè¡¨ç¤ºåˆ†åŒºç´¢å¼•çš„æ•´æ•°å€¼, funcå‡½æ•°è¿è¡Œåœ¨ä¸€ä¸ªRDDä¸Šå¿…é¡»æ˜¯ (Int, Iterator) =&gt; Iterator ç±»å‹. sample(withReplacement, frac, seed) : æ ¹æ®ç»™å®šçš„éšæœºç§å­seedï¼ŒéšæœºæŠ½æ ·å‡ºæ•°é‡ä¸ºfracçš„æ•°æ® union(otherDataset) : è¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”±åŸæ•°æ®é›†å’Œå‚æ•°è”åˆè€Œæˆ intersection(otherDataset) Return a new RDD that contains the intersection of elements in the source dataset and the argument. distinct([numTasks])) Return a new dataset that contains the distinct elements of the source dataset. groupByKey([numTasks]) : åœ¨ä¸€ä¸ªç”±ï¼ˆK,Vï¼‰å¯¹ç»„æˆçš„æ•°æ®é›†ä¸Šè°ƒç”¨ï¼Œè¿”å›ä¸€ä¸ªï¼ˆKï¼ŒSeq[V])å¯¹çš„æ•°æ®é›†ã€‚æ³¨æ„ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨8ä¸ªå¹¶è¡Œä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œä½ å¯ä»¥ä¼ å…¥numTaskå¯é€‰å‚æ•°ï¼Œæ ¹æ®æ•°æ®é‡è®¾ç½®ä¸åŒæ•°ç›®çš„Task reduceByKey(func, [numTasks]) : åœ¨ä¸€ä¸ªï¼ˆKï¼ŒV)å¯¹çš„æ•°æ®é›†ä¸Šä½¿ç”¨ï¼Œè¿”å›ä¸€ä¸ªï¼ˆKï¼ŒVï¼‰å¯¹çš„æ•°æ®é›†ï¼Œkeyç›¸åŒçš„å€¼ï¼Œéƒ½è¢«ä½¿ç”¨æŒ‡å®šçš„reduceå‡½æ•°èšåˆåˆ°ä¸€èµ·ã€‚å’Œgroupbykeyç±»ä¼¼ï¼Œä»»åŠ¡çš„ä¸ªæ•°æ˜¯å¯ä»¥é€šè¿‡ç¬¬äºŒä¸ªå¯é€‰å‚æ•°æ¥é…ç½®çš„ã€‚ aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral â€œzeroâ€ value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. sortByKey([ascending], [numTasks]) When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.-join(otherDataset, [numTasks]) : åœ¨ç±»å‹ä¸ºï¼ˆK,V)å’Œï¼ˆK,W)ç±»å‹çš„æ•°æ®é›†ä¸Šè°ƒç”¨ï¼Œè¿”å›ä¸€ä¸ªï¼ˆK,(V,W))å¯¹ï¼Œæ¯ä¸ªkeyä¸­çš„æ‰€æœ‰å…ƒç´ éƒ½åœ¨ä¸€èµ·çš„æ•°æ®é›† groupWith(otherDataset, [numTasks]) : åœ¨ç±»å‹ä¸ºï¼ˆK,V)å’Œ(K,W)ç±»å‹çš„æ•°æ®é›†ä¸Šè°ƒç”¨ï¼Œè¿”å›ä¸€ä¸ªæ•°æ®é›†ï¼Œç»„æˆå…ƒç´ ä¸ºï¼ˆK, Seq[V], Seq[W]) Tuplesã€‚è¿™ä¸ªæ“ä½œåœ¨å…¶å®ƒæ¡†æ¶ï¼Œç§°ä¸ºCoGroup cogroup(otherDataset, [numTasks]) When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable, Iterable)) tuples. This operation is also called groupWith. cartesian(otherDataset) : ç¬›å¡å°”ç§¯ã€‚ä½†åœ¨æ•°æ®é›†Tå’ŒUä¸Šè°ƒç”¨æ—¶ï¼Œè¿”å›ä¸€ä¸ª(Tï¼ŒUï¼‰å¯¹çš„æ•°æ®é›†ï¼Œæ‰€æœ‰å…ƒç´ äº¤äº’è¿›è¡Œç¬›å¡å°”ç§¯ã€‚ pipe(command, [envVars]) Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the processâ€™s stdin and lines output to its stdout are returned as an RDD of strings. coalesce(numPartitions) Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset. repartition(numPartitions) Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. repartitionAndSortWithinPartitions(partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery. ä»¥ä¸ŠTransformationç®—å­è¯¦ç»†åˆåˆ†ä¸ºå¦‚ä¸‹ä¸¤ç±»ï¼š1.Valueæ•°æ®ç±»å‹çš„Transformationç®—å­ã€‚è¾“å…¥åˆ†åŒºä¸è¾“å‡ºåˆ†åŒºä¸€å¯¹ä¸€å‹ mapã€flatMapã€mapPartitionsè¾“å…¥åˆ†åŒºä¸è¾“å‡ºåˆ†åŒºå¤šå¯¹ä¸€å‹ unionã€cartesianè¾“å…¥åˆ†åŒºä¸è¾“å‡ºåˆ†åŒºå¤šå¯¹å¤šå‹ groupByè¾“å‡ºåˆ†åŒºä¸ºè¾“å…¥åˆ†åŒºå­é›†å‹ filterã€distinctã€subtractã€sampleã€takeSampleCacheå‹ cacheã€persist 2.Key-Valueæ•°æ®ç±»å‹çš„Transfromationç®—å­ã€‚ç±»å‹ ç®—å­è¾“å…¥åˆ†åŒºä¸è¾“å‡ºåˆ†åŒºä¸€å¯¹ä¸€ mapValueså¯¹å•ä¸ªRDD combineByKeyã€reduceByKeyã€partitionByä¸¤ä¸ªRDDèšé›† Cogroupè¿æ¥ joinã€leftOutJoinã€rightOutJoin Actionsï¼š reduce(func) : é€šè¿‡å‡½æ•°funcèšé›†æ•°æ®é›†ä¸­çš„æ‰€æœ‰å…ƒç´ ã€‚Funcå‡½æ•°æ¥å—2ä¸ªå‚æ•°ï¼Œè¿”å›ä¸€ä¸ªå€¼ã€‚è¿™ä¸ªå‡½æ•°å¿…é¡»æ˜¯å…³è”æ€§çš„ï¼Œç¡®ä¿å¯ä»¥è¢«æ­£ç¡®çš„å¹¶å‘æ‰§è¡Œ collect() : åœ¨Driverçš„ç¨‹åºä¸­ï¼Œä»¥æ•°ç»„çš„å½¢å¼ï¼Œè¿”å›æ•°æ®é›†çš„æ‰€æœ‰å…ƒç´ ã€‚è¿™é€šå¸¸ä¼šåœ¨ä½¿ç”¨filteræˆ–è€…å…¶å®ƒæ“ä½œåï¼Œè¿”å›ä¸€ä¸ªè¶³å¤Ÿå°çš„æ•°æ®å­é›†å†ä½¿ç”¨ï¼Œç›´æ¥å°†æ•´ä¸ªRDDé›†Collectè¿”å›ï¼Œå¾ˆå¯èƒ½ä¼šè®©Driverç¨‹åºOOM count() : è¿”å›æ•°æ®é›†çš„å…ƒç´ ä¸ªæ•° take(n) : è¿”å›ä¸€ä¸ªæ•°ç»„ï¼Œç”±æ•°æ®é›†çš„å‰nä¸ªå…ƒç´ ç»„æˆã€‚æ³¨æ„ï¼Œè¿™ä¸ªæ“ä½œç›®å‰å¹¶éåœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šï¼Œå¹¶è¡Œæ‰§è¡Œï¼Œè€Œæ˜¯Driverç¨‹åºæ‰€åœ¨æœºå™¨ï¼Œå•æœºè®¡ç®—æ‰€æœ‰çš„å…ƒç´ (Gatewayçš„å†…å­˜å‹åŠ›ä¼šå¢å¤§ï¼Œéœ€è¦è°¨æ…ä½¿ç”¨ï¼‰ first() : è¿”å›æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆç±»ä¼¼äºtake(1)ï¼‰ take(n) Return an array with the first n elements of the dataset. takeSample(withReplacement, num, [seed]) Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed. takeOrdered(n, [ordering]) Return the first n elements of the RDD using either their natural order or a custom comparator. saveAsTextFile(path) : å°†æ•°æ®é›†çš„å…ƒç´ ï¼Œä»¥textfileçš„å½¢å¼ï¼Œä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œhdfsæˆ–è€…ä»»ä½•å…¶å®ƒhadoopæ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿã€‚Sparkå°†ä¼šè°ƒç”¨æ¯ä¸ªå…ƒç´ çš„toStringæ–¹æ³•ï¼Œå¹¶å°†å®ƒè½¬æ¢ä¸ºæ–‡ä»¶ä¸­çš„ä¸€è¡Œæ–‡æœ¬ saveAsSequenceFile(path) : å°†æ•°æ®é›†çš„å…ƒç´ ï¼Œä»¥sequencefileçš„æ ¼å¼ï¼Œä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸‹ï¼Œæœ¬åœ°ç³»ç»Ÿï¼Œhdfsæˆ–è€…ä»»ä½•å…¶å®ƒhadoopæ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿã€‚RDDçš„å…ƒç´ å¿…é¡»ç”±key-valueå¯¹ç»„æˆï¼Œå¹¶éƒ½å®ç°äº†Hadoopçš„Writableæ¥å£ï¼Œæˆ–éšå¼å¯ä»¥è½¬æ¢ä¸ºWritableï¼ˆSparkåŒ…æ‹¬äº†åŸºæœ¬ç±»å‹çš„è½¬æ¢ï¼Œä¾‹å¦‚Intï¼ŒDoubleï¼ŒStringç­‰ç­‰ï¼‰ saveAsObjectFile(path)(Java and Scala) Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile(). countByKey() Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. foreach(func) : åœ¨æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸Šï¼Œè¿è¡Œå‡½æ•°funcã€‚è¿™é€šå¸¸ç”¨äºæ›´æ–°ä¸€ä¸ªç´¯åŠ å™¨å˜é‡ï¼Œæˆ–è€…å’Œå¤–éƒ¨å­˜å‚¨ç³»ç»Ÿåšäº¤äº’]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windwosä¸‹å¯åŠ¨spark]]></title>
    <url>%2F2016%2F04%2F02%2Fwindwos%E4%B8%8B%E5%90%AF%E5%8A%A8spark%2F</url>
    <content type="text"><![CDATA[å®‰è£…scalaï¼ˆç•¥ï¼‰ä¸Šæ–‡å·²ç»å®‰è£…å¥½äº†hadoopä¸‹é¢å®‰è£…spark ä¸‹è½½å’ŒhadoopåŒ¹é…ç‰ˆæœ¬çš„sparkhttp://spark.apache.org/downloads.html åšä¸»ä¸‹è½½çš„spark-2.1.0-bin-hadoop2.6.tgzè§£è¯åˆ°æ·»åŠ ç¯å¢ƒå˜é‡D:\tool\spark-2.1.0-bin-hadoop2.6\bin åˆ°pathä¸‹ å°æç¤ºï¼šå¦‚æœè¿è¡Œsparkä»£ç çš„æ—¶å€™æç¤ºCould not locate executable null\bin\winutils.exe in the Hadoop binaries.è§£å†³æ–¹æ³•ï¼šSystem.setProperty(â€œhadoop.home.dirâ€, â€œD:\tool\hadoop-2.6.0â€); å¦‚æœå’Œwinutilsç›¸å…³çš„é—®é¢˜å¯èƒ½è¿˜éœ€è¦ä¸‹è½½winutilsåœ°å€ï¼šhttps://github.com/steveloughran/winutilsæŠŠå¯¹åº”hadoopç‰ˆæœ¬çš„winutilsæ”¾åœ¨hadoopçš„binç›®å½•ä¸‹ ç›´æ¥ç”¨spark-shellåœ¨cmdé‡Œå¯åŠ¨å³å¯ æµ‹è¯•ï¼šåŠ è½½æ–‡ä»¶val testlog=sc.textFile(â€œinst.iniâ€)è·å–ç¬¬ä¸€è¡Œæ•°æ®ã€testlog.firstè·å–è¡Œæ•°testlog.countè¿‡æ»¤åŒ…å«Generalçš„è¡Œval linesWithGeneral=testlog.filter(line =&gt; line.contains(â€œGeneralâ€))è¿‡æ»¤ä¸åŒ…å«Generalçš„è¡Œval linesNoWithGeneral=testlog.filter(line =&gt; !line.contains(â€œGeneralâ€))å¾ªç¯æ‰“å°linesNoWithGeneral.collect.foreach(println)æ±‚å•è¯æœ€å¤šçš„è¡Œå•è¯æ•°testlog.map(line =&gt; line.split(â€œ â€œ).size).reduce((a, b) =&gt; if (a &gt; b) a else b)ä¹Ÿå¯ä»¥å¼•ç”¨æ•°å­¦å·¥å…·ï¼Œå†™æˆtextFile.map(line =&gt; line.split(â€œ â€œ).size).reduce((a, b) =&gt; Math.max(a, b))è®¡ç®—wordcountï¼ˆæ­¤å¤„ä½¿ç”¨reduceByKeyä¼šç”ŸæˆShuffledRDDï¼‰val wordCounts = textFile.flatMap(line =&gt; line.split(â€œ â€œ)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b) å†™ä¸ªç¨‹åºåœ¨æœ¬åœ°è·‘ä¸€ä¸‹12345678910111213141516171819202122package cn.zwjf.test/** * Created by Administrator on 2017/4/5. */import org.apache.spark.SparkContextimport org.apache.spark.SparkConfobject SimpleApp &#123; def main(args: Array[String]) &#123; System.setProperty("hadoop.home.dir", "D:\\tool\\hadoop-2.6.0"); val logFile = "D:\\tool\\spark-2.1.0-bin-hadoop2.6\\README.md" // Should be some file on your system val conf = new SparkConf().setAppName("Simple Application").setMaster("local[2]") val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line =&gt; line.contains("Spark")).count() val numBs = logData.filter(line =&gt; line.contains("http")).count() println(s"Lines with Spark: $numAs, Lines with http: $numBs") sc.stop() &#125;&#125; è¾“å‡ºLines with a: 20, Lines with b: 10]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windwosä¸‹å¯åŠ¨hadoop]]></title>
    <url>%2F2016%2F04%2F02%2Fwindwos%E4%B8%8B%E5%90%AF%E5%8A%A8hadoop%2F</url>
    <content type="text"><![CDATA[å®‰è£…JDKï¼ˆç•¥ï¼‰ ä¸‹è½½hadoop2.6.0åœ°å€ï¼šhttps://archive.apache.org/dist/hadoop/common/hadoop-2.6.0/é€‰æ‹©ç¼–è¯‘åçš„æ–‡ä»¶hadoop-2.6.0.tar.gz ä¸‹è½½è§£å‹ï¼Œåšä¸»è§£å‹åˆ°D:\toolä¸‹ é…ç½®ç¯å¢ƒå˜é‡ï¼šHADOOP_HOMED:\tool\hadoop-2.6.0 ä¿®æ”¹hadoopé…ç½®æ–‡ä»¶core-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xmlæ²¡æœ‰å°±å°†mapred-site.xml.templateé‡å‘½åä¸ºmapred-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;hdfs://localhost:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011&lt;configuration&gt; &lt;!-- è¿™ä¸ªå‚æ•°è®¾ç½®ä¸º1ï¼Œå› ä¸ºæ˜¯å•æœºç‰ˆhadoop --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/D:/dev/hadoop-2.5.2/workplace/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop-env.cmd æ³¨é‡ŠåŸæ¥çš„JAVA_HOMEï¼Œå¢åŠ ä½ æœ¬åœ°çš„è·¯å¾„ï¼ˆæ³¨æ„JDKè·¯å¾„ä¸ºC:\Program Files\Java\jdk1.8.0_74ï¼‰123@rem set JAVA_HOME=%JAVA_HOME%set JAVA_HOME=C:\Progra~1\Java\jdk1.8.0_74 æ ¼å¼åŒ–hdfs namenode -format åˆ°hadoopçš„sbinç›®å½•æ‰§è¡Œâ€œstart-all.cmdâ€ï¼Œå®ƒå°†ä¼šå¯åŠ¨ä»¥ä¸‹è¿›ç¨‹ã€‚ æ‰“å¼€http://localhost:8088/cluster]]></content>
      <categories>
        <category>å¤§æ•°æ®</category>
      </categories>
      <tags>
        <tag>å¤§æ•°æ®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®¾ç½®æ¨¡å¼-è½¯ä»¶è®¾è®¡åŸåˆ™]]></title>
    <url>%2F2016%2F03%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99(1)%2F</url>
    <content type="text"><![CDATA[å¼€é—­åŸåˆ™Software entities should be open for extension,but closed for modificationfrom ã€ŠObject Oriented Software Constructionã€‹ by Bertrand Meyer at 1988 entities:å®ä½“å¯¹åƒextension:æ‰©å±• é€šä¿—æ¥è®²åœ¨è®¾è®¡ä¸€ä¸ªæ¨¡å—çš„æ—¶å€™,åº”å½“ä½¿è¿™ä¸ªæ¨¡å—å¯ä»¥åœ¨ä¸è¢«ä¿®æ”¹çš„å‰æä¸‹è¢«æ‰©å±•ä¾‹å¦‚ï¼šspringåœ¨è£…è½½ä¸€ä¸ªç±»åï¼Œæƒ³è¦å¢åŠ æ–°åŠŸèƒ½æ‰©å±•ä¸€ä¸ªç±»ï¼Œspringå°±ä¼šè‡ªåŠ¨è£…è½½è¿™ä¸ªç±» å’Œé¢å‘æ¥å£ç¼–ç¨‹ç»Ÿä¸€èµ·æ¥ åˆæˆ/èšåˆå¤ç”¨åŸåˆ™ä½¿ç”¨èšåˆï¼Œç»„åˆä»£æ›¿ç»§æ‰¿ butï¼Œ what is èšåˆç»„åˆï¼Ÿe.g.class Driver { //ä½¿ç”¨æˆå‘˜å˜é‡å½¢å¼å®ç°èšåˆå…³ç³» Car mycar; public void drive(){ mycar.run(); }}spring è£…é…å³æ˜¯è¿™ä¸ªåŸåˆ™ but, when use ç»§æ‰¿ï¼Ÿå½“å­ç±»æ˜¯çˆ¶ç±»çš„ä¸€ç§ç‰¹ä¾‹ï¼Œä»è¯­ä¹‰æ¥è¯´æ˜¯ä¸€ä¸ªå•ç‹¬çš„çˆ¶ç±»ï¼Œè€Œä¸æ˜¯ä¸ºäº†å¤ç”¨ä»£ç  23ç§è®¾è®¡æ¨¡å¼å¼•ç”¨ï¼šä»£ç†æ¨¡å¼ï¼šçœŸå®ä¸»é¢˜ä½œä¸ºä»£ç†ç±»çš„ä¸€éƒ¨åˆ†ã€‚ä¾‹å¦‚æ—¥å¿—ï¼Œå®‰å…¨ï¼Œç¼“å­˜å¯ä»¥æ”¾åœ¨ä»£ç†ç±»é‡Œï¼Œéœ€è¦è°ƒç”¨çœŸå®ä¸»é¢˜çš„æ—¶å€™æ‰ä¼šå»è°ƒç”¨spring çš„AOPå°±æ˜¯ä»£ç†æ¨¡å¼ é‡Œæ°ä»£æ¢åŸåˆ™å­ç±»å¯ä»¥åœ¨æ‰€æœ‰åœºåˆä»£æ›¿çˆ¶ç±»é€šä¿—æ¥è®²ï¼šå­ç±»å¯ä»¥æ‰©å±•çˆ¶ç±»çš„åŠŸèƒ½ï¼Œä½†ä¸èƒ½æ”¹å˜çˆ¶ç±»åŸæœ‰çš„åŠŸèƒ½ 23ç§è®¾è®¡æ¨¡å¼å¼•ç”¨ï¼šæ¨¡æ¿æ–¹æ³•ï¼šdo not call us,we will call you ä¾èµ–å€’ç½®åŸåˆ™Abstractions should not depend upon details. Details should depend upon abstractions.æŠ½è±¡ä¸åº”å½“ä¾èµ–äºç»†èŠ‚,ç»†èŠ‚åº”å½“ä¾èµ–äºæŠ½è±¡ e.g. å®¢æˆ·ç«¯ä¾èµ–æ¥å£ï¼Œæ¥å£æœ‰ä¸åŒçš„å®ç°ã€‚å®¢æˆ·ç«¯ä¸ä¾èµ–å®ç°çˆ±äººä¾èµ–äººè¿™ä¸ªå€Ÿå£ï¼Œäººå€Ÿå£ä¸‹é¢æœ‰é²å›½äººï¼Œç§¦å›½äººç­‰å®ç° æ¥å£éš”ç¦»åŸåˆ™Clients should not be forced to depend upon interfaces that they donâ€™t useâ€”â€”å®¢æˆ·ç«¯ä¸åº”è¯¥ä¾èµ–å®ƒä¸éœ€ç”¨çš„æ¥å£ã€‚The dependency of one class to another one should depend on the smallest possible interfaceâ€”â€”ç±»é—´çš„ä¾èµ–å…³ç³»åº”è¯¥å»ºç«‹åœ¨æœ€å°çš„æ¥å£ä¸Š è¿ªç±³ç‰¹æ³•åˆ™1ï¼‰åªä¸ä½ ç›´æ¥çš„æœ‹å‹ä»¬é€šä¿¡ã€‚2ï¼‰ä¸è¦è·Ÿâ€œé™Œç”Ÿäººâ€è¯´è¯ã€‚3ï¼‰æ¯ä¸€ä¸ªè½¯ä»¶å•ä½å¯¹å…¶ä»–çš„å•ä½éƒ½åªæœ‰æœ€å°‘çš„çŸ¥è¯†ï¼Œè€Œä¸”å±€é™äºé‚£äº›ä¸æœ¬å•ä½å¯†åˆ‡ç›¸å…³çš„è½¯ä»¶å•ä½ã€‚ ä½†å¹¶ä¸æ˜¯ç»å¯¹çš„ä¾‹å¦‚StringBuffer çš„appendæ–¹æ³•å¯ä»¥ä¸€ç›´appendä¸‹å»ï¼Œä½†æ˜¯æ¯æ¬¡è¿”å›çš„éƒ½æ˜¯å®ƒæœ¬èº«ï¼Œè¿™æ ·ä¸ä¼šé€ æˆç©ºæŒ‡é’ˆç­‰å¼‚å¸¸ 23ç§è®¾è®¡æ¨¡å¼å¼•ç”¨ï¼šé—¨é¢ï¼ˆå¤–è§‚ï¼‰æ¨¡å¼å’Œè°ƒåœè€…ï¼ˆä¸­ä»‹è€…ï¼‰æ¨¡å¼å®é™…ä¸Šå°±æ˜¯è¿ªç±³ç‰¹æ³•åˆ™çš„å…·ä½“åº”ç”¨ã€‚ å•ä¸€èŒè´£åŸåˆ™ä¸€ä¸ªç±»ï¼Œåº”è¯¥åªæœ‰ä¸€ä¸ªèŒè´£ã€‚æ¯ä¸€ä¸ªèŒè´£éƒ½æ˜¯å˜åŒ–çš„ä¸€ä¸ªè½´çº¿ï¼Œå¦‚æœä¸€ä¸ªç±»æœ‰ä¸€ä¸ªä»¥ä¸Šçš„èŒè´£ï¼Œè¿™äº›èŒè´£å°±è€¦åˆåœ¨äº†ä¸€èµ·ã€‚è¿™ä¼šå¯¼è‡´è„†å¼±çš„è®¾è®¡ã€‚å½“ä¸€ä¸ªèŒè´£å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå¯èƒ½ä¼šå½±å“å…¶å®ƒçš„èŒè´£ã€‚å¦å¤–ï¼Œå¤šä¸ªèŒè´£è€¦åˆåœ¨ä¸€èµ·ï¼Œä¼šå½±å“å¤ç”¨æ€§ã€‚æˆ‘ä»¬å¯èƒ½åªéœ€è¦å¤ç”¨è¯¥ç±»çš„æŸä¸€ä¸ªèŒè´£ï¼Œä½†è¿™ä¸ªèŒè´£è·Ÿå…¶å®ƒèŒè´£è€¦åˆåœ¨äº†ä¸€èµ·ï¼Œå¾ˆéš¾åˆ†ç¦»å‡ºæ¥]]></content>
      <categories>
        <category>è®¾è®¡æ¨¡å¼</category>
      </categories>
      <tags>
        <tag>è®¾è®¡æ¨¡å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-MLlib-æ•°æ®ç±»å‹å®æˆ˜]]></title>
    <url>%2F2016%2F03%2F30%2Fspark-MLlib-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[ç›‘ç£å­¦ä¹ ï¼šç»™å®šæ•°æ®é›†ï¼ŒçŸ¥é“è¦é¢„æµ‹ä»€ä¹ˆ éç›‘ç£å­¦ä¹ ï¼šä¸å‘Šè¯‰è®¡ç®—æœºè¦åšä»€ä¹ˆï¼Œè®©ä»–è‡ªå·±å»å‘ç°æ•°æ®çš„å†…éƒ¨æœºæ„ åŠç›‘ç£å­¦ä¹ ï¼šè€ƒè™‘å¦‚ä½•ç”¨å°‘é‡çš„æ ‡æ³¨æ ·æœ¬å’Œå¤§é‡çš„æœªæ ‡æ³¨æ ·æœ¬è¿›è¡Œè®­ç»ƒå’Œåˆ†ç±» å¼ºåŒ–å­¦ä¹ ï¼šå¼ºåŒ–å­¦ä¹ é€šè¿‡è§‚å¯Ÿæ¥å­¦ä¹ åŠ¨ä½œçš„å®Œæˆï¼Œæ¯ä¸ªåŠ¨ä½œéƒ½ä¼šå¯¹ç¯å¢ƒæœ‰æ‰€å½±å“ï¼Œå­¦ä¹ å¯¹è±¡æ ¹æ®è§‚å¯Ÿåˆ°çš„å‘¨å›´ç¯å¢ƒçš„åé¦ˆæ¥åšå‡ºåˆ¤æ–­ã€‚åœ¨è¿™ç§å­¦ä¹ æ¨¡å¼ä¸‹ï¼Œè¾“å…¥æ•°æ®ä½œä¸ºå¯¹æ¨¡å‹çš„åé¦ˆ å®æˆ˜ç»ƒä¹ ï¼šæœ¬åœ°å‘é‡// åˆ›å»ºä¸€ä¸ªå¯†é›†å‘é‡ (1.0, 0.0, 3.0).val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)// åˆ›å»ºä¸€ä¸ªç¨€ç–å‘é‡ (1.0, 0.0, 3.0) é€šè¿‡åˆ¶å®šå¯¹åº”äºéé›¶é¡¹çš„ç´¢å¼•å’Œå€¼val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))// åˆ›å»ºç¨€ç–å‘é‡ (1.0, 0.0, 3.0) é€šè¿‡æŒ‡å®šéé›¶é¡¹val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0))) è¡ŒçŸ©é˜µéœ€è¦å¼•å…¥å¦‚ä¸‹åŒ…123456789101112131415import org.apache.spark.mllib.linalg.&#123;Vector, Vectors&#125;import org.apache.spark.mllib.linalg.distributed.RowMatriximport org.apache.spark.rdd.RDDä»£ç ï¼šval dv1 : Vector = Vectors.dense(1.0,2.0,3.0)val dv2 : Vector = Vectors.dense(2.0,3.0,4.0)val rows : RDD[Vector] = sc.parallelize(Array(dv1,dv2))val mat: RowMatrix = new RowMatrix(rows)// Get its size.val m = mat.numRows()val n = mat.numCols()print(m) //2print(n) //3 æˆ‘ä»¬å¯ä»¥é€šè¿‡å…¶è‡ªå¸¦çš„computeColumnSummaryStatistics()æ–¹æ³•è·å–è¯¥çŸ©é˜µçš„ä¸€äº›ç»Ÿè®¡æ‘˜è¦ä¿¡æ¯ï¼Œå¹¶å¯ä»¥å¯¹å…¶è¿›è¡ŒQRåˆ†è§£ï¼ŒSVDåˆ†è§£å’ŒPCAåˆ†è§£1234567891011val summary = mat.computeColumnSummaryStatistics()val count = summary.countprintln(count) //2val max = summary.maxprintln(max) //[2.0,3.0,4.0]val variance = summary.varianceprintln(variance) //[0.5,0.5,0.5]val mean = summary.meanprintln(mean) //[1.5,2.5,3.5]val normL1 = summary.normL1println(normL1) //[3.0,5.0,7.0] ç´¢å¼•è¡ŒçŸ©é˜µéœ€è¦å¼•å…¥12import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.distributed.&#123;IndexedRow, IndexedRowMatrix&#125; ä»£ç 12345val idxr1 = IndexedRow(1,dv1)val idxr2 = IndexedRow(2,dv2)val idxrows = sc.parallelize(Array(idxr1,idxr2))val idxmat: IndexedRowMatrix = new IndexedRowMatrix(idxrows)idxmat.rows.foreach(println) IndexedRow(2,[2.0,3.0,4.0])IndexedRow(1,[1.0,2.0,3.0]) åæ ‡çŸ©é˜µMatrixEntryçš„å‚æ•°æ„ä¹‰ï¼š(i: Long, j: Long, value: Double)ï¼Œå…¶ä¸­iæ˜¯è¡Œç´¢å¼•ï¼Œjæ˜¯åˆ—ç´¢å¼•ï¼Œvalueæ˜¯è¯¥ä½ç½®çš„å€¼ã€‚åæ ‡çŸ©é˜µä¸€èˆ¬åœ¨çŸ©é˜µçš„ä¸¤ä¸ªç»´åº¦éƒ½å¾ˆå¤§ï¼Œä¸”çŸ©é˜µéå¸¸ç¨€ç–çš„æ—¶å€™ä½¿ç”¨éœ€è¦å¼•å…¥ï¼š1import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125; ä»£ç ï¼š1234567val conf = new SparkConf().setAppName("Matrix").setMaster("local[2]")val sc = new SparkContext(conf)var ent1 = new MatrixEntry(0,1,0.5)var ent2 = new MatrixEntry(2,2,1.8)val entries : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2))val coordMat: CoordinateMatrix = new CoordinateMatrix(entries)coordMat.entries.foreach(println) MatrixEntry(2,2,1.8)MatrixEntry(0,1,0.5) 12val transMat: CoordinateMatrix = coordMat.transpose() //è½¬ç½®transMat.entries.foreach(println) MatrixEntry(1,0,0.5)MatrixEntry(2,2,1.8) 12val indexedRowMatrix = transMat.toIndexedRowMatrix() //è½¬æ¢æˆç´¢å¼•è¡ŒçŸ©é˜µindexedRowMatrix.rows.foreach(println) IndexedRow(2,(3,[2],[1.8]))IndexedRow(1,(3,[0],[0.5])) åˆ†å—çŸ©é˜µåˆ†å—çŸ©é˜µæ˜¯åŸºäºçŸ©é˜µå—MatrixBlockæ„æˆçš„RDDçš„åˆ†å¸ƒå¼çŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸€ä¸ªçŸ©é˜µå—MatrixBlockéƒ½æ˜¯ä¸€ä¸ªå…ƒç»„((Int, Int), Matrix)ï¼Œå…¶ä¸­(Int, Int)æ˜¯å—çš„ç´¢å¼•ï¼Œè€ŒMatrixåˆ™æ˜¯åœ¨å¯¹åº”ä½ç½®çš„å­çŸ©é˜µï¼ˆsub-matrixï¼‰ï¼Œå…¶å°ºå¯¸ç”±rowsPerBlockå’ŒcolsPerBlockå†³å®šï¼Œé»˜è®¤å€¼å‡ä¸º1024ã€‚åˆ†å—çŸ©é˜µæ”¯æŒå’Œå¦ä¸€ä¸ªåˆ†å—çŸ©é˜µè¿›è¡ŒåŠ æ³•æ“ä½œå’Œä¹˜æ³•æ“ä½œï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ”¯æŒæ–¹æ³•validate()æ¥ç¡®è®¤åˆ†å—çŸ©é˜µæ˜¯å¦åˆ›å»ºæˆåŠŸã€‚åˆ†å—çŸ©é˜µå¯ç”±ç´¢å¼•è¡ŒçŸ©é˜µIndexedRowMatrixæˆ–åæ ‡çŸ©é˜µCoordinateMatrixè°ƒç”¨toBlockMatrix()æ–¹æ³•æ¥è¿›è¡Œè½¬æ¢ï¼Œè¯¥æ–¹æ³•å°†çŸ©é˜µåˆ’åˆ†æˆå°ºå¯¸é»˜è®¤ä¸º1024x1024çš„åˆ†å—ï¼Œå¯ä»¥åœ¨è°ƒç”¨toBlockMatrix(rowsPerBlock, colsPerBlock)æ–¹æ³•æ—¶ä¼ å…¥å‚æ•°æ¥è°ƒæ•´åˆ†å—çš„å°ºå¯¸ã€‚ä¸‹é¢ä»¥çŸ©é˜µAï¼ˆå¦‚å›¾ï¼‰ä¸ºä¾‹ï¼Œå…ˆåˆ©ç”¨çŸ©é˜µé¡¹MatrixEntryå°†å…¶æ„é€ æˆåæ ‡çŸ©é˜µï¼Œå†è½¬åŒ–æˆå¦‚å›¾æ‰€ç¤ºçš„4ä¸ªåˆ†å—çŸ©é˜µï¼Œæœ€åå¯¹çŸ©é˜µAä¸å…¶è½¬ç½®è¿›è¡Œä¹˜æ³•è¿ç®—ï¼š éœ€è¦å¼•å…¥12import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.BlockMatrix 123456789101112131415161718192021222324252627282930313233343536373839ent1 = new MatrixEntry(0,0,1)ent2 = new MatrixEntry(1,1,1)val ent3 = new MatrixEntry(2,0,-1)val ent4 = new MatrixEntry(2,1,2)val ent5 = new MatrixEntry(2,2,1)val ent6 = new MatrixEntry(3,0,1)val ent7 = new MatrixEntry(3,1,1)val ent8 = new MatrixEntry(3,3,1)val entries2 : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2,ent3,ent4,ent5,ent6,ent7,ent8))val coordMat2: CoordinateMatrix = new CoordinateMatrix(entries2)val matA: BlockMatrix = coordMat2.toBlockMatrix(2,2).cache()//å°†åæ ‡çŸ©é˜µè½¬æ¢æˆ2x2çš„åˆ†å—çŸ©é˜µå¹¶å­˜å‚¨matA.validate()println("å—çŸ©é˜µ")val localMatrix: Matrix = matA.toLocalMatrixprintln(localMatrix)/*1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */val numColBlocks = matA.numColBlocks //2println(numColBlocks)val numRowBlocks = matA.numColBlocks //2println(numRowBlocks)val ata = matA.transpose.multiply(matA)//è®¡ç®—å…¶è½¬ç½®çŸ©é˜µå’ŒçŸ©é˜µçš„ç§¯çŸ©é˜µval matrix = ata.toLocalMatrixprintln(matrix)/*3.0 -1.0 -1.0 1.0 -1.0 6.0 2.0 1.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ æ•´ä½“æµ‹è¯•ä»£ç ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package mllibimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.mllib.linalg.&#123;Matrix, Vector, Vectors&#125;import org.apache.spark.mllib.linalg.distributed.RowMatriximport org.apache.spark.rdd.RDDimport spark_streaming.LoggerLevelsimport org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.&#123;IndexedRow, IndexedRowMatrix&#125;import org.apache.spark.mllib.linalg.distributed.&#123;CoordinateMatrix, MatrixEntry&#125;import org.apache.spark.mllib.linalg.distributed.BlockMatrixclass VectorTets&#123;&#125;/** * Created by zhanghongming on 2017/4/8. */object VectorTets &#123; def main(args: Array[String]) &#123; val dv:Vector =Vectors.dense(1.0,2.0,3.0) /*val rows: RDD[Vector] = print(dv)*/ LoggerLevels.setStreamingLogLevels() //StreamingContext val conf = new SparkConf().setAppName("Matrix").setMaster("local[2]") val sc = new SparkContext(conf) val dv1 : Vector = Vectors.dense(1.0,2.0,3.0) val dv2 : Vector = Vectors.dense(2.0,3.0,4.0) val rows : RDD[Vector] = sc.parallelize(Array(dv1,dv2)) val mat: RowMatrix = new RowMatrix(rows) // Get its size. val m = mat.numRows() val n = mat.numCols() println(m) //2 println(n) //3 mat.rows.foreach(println) /* [2.0,3.0,4.0] [1.0,2.0,3.0] * */ val summary = mat.computeColumnSummaryStatistics() val count = summary.count println(count) //2 val max = summary.max println(max) //[2.0,3.0,4.0] val variance = summary.variance println(variance) //[0.5,0.5,0.5] val mean = summary.mean println(mean) //[1.5,2.5,3.5] val normL1 = summary.normL1 println(normL1) //[3.0,5.0,7.0] /////ç´¢å¼•è¡ŒçŸ©é˜µ val idxr1 = IndexedRow(1,dv1) val idxr2 = IndexedRow(2,dv2) val idxrows = sc.parallelize(Array(idxr1,idxr2)) val idxmat: IndexedRowMatrix = new IndexedRowMatrix(idxrows) idxmat.rows.foreach(println) /////åæ ‡çŸ©é˜µ var ent1 = new MatrixEntry(0, 1, 0.5) var ent2 = new MatrixEntry(2, 2, 1.8) var entries : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2)) val coordMat: CoordinateMatrix = new CoordinateMatrix(entries) coordMat.entries.foreach(println) val transMat: CoordinateMatrix = coordMat.transpose() //è½¬ç½® transMat.entries.foreach(println) val indexedRowMatrix = transMat.toIndexedRowMatrix() //è½¬æ¢æˆç´¢å¼•è¡ŒçŸ©é˜µ indexedRowMatrix.rows.foreach(println) //å—çŸ©é˜µ ent1 = new MatrixEntry(0,0,1) ent2 = new MatrixEntry(1,1,1) val ent3 = new MatrixEntry(2,0,-1) val ent4 = new MatrixEntry(2,1,2) val ent5 = new MatrixEntry(2,2,1) val ent6 = new MatrixEntry(3,0,1) val ent7 = new MatrixEntry(3,1,1) val ent8 = new MatrixEntry(3,3,1) val entries2 : RDD[MatrixEntry] = sc.parallelize(Array(ent1,ent2,ent3,ent4,ent5,ent6,ent7,ent8)) val coordMat2: CoordinateMatrix = new CoordinateMatrix(entries2) val matA: BlockMatrix = coordMat2.toBlockMatrix(2,2).cache()//å°†åæ ‡çŸ©é˜µè½¬æ¢æˆ2x2çš„åˆ†å—çŸ©é˜µå¹¶å­˜å‚¨ matA.validate() println("å—çŸ©é˜µ") val localMatrix: Matrix = matA.toLocalMatrix println(localMatrix) /* 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ val numColBlocks = matA.numColBlocks//2 println(numColBlocks) val numRowBlocks = matA.numColBlocks//2 println(numRowBlocks) println("è®¡ç®—å…¶è½¬ç½®çŸ©é˜µ") println( matA.transpose.toLocalMatrix()) println("è®¡ç®—å…¶è½¬ç½®çŸ©é˜µå’ŒçŸ©é˜µçš„ç§¯çŸ©é˜µ") val ata = matA.transpose.multiply(matA)//è®¡ç®—å…¶è½¬ç½®çŸ©é˜µå’ŒçŸ©é˜µçš„ç§¯çŸ©é˜µ val matrix = ata.toLocalMatrix println(matrix) /* 3.0 -1.0 -1.0 1.0 -1.0 6.0 2.0 1.0 -1.0 2.0 1.0 0.0 1.0 1.0 0.0 1.0 */ &#125;&#125; pomæ–‡ä»¶1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.10.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-çº¿ç¨‹]]></title>
    <url>%2F2016%2F03%2F08%2Fjava%20thread%2F</url>
    <content type="text"><![CDATA[ThreadLocalï¼šçº¿ç¨‹æœ¬åœ°å˜é‡ï¼ˆThreadLocalä¸ºå˜é‡åœ¨æ¯ä¸ªçº¿ç¨‹ä¸­éƒ½åˆ›å»ºäº†ä¸€ä¸ªå‰¯æœ¬ï¼‰ å…ˆäº†è§£ä¸€ä¸‹ThreadLocalç±»æä¾›çš„å‡ ä¸ªæ–¹æ³•ï¼š public T get() { }public void set(T value) { }public void remove() { }protected T initialValue() { } get()æ–¹æ³•æ˜¯ç”¨æ¥è·å–ThreadLocalåœ¨å½“å‰çº¿ç¨‹ä¸­ä¿å­˜çš„å˜é‡å‰¯æœ¬ï¼Œé€šè¿‡ä»–è·å–ThreadLocalMapï¼ˆå½“å‰çº¿ç¨‹tä¸­çš„ä¸€ä¸ªæˆå‘˜å˜é‡threadLocalsã€‚ï¼‰ThreadLocalMapçš„Entryç»§æ‰¿äº†WeakReferenceset()ç”¨æ¥è®¾ç½®å½“å‰çº¿ç¨‹ä¸­å˜é‡çš„å‰¯æœ¬ï¼Œremove()ç”¨æ¥ç§»é™¤å½“å‰çº¿ç¨‹ä¸­å˜é‡çš„å‰¯æœ¬initialValue()æ˜¯ä¸€ä¸ªprotectedæ–¹æ³•,ç›®å‰è¿”å›null ä»å¦‚ä¸Šå¼•å‡ºä¸€ä¸ªé—®é¢˜ï¼šWeakReferenceï¼šå½“ä¸€ä¸ªå¯¹è±¡oè¢«åˆ›å»ºæ—¶, å®ƒè¢«æ”¾åœ¨Heapé‡Œ. å½“GCè¿è¡Œçš„æ—¶å€™, å¦‚æœå‘ç°æ²¡æœ‰ä»»ä½•å¼•ç”¨æŒ‡å‘o, oå°±ä¼šè¢«å›æ”¶ä»¥è…¾å‡ºå†…å­˜ç©ºé—´. æˆ–è€…æ¢å¥è¯è¯´, ä¸€ä¸ªå¯¹è±¡è¢«å›æ”¶, å¿…é¡»æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶: 1)æ²¡æœ‰ä»»ä½•å¼•ç”¨æŒ‡å‘å®ƒ 2)GCè¢«è¿è¡Œ. å½“ä¸€ä¸ªå¯¹è±¡ä»…ä»…è¢«weak referenceæŒ‡å‘, è€Œæ²¡æœ‰ä»»ä½•å…¶ä»–strong referenceæŒ‡å‘çš„æ—¶å€™, å¦‚æœGCè¿è¡Œ, é‚£ä¹ˆè¿™ä¸ªå¯¹è±¡å°±ä¼šè¢«å›æ”¶. å¤šçº¿ç¨‹çš„å‡ ä¸ªæ–¹æ³•ï¼šyield()ï¼šåšçš„æ˜¯è®©å½“å‰è¿è¡Œçº¿ç¨‹å›åˆ°å¯è¿è¡ŒçŠ¶æ€ï¼Œä»¥å…è®¸å…·æœ‰ç›¸åŒä¼˜å…ˆçº§çš„å…¶ä»–çº¿ç¨‹è·å¾—è¿è¡Œæœºä¼šã€‚ å› æ­¤ï¼Œä½¿ç”¨yield()çš„ç›®çš„æ˜¯è®©ç›¸åŒä¼˜å…ˆçº§çš„çº¿ç¨‹ä¹‹é—´èƒ½é€‚å½“çš„è½®è½¬æ‰§è¡Œã€‚ä½†æ˜¯ï¼Œå®é™…ä¸­æ— æ³•ä¿è¯yield()è¾¾åˆ°è®©æ­¥ç›®çš„ï¼Œå› ä¸ºè®©æ­¥çš„çº¿ç¨‹è¿˜æœ‰å¯èƒ½è¢«çº¿ç¨‹è°ƒåº¦ç¨‹åºå†æ¬¡é€‰ä¸­ã€‚ join(): æ–¹æ³•çš„ä¸»è¦ä½œç”¨å°±æ˜¯åŒæ­¥ï¼Œå®ƒå¯ä»¥ä½¿å¾—çº¿ç¨‹ä¹‹é—´çš„å¹¶è¡Œæ‰§è¡Œå˜ä¸ºä¸²è¡Œæ‰§è¡Œã€‚åŸç†æ˜¯ï¼šjoinæ–¹æ³•æ˜¯é€šè¿‡è°ƒç”¨çº¿ç¨‹çš„waitæ–¹æ³•æ¥è¾¾åˆ°åŒæ­¥çš„ç›®çš„çš„ã€‚ä¾‹å¦‚ï¼ŒAçº¿ç¨‹ä¸­è°ƒç”¨äº†Bçº¿ç¨‹çš„joinæ–¹æ³•ï¼Œåˆ™ç›¸å½“äºAçº¿ç¨‹è°ƒç”¨äº†Bçº¿ç¨‹çš„waitæ–¹æ³•ï¼Œåœ¨è°ƒç”¨äº†Bçº¿ç¨‹çš„waitæ–¹æ³•åï¼ŒAçº¿ç¨‹å°±ä¼šè¿›å…¥é˜»å¡çŠ¶æ€]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è®¡ç®—æœºç§‘å­¦å¯¼è®º]]></title>
    <url>%2F2016%2F02%2F07%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AF%BC%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[è®°å½•ä¸ºä»€ä¹ˆè¦ç”¨æœºå™¨è¯­è¨€è®©è®¡ç®—æœºæ¥æ‰§è¡ŒæŒ‡ä»¤è€Œä¸ç”¨æˆ‘ä»¬æ‰€å­¦çš„è‹±è¯­æˆ–å…¶ä»–è¯­è¨€å‘¢ï¼Ÿä¸¾ä¸ªä¾‹å­ï¼šå¦‚æœä½ è®©æœºå™¨å»æ‰§è¡Œbiweeklyã€‚è®¾å®šå‘å·¥èµ„æ˜¯æŒ‰ç…§biweeklyæ¥æ‰§è¡Œï¼Œé‚£ä¹ˆæœºå™¨ä¼šå»æŒ‰ç…§æ¯ä¸¤å‘¨å‘ä¸€æ¬¡ï¼Œè¿˜æ˜¯æ¯å‘¨å‘ä¸¤æ¬¡å‘¢ï¼Ÿè‡ªç„¶è¯­è¨€çš„è¯­ä¹‰å¤šç§å¤šæ ·ã€‚ pythonä¸èƒ½ä½¿ç”¨å­—ç¬¦ä¸²+æ•°å­—ï¼Œä½†æ˜¯å¯ä»¥ä½¿ç”¨å­—ç¬¦ä¸²*æ•°å­—]]></content>
      <categories>
        <category>è®¡ç®—æœºç§‘å­¦å¯¼è®º</category>
      </categories>
      <tags>
        <tag>å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAPåŸç†]]></title>
    <url>%2F2015%2F03%2F03%2FCAP%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Consistency ä¸€è‡´æ€§Ã˜ é€šè¿‡æŸä¸ªèŠ‚ç‚¹çš„å†™æ“ä½œç»“æœå¯¹åé¢é€šè¿‡å…¶å®ƒèŠ‚ç‚¹çš„è¯»æ“ä½œå¯è§Ã˜ å¦‚æœæ›´æ–°æ•°æ®åï¼Œå¹¶å‘è®¿é—®æƒ…å†µä¸‹å¯ç«‹å³æ„ŸçŸ¥è¯¥æ›´æ–°ï¼Œç§°ä¸ºå¼ºä¸€è‡´æ€§Ã˜ å¦‚æœå…è®¸ä¹‹åéƒ¨åˆ†æˆ–è€…å…¨éƒ¨æ„ŸçŸ¥ä¸åˆ°è¯¥æ›´æ–°ï¼Œç§°ä¸ºå¼±ä¸€è‡´æ€§Ã˜ è‹¥åœ¨ä¹‹åçš„ä¸€æ®µæ—¶é—´ï¼ˆé€šå¸¸è¯¥æ—¶é—´ä¸å›ºå®šï¼‰åï¼Œä¸€å®šå¯ä»¥æ„ŸçŸ¥è¯¥æ›´æ–°ï¼Œç§°ä¸ºæœ€ç»ˆä¸€è‡´æ€§ Availability å¯ç”¨æ€§Ã˜ ä»»ä½•ä¸€ä¸ªæ²¡æœ‰å‘ç”Ÿæ•…éšœçš„èŠ‚ç‚¹å¿…é¡»åœ¨æœ‰é™çš„æ—¶é—´å†…è¿”å›åˆç†çš„ç»“æœ Partition tolerance åˆ†åŒºå®¹å¿æ€§Ã˜ éƒ¨åˆ†èŠ‚ç‚¹å®•æœºæˆ–è€…æ— æ³•ä¸å…¶å®ƒèŠ‚ç‚¹é€šä¿¡æ—¶ï¼Œå„åˆ†åŒºé—´è¿˜å¯ä¿æŒåˆ†å¸ƒå¼ç³»ç»Ÿçš„åŠŸèƒ½ ç»“è®ºï¼šåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿä¸­ï¼Œä¸€è‡´æ€§ï¼Œå¯ç”¨æ€§ï¼Œåˆ†åŒºå®¹å¿æ€§åªèƒ½æ»¡è¶³ä¸¤ä¸ªï¼Œä¸èƒ½æ»¡è¶³ä¸‰ä¸ªã€‚ç”±äºå½“å‰çš„ç½‘ç»œç¡¬ä»¶è‚¯å®šä¼šå‡ºç°å»¶è¿Ÿä¸¢åŒ…ç­‰é—®é¢˜ï¼Œæ‰€ä»¥åˆ†åŒºå®¹å¿æ€§æ˜¯æˆ‘ä»¬å¿…é¡»éœ€è¦å®ç°çš„ã€‚å› æ­¤å¾ˆå¤šæ—¶å€™æ˜¯åœ¨å¯ç”¨æ€§å’Œä¸€è‡´æ€§ä¹‹é—´æƒè¡¡ã€‚ã€‚]]></content>
      <categories>
        <category>åŸç†</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jdk8 Optional]]></title>
    <url>%2F2014%2F12%2F04%2Fjdk8-Optional%2F</url>
    <content type="text"><![CDATA[java 8 Optional1.ofæ–¹æ³•ï¼šä¸ºénullçš„å€¼åˆ›å»ºä¸€ä¸ªOptionalã€‚ ofæ–¹æ³•é€šè¿‡å·¥å‚æ–¹æ³•åˆ›å»ºOptionalç±»ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåˆ›å»ºå¯¹è±¡æ—¶ä¼ å…¥çš„å‚æ•°ä¸èƒ½ä¸ºnullã€‚å¦‚æœä¼ å…¥å‚æ•°ä¸ºnullï¼Œåˆ™æŠ›å‡ºNullPointerException ã€‚ //è°ƒç”¨å·¥å‚æ–¹æ³•åˆ›å»ºOptionalå®ä¾‹Optional name = Optional.of(â€œSanaullaâ€);//ok//ä¼ å…¥å‚æ•°ä¸ºnullï¼ŒæŠ›å‡ºNullPointerException.Optional someNull = Optional.of(null);//java.lang.NullPointerException ofNullableä¸ºæŒ‡å®šçš„å€¼åˆ›å»ºä¸€ä¸ªOptionalï¼Œå¦‚æœæŒ‡å®šçš„å€¼ä¸ºnullï¼Œåˆ™è¿”å›ä¸€ä¸ªç©ºçš„Optionalã€‚ ofNullableä¸ofæ–¹æ³•ç›¸ä¼¼ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯å¯ä»¥æ¥å—å‚æ•°ä¸ºnullçš„æƒ…å†µã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š Optional empty = Optional.ofNullable(null);isPresentå¦‚æœå€¼å­˜åœ¨è¿”å›trueï¼Œå¦åˆ™è¿”å›falseã€‚ Optional name = Optional.of(â€œnameâ€);Optional name2 = Optional.ofNullable(null);if (name.isPresent()) { //åœ¨Optionalå®ä¾‹å†…è°ƒç”¨get()è¿”å›å·²å­˜åœ¨çš„å€¼ System.out.println(name.get());//names} orElseå¦‚æœæœ‰å€¼åˆ™å°†å…¶è¿”å›ï¼Œå¦åˆ™è¿”å›æŒ‡å®šçš„å…¶å®ƒå€¼ã€‚ Optional name = Optional.ofNullable(null);System.out.println(name.orElse(â€œThere is some value!â€)); orElseGetorElseGetä¸orElseæ–¹æ³•ç±»ä¼¼ï¼ŒåŒºåˆ«åœ¨äºå¾—åˆ°çš„é»˜è®¤å€¼ã€‚orElseæ–¹æ³•å°†ä¼ å…¥çš„å­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤ System.out.println(name.orElseGet(() -&gt; â€œDefault Valueâ€));]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jdk8 å¼•ç”¨æ–¹æ³•]]></title>
    <url>%2F2014%2F12%2F03%2Fjdk8-%E5%BC%95%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[java 8 å¼•ç”¨æ–¹æ³•æˆ‘ä»¬é€šå¸¸ä½¿ç”¨lambdaè¡¨è¾¾å¼æ¥åˆ›å»ºåŒ¿åæ–¹æ³•ã€‚ç„¶è€Œï¼Œæœ‰æ—¶å€™æˆ‘ä»¬ä»…ä»…æ˜¯è°ƒç”¨äº†ä¸€ä¸ªå·²å­˜åœ¨çš„æ–¹æ³•ã€‚å¦‚ä¸‹: Arrays.sort(stringsArray,(s1,s2)-&gt;s1.compareToIgnoreCase(s2)); åœ¨Java8ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥é€šè¿‡æ–¹æ³•å¼•ç”¨æ¥ç®€å†™lambdaè¡¨è¾¾å¼ä¸­å·²ç»å­˜åœ¨çš„æ–¹æ³•ã€‚ Arrays.sort(stringsArray, String::compareToIgnoreCase); Person [] persons = new Person[10]; //ä½¿ç”¨åŒ¿åç±»Arrays.sort(persons, new Comparator() { @Override public int compare(Person o1, Person o2) { return o1.birthday.compareTo(o2.birthday); } }); //ä½¿ç”¨lambdaè¡¨è¾¾å¼Arrays.sort(persons, (o1, o2) -&gt; o1.birthday.compareTo(o2.birthday)); //ä½¿ç”¨lambdaè¡¨è¾¾å¼å’Œç±»çš„é™æ€æ–¹æ³•Arrays.sort(persons, (o1, o2) -&gt; Person.compareByAge(o1,o2)); //ä½¿ç”¨æ–¹æ³•å¼•ç”¨//å¼•ç”¨çš„æ˜¯ç±»çš„é™æ€æ–¹æ³•Arrays.sort(persons, Person::compareByAge);]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[è§£å¯†JDK8 æšä¸¾]]></title>
    <url>%2F2014%2F12%2F02%2Fjdk8-enum%2F</url>
    <content type="text"><![CDATA[å†™ä¸€ä¸ªæšä¸¾ç±»123456public enum Season &#123; SPRING, SUMMER, AUTUMN, WINTER&#125; ç„¶åæˆ‘ä»¬ä½¿ç”¨javacç¼–è¯‘ä¸Šé¢çš„ç±»,å¾—åˆ°classæ–‡ä»¶ç„¶å,æˆ‘ä»¬åˆ©ç”¨åç¼–è¯‘çš„æ–¹æ³•æ¥çœ‹çœ‹å­—èŠ‚ç æ–‡ä»¶ç©¶ç«Ÿæ˜¯ä»€ä¹ˆ.è¿™é‡Œä½¿ç”¨çš„å·¥å…·æ˜¯javapçš„ç®€å•å‘½ä»¤,å…ˆåˆ—ä¸¾ä¸€ä¸‹è¿™ä¸ªSeasonä¸‹çš„å…¨éƒ¨å…ƒç´ . 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190Classfile /C:/Season.class Last modified 2016-12-8; size 956 bytes MD5 checksum 7f6dfb988d182327a1a73ee986a9d3fa Compiled from "Season.java"public final class cn.redis.model.Season extends java.lang.Enum&lt;cn.redis.model.Season&gt; minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_FINAL, ACC_SUPER, ACC_ENUMConstant pool: #1 = Fieldref #4.#38 // cn/redis/model/Season.$VALUES:[Lcn/redis/model/Season; #2 = Methodref #39.#40 // "[Lcn/redis/model/Season;".clone:()Ljava/lang/Object; #3 = Class #23 // "[Lcn/redis/model/Season;" #4 = Class #41 // cn/redis/model/Season #5 = Methodref #16.#42 // java/lang/Enum.valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; #6 = Methodref #16.#43 // java/lang/Enum."&lt;init&gt;":(Ljava/lang/String;I)V #7 = String #17 // SPRING #8 = Methodref #4.#43 // cn/redis/model/Season."&lt;init&gt;":(Ljava/lang/String;I)V #9 = Fieldref #4.#44 // cn/redis/model/Season.SPRING:Lcn/redis/model/Season; #10 = String #19 // SUMMER #11 = Fieldref #4.#45 // cn/redis/model/Season.SUMMER:Lcn/redis/model/Season; #12 = String #20 // AUTUMN #13 = Fieldref #4.#46 // cn/redis/model/Season.AUTUMN:Lcn/redis/model/Season; #14 = String #21 // WINTER #15 = Fieldref #4.#47 // cn/redis/model/Season.WINTER:Lcn/redis/model/Season; #16 = Class #48 // java/lang/Enum #17 = Utf8 SPRING #18 = Utf8 Lcn/redis/model/Season; #19 = Utf8 SUMMER #20 = Utf8 AUTUMN #21 = Utf8 WINTER #22 = Utf8 $VALUES #23 = Utf8 [Lcn/redis/model/Season; #24 = Utf8 values #25 = Utf8 ()[Lcn/redis/model/Season; #26 = Utf8 Code #27 = Utf8 LineNumberTable #28 = Utf8 valueOf #29 = Utf8 (Ljava/lang/String;)Lcn/redis/model/Season; #30 = Utf8 &lt;init&gt; #31 = Utf8 (Ljava/lang/String;I)V #32 = Utf8 Signature #33 = Utf8 ()V #34 = Utf8 &lt;clinit&gt; #35 = Utf8 Ljava/lang/Enum&lt;Lcn/redis/model/Season;&gt;; #36 = Utf8 SourceFile #37 = Utf8 Season.java #38 = NameAndType #22:#23 // $VALUES:[Lcn/redis/model/Season; #39 = Class #23 // "[Lcn/redis/model/Season;" #40 = NameAndType #49:#50 // clone:()Ljava/lang/Object; #41 = Utf8 cn/redis/model/Season #42 = NameAndType #28:#51 // valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; #43 = NameAndType #30:#31 // "&lt;init&gt;":(Ljava/lang/String;I)V #44 = NameAndType #17:#18 // SPRING:Lcn/redis/model/Season; #45 = NameAndType #19:#18 // SUMMER:Lcn/redis/model/Season; #46 = NameAndType #20:#18 // AUTUMN:Lcn/redis/model/Season; #47 = NameAndType #21:#18 // WINTER:Lcn/redis/model/Season; #48 = Utf8 java/lang/Enum #49 = Utf8 clone #50 = Utf8 ()Ljava/lang/Object; #51 = Utf8 (Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum;&#123; public static final cn.redis.model.Season SPRING; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season SUMMER; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season AUTUMN; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static final cn.redis.model.Season WINTER; descriptor: Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC, ACC_FINAL, ACC_ENUM public static cn.redis.model.Season[] values(); descriptor: ()[Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=0, args_size=0 0: getstatic #1 // Field $VALUES:[Lcn/redis/model/Season; 3: invokevirtual #2 // Method "[Lcn/redis/model/Season;".clone:()Ljava/lang/Object; 6: checkcast #3 // class "[Lcn/redis/model/Season;" 9: areturn LineNumberTable: line 3: 0 public static cn.redis.model.Season valueOf(java.lang.String); descriptor: (Ljava/lang/String;)Lcn/redis/model/Season; flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=1, args_size=1 0: ldc #4 // class cn/redis/model/Season 2: aload_0 3: invokestatic #5 // Method java/lang/Enum.valueOf:(Ljava/lang/Class;Ljava/lang/String;)Ljava/lang/Enum; 6: checkcast #4 // class cn/redis/model/Season 9: areturn LineNumberTable: line 3: 0 static &#123;&#125;; descriptor: ()V flags: ACC_STATIC Code: stack=4, locals=0, args_size=0 0: new #4 // class cn/redis/model/Season 3: dup 4: ldc #7 // String SPRING 6: iconst_0 7: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 10: putstatic #9 // Field SPRING:Lcn/redis/model/Season; 13: new #4 // class cn/redis/model/Season 16: dup 17: ldc #10 // String SUMMER 19: iconst_1 20: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 23: putstatic #11 // Field SUMMER:Lcn/redis/model/Se ason; 26: new #4 // class cn/redis/model/Season 29: dup 30: ldc #12 // String AUTUMN 32: iconst_2 33: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 36: putstatic #13 // Field AUTUMN:Lcn/redis/model/Season; 39: new #4 // class cn/redis/model/Season 42: dup 43: ldc #14 // String WINTER 45: iconst_3 46: invokespecial #8 // Method "&lt;init&gt;":(Ljava/lang/String;I)V 49: putstatic #15 // Field WINTER:Lcn/redis/model/Season; 52: iconst_4 53: anewarray #4 // class cn/redis/model/Season 56: dup 57: iconst_0 58: getstatic #9 // Field SPRING:Lcn/redis/model/Season; 61: aastore 62: dup 63: iconst_1 64: getstatic #11 // Field SUMMER:Lcn/redis/model/Season; 67: aastore 68: dup 69: iconst_2 70: getstatic #13 // Field AUTUMN:Lcn/redis/model/Season; 73: aastore 74: dup 75: iconst_3 76: getstatic #15 // Field WINTER:Lcn/redis/model/Season; 79: aastore 80: putstatic #1 // Field $VALUES:[Lcn/redis/model/Season; 83: return LineNumberTable: line 4: 0 line 5: 13 line 6: 26 line 7: 39 line 3: 52 &#125;Signature: #35 // Ljava/lang/Enum&lt;Lcn/redis/model/Season;&gt;;SourceFile: "Season.java" ä»ä¸Šåç¼–è¯‘ç»“æœå¯çŸ¥ javaä»£ç ä¸­çš„Seasonè½¬æ¢æˆäº†ç»§æ‰¿è‡ªçš„java.lang.enumçš„ç±» æ—¢ç„¶éšå¼ç»§æ‰¿è‡ªjava.lang.enum,ä¹Ÿå°±æ„å‘³javaä»£ç ä¸­,Seasonä¸èƒ½å†ç»§æ‰¿å…¶ä»–çš„ç±» Seasonè¢«æ ‡è®°æˆäº†final,æ„å‘³ç€å®ƒä¸èƒ½è¢«ç»§æ‰¿ é™æ€å—static {}; 0~52ä¸ºå®ä¾‹åŒ–SPRING, SUMMER, AUTUMN, WINTER 53~83ä¸ºåˆ›å»ºSeason[]æ•°ç»„$VALUES,å¹¶å°†ä¸Šé¢çš„å››ä¸ªå¯¹è±¡æ”¾å…¥æ•°ç»„çš„æ“ä½œ. 8: invokevirtual #4 åœ¨switch-caseä¸­,è¿˜æ˜¯å°†Enumè½¬æˆäº†intå€¼(é€šè¿‡è°ƒç”¨Enum.oridinal()æ–¹æ³•)]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2014%2F11%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy æ¬¢è¿å¤§å®¶æ¥åˆ°æˆ‘çš„åšå®¢ More info: Deployment]]></content>
      <categories>
        <category>å…¶ä»–</category>
      </categories>
  </entry>
</search>
