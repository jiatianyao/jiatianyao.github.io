<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="人生在勤 勤则不匮">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="人生在勤 勤则不匮">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="人生在勤 勤则不匮">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> Hexo </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?f1d5376670df380fc71bd039c45d6049";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Hexo</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/28/spring boot solr/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/28/spring boot solr/" itemprop="url">
                  spring boot solr
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-28T23:33:30+08:00">
                2016-12-28
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/28/spring boot solr/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/28/spring boot solr/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>pom引入solr的jar包<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;spring-boot-starter-data-solr&lt;/artifactId&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/19/docker玩kafka集群/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/19/docker玩kafka集群/" itemprop="url">
                  docker版Kafka集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-19T12:33:30+08:00">
                2016-12-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/19/docker玩kafka集群/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/19/docker玩kafka集群/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="入手centos7"><a href="#入手centos7" class="headerlink" title="入手centos7"></a>入手centos7</h3><p>首先打开centos 官网下载<br><a href="https://www.centos.org/download/" target="_blank" rel="external">https://www.centos.org/download/</a></p>
<p>选择DVD ISO</p>
<p>找到地址并下载<br><a href="http://101.96.8.151/isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso" target="_blank" rel="external">http://101.96.8.151/isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso</a></p>
<p>安装参考：<br><a href="http://blog.csdn.net/alex_my/article/details/38142229" target="_blank" rel="external">http://blog.csdn.net/alex_my/article/details/38142229</a></p>
<h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><p>安装详情可参考官网<br><a href="https://docs.docker.com/engine/installation/linux/centos/" target="_blank" rel="external">https://docs.docker.com/engine/installation/linux/centos/</a></p>
<p>博主使用另外一种简单方式安装<br>安装docker<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install docker</div></pre></td></tr></table></figure></p>
<p>启动服务（CENTOS7之前的版本）<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">service docker start</div><div class="line">chkconfig docker on</div></pre></td></tr></table></figure></p>
<p>Centos之后的：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl  start docker.service</div><div class="line">systemctl  enable docker.service</div></pre></td></tr></table></figure></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">docker version</div><div class="line">Client:</div><div class="line"> Version:         <span class="number">1.10</span><span class="number">.3</span></div><div class="line"> API version:     <span class="number">1.22</span></div><div class="line"> Package version: docker-common<span class="number">-1.10</span><span class="number">.3</span><span class="number">-59.</span>el7.centos.x86_64</div><div class="line"> Go version:      go1<span class="number">.6</span><span class="number">.3</span></div><div class="line"> Git commit:      <span class="number">3999</span>ccb-unsupported</div><div class="line"> Built:           Thu Dec <span class="number">15</span> <span class="number">17</span>:<span class="number">24</span>:<span class="number">43</span> <span class="number">2016</span></div><div class="line"> OS/Arch:         linux/amd64</div><div class="line"></div><div class="line">Server:</div><div class="line"> Version:         <span class="number">1.10</span><span class="number">.3</span></div><div class="line"> API version:     <span class="number">1.22</span></div><div class="line"> Package version: docker-common<span class="number">-1.10</span><span class="number">.3</span><span class="number">-59.</span>el7.centos.x86_64</div><div class="line"> Go version:      go1<span class="number">.6</span><span class="number">.3</span></div><div class="line"> Git commit:      <span class="number">3999</span>ccb-unsupported</div><div class="line"> Built:           Thu Dec <span class="number">15</span> <span class="number">17</span>:<span class="number">24</span>:<span class="number">43</span> <span class="number">2016</span></div><div class="line"> OS/Arch:         linux/amd64</div></pre></td></tr></table></figure>
<h3 id="安装docker-compose"><a href="#安装docker-compose" class="headerlink" title="安装docker-compose"></a>安装docker-compose</h3><p>博主当时有1.7.1的版本在安装完之后启动报错<br>Cannot open self /usr/bin/docker-compose or archive /usr/bin/docker-compose.pkg</p>
<p>后改用早一点的版本1.7.0测试没有问题<br>下载和安装地址可参考：<br><a href="https://github.com/docker/compose/releases?after=docs-v1.7.1-2016-05-31" target="_blank" rel="external">https://github.com/docker/compose/releases?after=docs-v1.7.1-2016-05-31</a><br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">curl -L https:<span class="comment">//github.com/docker/compose/releases/download/1.7.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose</span></div><div class="line">chmod +x /usr/local/bin/docker-compose</div><div class="line"></div><div class="line">docker-compose version</div><div class="line">docker-compose version <span class="number">1.7</span><span class="number">.0</span>, build <span class="number">0</span>d7bf73</div><div class="line">docker-py version: <span class="number">1.8</span><span class="number">.0</span></div><div class="line">CPython version: <span class="number">2.7</span><span class="number">.9</span></div><div class="line">OpenSSL version: OpenSSL <span class="number">1.0</span><span class="number">.1</span>e <span class="number">11</span> Feb <span class="number">2013</span></div></pre></td></tr></table></figure></p>
<p>编写docker文件，此处参考Jason大神的，尊重原创，转载请标注<br>kafka.Dockerfile<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">FROM centos:<span class="number">6.6</span></div><div class="line"></div><div class="line">RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\</div><div class="line">	mv /etc/yum.repos.d<span class="comment">/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\</span></div><div class="line">	curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo</div><div class="line"></div><div class="line">RUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pip</div><div class="line"></div><div class="line">RUN mkdir /opt/java &amp;&amp;\</div><div class="line">	wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/java</div><div class="line"></div><div class="line">ENV KAFKA_VERSION "0.8.2.2"</div><div class="line"></div><div class="line">RUN mkdir /opt/kafka &amp;&amp;\</div><div class="line">	wget http://apache.fayea.com/kafka/$KAFKA_VERSION/kafka_2.11-$KAFKA_VERSION.tgz -P /opt/kafka</div><div class="line"></div><div class="line">RUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\</div><div class="line">	JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\</div><div class="line">	sed -i "/^PATH/i export JAVA_HOME=$JAVA_HOME" /root/.bash_profile &amp;&amp;\</div><div class="line">	sed -i "s%^PATH.*$%&amp;:$JAVA_HOME/bin%g" /root/.bash_profile &amp;&amp;\</div><div class="line">	source /root/.bash_profile</div><div class="line"></div><div class="line">RUN tar zxvf /opt/kafka/kafka*.tgz -C /opt/kafka &amp;&amp;\</div><div class="line">	sed -i 's/num.partitions.*$/num.partitions=3/g' /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.properties</div><div class="line"></div><div class="line">RUN echo "source /root/.bash_profile" &gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "cd /opt/kafka/kafka_2.11-"$KAFKA_VERSION &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	#echo "sed -i 's%zookeeper.connect=.*$%zookeeper.connect=zookeeper:2181%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "[ ! -z $""ZOOKEEPER_CONNECT"" ] &amp;&amp; sed -i 's%.*zookeeper.connect=.*$%zookeeper.connect='$""ZOOKEEPER_CONNECT'""%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "[ ! -z $""BROKER_ID"" ] &amp;&amp; sed -i 's%broker.id=.*$%broker.id='$""BROKER_ID'""%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "[ ! -z $""BROKER_PORT"" ] &amp;&amp; sed -i 's%port=.*$%port='$""BROKER_PORT'""%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "sed -i 's%#advertised.host.name=.*$%advertised.host.name='$""(hostname -i)'""%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "[ ! -z $""ADVERTISED_HOST_NAME"" ] &amp;&amp; sed -i 's%.*advertised.host.name=.*$%advertised.host.name='$""ADVERTISED_HOST_NAME'""%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "sed -i 's%#host.name=.*$%host.name='$""(hostname -i)'""%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "[ ! -z $""HOST_NAME"" ] &amp;&amp; sed -i 's%.*host.name=.*$%host.name='$""HOST_NAME'""%g'  /opt/kafka/kafka_2.11-"$KAFKA_VERSION"/config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	echo "delete.topic.enable=true" &gt;&gt; /opt/kafka/kafka_2.11-$KAFKA_VERSION/config/server.properties &amp;&amp;\</div><div class="line">	echo "bin/kafka-server-start.sh config/server.properties" &gt;&gt; /opt/kafka/start.sh &amp;&amp;\</div><div class="line">	chmod a+x /opt/kafka/start.sh</div><div class="line"></div><div class="line">RUN yum install -y nc</div><div class="line"></div><div class="line">EXPOSE 9092</div><div class="line"></div><div class="line">WORKDIR /opt/kafka/kafka_2.11-$KAFKA_VERSION</div><div class="line"></div><div class="line">ENTRYPOINT ["sh", "/opt/kafka/start.sh"]</div></pre></td></tr></table></figure></p>
<p>zookeeper.Dockerfile<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">FROM centos:6.6</div><div class="line"></div><div class="line">RUN mkdir /etc/yum.repos.d/backup &amp;&amp;\</div><div class="line">	mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ &amp;&amp;\</div><div class="line">	curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo</div><div class="line"></div><div class="line">RUN yum -y install vim lsof wget tar bzip2 unzip vim-enhanced passwd sudo yum-utils hostname net-tools rsync man git make automake cmake patch logrotate python-devel libpng-devel libjpeg-devel pwgen python-pip</div><div class="line"></div><div class="line">RUN mkdir /opt/java &amp;&amp;\</div><div class="line">	wget --no-check-certificate --no-cookies --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.tar.gz -P /opt/java</div><div class="line"></div><div class="line">RUN tar zxvf /opt/java/jdk-8u102-linux-x64.tar.gz -C /opt/java &amp;&amp;\</div><div class="line">	JAVA_HOME=/opt/java/jdk1.8.0_102 &amp;&amp;\</div><div class="line">	sed -i &quot;/^PATH/i export JAVA_HOME=$JAVA_HOME&quot; /root/.bash_profile &amp;&amp;\</div><div class="line">	sed -i &quot;s%^PATH.*$%&amp;:$JAVA_HOME/bin%g&quot; /root/.bash_profile &amp;&amp;\</div><div class="line">	source /root/.bash_profile</div><div class="line"></div><div class="line">ENV ZOOKEEPER_VERSION &quot;3.4.6&quot;</div><div class="line"></div><div class="line">RUN mkdir /opt/zookeeper &amp;&amp;\</div><div class="line">	wget http://mirror.olnevhost.net/pub/apache/zookeeper/zookeeper-$ZOOKEEPER_VERSION/zookeeper-$ZOOKEEPER_VERSION.tar.gz -P /opt/zookeeper</div><div class="line"></div><div class="line">RUN tar zxvf /opt/zookeeper/zookeeper*.tar.gz -C /opt/zookeeper</div><div class="line"></div><div class="line">RUN echo &quot;source /root/.bash_profile&quot; &gt; /opt/zookeeper/start.sh &amp;&amp;\</div><div class="line">	echo &quot;cp /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo_sample.cfg /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\</div><div class="line">	echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_PORT&quot;&quot; ] &amp;&amp; sed -i &apos;s%.*clientPort=.*$%clientPort=&apos;$&quot;&quot;ZOOKEEPER_PORT&apos;&quot;&quot;%g&apos;  /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\</div><div class="line">	echo &quot;[ ! -z $&quot;&quot;ZOOKEEPER_ID&quot;&quot; ] &amp;&amp; mkdir -p /tmp/zookeeper &amp;&amp; echo $&quot;&quot;ZOOKEEPER_ID &gt; /tmp/zookeeper/myid&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\</div><div class="line">	echo &quot;[[ ! -z $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot; ]] &amp;&amp; for server in $&quot;&quot;ZOOKEEPER_SERVERS&quot;&quot;; do echo $&quot;&quot;server&quot;&quot; &gt;&gt; /opt/zookeeper/zookeeper-&quot;$ZOOKEEPER_VERSION&quot;/conf/zoo.cfg; done&quot; &gt;&gt; /opt/zookeeper/start.sh &amp;&amp;\</div><div class="line">	echo &quot;/opt/zookeeper/zookeeper-$&quot;ZOOKEEPER_VERSION&quot;/bin/zkServer.sh start-foreground&quot; &gt;&gt; /opt/zookeeper/start.sh</div><div class="line"></div><div class="line">RUN yum install -y nc</div><div class="line"></div><div class="line">EXPOSE 2181</div><div class="line"></div><div class="line">WORKDIR /opt/zookeeper/zookeeper-$ZOOKEEPER_VERSION</div><div class="line"></div><div class="line">ENTRYPOINT [&quot;sh&quot;, &quot;/opt/zookeeper/start.sh&quot;]</div></pre></td></tr></table></figure></p>
<p>docker-compose.yml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div></pre></td><td class="code"><pre><div class="line">version: &apos;2.0&apos;</div><div class="line">services:</div><div class="line">  zookeeper0:</div><div class="line">    build:</div><div class="line">      context: .</div><div class="line">      dockerfile: zookeeper.Dockerfile</div><div class="line">    image: jason/zookeeper:3.4.6</div><div class="line">    container_name: zookeeper0</div><div class="line">    hostname: zookeeper0</div><div class="line">    ports:</div><div class="line">      - &quot;2181:2181&quot;</div><div class="line">      - &quot;2888:2888&quot;</div><div class="line">      - &quot;3888:3888&quot;</div><div class="line">    expose:</div><div class="line">      - 2181</div><div class="line">      - 2888</div><div class="line">      - 3888</div><div class="line">    environment:</div><div class="line">      ZOOKEEPER_PORT: 2181</div><div class="line">      ZOOKEEPER_ID: 0</div><div class="line">      ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882</div><div class="line">  zookeeper1:</div><div class="line">    build:</div><div class="line">      context: .</div><div class="line">      dockerfile: zookeeper.Dockerfile</div><div class="line">    image: jason/zookeeper:3.4.6</div><div class="line">    container_name: zookeeper1</div><div class="line">    hostname: zookeeper1</div><div class="line">    ports:</div><div class="line">      - &quot;2182:2182&quot;</div><div class="line">      - &quot;28881:28881&quot;</div><div class="line">      - &quot;38881:38881&quot;</div><div class="line">    expose:</div><div class="line">      - 2182</div><div class="line">      - 2888</div><div class="line">      - 3888</div><div class="line">    environment:</div><div class="line">      ZOOKEEPER_PORT: 2182</div><div class="line">      ZOOKEEPER_ID: 1</div><div class="line">      ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882</div><div class="line">#    depends_on:</div><div class="line">#      - zookeeper0</div><div class="line">  zookeeper2:</div><div class="line">    build:</div><div class="line">      context: .</div><div class="line">      dockerfile: zookeeper.Dockerfile</div><div class="line">    image: jason/zookeeper:3.4.6</div><div class="line">    container_name: zookeeper2</div><div class="line">    hostname: zookeeper2</div><div class="line">    ports:</div><div class="line">      - &quot;2183:2183&quot;</div><div class="line">      - &quot;28882:28882&quot;</div><div class="line">      - &quot;38882:38882&quot;</div><div class="line">    expose:</div><div class="line">      - 2183</div><div class="line">      - 2888</div><div class="line">      - 3888</div><div class="line">    environment:</div><div class="line">      ZOOKEEPER_PORT: 2183</div><div class="line">      ZOOKEEPER_ID: 2</div><div class="line">      ZOOKEEPER_SERVERS: server.0=zookeeper0:2888:3888 server.1=zookeeper1:28881:38881 server.2=zookeeper2:28882:38882</div><div class="line">#    depends_on:</div><div class="line">#        - zookeeper1</div><div class="line">  kafka0:</div><div class="line">    build:</div><div class="line">      context: .</div><div class="line">      dockerfile: kafka.Dockerfile</div><div class="line">    image: jason/kafka:0.8.2.2</div><div class="line">    container_name: kafka0</div><div class="line">    hostname: kafka0</div><div class="line">    ports:</div><div class="line">      - &quot;9092:9092&quot;</div><div class="line">    environment:</div><div class="line">      ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka</div><div class="line">      BROKER_ID: 0</div><div class="line">      BROKER_PORT: 9092</div><div class="line">      ADVERTISED_HOST_NAME: kafka0</div><div class="line">      HOST_NAME: kafka0</div><div class="line">    volumes:</div><div class="line">      - /var/run/docker.sock:/var/run/docker.sock</div><div class="line">    depends_on:</div><div class="line">        - zookeeper0</div><div class="line">        - zookeeper1</div><div class="line">        - zookeeper2</div><div class="line">    expose:</div><div class="line">      - 9092</div><div class="line">#    links:</div><div class="line">#      - zookeeper</div><div class="line">  kafka1:</div><div class="line">    build:</div><div class="line">      context: .</div><div class="line">      dockerfile: kafka.Dockerfile</div><div class="line">    image: jason/kafka:0.8.2.2</div><div class="line">    container_name: kafka1</div><div class="line">    hostname: kafka1</div><div class="line">    ports:</div><div class="line">      - &quot;9093:9093&quot;</div><div class="line">    environment:</div><div class="line">      ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka</div><div class="line">      BROKER_ID: 1</div><div class="line">      BROKER_PORT: 9093</div><div class="line">      ADVERTISED_HOST_NAME: kafka1</div><div class="line">      HOST_NAME: kafka1</div><div class="line">    volumes:</div><div class="line">      - /var/run/docker.sock:/var/run/docker.sock</div><div class="line">    depends_on:</div><div class="line">        - zookeeper0</div><div class="line">        - zookeeper1</div><div class="line">        - zookeeper2</div><div class="line">    expose:</div><div class="line">      - 9093</div><div class="line">#    links:</div><div class="line">#      - zookeeper</div><div class="line">  kafka2:</div><div class="line">    build: .</div><div class="line">    build:</div><div class="line">      context: .</div><div class="line">      dockerfile: kafka.Dockerfile</div><div class="line">    image: jason/kafka:0.8.2.2</div><div class="line">    container_name: kafka2</div><div class="line">    hostname: kafka2</div><div class="line">    ports:</div><div class="line">      - &quot;9094:9094&quot;</div><div class="line">    environment:</div><div class="line">      ZOOKEEPER_CONNECT: zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka</div><div class="line">      BROKER_ID: 2</div><div class="line">      BROKER_PORT: 9094</div><div class="line">      ADVERTISED_HOST_NAME: kafka2</div><div class="line">      HOST_NAME: kafka2</div><div class="line">    volumes:</div><div class="line">      - /var/run/docker.sock:/var/run/docker.sock</div><div class="line">    depends_on:</div><div class="line">        - zookeeper0</div><div class="line">        - zookeeper1</div><div class="line">        - zookeeper2</div><div class="line">    expose:</div><div class="line">      - 9094</div><div class="line">#   links:</div><div class="line">#     - zookeeper</div></pre></td></tr></table></figure></p>
<p>启动docker-compose<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 docker-file]# docker-compose up -d</div></pre></td></tr></table></figure></p>
<p>如果曾经有启动的需要先停止移除在启动<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 docker-file]# docker-compose stop;docker-compose rm -f</div></pre></td></tr></table></figure></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 docker-file]# docker ps</div><div class="line">CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS                                                                                                      NAMES</div><div class="line">891585b4767a        jason/kafka:0.8.2.2     "sh /opt/kafka/start."   About an hour ago   Up About an hour    9092/tcp, 0.0.0.0:9093-&gt;9093/tcp                                                                           kafka1</div><div class="line">81f90ccb917a        jason/kafka:0.8.2.2     "sh /opt/kafka/start."   About an hour ago   Up About an hour    9092/tcp, 0.0.0.0:9094-&gt;9094/tcp                                                                           kafka2</div><div class="line">2edb39ed0e97        jason/kafka:0.8.2.2     "sh /opt/kafka/start."   About an hour ago   Up About an hour    0.0.0.0:9092-&gt;9092/tcp                                                                                     kafka0</div><div class="line">b21ac8fb0f72        jason/zookeeper:3.4.6   "sh /opt/zookeeper/st"   About an hour ago   Up About an hour    2181/tcp, 2888/tcp, 0.0.0.0:2183-&gt;2183/tcp, 0.0.0.0:28882-&gt;28882/tcp, 3888/tcp, 0.0.0.0:38882-&gt;38882/tcp   zookeeper2</div><div class="line">48f5ada24ff7        jason/zookeeper:3.4.6   "sh /opt/zookeeper/st"   About an hour ago   Up About an hour    0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2888-&gt;2888/tcp, 0.0.0.0:3888-&gt;3888/tcp                                     zookeeper0</div><div class="line">d247a8ac6a30        jason/zookeeper:3.4.6   "sh /opt/zookeeper/st"   About an hour ago   Up About an hour    2181/tcp, 2888/tcp, 0.0.0.0:2182-&gt;2182/tcp, 0.0.0.0:28881-&gt;28881/tcp, 3888/tcp, 0.0.0.0:38881-&gt;38881/tcp   zookeeper1</div></pre></td></tr></table></figure>
<p>此时可以进入容器内查看：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">docker exec -it zookeeper0 bash</div><div class="line">source /root/.bash_profile</div><div class="line">bin/zkCli.sh -server zookeeper0:<span class="number">2181</span></div></pre></td></tr></table></figure></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[zk: zookeeper0:<span class="number">2182</span>(CONNECTED) <span class="number">0</span>] ls /</div><div class="line">[zookeeper, kafka]</div><div class="line">[zk: zookeeper0:<span class="number">2182</span>(CONNECTED) <span class="number">1</span>] ls /kafka</div><div class="line">[admin, consumers, controller, controller_epoch, brokers, config]</div><div class="line">[zk: zookeeper0:<span class="number">2182</span>(CONNECTED) <span class="number">2</span>] ls /kafka/brokers/ids</div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</div><div class="line">[zk: zookeeper0:<span class="number">2182</span>(CONNECTED) <span class="number">3</span>]</div></pre></td></tr></table></figure>
<p>此处注意，因为docker-compose.yml文件里配置的kafka引用zookeeper根目录下的kafka目录，故在写zookeeper的时候要增加/kafka<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 docker-file]# docker exec -it kafka0 bash</div><div class="line">[root@kafka0 kafka_2.11-0.8.2.2]# source /root/.bash_profile</div><div class="line">[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --create --partitions 3 --replication-factor 1</div><div class="line">Created topic "topic1".</div><div class="line">[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-topics.sh --zookeeper zookeeper0:2181/kafka --topic topic1 --describe                                    </div><div class="line">Topic:topic1    PartitionCount:3        ReplicationFactor:1     Configs:</div><div class="line">        Topic: topic1   Partition: 0    Leader: 1       Replicas: 1     Isr: 1</div><div class="line">        Topic: topic1   Partition: 1    Leader: 2       Replicas: 2     Isr: 2</div><div class="line">        Topic: topic1   Partition: 2    Leader: 0       Replicas: 0     Isr: 0</div></pre></td></tr></table></figure></p>
<h3 id="kafka术语"><a href="#kafka术语" class="headerlink" title="kafka术语"></a>kafka术语</h3><h4 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h4><p>Topic,是KAFKA对消息分类的依据;一条消息,必须有一个与之对应的Topic;<br>比如现在又两个Topic,分别是TopicA和TopicB,Producer向TopicA发送一个消息messageA,然后向TopicB发送一个消息messaeB;那么,订阅TopicA的Consumer就会收到消息messageA,订阅TopicB的Consumer就会收到消息messaeB;(每个Consumer可以同时订阅多个Topic,也即是说,同时订阅TopicA和TopicB的Consumer可以收到messageA和messaeB)。<br>同一个Group id的consumers在同一个Topic的同一条消息只能被一个consumer消费，实现了点对点模式，不同Group id的Consumers在同一个Topic上的同一条消息可以同时消费到，则实现了发布订阅模式。通过Consumer的Group id实现了JMS的消息模式</p>
<h4 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h4><p>每一个Topic可以有多个Partition,这样做是为了提高KAFKA系统的并发能力，每个Partition中按照消息发送的顺序保存着Producer发来的消息,每个消息用ID标识,代表这个消息在改Partition中的偏移量,这样,知道了ID,就可以方便的定位一个消息了;每个新提交过来的消息,被追加到Partition的尾部;如果一个Partition被写满了,就不再追加;(注意,KAFKA不保证不同Partition之间的消息有序保存)</p>
<h4 id="Leader"><a href="#Leader" class="headerlink" title="Leader"></a>Leader</h4><p>Partition中负责消息读写的节点;Leader是从Partition的节点中随机选取的。每个Partition都会在集中的其中一台服务器存在Leader。一个Topic如果有多个Partition，则会有多个Leader。</p>
<h4 id="ReplicationFactor"><a href="#ReplicationFactor" class="headerlink" title="ReplicationFactor"></a>ReplicationFactor</h4><p>一个Partition中复制数据的所有节点,包括已经挂了的;数量不会超过集群中broker的数量</p>
<h4 id="isr"><a href="#isr" class="headerlink" title="isr"></a>isr</h4><p>ReplicationFactor的子集,存活的且和Leader保持同步的节点;leade会维护一个与其基本保持同步的Replica列表，该列表成为ISR（in-sync Replica）.如果一个Follower比leader落后太多，或者超过一段时间未发起数据复制请求（kafka的数据复制是follower的pull形式），则leader将其从ISR中移除</p>
<p>balabala..那么多，来个实例玩玩吧</p>
<p>打开另外一个终端，进入kafka1启动个消费者，此处要注意消费者路径也要指定kafka<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginning</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@kafka0 kafka_2.11-0.8.2.2]# bin/kafka-console-producer.sh --broker-list kafka0:9092 --topic topic3</div></pre></td></tr></table></figure>
<p>输入hello world</p>
<p>另外一个终端显示<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@kafka1 kafka_2.11-0.8.2.2]# bin/kafka-console-consumer.sh --zookeeper zookeeper0:2181,zookeeper1:2182,zookeeper2:2183/kafka --topic topic3 --from-beginning</div><div class="line">hello world</div></pre></td></tr></table></figure></p>
<p>如果设置topic的时候设置log.cleanup.policy为compact，则消费的消息会被压缩</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/13/码云操作/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/13/码云操作/" itemprop="url">
                  用git上传代码到码云
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-13T10:33:30+08:00">
                2016-12-13
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/13/码云操作/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/13/码云操作/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>用git上传代码到码云</p>
<p>下载git客户端<br>Window 下的安装<br>从 <a href="http://git-scm.com/download" target="_blank" rel="external">http://git-scm.com/download</a> 上下载window版的客户端，然后一直下一步下一步安装git即可，请注意，如果你不熟悉每个选项的意思，请保持默认的选项</p>
<p>git config –global user.name Jenick<br>git config –global user.email 258409707@qq.com</p>
<p>以下内容Wie转载：<br>1.先在远程库创建项目，然后复制远程库里面的项目的地址；</p>
<p>2.打开命令行（电脑上用windows+R键），创建一个文件夹用于从远程库里克</p>
<p>隆所要更改的文件：命令为：mkdir + “目录名”；</p>
<p>3.用cd + “目录名”跳转到所创建的目录名下，用命令git clone +远程库</p>
<p>文件地址；此时刷新所创建的目录，会发现新出现了一个文件夹，那就是</p>
<p>从远程库里面复制的文件目录（里面包含有后缀为.git的文件）</p>
<p>4.用cd 命令跳转到新的文件夹里，把所需要提交的文件复制到这个文件夹</p>
<p>里；</p>
<p>5.用git status命令能看到所改变了的文件目录列表，用git diff命令能看</p>
<p>到具体改变了的哪一行代码；</p>
<p>6.用命令git branch + “分支名”创建本地分支，用命令git branch可查看</p>
<p>目前所在分支，用 git checkout “你所创建的分支名”；跳转到你所创建的分</p>
<p>支；</p>
<p>7.用git  add+(空格)+ “.”将所有的修改追加到文件中去；</p>
<p>8.用git commit -m“更改的文件备注”命令将你的文件提交到本地库；</p>
<p>9.用git checkout master 切换回到主分支；</p>
<p>10.用git pull  origin  master 命令将主分支从远程仓库里面拉过来；</p>
<p>11.用git merge+”你所创建的分支名”合并分支；</p>
<p>12.用git push  origin  master 命令将合并分支提交到远程库；</p>
<p>13.刷新一下远程库，就能发现所要提交的文件了</p>
<p>文／玄薛烨（简书作者）<br>原文链接：<a href="http://www.jianshu.com/p/edf037f921c7" target="_blank" rel="external">http://www.jianshu.com/p/edf037f921c7</a><br>著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-kafka " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/03/codis ha/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/03/codis ha/" itemprop="url">
                  codis ha
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-03T22:33:30+08:00">
                2016-12-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/03/codis ha/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/03/codis ha/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>启动多个实例：<br>cd /usr/local/codis/src/github.com/CodisLabs/codis<br>[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf –protected-mode no<br>[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf –protected-mode no<br>[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6381.conf –protected-mode no<br>[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6382.conf –protected-mode no</p>
<p>启动zookeeper<br>[root@zhm1 codis]# zkServer.sh start</p>
<p>启动dashboard<br>[root@zhm1 codis]# zkServer.sh startnohup bin/codis-dashboard –ncpu=2 –config=dashboard.conf –log=dashboard.log –log-level=WARN &amp;</p>
<p>启动codis-proxy<br>[root@zhm1 codis]# nohup bin/codis-proxy –ncpu=2 –config=proxy.conf –log =proxy.log –log-level=WARN &amp;</p>
<p>启动codis-fe<br>[root@zhm1 codis]# ./bin/codis-fe –ncpu=2 –log=fe.log –log-level=WARN –dashboard-list=conf/codis.json –listen=192.168.110.129:18090 &amp;</p>
<p>将实例添加到group<br><img src="http://oh6ybr0jg.bkt.clouddn.com/coids-group3.jpg" alt="codis-group"></p>
<p>分配slot<br><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-slot5.jpg" alt="codis-slot5"></p>
<p>启动codis-ha<br>nohup ./bin/codis-ha –log=ha.log –log-level=WARN –dashboard=127.0.0.1:18080 &amp;</p>
<p>[root@zhm1 codis]# ps -ef | grep codis<br>root       3096   2884  0 09:51 pts/0    00:00:32 bin/codis-dashboard –ncpu=2 –config=dashboard.conf –log=dashboard.log –log-level=WARN<br>root       3114   2884  0 09:52 pts/0    00:00:04 bin/codis-proxy –ncpu=2 –config=proxy.conf –log =proxy.log –log-level=WARN<br>root       3149   2884  0 09:53 pts/0    00:00:02 ./bin/codis-fe –ncpu=2 –log=fe.log –log-level=WARN –dashboard-list=conf/codis.json –listen=192.168.110.129:18090<br>root       3175      1  0 09:54 ?        00:00:08 ./bin/codis-server <em>:6379<br>root       3179      1  0 09:54 ?        00:00:08 ./bin/codis-server </em>:6380<br>root       3888      1  0 10:46 ?        00:00:02 ./bin/codis-server <em>:6382<br>root       3892      1  0 10:46 ?        00:00:02 ./bin/codis-server </em>:6381<br>root       4680   2884  0 11:12 pts/0    00:00:00 ./bin/codis-ha –log=ha.log –log-level=WARN –dashboard=127.0.0.1:18080<br>root       4712   2884  0 11:15 pts/0    00:00:00 grep –color=auto codis</p>
<p>kill6379 的端口<br>kill -9 3175<br>查看6380 的日志<br>3179:M 23 Dec 11:19:07.739 * MASTER MODE enabled (user request from ‘id=7 addr=192.168.110.129:51438 fd=8 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof’)<br>看到6380成为主</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-group4.jpg" alt="codis-group4"></p>
<p>再启动6379 并没有成为6380的主，需要手动点击web上绿色的工具扳手<br>怀疑可能是我的配置文件没有写slave of的原因</p>
<p>再6380输入内容<br>[root@zhm1 codis]# ./bin/redis-cli  -p 6380<br>127.0.0.1:6380&gt; set aaa 123<br>OK<br>127.0.0.1:6380&gt; </p>
<p>启动6379看<br>[root@zhm1 codis]# ./bin/redis-cli  -p 6379<br>127.0.0.1:6379&gt; get aaa<br>“123”</p>
<p>总结codis和redis集群的区别</p>
<p>a. Redis Cluster 的集群信息存储在每个集群节点上，而Codis 集群的信息<br>存储在一个独立的存储系统（Zookeeper）里。<br>b. 外部访问集群：对Redis Cluster，直接通过以集群模式启动的redis 客户<br>端 “redis-cli -c”来访问。对 Codis 集群通过独立的 codis-proxy 节点来<br>访问。<br>c. Redis Cluster 有 16384 个slot 可以分配，而 Codis 集群只有1024 个<br>slot 可以分配。<br>d. 监控和操作：Codis 集群有图形化的 Code FE 管理工具（可以完成<br>Codis Proxy，Codis Group、Codis Server 的添加和删除，分配 Slot、<br>提升Slave 为Master 等操作），而 Redis Cluster 好像没有这类工具。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/02/KafkaStream/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/02/KafkaStream/" itemprop="url">
                  Kafka Stream
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-02T23:33:30+08:00">
                2016-12-02
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/02/KafkaStream/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/02/KafkaStream/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>以下内容摘自官网，熟悉的同学可跳过</p>
<h2 id="Use-Kafka-Streams-to-process-data"><a href="#Use-Kafka-Streams-to-process-data" class="headerlink" title="Use Kafka Streams to process data"></a>Use Kafka Streams to process data</h2><p>Kafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the WordCountDemo example code (converted to use Java 8 lambda expressions for easy reading).<br>Kafka Streams是Kafka中用于客户端的库，主要用于获取实时流处理以及分析Kafka brokers中存储的数据。这个例子将会展示如何使用这个库来运行一个流式处理应用。这里有一个WordCountDemo的主要代码（转换成Java8 lambda表达式更易读）：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">KTable wordCounts = textLines</div><div class="line">    <span class="comment">// Split each text line, by whitespace, into words.</span></div><div class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</div><div class="line"></div><div class="line">    <span class="comment">// Ensure the words are available as record keys for the next aggregate operation.</span></div><div class="line">    .map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(value, value))</div><div class="line"></div><div class="line">    <span class="comment">// Count the occurrences of each word (record key) and store the results into a table named "Counts".</span></div><div class="line">    .countByKey(<span class="string">"Counts"</span>)</div></pre></td></tr></table></figure>
<p> It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on an infinite, unbounded stream of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data.</p>
<p>它实现了WordCount算法，计算了输入文本中的词频。然而，并不像其他的WordCount的例子，都是计算固定大小的数据，这个WordCount demo应用稍微有点不同，它是基于不会终止的数据流计算的。和计算固定数据的模型比较形似的是，它也会不停的更新词频计算结果。然而，由于它是基于永不停止的数据流，所以会周期性的输出当前的计算结果，他会不停的处理更多的数据，因为它也不知道何时它处理过“所有”的输入数据。</p>
<p>We will now prepare input data to a Kafka topic, which will subsequently be processed by a Kafka Streams application.<br>现在我们将输入数据导入Kafka topic，这些数据将会被Kafka Streams应用处理</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; echo -e <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; file-input.txt</div></pre></td></tr></table></figure>
<p>Or on Windows:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; echo all streams lead to kafka&gt; file-input.txt</div><div class="line">&gt; echo hello kafka streams&gt;&gt; file-input.txt</div><div class="line">&gt; echo|set /p=join kafka summit&gt;&gt; file-input.txt</div></pre></td></tr></table></figure></p>
<p>Next, we send this input data to the input topic named streams-file-input using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):<br>接着，我们使用终端producer来将这些输入数据发送到名为streams-file-input的topic（在实践中，流数据会持续不断的流入kafka，当应用将会启动并运行时）：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --create \</div><div class="line">            --zookeeper localhost:<span class="number">2181</span> \</div><div class="line">            --replication-factor <span class="number">1</span> \</div><div class="line">            --partitions <span class="number">1</span> \</div><div class="line">            --topic streams-file-input</div></pre></td></tr></table></figure></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-<span class="built_in">console</span>-producer.sh --broker-list localhost:<span class="number">9092</span> --topic streams-file-input &lt; file-input.txt</div></pre></td></tr></table></figure>
<p>We can now run the WordCount demo application to process the input data:<br>我们可以运行WordCount demo应用来处理输入数据<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-run-<span class="class"><span class="keyword">class</span>.<span class="title">sh</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">kafka</span>.<span class="title">streams</span>.<span class="title">examples</span>.<span class="title">wordcount</span>.<span class="title">WordCountDemo</span></span></div></pre></td></tr></table></figure></p>
<p>There won’t be any STDOUT output except log entries as the results are continuously written back into another topic named streams-wordcount-output in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.<br>不会有任何的stdout输出除了日志条目，结果会持续不断的写回kafka中另一个名为streams-wordcount-output的topic。这个demo将会运行数秒，不会像典型的流处理应用，自动终止。</p>
<p>We can now inspect the output of the WordCount demo application by reading from its output topic:<br>我们现在通过阅读主题的数来来检查WordCount demo应用程序的输出： </p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server localhost:<span class="number">9092</span> \</div><div class="line">            --topic streams-wordcount-output \</div><div class="line">            --<span class="keyword">from</span>-beginning \</div><div class="line">            --formatter kafka.tools.DefaultMessageFormatter \</div><div class="line">            --property print.key=<span class="literal">true</span> \</div><div class="line">            --property print.value=<span class="literal">true</span> \</div><div class="line">            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \</div><div class="line">            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</div></pre></td></tr></table></figure>
<p>with the following output data being printed to the console:<br>终端会打印出以下数据：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">all     <span class="number">1</span></div><div class="line">lead    <span class="number">1</span></div><div class="line">to      <span class="number">1</span></div><div class="line">hello   <span class="number">1</span></div><div class="line">streams <span class="number">2</span></div><div class="line">join    <span class="number">1</span></div><div class="line">kafka   <span class="number">3</span></div><div class="line">summit  <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>Here, the first column is the Kafka message key, and the second column is the message value, both in in java.lang.String format. Note that the output is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word, aka record key such as “kafka”. For multiple records with the same key, each later record is an update of the previous one.<br>第一列是Kafka消息的key，第二列是消息value，两者都是java.lang.String格式。注意，输出实际上应该是持续的更新数据流，数据流中的每一个记录（例如，上面输出的每一行）都是一个单独词汇的数量，或者是记录了key的数量，例如上面的“kafka”。对于多条记录的key一致这种情况，每一条后面的记录都是对前一条记录的更新。</p>
<p>Now you can write more input messages to the streams-file-input topic and observe additional messages added to streams-wordcount-output topic, reflecting updated word counts (e.g., using the console producer and the console consumer, as described above).<br>现在你可以写入更多的消息到streams-file-input这个topic，可以观察到更多的消息会发送到streams-wordcount-output这个topic，反映了更新之后的词汇数量。</p>
<p>You can stop the console consumer via Ctrl-C.<br>你可以使用Ctrl+C结束控制台的消费者</p>
<p>Stream Processing<br>Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might normalize or deduplicate this content and published the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza.<br>很多用户将kafka用作多级数据处理之间的消息管道：原始数据存放于Kafka不同的topics中，然后经过聚合、增强、或者其他的转换之后，导入Kafka新的topics中，以供后面的消费。例如，对于新闻推荐的处理流程来所：首先从RSS信息流中获取文章内容，然后导入名为“articles”的topic; 其次，后面的处理可能是对这些内容进行规范化或者精简操作，然后将经过上述处理的内容导入新的topic;最后的处理可能是试图将这些内容推荐给用户。这样的处理流程实际展现了实时流在独立的topics之间流动的流程图。从0.10.0.0开始，Apache Kafka推出了一款称为Kafka Streams的流式处理库，优点是轻量级同时性能很好，它可以完成上面所描述的多级处理。除了Kafka streams之外，还有一些开源流式处理工具可以选用，包括Apache  Storm和Samza。</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Kafka Streams is a client library for processing and analyzing data stored in Kafka and either write the resulting data back to Kafka or send the final output to an external system. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management of application state. Kafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka’s parallelism model.<br>Kafka Streams是一个客户端程序库用于处理和分析存储在Kafka中的数据，并将得到的数据写入Kafka或发送最终输出到外部系统。它建立在如适当区分事件的时间和加工时间，窗口函数的支持，和简单而高效的应用程序状态管理。Kafka Streams有一个低门槛进入：你可以快速编写和运行一个小规模的概念证明在一台机器上，你只需要运行在多台机器上的应用程序的额外的实例扩展到高容量的生产工作负载。Kafka Streams 透明地处理相同的应用程序通过利用Kafka 的并行模型的多个实例的负载平衡。</p>
<p>Some highlights of Kafka Streams:<br>Kafka Streams的一些亮点</p>
<ul>
<li>Designed as a simple and lightweight client library, which can be<br>easily embedded in any Java application and integrated with any<br>existing packaging, deployment and operational tools that users have<br>for their streaming applications.</li>
<li>作为一个简单而轻量级的客户端库，它可以方便的嵌入任何java应用和集成任何现有的包，部署和运营工具，用户有对于他们的流应用。</li>
<li>Has no external dependencies on systems other than Apache Kafka<br>itself as the internal messaging layer; notably, it uses Kafka’s<br>partitioning model to horizontally scale processing while maintaining<br>strong ordering guarantees.</li>
<li>对其他比Apache Kafka 没有外部依赖它本身作为内部消息层，特别是，它使用Kafka的 分割模型在保持同时进行水平缩放处理的分区模型强排序保证。</li>
<li>Supports fault-tolerant local state, which enables very fast and<br>efficient stateful operations like joins and windowed aggregations.</li>
<li>支持容错的本地状态，使非常快速和高效的状态操作的加入和窗口聚集。</li>
<li>Employs one-record-at-a-time processing to achieve low processing<br>latency, and supports event-time based windowing operations.</li>
<li>采用同一时刻只有一条记录处理实现低的处理延迟，并支持基于时间事件的窗口操作。</li>
<li>Offers necessary stream processing primitives, along with a<br>high-level Streams DSL and a low-level Processor API.</li>
<li>提供必要的流处理基元，以及 high-level Streams DSL和 low-level Processor API。</li>
</ul>
<h3 id="Developer-Guide-开发者指南"><a href="#Developer-Guide-开发者指南" class="headerlink" title="Developer Guide 开发者指南"></a>Developer Guide 开发者指南</h3><p>There is a quickstart example that provides how to run a stream processing program coded in the Kafka Streams library. This section focuses on how to write, configure, and execute a Kafka Streams application.</p>
<p>有一个快速入门示例提供了如何运行一个流处理程序在卡夫卡流的库代码。本节重点介绍如何编写、配置和执行卡夫卡流应用程序。</p>
<h3 id="Core-Concepts-核心概念"><a href="#Core-Concepts-核心概念" class="headerlink" title="Core Concepts 核心概念"></a>Core Concepts 核心概念</h3><p>We first summarize the key concepts of Kafka Streams.<br>我们首先总结了Kafka Streams的关键概念。</p>
<h3 id="Stream-Processing-Topology-流处理Topology"><a href="#Stream-Processing-Topology-流处理Topology" class="headerlink" title="Stream Processing Topology 流处理Topology"></a>Stream Processing Topology 流处理Topology</h3><ul>
<li>A stream is the most important abstraction provided by Kafka Streams:<br>it represents an unbounded, continuously updating data set. A stream<br>is an ordered, replayable, and fault-tolerant sequence of immutable<br>data records, where a data record is defined as a key-value pair.</li>
<li>流是Kafka Streams提供的最重要的抽象：它表示一个无界的，不断更新的数据集。一个流是一个有序的、可重复的，和不变的容错序列数据记录，其中一个数据记录被定义为一个键值对。</li>
<li>A stream processing application written in Kafka Streams defines its<br>computational logic through one or more processor topologies, where a<br>processor topology is a graph of stream processors (nodes) that are<br>connected by streams (edges).<br>在Kafka Streams中写的流处理应用程序定义了它的计算逻辑通过一个或多个处理器的topologies，其中处理器的topology是一个流处理器（节点）的图形由流连接（边缘）。</li>
<li>A stream processor is a node in the processor topology; it represents<br>a processing step to transform data in streams by receiving one input<br>record at a time from its upstream processors in the topology,<br>applying its operation to it, and may subsequently producing one or<br>more output records to its downstream processors.</li>
<li>流处理器是处理器topology中的一个节点；它表示通过接收一个输入来变换流中的数据的处理步骤在topology中的上游处理器上记录的时间，应用它的操作，并可能随后产生一个或向下游处理器的更多输出记录。</li>
</ul>
<p>Kafka Streams offers two ways to define the stream processing topology: the Kafka Streams DSL provides the most common data transformation operations such as map and filter; the lower-level Processor API allows developers define and connect custom processors as well as to interact with state stores.<br>Kafka Streams 提供了两种方式来定义流处理topology：Kafka Streams DSL提供了最常用的数据转换操作，如map和filter；lower-level Processor API允许开发者定义和连接定制处理器以及存储交互的状态。</p>
<h3 id="Time-时间"><a href="#Time-时间" class="headerlink" title="Time 时间"></a>Time 时间</h3><p>A critical aspect in stream processing is the notion of time, and how it is modeled and integrated. For example, some operations such as windowing are defined based on time boundaries.<br>流处理中的一个关键方面是时间的概念，以及它是如何建模和集成。例如，一些操作如窗口是基于时间边界的定义。</p>
<p>Common notions of time in streams are:<br>流中的时间的共同概念是： </p>
<ul>
<li>Event time - The point in time when an event or data record occurred,<br>i.e. was originally created “at the source”.</li>
<li>事件时间 - 当发生事件或数据记录时的时间点，即最初创建的“在源头上”。</li>
<li>Processing time - The point in time when the event or data record<br>happens to be processed by the stream processing application, i.e.<br>when the record is being consumed. The processing time may be<br>milliseconds, hours, or days etc. later than the original event time.</li>
<li>处理时间 - 事件或数据记录的时间点碰巧被流处理应用程序处理，即当记录被消耗。处理时间可能是比原始事件时间晚的毫秒数、小时或数天等。</li>
<li>Ingestion time - The point in time when an event or data record is<br>stored in a topic partition by a Kafka broker. The difference to<br>event time is that this ingestion timestamp is generated when the<br>record is appended to the target topic by the Kafka broker, not when<br>the record is created “at the source”. The difference to processing<br>time is that processing time is when the stream processing<br>application processes the record. For example, if a record is never<br>processed, there is no notion of processing time for it, but it still<br>has an ingestion time.</li>
<li>摄取时间 - 当一个事件或数据记录的时间点存储在Kafka broker的主题分区中。<br>事件时间不同的是，这种摄取时间戳时产生的记录追加到目标主题由Kafka broker ，而不是当记录是在“源”创建的。处理差异时间是处理时间的时候是流处理的应用程序处理记录。例如，如果一个记录是从来没有处理，没有处理时间的概念，但它仍然有一个摄取时间</li>
</ul>
<p>The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka’s configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps.<br>事件的时间和摄取时间之间的选择实际上是通过Kafka的配置（不是Kafka Streams）：从Kafka0.10.x起，时间戳被自动嵌入到Kafka的消息中。根据Kafka的配置这些时间戳表示事件时间或摄取时间。各自的Kafka配置设置可以在broker级别或每个主题上指定。在Kafka Streams的默认时间戳提取器将检索这些嵌入时间戳as-is。因此，您的应用程序的有效时间语义依赖于有效的Kafka配置这些嵌入时间戳。</p>
<p>Kafka Streams assigns a timestamp to every data record via the TimestampExtractor interface. Concrete implementations of this interface may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce different notions of time depending on their business needs. For example, per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins.<br>Kafka Streams分配一个时间戳的每一个数据记录通过TimestampExtractor接口。这个接口的具体实现可以检索或计算基于数据记录如嵌入时间戳字段提供事件时间语义内容的时间戳，或使用任何其他的方法，如加工时返回当前时钟时间，从而产生语义流处理应用程序的处理时间。因此，开发人员可以执行不同的时间概念，这取决于他们的业务需求。例如，每个记录时间戳的描述关于流时间的进展（虽然记录可能会超出流）和促使时间依赖操作例如 joins。</p>
<p>Finally, whenever a Kafka Streams application writes records to Kafka, then it will also assign timestamps to these new records. The way the timestamps are assigned depends on the context:<br>最后，当Kafka记录写入到Kafka Streams应用，那么它也会对新纪录指定时间戳。时间戳是分配方式取决于context： </p>
<ul>
<li>When new output records are generated via processing some input<br>record, for example, context.forward() triggered in the process()<br>function call, output record timestamps are inherited from input<br>record timestamps directly.</li>
<li>当通过处理一些输入而产生新的输出记录时记录，例如，context.forward()引发的process()函数调用，输出记录的时间戳是继承自输入直接记录时间戳。</li>
<li>When new output records are generated via periodic functions such as<br>punctuate(), the output record timestamp is defined as the current<br>internal time (obtained through context.timestamp()) of the stream<br>task.</li>
<li>当新的输出记录通过诸如punctuate()周期函数生成的输出记录时间戳定义为当前的内部时间（context.timestamp()获得）的流任务。</li>
<li>For aggregations, the timestamp of a resulting aggregate update<br>record will be that of the latest arrived input record that triggered<br>the update.</li>
<li>对于一个聚合，形成的聚合更新记录的时间戳将最新到达的输入记录触发更新。</li>
</ul>
<h3 id="States-状态"><a href="#States-状态" class="headerlink" title="States 状态"></a>States 状态</h3><p>Some stream processing applications don’t require state, which means the processing of a message is independent from the processing of all other messages. However, being able to maintain state opens up many possibilities for sophisticated stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.<br>一些流处理应用程序不需要状态，这意味着消息的处理是独立于所有其他消息的处理。然而，能够保持状态为复杂的流处理应用程序打开了许多可能性：您可以加入输入流，或组和汇总数据记录。许多这样的状态的操作，由Kafka Streams DSL提供。</p>
<p>Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data. This is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a persistent key-value store, an in-memory hashmap, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.<br>Kafka Streams提供了所谓的状态存储，它可以用于流处理应用程序来存储和查询数据。当实施状态操作时这是一个重要的能力。在Kafka Streams的每一项任务的一个或多个状态存储将可以通过API来存储和查询处理所需的数据访问。这些状态存储可以是一个持续的键值存储，内存中的HashMap，或另一个方便的数据结构。Kafka Streams提供了容错和本地状态存储的自动恢复。</p>
<p>Kafka Streams allows direct read-only queries of the state stores by methods, threads, processes or applications external to the stream processing application that created the state stores. This is provided through a feature called Interactive Queries. All stores are named and Interactive Queries exposes only the read operations of the underlying implementation.<br>Kafka Streams允许通过方法，线程，进程或应用程序的外部的流处理应用程序创建的状态存储的状态存储的直接只读查询。这是通过一个被称为交互式查询的功能。所有的存储都被命名和交互查询只公开底层实现的读操作。</p>
<p>As we have mentioned above, the computational logic of a Kafka Streams application is defined as a processor topology. Currently Kafka Streams provides two sets of APIs to define the processor topology, which will be described in the subsequent sections.<br>正如我们上文所提到的，Kafka Streams应用程序的计算逻辑被定义为一个处理topology。目前，Kafka Streams提供了两组的接口来定义处理的topology，这将在随后的章节中描述。 </p>
<h2 id="Low-Level-Processor-API"><a href="#Low-Level-Processor-API" class="headerlink" title="Low-Level Processor API"></a>Low-Level Processor API</h2><h3 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a>Processor</h3><p>Developers can define their customized processing logic by implementing the Processor interface, which provides process and punctuate methods. The process method is performed on each of the received record; and the punctuate method is performed periodically based on elapsed time. In addition, the processor can maintain the current ProcessorContext instance variable initialized in the init method, and use the context to schedule the punctuation period (context().schedule), to forward the modified / new key-value pair to downstream processors (context().forward), to commit the current processing progress (context().commit), etc.<br>开发者可以通过处理器接口定义自己的定制的处理逻辑，它提供了方法和标点的方法。process方法是对每个接收的记录执行；和标点法是基于时间进行定期。此外，该处理器可以维持目前的ProcessorContext实例变量在init方法初始化，并使用context安排标点符号周期（context().schedule），提出修改/新的键值对下游处理器（context().forward），把当前的处理进度（context().commit），等。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MyProcessor</span> <span class="keyword">extends</span> <span class="title">Processor</span> </span>&#123;</div><div class="line">        private ProcessorContext context;</div><div class="line">        private KeyValueStore kvStore;</div><div class="line">        @Override</div><div class="line">        @SuppressWarnings(<span class="string">"unchecked"</span>)</div><div class="line">        public <span class="keyword">void</span> init(ProcessorContext context) &#123;</div><div class="line">            <span class="keyword">this</span>.context = context;</div><div class="line">            <span class="keyword">this</span>.context.schedule(<span class="number">1000</span>);</div><div class="line">            <span class="keyword">this</span>.kvStore = (KeyValueStore) context.getStateStore(<span class="string">"Counts"</span>);</div><div class="line">        &#125;</div><div class="line">        @Override</div><div class="line">        public <span class="keyword">void</span> process(<span class="built_in">String</span> dummy, <span class="built_in">String</span> line) &#123;</div><div class="line">            <span class="built_in">String</span>[] words = line.toLowerCase().split(<span class="string">" "</span>);</div><div class="line">            <span class="keyword">for</span> (<span class="built_in">String</span> word : words) &#123;</div><div class="line">                Integer oldValue = <span class="keyword">this</span>.kvStore.get(word);</div><div class="line">                <span class="keyword">if</span> (oldValue == <span class="literal">null</span>) &#123;</div><div class="line">                    <span class="keyword">this</span>.kvStore.put(word, <span class="number">1</span>);</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    <span class="keyword">this</span>.kvStore.put(word, oldValue + <span class="number">1</span>);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        @Override</div><div class="line">        public <span class="keyword">void</span> punctuate(long timestamp) &#123;</div><div class="line">            KeyValueIterator iter = <span class="keyword">this</span>.kvStore.all();</div><div class="line"></div><div class="line">            <span class="keyword">while</span> (iter.hasNext()) &#123;</div><div class="line">                KeyValue entry = iter.next();</div><div class="line">                context.forward(entry.key, entry.value.toString());</div><div class="line">            &#125;</div><div class="line">            iter.close();</div><div class="line">            context.commit();</div><div class="line">        &#125;</div><div class="line">        @Override</div><div class="line">        public <span class="keyword">void</span> close() &#123;</div><div class="line">            <span class="keyword">this</span>.kvStore.close();</div><div class="line">        &#125;</div><div class="line">    &#125;;</div></pre></td></tr></table></figure>
<p>In the above implementation, the following actions are performed:<br>在上面的实现中，执行以下操作：</p>
<ul>
<li>In the init method, schedule the punctuation every 1 second and<br>retrieve the local state store by its name “Counts”.</li>
<li>在init方法，schedule每1秒和标点符号检索本地状态存储由它的名称“计数”。</li>
<li>In the process method, upon each received record, split the value<br>string into words, and update their counts into the state store (we<br>will talk about this feature later in the section).</li>
<li>在处理方法中，在每个接收到的记录中，将值字符串分割成单词，并更新他们的计数到状态存储区（我们将在本节中讨论这个功能）。</li>
<li>In the punctuate method, iterate the local state store and send the<br>aggregated counts to the downstream processor, and commit the current<br>stream state.</li>
<li>在标点法，迭代局部状态存储和发送汇总计数到下游的处理器，并承诺目前流状态。</li>
</ul>
<h3 id="Processor-Topology"><a href="#Processor-Topology" class="headerlink" title="Processor Topology"></a>Processor Topology</h3><p>With the customized processors defined in the Processor API, developers can use the TopologyBuilder to build a processor topology by connecting these processors together:<br>与定制的处理器在Processor API，开发者可以使用TopologyBuilder来连接这些处理器一起建立一个topology：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</div><div class="line">builder.addSource(<span class="string">"SOURCE"</span>, <span class="string">"src-topic"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS1"</span>, <span class="attr">MyProcessor1</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor1 */</span>, <span class="string">"SOURCE"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS2"</span>, <span class="attr">MyProcessor2</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor2 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS3"</span>, <span class="attr">MyProcessor3</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor3 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addSink(<span class="string">"SINK1"</span>, <span class="string">"sink-topic1"</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addSink(<span class="string">"SINK2"</span>, <span class="string">"sink-topic2"</span>, <span class="string">"PROCESS2"</span>)</div><div class="line">        .addSink(<span class="string">"SINK3"</span>, <span class="string">"sink-topic3"</span>, <span class="string">"PROCESS3"</span>);</div></pre></td></tr></table></figure></p>
<p>There are several steps in the above code to build the topology, and here is a quick walk through:<br>在上面的代码中有几个步骤来构建topology，这里是一个快速的步行通过：</p>
<ul>
<li>First of all a source node named “SOURCE” is added to the topology<br>using the addSource method, with one Kafka topic “src-topic” fed to<br>it.</li>
<li>首先，源节点命名为“源”添加到topology使用addSource方法，使用“src-topic”这个Kafka 主题。</li>
<li>Three processor nodes are then added using the addProcessor method;<br>here the first processor is a child of the “SOURCE” node, but is the<br>parent of the other two processors.</li>
<li>三个processor节点，然后使用addProcessor方法添加；这里第一个processor是一个“源”节点的子节点，但是其他两处processors</li>
<li>Finally three sink nodes are added to complete the topology using the<br>addSink method, each piping from a different parent processor node<br>and writing to a separate topic.</li>
<li>最后，添加三个汇聚节点来完成使用的topology addSink方法，每个管道从不同的父节点的processor和写到一个单独的主题。</li>
</ul>
<h3 id="Local-State-Store本地状态存储"><a href="#Local-State-Store本地状态存储" class="headerlink" title="Local State Store本地状态存储"></a>Local State Store本地状态存储</h3><p>Note that the Processor API is not limited to only accessing the current records as they arrive, but can also maintain local state stores that keep recently arrived records to use in stateful processing operations such as aggregation or windowed joins. To take advantage of this local states, developers can use the TopologyBuilder.addStateStore method when building the processor topology to create the local state and associate it with the processor nodes that needs to access it; or they can connect a created local state store with the existing processor nodes through TopologyBuilder.connectProcessorAndStateStores.<br>注意Processor API不仅限于访问当前记录，还可以维持本地状态村粗，使记录使用状态的处理操作，如聚集或窗口的加入。利用局部状态，开发人员可以使用TopologyBuilder.addStateStore方法当搭建processor topology创建本地状态，它与processor节点需要访问它的联想；或者他们可以连接创建的局部状态存储与现有的处理器节点通过TopologyBuilder.connectProcessorAndStateStores.<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</div><div class="line">    builder.addSource(<span class="string">"SOURCE"</span>, <span class="string">"src-topic"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS1"</span>, <span class="attr">MyProcessor1</span>::<span class="keyword">new</span>, <span class="string">"SOURCE"</span>)</div><div class="line">        <span class="comment">// create the in-memory state store "COUNTS" associated with processor "PROCESS1"</span></div><div class="line">        .addStateStore(Stores.create(<span class="string">"COUNTS"</span>).withStringKeys().withStringValues().inMemory().build(), <span class="string">"PROCESS1"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS2"</span>, <span class="attr">MyProcessor3</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor3 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS3"</span>, <span class="attr">MyProcessor3</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor3 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        <span class="comment">// connect the state store "COUNTS" with processor "PROCESS2"</span></div><div class="line">        .connectProcessorAndStateStores(<span class="string">"PROCESS2"</span>, <span class="string">"COUNTS"</span>);</div><div class="line">        .addSink(<span class="string">"SINK1"</span>, <span class="string">"sink-topic1"</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addSink(<span class="string">"SINK2"</span>, <span class="string">"sink-topic2"</span>, <span class="string">"PROCESS2"</span>)</div><div class="line">        .addSink(<span class="string">"SINK3"</span>, <span class="string">"sink-topic3"</span>, <span class="string">"PROCESS3"</span>);</div></pre></td></tr></table></figure></p>
<p>例子：<br>搭建kafka_2.11-0.10.1.0 集群：<br>设置server.properties<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">#broker的全局唯一编号，不能重复</div><div class="line">broker.id=0</div><div class="line"></div><div class="line">#用来监听链接的端口，producer或consumer将在此端口建立连接</div><div class="line">port=9092</div><div class="line"></div><div class="line">#处理网络请求的线程数量</div><div class="line">num.network.threads=3</div><div class="line"></div><div class="line">#用来处理磁盘IO的线程数量</div><div class="line">num.io.threads=8</div><div class="line"></div><div class="line">#发送套接字的缓冲区大小</div><div class="line">socket.send.buffer.bytes=102400</div><div class="line"></div><div class="line">#接受套接字的缓冲区大小</div><div class="line">socket.receive.buffer.bytes=102400</div><div class="line"></div><div class="line">#请求套接字的缓冲区大小</div><div class="line">socket.request.max.bytes=104857600</div><div class="line"></div><div class="line">#kafka运行日志存放的路径</div><div class="line">log.dirs=/home/hadoop/apps/kafka_2.11-0.10.1.0/logs/kafka</div><div class="line"></div><div class="line">#topic在当前broker上的分片个数</div><div class="line">num.partitions=2</div><div class="line"></div><div class="line">#用来恢复和清理data下数据的线程数量</div><div class="line">num.recovery.threads.per.data.dir=1</div><div class="line"></div><div class="line">#segment文件保留的最长时间，超时将被删除</div><div class="line">log.retention.hours=168</div><div class="line"></div><div class="line">#滚动生成新的segment文件的最大时间</div><div class="line">log.roll.hours=168</div><div class="line"></div><div class="line">#日志文件中每个segment的大小，默认为1G</div><div class="line">log.segment.bytes=1073741824</div><div class="line"></div><div class="line">#周期性检查文件大小的时间</div><div class="line">log.retention.check.interval.ms=300000</div><div class="line"></div><div class="line">#日志清理是否打开</div><div class="line">log.cleaner.enable=true</div><div class="line"></div><div class="line">#broker需要使用zookeeper保存meta数据</div><div class="line">zookeeper.connect=www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka0.10.1.0</div><div class="line"></div><div class="line">#zookeeper链接超时时间</div><div class="line">zookeeper.connection.timeout.ms=6000</div><div class="line"></div><div class="line">#partion buffer中，消息的条数达到阈值，将触发flush到磁盘</div><div class="line">log.flush.interval.messages=10000</div><div class="line"></div><div class="line">#消息buffer的时间，达到阈值，将触发flush到磁盘</div><div class="line">log.flush.interval.ms=3000</div><div class="line"></div><div class="line">#删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除</div><div class="line">delete.topic.enable=true</div><div class="line"></div><div class="line">#此处的host.name为本机IP(重要),如果不改,则客户端会抛出:Producer connection to localhost:9092 unsuccessful 错误!</div><div class="line">host.name=www.hadoop01.com</div></pre></td></tr></table></figure></p>
<p>分发到其他机器<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scp -rp kafka_2<span class="number">.11</span><span class="number">-0.10</span><span class="number">.1</span><span class="number">.0</span>/ hadoop@www.hadoop02.com:<span class="regexp">/home/</span>hadoop/apps/</div><div class="line">scp -rp kafka_2<span class="number">.11</span><span class="number">-0.10</span><span class="number">.1</span><span class="number">.0</span>/ hadoop@www.hadoop03.com:<span class="regexp">/home/</span>hadoop/apps/</div></pre></td></tr></table></figure></p>
<p>修改server.properties 以下两个参数<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">broker.id=<span class="number">0</span></div><div class="line">host.name=www.hadoop01.com</div></pre></td></tr></table></figure></p>
<p>zookeeper设置，启动略。</p>
<p>创建主题</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --replication-factor <span class="number">1</span> --partitions <span class="number">1</span> --topic words</div><div class="line">Created topic <span class="string">"words"</span>.</div><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --replication-factor <span class="number">1</span> --partitions <span class="number">1</span> --topic counts</div><div class="line">Created topic <span class="string">"counts"</span>.</div><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh  --describe --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --topic words</div><div class="line">        Topic:words     PartitionCount:<span class="number">1</span>        ReplicationFactor:<span class="number">1</span>     Configs:</div><div class="line">        Topic: words    Partition: <span class="number">0</span>    Leader: <span class="number">2</span>       Replicas: <span class="number">2</span>     Isr: <span class="number">2</span></div><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh  --describe --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --topic counts</div><div class="line">        Topic:counts    PartitionCount:<span class="number">1</span>        ReplicationFactor:<span class="number">1</span>     Configs:</div><div class="line">        Topic: counts   Partition: <span class="number">0</span>    Leader: <span class="number">2</span>       Replicas: <span class="number">2</span>     Isr: <span class="number">2</span></div></pre></td></tr></table></figure>
<p>依次启动：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicLong;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.DoubleDeserializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.IntegerDeserializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</div><div class="line"></div><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">DemoConsumerManualCommit</span> </span>&#123;</div><div class="line"></div><div class="line">	public <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) throws Exception &#123;</div><div class="line">		args = <span class="keyword">new</span> <span class="built_in">String</span>[] &#123; <span class="string">"www.hadoop01.com:9092"</span>, <span class="string">"gender-amount"</span>, <span class="string">"group4"</span>, <span class="string">"consumer2"</span> &#125;;</div><div class="line">		<span class="keyword">if</span> (args == <span class="literal">null</span> || args.length != <span class="number">4</span>) &#123;</div><div class="line">			System.err.println(</div><div class="line">					<span class="string">"Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"</span>);</div><div class="line">			System.exit(<span class="number">1</span>);</div><div class="line">		&#125;</div><div class="line">		<span class="built_in">String</span> bootstrap = args[<span class="number">0</span>];</div><div class="line">		<span class="built_in">String</span> topic = args[<span class="number">1</span>];</div><div class="line">		<span class="built_in">String</span> groupid = args[<span class="number">2</span>];</div><div class="line">		<span class="built_in">String</span> clientid = args[<span class="number">3</span>];</div><div class="line">		</div><div class="line">		Properties props = <span class="keyword">new</span> Properties();</div><div class="line">		props.put(<span class="string">"bootstrap.servers"</span>, bootstrap);</div><div class="line">		props.put(<span class="string">"group.id"</span>, groupid);</div><div class="line">		props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</div><div class="line">		props.put(<span class="string">"key.deserializer"</span>, StringDeserializer.class.getName());</div><div class="line">		<span class="comment">//props.put("value.deserializer", DoubleDeserializer.class.getName());</span></div><div class="line">		props.put(<span class="string">"value.deserializer"</span>, IntegerDeserializer.class.getName());</div><div class="line">		props.put(<span class="string">"max.poll.interval.ms"</span>, <span class="string">"300000"</span>);</div><div class="line">		props.put(<span class="string">"max.poll.records"</span>, <span class="string">"500"</span>);</div><div class="line">		props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"earliest"</span>);</div><div class="line">		KafkaConsumer&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</div><div class="line">		consumer.subscribe(Arrays.asList(topic));</div><div class="line">		AtomicLong atomicLong = <span class="keyword">new</span> AtomicLong();</div><div class="line">		<span class="keyword">while</span> (<span class="literal">true</span>) &#123;</div><div class="line">			ConsumerRecords&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; records = consumer.poll(<span class="number">100</span>);</div><div class="line">			records.forEach(record -&gt; &#123;</div><div class="line">				System.out.printf(<span class="string">"client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n"</span>,</div><div class="line">						clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value());</div><div class="line">				<span class="keyword">if</span> (atomicLong.get() % <span class="number">10</span> == <span class="number">0</span>) &#123;</div><div class="line"><span class="comment">//					consumer.commitSync();</span></div><div class="line">				&#125;</div><div class="line">			&#125;);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.IntegerSerializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TopologyBuilder;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.Stores;</div><div class="line"></div><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">WordCountTopology</span> </span>&#123;</div><div class="line"></div><div class="line">	public <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) throws IOException &#123;</div><div class="line">		Properties props = <span class="keyword">new</span> Properties();</div><div class="line">        props.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"streams-wordcount-processor"</span>);</div><div class="line">        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"www.hadoop01.com:9092"</span>);</div><div class="line">        props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, <span class="string">"www.hadoop01.com:2181/kafka0.10.1.0"</span>);</div><div class="line">        props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</div><div class="line">        props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());</div><div class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</div><div class="line">		</div><div class="line">		TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</div><div class="line">		builder.addSource(<span class="string">"SOURCE"</span>, <span class="keyword">new</span> StringDeserializer(), <span class="keyword">new</span> StringDeserializer(), <span class="string">"words"</span>)</div><div class="line">				.addProcessor(<span class="string">"WordCountProcessor"</span>, <span class="attr">WordCountProcessor</span>::<span class="keyword">new</span>, <span class="string">"SOURCE"</span>)</div><div class="line">				.addStateStore(Stores.create(<span class="string">"Counts"</span>).withStringKeys().withIntegerValues().inMemory().build(), <span class="string">"WordCountProcessor"</span>)</div><div class="line"><span class="comment">//				.connectProcessorAndStateStores("WordCountProcessor", "Counts")</span></div><div class="line">				.addSink(<span class="string">"SINK"</span>, <span class="string">"count"</span>, <span class="keyword">new</span> StringSerializer(), <span class="keyword">new</span> IntegerSerializer(), <span class="string">"WordCountProcessor"</span>);</div><div class="line">		</div><div class="line">        KafkaStreams stream = <span class="keyword">new</span> KafkaStreams(builder, props);</div><div class="line">        stream.start();</div><div class="line">        System.in.read();</div><div class="line">        stream.close();</div><div class="line">        stream.cleanUp();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>启动producer<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kafka-<span class="built_in">console</span>-producer.sh --broker-list www.hadoop01.com:<span class="number">9092</span> --topic words</div><div class="line">hello apache hello kafka</div></pre></td></tr></table></figure></p>
<p>报错：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Exception <span class="keyword">in</span> thread <span class="string">"StreamThread-1"</span> org.apache.kafka.streams.errors.StreamsException: Extracted timestamp value is negative, which is not allowed.</div><div class="line">	at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:<span class="number">111</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:<span class="number">117</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:<span class="number">144</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:<span class="number">415</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:<span class="number">242</span>)</div></pre></td></tr></table></figure></p>
<p>为时间戳的原因<br>kafka 18May的时候何如了时间戳的东东<br><img src="http://oh6ybr0jg.bkt.clouddn.com/kafka-stream%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3.png" alt="image"><br>后来更改了主题：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/kafka-topics.sh --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --create --topic word --partitions <span class="number">1</span> --replication-factor <span class="number">1</span> --config message.timestamp.type=LogAppendTime</div><div class="line">bin/kafka-topics.sh --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --create --topic count --partitions <span class="number">1</span> --replication-factor <span class="number">1</span> --config message.timestamp.type=LogAppendTime</div></pre></td></tr></table></figure></p>
<p>先启动DemoConsumerManualCommit再启动WordCountTopology</p>
<p>命令行输入<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kafka-<span class="built_in">console</span>-producer.sh --broker-list www.hadoop01.com:<span class="number">9092</span> --topic word_test</div><div class="line">hello apache kafka hello apache spark hello storm</div></pre></td></tr></table></figure></p>
<p>最后控制台输出</p>
<p>应该是<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">0</span>, key = apache, value = <span class="number">2</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">1</span>, key = hello, value = <span class="number">3</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">2</span>, key = kafka, value = <span class="number">1</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">3</span>, key = spark, value = <span class="number">1</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">4</span>, key = storm, value = <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>查看所有主题<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/kafka-topics.sh --list --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span></div></pre></td></tr></table></figure></p>
<p>发现多了一个<br>streams-wordcount-processor-Counts-changelog</p>
<p>In the next section we present another way to build the processor topology: the Kafka Streams DSL. </p>
<h2 id="High-Level-Streams-DSL"><a href="#High-Level-Streams-DSL" class="headerlink" title="High-Level Streams DSL"></a>High-Level Streams DSL</h2><p>To build a processor topology using the Streams DSL, developers can apply the KStreamBuilder class, which is extended from the TopologyBuilder. A simple example is included with the source code for Kafka in the streams/examples package. The rest of this section will walk through some code to demonstrate the key steps in creating a topology using the Streams DSL, but we recommend developers to read the full example source codes for details.<br>使用DSL Streams创建topology，开发人员可以应用KStreamBuilder类，这是从TopologyBuilder延伸。一个简单的例子是包含了streams/examples package的源代码。本节的其余部分将通过一些代码来演示使用流DSL创建topology的关键步骤，但我们建议开发者阅读完整的示例源代码的细节。</p>
<p>KStream and KTable</p>
<p>The DSL uses two main abstractions. A KStream is an abstraction of a record stream, where each data record represents a self-contained datum in the unbounded data set. A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered a create). To illustrate the difference between KStreams and KTables, let’s imagine the following two data records are being sent to the stream: (“alice”, 1) –&gt; (“alice”, 3). If these records a KStream and the stream processing application were to sum the values it would return 4. If these records were a KTable, the return would be 3, since the last record would be considered as an update.</p>
<p>DSL使用的两种主要的抽象。一个KStream是记录流的一个抽象的概念，其中每个数据记录代表一个独立的数据的数据集。一个KTable是一个变更的流的一个抽象的概念，其中每个数据记录的更新。更精确地说，数据记录中的值被认为是同一个记录键的最后一个值的更新，如果有（如果一个相应的key不存在，则该更新将被视为一个创建）。说明kstreams和KTables之间的差异，让我们想象一下以下两个数据记录被发送到流：(“alice”, 1)——&gt;(“alice”, 3)。如果这些记录KStream和流处理应用进行总结的值将返回4。如果这些记录是一个KTable，返回的返回的将是3，因为过去的记录将被视为一种更新。</p>
<p>Create Source Streams from Kafka<br>创建Kafka Streams</p>
<p>Either a record stream (defined as KStream) or a changelog stream (defined as KTable) can be created as a source stream from one or more Kafka topics (for KTable you can only create the source stream from a single topic).<br>一个记录流（定义为KStream）或更新流（定义为KTable）可以创建从一个或多个Kafka主题的源流（为KTable你只能从一个单一的主题创建源流）。 </p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</div><div class="line">KStream source1 = builder.stream(<span class="string">"topic1"</span>, <span class="string">"topic2"</span>);</div><div class="line">KTable source2 = builder.table(<span class="string">"topic3"</span>, <span class="string">"stateStoreName"</span>);</div></pre></td></tr></table></figure>
<p>Windowing a stream 窗口流<br>A stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for join and aggregation operations, etc. Kafka Streams currently defines the following types of windows:</p>
<p>流处理器可能需要将数据记录划分成时间桶，即通过时间窗口的流。这通常是需要的连接和聚合操作等。Kafka流目前定义了以下类型的窗口： </p>
<ul>
<li><p>Hopping time windows are windows based on time intervals. They model<br>fixed-sized, (possibly) overlapping windows. A hopping window is<br>defined by two properties: the window’s size and its advance interval<br>(aka “hop”). The advance interval specifies by how much a window<br>moves forward relative to the previous one. For example, you can<br>configure a hopping window with a size 5 minutes and an advance<br>interval of 1 minute. Since hopping windows can overlap a data record<br>may belong to more than one such windows.<br>跳跃的时间（跳时?）窗口是基于时间间隔的窗口。他们模型固定大小的，（可能）重叠的窗口。跳跃窗口是由两个属性定义的：窗口的大小和它的提前间隔（又名”hop”）。提前间隔由一个窗口相对于前一个窗口移动的多少来指定。例如，您可以配置一个具有大小5分钟和一个提前间隔1分钟的跳跃窗口。由于跳跃窗口可以重叠数据记录，可能属于多个这样的窗口。</p>
</li>
<li><p>Tumbling time windows are a special case of hopping time windows and,<br>like the latter, are windows based on time intervals. They model<br>fixed-size, non-overlapping, gap-less windows. A tumbling window is<br>defined by a single property: the window’s size. A tumbling window is<br>a hopping window whose window size is equal to its advance interval.<br>Since tumbling windows never overlap, a data record will belong to<br>one and only one window.<br>翻滚时间窗口是一个特殊的情况下的跳跃时间窗口，和后者一样，是基于时间间隔的窗口。他们模型固定的大小，不重叠，无缝隙的窗口。一个翻滚窗口是由一个单一属性定义的：窗口的大小。一个翻滚窗口是一个跳跃的窗口，它的窗口大小等于它的预先间隔。由于翻滚的窗口永远不会重叠，数据记录将属于一个并且只有一个窗口。 </p>
</li>
<li><p>Sliding windows model a fixed-size window that slides continuously<br>over the time axis; here, two data records are said to be included in<br>the same window if the difference of their timestamps is within the<br>window size. Thus, sliding windows are not aligned to the epoch, but<br>on the data record timestamps. In Kafka Streams, sliding windows are<br>used only for join operations, and can be specified through the<br>JoinWindows class.<br>滑动窗口模型一个固定大小的窗口，幻灯片不断在时间轴上；在这里，两个数据记录，说是如果他们的时间差异是在窗口的大小，包括在同一个窗口。因此，滑动窗口不一致的时代，但在数据记录的时间戳。Kafka流，滑动窗口只用于连接操作，并可以通过JoinWindows类指定。</p>
</li>
</ul>
<h3 id="Joins"><a href="#Joins" class="headerlink" title="Joins"></a>Joins</h3><p>A join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely. In Kafka Streams, you may perform the following join operations:<br>一个连接操作基于它们的数据记录的键合并两个流，并产生一个新的流。在记录流的加入通常需要执行在视窗基础否则记录必须保持履行加入的数量可以无限增长。在Kafka Streams中，您可以执行以下连接操作：</p>
<ul>
<li>KStream-to-KStream Joins are always windowed joins, since otherwise<br>the memory and state required to compute the join would grow<br>infinitely in size. Here, a newly received record from one of the<br>streams is joined with the other stream’s records within the<br>specified window interval to produce one result for each matching<br>pair based on user-provided ValueJoiner. A new KStream instance<br>representing the result stream of the join is returned from this<br>operator.<br>KStream-to-KStream Joins总是窗口连接，否则内存和计算所需的join会无限增长的大小。在这里，新接收的记录从一条数据流与其他流的记录在指定的窗口间隔为对每一个匹配的基于用户提供的ValueJoiner产生的一个结果。一个新的KStream实例表示的加入导致流从这个操作符返回。</li>
<li>KTable-to-KTable Joins are join operations designed to be consistent<br>with the ones in relational databases. Here, both changelog streams<br>are materialized into local state stores first. When a new record is<br>received from one of the streams, it is joined with the other<br>stream’s materialized state stores to produce one result for each<br>matching pair based on user-provided ValueJoiner. A new KTable<br>instance representing the result stream of the join, which is also a<br>changelog stream of the represented table, is returned from this<br>operator.<br>KTable-to-KTable Joins 操作设计与关系数据库中的一致行动。在这里，无论是修改流物化在当地商店。当一个新的记录从一个流的接收，它与其他流的物化状态存储为对每一个匹配的基于用户提供的ValueJoiner产生的一个结果。一个新的KTable实例代表了连接流，这也是一个代表表更新流，是从这个操作符返回。 </li>
<li>KStream-to-KTable Joins allow you to perform table lookups against a<br>changelog stream (KTable) upon receiving a new record from another<br>record stream (KStream). An example use case would be to enrich a<br>stream of user activities (KStream) with the latest user profile<br>information (KTable). Only records received from the record stream<br>will trigger the join and produce results via ValueJoiner, not vice<br>versa (i.e., records received from the changelog stream will be used<br>only to update the materialized state store). A new KStream instance<br>representing the result stream of the join is returned from this<br>operator.<br>KStream-to-KTable Joins允许你执行表查找和修改流（ktable）在从另一个记录流接收一个新的记录（KStream）。一个例子使用案例将丰富用户活动流（KStream）最新的用户配置文件信息（KTable）。只记录收到的记录流会触发连接并通过ValueJoiner产生结果，反之亦然（即记录收到更新流只会被用来更新物化状态存储）。一个新的KStream实例表示的加入导致流从这个操作符返回。</li>
</ul>
<p>Depending on the operands the following join operations are supported: inner joins, outer joins and left joins. Their semantics are similar to the corresponding operators in relational databases. a<br>Transform a stream</p>
<p>There is a list of transformation operations provided for KStream and KTable respectively. Each of these operations may generate either one or more KStream and KTable objects and can be translated into one or more connected processors into the underlying processor topology. All these transformation methods can be chained together to compose a complex processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types.</p>
<p>Among these transformations, filter, map, mapValues, etc, are stateless transformation operations and can be applied to both KStream and KTable, where users can usually pass a customized function to these functions as a parameter, such as Predicate for filter, KeyValueMapper for map, etc:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// written in Java 8+, using lambda expressions</span></div><div class="line">KStream mapped = source1.mapValue(record -&gt; record.get(<span class="string">"category"</span>));</div></pre></td></tr></table></figure></p>
<p>Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor; Stateful transformations, on the other hand, require accessing an associated state for processing and producing outputs. For example, in join and aggregate operations, a windowing state is usually used to store all the received records within the defined window boundary so far. The operators can then access these accumulated records in the store and compute based on them.<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// written in Java 8+, using lambda expressions</span></div><div class="line">KTable, Long&gt; counts = source1.groupByKey().aggregate(</div><div class="line">    () -&gt; <span class="number">0</span>L,  <span class="comment">// initial value</span></div><div class="line">    (aggKey, value, aggregate) -&gt; aggregate + <span class="number">1</span>L,   <span class="comment">// aggregating value</span></div><div class="line">    TimeWindows.of(<span class="string">"counts"</span>, <span class="number">5000</span>L).advanceBy(<span class="number">1000</span>L), <span class="comment">// intervals in milliseconds</span></div><div class="line">    Serdes.Long() <span class="comment">// serde for aggregated value</span></div><div class="line">);</div><div class="line"></div><div class="line">KStream joined = source1.leftJoin(source2,</div><div class="line">    (record1, record2) -&gt; record1.get(<span class="string">"user"</span>) + <span class="string">"-"</span> + record2.get(<span class="string">"region"</span>);</div><div class="line">);</div></pre></td></tr></table></figure></p>
<p>Write streams back to Kafka</p>
<p>At the end of the processing, users can choose to (continuously) write the final resulted streams back to a Kafka topic through KStream.to and KTable.to.<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">joined.to(<span class="string">"topic4"</span>);</div></pre></td></tr></table></figure></p>
<p>If your application needs to continue reading and processing the records after they have been materialized to a topic via to above, one option is to construct a new stream that reads from the output topic; Kafka Streams provides a convenience method called through:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// equivalent to</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// joined.to("topic4");</span></div><div class="line"><span class="comment">// materialized = builder.stream("topic4");</span></div><div class="line">KStream materialized = joined.through(<span class="string">"topic4"</span>);</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-kafka " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/02/codis 集群/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/02/codis 集群/" itemprop="url">
                  codis 集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-02T23:33:30+08:00">
                2016-12-02
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/02/codis 集群/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/02/codis 集群/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>安装go语言<br><a href="http://www.golangtc.com/download" target="_blank" rel="external">http://www.golangtc.com/download</a></p>
<p>选择1.7.3下载<br><img src="http://oh6ybr0jg.bkt.clouddn.com/go1.7.3.jpg" alt="go1.7.3"><br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar -zxvf go1<span class="number">.7</span><span class="number">.3</span>.linux-amd64.tar.gz -C /usr/local/</div></pre></td></tr></table></figure></p>
<p>下载jdk和zookeeper并解压</p>
<p>配置zookeeper</p>
<p>创建文件夹 zkData和zkLog<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cp zoo_sample.cfg zoo.cfg</div><div class="line"></div><div class="line">vi zoo.cfg </div><div class="line">dataDir=<span class="regexp">/root/</span>zookeeper/zookeeper<span class="number">-3.4</span><span class="number">.6</span>/zkData</div><div class="line">dataLogDir=<span class="regexp">/usr/</span>bigdata/zookeeper<span class="number">-3.4</span><span class="number">.6</span>/zkLog</div><div class="line">server<span class="number">.1</span>=master:<span class="number">2888</span>:<span class="number">3888</span></div></pre></td></tr></table></figure></p>
<p>进入zkData<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo <span class="number">1</span> &gt; myid</div></pre></td></tr></table></figure></p>
<p>– 配置环境变量<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">export</span> JAVA_HOME=<span class="regexp">/root/</span>java/jdk1<span class="number">.8</span><span class="number">.0</span>_111</div><div class="line"><span class="keyword">export</span> GOROOT=<span class="regexp">/usr/</span>local/go</div><div class="line"><span class="keyword">export</span> GOPATH=<span class="regexp">/usr/</span>local/codis</div><div class="line"><span class="keyword">export</span> ZOOKEEPER_HOME=<span class="regexp">/root/</span>zookeeper/zookeeper<span class="number">-3.4</span><span class="number">.6</span></div><div class="line"><span class="keyword">export</span> PATH=$PATH:$JAVA_HOME/bin:$GOROOT/bin:$GOPATH/bin:$ZOOKEEPER_HOME/bin</div></pre></td></tr></table></figure></p>
<p>下载codis<br><a href="https://github.com/CodisLabs/codis/releases" target="_blank" rel="external">https://github.com/CodisLabs/codis/releases</a><br>选择与go版本一样的codis版本<br><img src="http://oh6ybr0jg.bkt.clouddn.com/codis3.1.0-go1.7.3.png" alt="codis3.1.0-go1.7.1"></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir -p $GOPATH/src/github.com/CodisLabs</div><div class="line">tar -xzf codis<span class="number">-3.1</span><span class="number">.0</span>.tar.gz -C /usr/local/codis/src/github.com/CodisLabs/</div><div class="line">cd /usr/local/codis/src/github.com/CodisLabs/</div><div class="line">mv codis<span class="number">-3.1</span><span class="number">.0</span>/ codis</div><div class="line">cd codis/</div><div class="line">make</div></pre></td></tr></table></figure>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis_make.jpg" alt="codis_make"><br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">make gotest</div></pre></td></tr></table></figure></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis_gotest.jpg" alt="codis_gotest"></p>
<p>执行成功后进入bin 目录<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/codis/src/github.com/CodisLabs/codis/bin</div></pre></td></tr></table></figure></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-bin.jpg" alt="codis-bin"></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir /etc/codis</div><div class="line">cd /usr/local/codis/src/github.com/CodisLabs/codis/extern</div><div class="line">cp redis<span class="number">-2.8</span><span class="number">.21</span>/redis.conf /etc/codis/redis6379.conf</div></pre></td></tr></table></figure>
<p>创建redis 配置文件中所指定的目录<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 codis]# cd /opt</div><div class="line">[root@zhm1 opt]# mkdir -p codisapp/run</div><div class="line">[root@zhm1 opt]# mkdir -p codisapp/logs</div><div class="line">[root@zhm1 opt]# mkdir -p codisapp/data/6379 codisapp/data/6380</div><div class="line">[root@zhm1 opt]# </div><div class="line"></div><div class="line">vi /etc/codis/redis6379.conf</div><div class="line"></div><div class="line">daemonize yes</div><div class="line">pidfile /opt/codisapp/run/redis6379.pid</div><div class="line">port 6379</div><div class="line">logfile "/opt/codisapp/logs/redis6379.log"</div><div class="line">dbfilename dump.rdb</div><div class="line">dir /opt/codisapp/data/6379</div><div class="line"></div><div class="line">cp /etc/codis/redis6379.conf /etc/codis/redis6380.conf</div><div class="line">vi /etc/codis/redis6380.conf</div><div class="line"></div><div class="line">daemonize yes</div><div class="line">pidfile /opt/codisapp/run/redis6380.pid</div><div class="line">port 6380</div><div class="line">logfile "/opt/codisapp/logs/redis6380.log"</div><div class="line">dbfilename dump.rdb</div><div class="line">dir /opt/codisapp/data/6380</div></pre></td></tr></table></figure></p>
<p>启动redis<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 opt]# cd /usr/local/codis/src/github.com/CodisLabs/codis/</div><div class="line">[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6379.conf --protected-mode no </div><div class="line">[root@zhm1 codis]# ./bin/codis-server /etc/codis/redis6380.conf --protected-mode no</div></pre></td></tr></table></figure></p>
<p>此处启动增加–protected-mode no  参数可以防止后面添加server的时候提示<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Cause<span class="string">": "</span>DENIED Redis is running <span class="keyword">in</span> protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In <span class="keyword">this</span> mode connections are only accepted <span class="keyword">from</span> the loopback interface. If you want to connect <span class="keyword">from</span> external computers to Redis you may adopt one <span class="keyword">of</span> the following solutions: <span class="number">1</span>) Just disable protected mode sending the command <span class="string">'CONFIG SET protected-mode no'</span> <span class="keyword">from</span> the loopback interface by connecting to Redis <span class="keyword">from</span> the same host the server is running, however MAKE SURE Redis is not publicly accessible <span class="keyword">from</span> internet <span class="keyword">if</span> you <span class="keyword">do</span> so. Use CONFIG REWRITE to make <span class="keyword">this</span> change permanent. <span class="number">2</span>) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to <span class="string">'no'</span>, and then restarting the server. <span class="number">3</span>) If you started the server manually just <span class="keyword">for</span> testing, restart it <span class="keyword">with</span> the <span class="string">'--protected-mode no'</span> option. <span class="number">4</span>) Setup a bind address or an authentication password. NOTE: You only need to <span class="keyword">do</span> one <span class="keyword">of</span> the above things <span class="keyword">in</span> order <span class="keyword">for</span> the server to start accepting connections <span class="keyword">from</span> the outside.</div></pre></td></tr></table></figure></p>
<p>启动dashboard<br>自定义dashboard 配置文件<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/codis-dashboard --<span class="keyword">default</span>-config | tee dashboard.conf</div></pre></td></tr></table></figure></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-dashboard.jpg" alt="codis-dashboard"></p>
<p>修改IP<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 codis]# vi dashboard.conf </div><div class="line">##################################################</div><div class="line">#                                                #</div><div class="line">#                  Codis-Dashboard               #</div><div class="line">#                                                #</div><div class="line">##################################################</div><div class="line"></div><div class="line"># Set Coordinator, only accept "zookeeper" &amp; "etcd".</div><div class="line">coordinator_name = "zookeeper"</div><div class="line">coordinator_addr = "192.168.110.129:2181"</div><div class="line"></div><div class="line"># Set Codis Product Name/Auth.</div><div class="line">product_name = "codis-demo"</div><div class="line">product_auth = ""</div><div class="line"></div><div class="line"># Set bind address for admin(rpc), tcp only.</div><div class="line">admin_addr = "192.168.110.129:18080"</div><div class="line"></div><div class="line"># Set quorum value for sentinel, default is 2.</div><div class="line">sentinel_quorum = 2</div></pre></td></tr></table></figure></p>
<p>启动zookeeper<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 codis]# zkServer.sh start</div><div class="line">JMX enabled by default</div><div class="line">Using config: /root/zookeeper/zookeeper-3.4.6/bin/../conf/zoo.cfg</div><div class="line">Starting zookeeper ... STARTED</div></pre></td></tr></table></figure></p>
<p>启动dashboard<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 codis]# nohup bin/codis-dashboard --ncpu=2 --config=dashboard.conf --log=dashboard.log --log-level=WARN &amp;</div></pre></td></tr></table></figure></p>
<p>查看日志输出，连接zookeeper 成功<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 codis]# cat dashboard.log.2016-12-21</div><div class="line">2016/12/21 11:18:34 main.go:77: [WARN] set ncpu = 2</div><div class="line">2016/12/21 11:18:34 topom.go:110: [WARN] create new topom:</div><div class="line">&#123;</div><div class="line">    "token": "6047b5ce9ef2826c1eb40692023b9bf4",</div><div class="line">    "start_time": "2016-12-21 11:18:34.481106135 +0800 CST",</div><div class="line">    "admin_addr": "192.168.110.129:18080",</div><div class="line">    "product_name": "codis-demo",</div><div class="line">    "pid": 66994,</div><div class="line">    "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis",</div><div class="line">    "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux"</div><div class="line">&#125;</div><div class="line">2016/12/21 11:18:35 main.go:124: [WARN] create topom with config</div><div class="line">coordinator_name = "zookeeper"</div><div class="line">coordinator_addr = "192.168.110.129:2181"</div><div class="line">admin_addr = "192.168.110.129:18080"</div><div class="line">product_name = "codis-demo"</div><div class="line">product_auth = ""</div><div class="line">sentinel_quorum = 2</div><div class="line">2016/12/21 11:18:36 topom.go:381: [WARN] admin start service on 192.168.110.129:18080</div></pre></td></tr></table></figure></p>
<p>启动codis-proxy<br>编译codis-proxy 配置文件<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 codis]# ./bin/codis-proxy --default-config | tee proxy.conf</div><div class="line"></div><div class="line">##################################################</div><div class="line">#                                                #</div><div class="line">#                  Codis-Proxy                   #</div><div class="line">#                                                #</div><div class="line">##################################################</div><div class="line"></div><div class="line"># Set Codis Product Name/Auth.</div><div class="line">product_name = "codis-demo"</div><div class="line">product_auth = ""</div><div class="line"></div><div class="line"># Set bind address for admin(rpc), tcp only.</div><div class="line">admin_addr = "0.0.0.0:11080"</div><div class="line"></div><div class="line"># Set bind address for proxy, proto_type can be "tcp", "tcp4", "tcp6", "unix" or "unixpacket".</div><div class="line">proto_type = "tcp4"</div><div class="line">proxy_addr = "0.0.0.0:19000"</div><div class="line"></div><div class="line"># Set jodis address &amp; session timeout, only accept "zookeeper" &amp; "etcd".</div><div class="line">jodis_name = ""</div><div class="line">jodis_addr = ""</div><div class="line">jodis_timeout = "20s"</div><div class="line">jodis_compatible = false</div><div class="line"></div><div class="line"># Set datacenter of proxy.</div><div class="line">proxy_datacenter = ""</div><div class="line"></div><div class="line"># Set max number of alive sessions.</div><div class="line">proxy_max_clients = 1000</div><div class="line"></div><div class="line"># Set max offheap memory size. (0 to disable)</div><div class="line">proxy_max_offheap_size = "1024mb"</div><div class="line"></div><div class="line"># Set heap placeholder to reduce GC frequency.</div><div class="line">proxy_heap_placeholder = "256mb"</div><div class="line"></div><div class="line"># Proxy will ping backend redis in a predefined interval. (0 to disable)</div><div class="line">backend_ping_period = "5s"</div><div class="line"></div><div class="line"># Set backend recv buffer size &amp; timeout.</div><div class="line">backend_recv_bufsize = "128kb"</div><div class="line">backend_recv_timeout = "30s"</div><div class="line"></div><div class="line"># Set backend send buffer &amp; timeout.</div><div class="line">backend_send_bufsize = "128kb"</div><div class="line">backend_send_timeout = "30s"</div><div class="line"></div><div class="line"># Set backend pipeline buffer size.</div><div class="line">backend_max_pipeline = 1024</div><div class="line"></div><div class="line"># Set backend never read replica groups, default is false</div><div class="line">backend_primary_only = false</div><div class="line"></div><div class="line"># Set backend parallel connections per server</div><div class="line">backend_primary_parallel = 1</div><div class="line">backend_replica_parallel = 1</div><div class="line"></div><div class="line"># Set backend tcp keepalive period. (0 to disable)</div><div class="line">backend_keepalive_period = "75s"</div><div class="line"></div><div class="line"># If there is no request from client for a long time, the connection will be closed. (0 to disable)</div><div class="line"># Set session recv buffer size &amp; timeout.</div><div class="line">session_recv_bufsize = "128kb"</div><div class="line">session_recv_timeout = "30m"</div><div class="line"></div><div class="line"># Set session send buffer size &amp; timeout.</div><div class="line">session_send_bufsize = "64kb"</div><div class="line">session_send_timeout = "30s"</div><div class="line"></div><div class="line"># Make sure this is higher than the max number of requests for each pipeline request, or your client may be blocked.</div><div class="line"># Set session pipeline buffer size.</div><div class="line">session_max_pipeline = 512</div><div class="line"></div><div class="line"># Set session tcp keepalive period. (0 to disable)</div><div class="line">session_keepalive_period = "75s"</div><div class="line"></div><div class="line"># Set metrics server (such as http://localhost:28000), proxy will report json formatted metrics to specified server in a predefined period.</div><div class="line">metrics_report_server = ""</div><div class="line">metrics_report_period = "1s"</div><div class="line"></div><div class="line"># Set influxdb server (such as http://localhost:8086), proxy will report metrics to influxdb.</div><div class="line">metrics_report_influxdb_server = ""</div><div class="line">metrics_report_influxdb_period = "1s"</div><div class="line">metrics_report_influxdb_username = ""</div><div class="line">metrics_report_influxdb_password = ""</div><div class="line">metrics_report_influxdb_database = ""</div></pre></td></tr></table></figure></p>
<p>product_name 集群名称，参考dashboard 参数说明<br>jodis_addr Jodis 注册zookeeper 地址<br>此处把IP修改一下<br>jodis_addr = “192.168.110.129:2181”</p>
<p>启动codis-proxy</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">nohup bin/codis-proxy --ncpu=2 --config=proxy.conf --log =proxy.log --log-level=WARN &amp;</div><div class="line"></div><div class="line">[root@zhm1 codis]# cat =proxy.log.2016-12-21</div><div class="line">2016/12/21 11:25:01 main.go:100: [WARN] set ncpu = 2, max-ncpu = 0</div><div class="line">2016/12/21 11:25:01 proxy.go:89: [WARN] [0xc42010e840] create new proxy:</div><div class="line">&#123;</div><div class="line">    "token": "673cc48d563a3614c13a3a294067f0b9",</div><div class="line">    "start_time": "2016-12-21 11:25:01.70765896 +0800 CST",</div><div class="line">    "admin_addr": "192.168.110.129:11080",</div><div class="line">    "proto_type": "tcp4",</div><div class="line">    "proxy_addr": "192.168.110.129:19000",</div><div class="line">    "product_name": "codis-demo",</div><div class="line">    "pid": 67097,</div><div class="line">    "pwd": "/usr/local/codis/src/github.com/CodisLabs/codis",</div><div class="line">    "sys": "Linux zhm1.cn 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux",</div><div class="line">    "hostname": "zhm1.cn",</div><div class="line">    "datacenter": ""</div><div class="line">&#125;</div><div class="line">2016/12/21 11:25:01 proxy.go:353: [WARN] [0xc42010e840] admin start service on [::]:11080</div><div class="line">2016/12/21 11:25:01 main.go:159: [WARN] create proxy with config</div><div class="line">proto_type = "tcp4"</div><div class="line">proxy_addr = "0.0.0.0:19000"</div><div class="line">admin_addr = "0.0.0.0:11080"</div><div class="line">jodis_name = ""</div><div class="line">jodis_addr = "192.168.110.129:2181"</div><div class="line">jodis_timeout = "20s"</div><div class="line">jodis_compatible = false</div><div class="line">product_name = "codis-demo"</div><div class="line">product_auth = ""</div><div class="line">proxy_datacenter = ""</div><div class="line">proxy_max_clients = 1000</div><div class="line">proxy_max_offheap_size = "1gb"</div><div class="line">proxy_heap_placeholder = "256mb"</div><div class="line">backend_ping_period = "5s"</div><div class="line">backend_recv_bufsize = "128kb"</div><div class="line">backend_recv_timeout = "30s"</div><div class="line">backend_send_bufsize = "128kb"</div><div class="line">backend_send_timeout = "30s"</div><div class="line">backend_max_pipeline = 1024</div><div class="line">backend_primary_only = false</div><div class="line">backend_primary_parallel = 1</div><div class="line">backend_replica_parallel = 1</div><div class="line">backend_keepalive_period = "75s"</div><div class="line">session_recv_bufsize = "128kb"</div><div class="line">session_recv_timeout = "30m"</div><div class="line">session_send_bufsize = "64kb"</div><div class="line">session_send_timeout = "30s"</div><div class="line">session_max_pipeline = 512</div><div class="line">session_keepalive_period = "75s"</div><div class="line">metrics_report_server = ""</div><div class="line">metrics_report_period = "1s"</div><div class="line">metrics_report_influxdb_server = ""</div><div class="line">metrics_report_influxdb_period = "1s"</div><div class="line">metrics_report_influxdb_username = ""</div><div class="line">metrics_report_influxdb_password = ""</div><div class="line">metrics_report_influxdb_database = ""</div><div class="line">2016/12/21 11:25:01 main.go:180: [WARN] [0xc42010e840] proxy waiting online ...</div><div class="line">2016/12/21 11:25:01 proxy.go:377: [WARN] [0xc42010e840] proxy start service on 0.0.0.0:19000</div><div class="line">2016/12/21 11:25:02 main.go:180: [WARN] [0xc42010e840] proxy waiting online ...</div></pre></td></tr></table></figure>
<p>codis-proxy 启动后，处于waiting 状态，监听proxy_addr 地址，但是不会accept 连接，添加到集群并完成集群状态的同步，才能改变状态为online。</p>
<p>添加codis-proxy 到集群<br>添加的方法有以下两种：<br>通过codis-fe 添加，通过Add Proxy 按钮，将admin_addr 加入到集群中；<br>通过codis-admin 命令行工具添加：<br>./bin/codis-admin –dashboard=192.168.110.129:18080 –create-proxy -x 192.168.110.129:11080</p>
<p>查看zookeeper 存储的数据<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">zkCli.sh</div><div class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">0</span>] ls /</div><div class="line">[codis3, zookeeper, kafka]</div><div class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">1</span>] ls /codis3</div><div class="line">[codis-demo]</div><div class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">2</span>] ls /codis3/codis-demo</div><div class="line">[proxy, topom]</div><div class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">3</span>] ls /codis3/codis-demo/proxy</div><div class="line">[proxy<span class="number">-673</span>cc48d563a3614c13a3a294067f0b9]</div><div class="line">[zk: localhost:<span class="number">2181</span>(CONNECTED) <span class="number">4</span>] ls /codis3/codis-demo/topom</div><div class="line">[]</div></pre></td></tr></table></figure></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/zk-codis.jpg" alt="zk-codis"></p>
<p>配置启动Cdis FE 集群管理界面<br>创建conf 目录，放下面生成的codis.json 文件<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">mkdir conf</div><div class="line">./bin/codis-admin --dashboard-list --zookeeper=<span class="number">192.168</span><span class="number">.110</span><span class="number">.129</span> | tee conf/codis.json</div><div class="line"><span class="number">2016</span>/<span class="number">12</span>/<span class="number">21</span> <span class="number">11</span>:<span class="number">32</span>:<span class="number">15</span> zkclient.go:<span class="number">23</span>: [INFO] zookeeper - zkclient setup <span class="keyword">new</span> connection to <span class="number">192.168</span><span class="number">.110</span><span class="number">.129</span></div><div class="line"><span class="number">2016</span>/<span class="number">12</span>/<span class="number">21</span> <span class="number">11</span>:<span class="number">32</span>:<span class="number">15</span> zkclient.go:<span class="number">23</span>: [INFO] zookeeper - Connected to <span class="number">192.168</span><span class="number">.110</span><span class="number">.129</span>:<span class="number">2181</span></div><div class="line"><span class="number">2016</span>/<span class="number">12</span>/<span class="number">21</span> <span class="number">11</span>:<span class="number">32</span>:<span class="number">15</span> zkclient.go:<span class="number">23</span>: [INFO] zookeeper - Authenticated: id=<span class="number">25085718467510274</span>, timeout=<span class="number">40000</span></div><div class="line">[</div><div class="line">    &#123;</div><div class="line">        <span class="string">"name"</span>: <span class="string">"codis-demo"</span>,</div><div class="line">        <span class="string">"dashboard"</span>: <span class="string">"192.168.110.129:18080"</span></div><div class="line">    &#125;</div><div class="line">]</div><div class="line"><span class="number">2016</span>/<span class="number">12</span>/<span class="number">21</span> <span class="number">11</span>:<span class="number">32</span>:<span class="number">15</span> zkclient.go:<span class="number">23</span>: [INFO] zookeeper - Recv loop terminated: err=EOF</div><div class="line"><span class="number">2016</span>/<span class="number">12</span>/<span class="number">21</span> <span class="number">11</span>:<span class="number">32</span>:<span class="number">15</span> zkclient.go:<span class="number">23</span>: [INFO] zookeeper - Send loop terminated: err=&lt;nil&gt;</div></pre></td></tr></table></figure></p>
<p>启动codis-fe<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">./bin/codis-fe --ncpu=2 --log=fe.log --log-level=WARN --dashboard-list=conf/codis.json --listen=192.168.110.129:18090 &amp;</div><div class="line">[3] 67271</div><div class="line">[root@zhm1 codis]# cat fe.log.2016-12-21 </div><div class="line">2016/12/21 11:33:34 main.go:102: [WARN] set ncpu = 2</div><div class="line">2016/12/21 11:33:34 main.go:105: [WARN] set listen = 192.168.110.129:18090</div><div class="line">2016/12/21 11:33:34 main.go:117: [WARN] set assets = /usr/local/codis/src/github.com/CodisLabs/codis/bin/assets</div><div class="line">2016/12/21 11:33:34 main.go:132: [WARN] set --dashboard-list = conf/codis.json</div></pre></td></tr></table></figure></p>
<p>至此codis 的图形界面已经能够显示。<br>打开浏览器访问<a href="http://192.168.110.129:18090/，通过管理界面操作codis" target="_blank" rel="external">http://192.168.110.129:18090/，通过管理界面操作codis</a></p>
<p>如果是机器外访问需要关闭防火墙<br>centos7的命令是<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 codis]# systemctl stop firewalld.service</div><div class="line">[root@zhm1 codis]# systemctl disable firewalld.service</div><div class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</div><div class="line">Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service.</div></pre></td></tr></table></figure></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-demo-web.jpg" alt="codis-web1"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-demo-web2.jpg" alt="codis-web2"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-demo-web3.jpg" alt="codis-web3"></p>
<p>创建组和实例<br>操作步骤：<br>在New Group 后面输入1（表示增加编号为1 的组），点击New Group 完成组的创建；<br>然后，Add Server，后面设置ip 和端口，并指向组编号1，点击Add Server 完成redis 实例的创建；</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-group1.jpg" alt="codis-group1"></p>
<p>添加第2 个组，以及第2 个组的实例，操作步骤同上。</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/codis-group2.jpg" alt="codis-group2"></p>
<p>对slot 进行分组<br>输入slot范围和组</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/migrate-slots1.jpg" alt="migrate-slots1.jpg"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/migrate-slots2.jpg" alt="migrate-slots2.jpg"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/migrate-slots3.jpg" alt="migrate-slots3.jpg"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/migrate-slots4.jpg" alt="migrate-slots4.jpg"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/migrate-slots5.jpg" alt="migrate-slots5.jpg"></p>
<p>另外分享一个github上的好文<br>docker玩codis集群<br><a href="https://github.com/ruo91/docker-codis" target="_blank" rel="external">https://github.com/ruo91/docker-codis</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/02/redis集群/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/02/redis集群/" itemprop="url">
                  Redis 集群
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-02T23:33:30+08:00">
                2016-12-02
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/02/redis集群/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/02/redis集群/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>同时在每台机器配置如下内容：<br>创建一个空的集群文件</p>
<pre><code>vi nodes-6379.conf
:wq
</code></pre><p>修改配置文件，开启集群模式</p>
<pre><code>vi redis.conf
cluster-enabled yes
cluster-config-file nodes-6379.conf
</code></pre><p>启动：</p>
<pre><code>./src/redis-server redis.conf
</code></pre><p>然后输入查看命令<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[hadoop@www.hadoop01.com ~]$ps -ef | grep redis</div><div class="line">hadoop  <span class="number">2657</span>  <span class="number">2506</span>  <span class="number">0</span> <span class="number">21</span>:<span class="number">59</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> ./src/redis-server *:<span class="number">6379</span> [cluster]</div><div class="line">hadoop  <span class="number">2695</span>  <span class="number">2670</span>  <span class="number">0</span> <span class="number">22</span>:<span class="number">00</span> pts/<span class="number">1</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep redis</div><div class="line">    </div><div class="line">[hadoop@www.hadoop02.com ~]$ps -ef | grep redis</div><div class="line">hadoop  <span class="number">2991</span>  <span class="number">2662</span>  <span class="number">0</span> <span class="number">10</span>:<span class="number">12</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">01</span> ./src/redis-server *:<span class="number">6379</span> [cluster]</div><div class="line">hadoop  <span class="number">3039</span>  <span class="number">2999</span>  <span class="number">0</span> <span class="number">10</span>:<span class="number">21</span> pts/<span class="number">1</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep redis</div><div class="line">    </div><div class="line">[hadoop@www.hadoop03.com ~]$ps -ef | grep redis</div><div class="line">hadoop  <span class="number">2482</span>  <span class="number">2417</span>  <span class="number">0</span> <span class="number">13</span>:<span class="number">41</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">01</span> ./src/redis-server *:<span class="number">6379</span> [cluster]</div><div class="line">hadoop  <span class="number">2530</span>  <span class="number">2490</span>  <span class="number">0</span> <span class="number">13</span>:<span class="number">51</span> pts/<span class="number">1</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep redis</div></pre></td></tr></table></figure></p>
<p>每个后面都有一个cluster<br>此时并没有把节点添加到一个集群，用命令查看nodes为1<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[hadoop@www.hadoop01.com redis<span class="number">-3.0</span><span class="number">.7</span>]$./src/redis-cli</div><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster info</div><div class="line">cluster_state:fail</div><div class="line">cluster_slots_assigned:<span class="number">5</span></div><div class="line">cluster_slots_ok:<span class="number">5</span></div><div class="line">cluster_slots_pfail:<span class="number">0</span></div><div class="line">cluster_slots_fail:<span class="number">0</span></div><div class="line">cluster_known_nodes:<span class="number">1</span></div><div class="line">cluster_size:<span class="number">1</span></div><div class="line">cluster_current_epoch:<span class="number">0</span></div><div class="line">cluster_my_epoch:<span class="number">0</span></div><div class="line">cluster_stats_messages_sent:<span class="number">0</span></div><div class="line">cluster_stats_messages_received:<span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>这时需要添加集群节点：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; Cluster meet www.hadoop02.com <span class="number">6379</span></div><div class="line">(error) ERR Invalid node address specified: www.hadoop02.com:<span class="number">6379</span></div><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; Cluster meet <span class="number">192.168</span><span class="number">.247</span><span class="number">.152</span> <span class="number">6379</span></div><div class="line">OK</div><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; Cluster meet <span class="number">192.168</span><span class="number">.247</span><span class="number">.154</span> <span class="number">6379</span></div><div class="line">OK</div><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster info</div><div class="line">cluster_state:fail</div><div class="line">cluster_slots_assigned:<span class="number">5</span></div><div class="line">cluster_slots_ok:<span class="number">5</span></div><div class="line">cluster_slots_pfail:<span class="number">0</span></div><div class="line">cluster_slots_fail:<span class="number">0</span></div><div class="line">cluster_known_nodes:<span class="number">3</span></div><div class="line">cluster_size:<span class="number">1</span></div><div class="line">cluster_current_epoch:<span class="number">1</span></div><div class="line">cluster_my_epoch:<span class="number">1</span></div><div class="line">cluster_stats_messages_sent:<span class="number">27</span></div><div class="line">cluster_stats_messages_received:<span class="number">26</span></div></pre></td></tr></table></figure></p>
<p>可以看到节点数为3-&gt;cluster_known_nodes:3，但是集群状态是失败-&gt;cluster_state:fail</p>
<p>分配槽位一共16384个槽位<br>cluster addslots 0 1 2 …</p>
<p>写个脚本：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">vi fenpeicaowei.sh</div><div class="line">#bash</div><div class="line">for i in &#123;10000..16384&#125;; do ./src/redis-cli cluster addslots $i; done</div><div class="line"></div><div class="line"></div><div class="line">chmod a+x fenpeicaowei.sh</div><div class="line">./fenpeicaowei.sh</div></pre></td></tr></table></figure></p>
<p>查看有哪些节点(因为之前配置了主从，读者忽略slave节点，另外从节点分配槽位是不会被同步的)<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">./src/redis-cli -c</div><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster nodes</div><div class="line"><span class="number">276</span>a3675a706d4626a25c54892c368ca6c37002c <span class="number">192.168</span><span class="number">.247</span><span class="number">.154</span>:<span class="number">6379</span> slave <span class="number">64</span>a2d824459edc01c7b6b205021d01da96513121 <span class="number">0</span> <span class="number">1481166569926</span> <span class="number">1</span> connected</div><div class="line">da0322b3160917ff99e363b7488e833be959205d <span class="number">192.168</span><span class="number">.247</span><span class="number">.150</span>:<span class="number">6379</span> slave <span class="number">64</span>a2d824459edc01c7b6b205021d01da96513121 <span class="number">0</span> <span class="number">1481166571948</span> <span class="number">1</span> connected</div><div class="line"><span class="number">64</span>a2d824459edc01c7b6b205021d01da96513121 <span class="number">192.168</span><span class="number">.247</span><span class="number">.152</span>:<span class="number">6379</span> myself,master - <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> connected <span class="number">1584</span> <span class="number">2171</span> <span class="number">5649</span> <span class="number">9842</span> <span class="number">10000</span><span class="number">-16383</span></div></pre></td></tr></table></figure></p>
<p>查看槽位分配到哪些节点<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster slots</div><div class="line"><span class="number">1</span>) <span class="number">1</span>) (integer) <span class="number">1584</span></div><div class="line">   <span class="number">2</span>) (integer) <span class="number">1584</span></div><div class="line">   <span class="number">3</span>) <span class="number">1</span>) <span class="string">"192.168.247.152"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">4</span>) <span class="number">1</span>) <span class="string">"192.168.247.150"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">5</span>) <span class="number">1</span>) <span class="string">"192.168.247.154"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line"><span class="number">2</span>) <span class="number">1</span>) (integer) <span class="number">2171</span></div><div class="line">   <span class="number">2</span>) (integer) <span class="number">2171</span></div><div class="line">   <span class="number">3</span>) <span class="number">1</span>) <span class="string">"192.168.247.152"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">4</span>) <span class="number">1</span>) <span class="string">"192.168.247.150"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">5</span>) <span class="number">1</span>) <span class="string">"192.168.247.154"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line"><span class="number">3</span>) <span class="number">1</span>) (integer) <span class="number">5649</span></div><div class="line">   <span class="number">2</span>) (integer) <span class="number">5649</span></div><div class="line">   <span class="number">3</span>) <span class="number">1</span>) <span class="string">"192.168.247.152"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">4</span>) <span class="number">1</span>) <span class="string">"192.168.247.150"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">5</span>) <span class="number">1</span>) <span class="string">"192.168.247.154"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line"><span class="number">4</span>) <span class="number">1</span>) (integer) <span class="number">9842</span></div><div class="line">   <span class="number">2</span>) (integer) <span class="number">9842</span></div><div class="line">   <span class="number">3</span>) <span class="number">1</span>) <span class="string">"192.168.247.152"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">4</span>) <span class="number">1</span>) <span class="string">"192.168.247.150"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">5</span>) <span class="number">1</span>) <span class="string">"192.168.247.154"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line"><span class="number">5</span>) <span class="number">1</span>) (integer) <span class="number">10000</span></div><div class="line">   <span class="number">2</span>) (integer) <span class="number">16383</span></div><div class="line">   <span class="number">3</span>) <span class="number">1</span>) <span class="string">"192.168.247.152"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">4</span>) <span class="number">1</span>) <span class="string">"192.168.247.150"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div><div class="line">   <span class="number">5</span>) <span class="number">1</span>) <span class="string">"192.168.247.154"</span></div><div class="line">      <span class="number">2</span>) (integer) <span class="number">6379</span></div></pre></td></tr></table></figure></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster info</div><div class="line">cluster_state:fail</div><div class="line">cluster_slots_assigned:<span class="number">6388</span></div><div class="line">cluster_slots_ok:<span class="number">6388</span></div><div class="line">cluster_slots_pfail:<span class="number">0</span></div><div class="line">cluster_slots_fail:<span class="number">0</span></div><div class="line">cluster_known_nodes:<span class="number">3</span></div><div class="line">cluster_size:<span class="number">1</span></div><div class="line">cluster_current_epoch:<span class="number">1</span></div><div class="line">cluster_my_epoch:<span class="number">1</span></div><div class="line">cluster_stats_messages_sent:<span class="number">3002</span></div><div class="line">cluster_stats_messages_received:<span class="number">2999</span></div></pre></td></tr></table></figure>
<p>全部分配完成，就可以看到<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster info</div><div class="line">cluster_state:ok</div><div class="line">cluster_slots_assigned:<span class="number">16384</span></div><div class="line">cluster_slots_ok:<span class="number">16384</span></div><div class="line">cluster_slots_pfail:<span class="number">0</span></div><div class="line">cluster_slots_fail:<span class="number">0</span></div><div class="line">cluster_known_nodes:<span class="number">3</span></div><div class="line">cluster_size:<span class="number">1</span></div><div class="line">cluster_current_epoch:<span class="number">1</span></div><div class="line">cluster_my_epoch:<span class="number">1</span></div><div class="line">cluster_stats_messages_sent:<span class="number">792</span></div><div class="line">cluster_stats_messages_received:<span class="number">798</span></div></pre></td></tr></table></figure></p>
<p>查看某个key对应的槽位<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster keyslot <span class="number">3</span></div><div class="line">(integer) <span class="number">1584</span></div></pre></td></tr></table></figure></p>
<p>重新分片<br>redis提供一个工具：redis-trib.rb</p>
<p>但是要用这个工具需要安装ruby环境</p>
<p>下载repo文件<br>wget <a href="http://mirrors.163.com/.help/CentOS6-Base-163.repo" target="_blank" rel="external">http://mirrors.163.com/.help/CentOS6-Base-163.repo</a></p>
<p>备份并替换系统的repo文件<br>cd /etc/yum.repos.d/<br>mv CentOS-Base.repo CentOS-Base.repo.bak<br>mv CentOS6-Base-163.repo CentOS-Base.repo</p>
<p>执行yum源更新命令<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">yum clean all</div><div class="line">yum makecache</div><div class="line">yum update</div><div class="line"></div><div class="line">yum install -y ruby* --skip-broken </div><div class="line">yum install rubygems</div><div class="line">wget https:<span class="comment">//rubygems.global.ssl.fastly.net/gems/redis-3.2.2.gem</span></div><div class="line">gem install redis</div><div class="line">gem install -l ./redis<span class="number">-3.2</span><span class="number">.2</span>.gem</div></pre></td></tr></table></figure></p>
<p>用redis-trib.rb创建集群<br>./src/redis-trib.rb  create –replicas 1 192.168.1.150:6379 192.168.1.152:6379 192.168.1.154:6379<br>可能会报错，是版本太低的原因<br>解决方案：<br>ftp://ftp.ruby-lang.org/pub/ruby/  下载最新的ruby安装包  ruby-2.3.3.tar.gz<br>gem update –system  升级到最新版本2.5.1<br>gem install redis  安装redis模块</p>
<p>重新执行<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[hadoop@www.hadoop02.com redis<span class="number">-3.0</span><span class="number">.7</span>]$./src/redis-trib.rb  create --replicas <span class="number">1</span> <span class="number">192.168</span><span class="number">.247</span><span class="number">.150</span>:<span class="number">6379</span> <span class="number">192.168</span><span class="number">.247</span><span class="number">.152</span>:<span class="number">6379</span> <span class="number">192.168</span><span class="number">.247</span><span class="number">.154</span>:<span class="number">6379</span></div><div class="line">&gt;&gt;&gt; Creating cluster</div><div class="line">[ERR] Node <span class="number">192.168</span><span class="number">.247</span><span class="number">.150</span>:<span class="number">6379</span> is not empty. Either the node already knows other nodes (check <span class="keyword">with</span> CLUSTER NODES) or contains some key <span class="keyword">in</span> database <span class="number">0.</span></div></pre></td></tr></table></figure></p>
<p>因为刚才已经分配了节点</p>
<p>现在做重新分区<br>查看分区前：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6379</span>&gt; cluster nodes</div><div class="line"><span class="number">276</span>a3675a706d4626a25c54892c368ca6c37002c <span class="number">192.168</span><span class="number">.247</span><span class="number">.154</span>:<span class="number">6379</span> slave <span class="number">64</span>a2d824459edc01c7b6b205021d01da96513121 <span class="number">0</span> <span class="number">1481242407249</span> <span class="number">1</span> connected</div><div class="line"><span class="number">64</span>a2d824459edc01c7b6b205021d01da96513121 <span class="number">192.168</span><span class="number">.247</span><span class="number">.152</span>:<span class="number">6379</span> master - <span class="number">0</span> <span class="number">1481242405165</span> <span class="number">1</span> connected <span class="number">0</span><span class="number">-16383</span></div><div class="line">da0322b3160917ff99e363b7488e833be959205d <span class="number">192.168</span><span class="number">.247</span><span class="number">.150</span>:<span class="number">6379</span> myself,slave <span class="number">64</span>a2d824459edc01c7b6b205021d01da96513121 <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> connected</div></pre></td></tr></table></figure></p>
<p>分区：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">[hadoop@www.hadoop02.com redis-3.0.7]$./src/redis-trib.rb reshard 127.0.0.1:6379</div><div class="line">&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6379)</div><div class="line">M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379</div><div class="line">   slots:0-16383 (16384 slots) master</div><div class="line">   2 additional replica(s)</div><div class="line">S: 276a3675a706d4626a25c54892c368ca6c37002c 192.168.247.154:6379</div><div class="line">   slots: (0 slots) slave</div><div class="line">   replicates 64a2d824459edc01c7b6b205021d01da96513121</div><div class="line">S: da0322b3160917ff99e363b7488e833be959205d 192.168.247.150:6379</div><div class="line">   slots: (0 slots) slave</div><div class="line">   replicates 64a2d824459edc01c7b6b205021d01da96513121</div><div class="line">[OK] All nodes agree about slots configuration.</div><div class="line">&gt;&gt;&gt; Check for open slots...</div><div class="line">&gt;&gt;&gt; Check slots coverage...</div><div class="line">[OK] All 16384 slots covered.</div><div class="line">How many slots do you want to move (from 1 to 16384)? 5000</div><div class="line">What is the receiving node ID? 276a3675a706d4626a25c54892c368ca6c37002c</div><div class="line">*** The specified node is not known or not a master, please retry.</div><div class="line">What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da9651312</div><div class="line">*** The specified node is not known or not a master, please retry.</div><div class="line">What is the receiving node ID? 64a2d824459edc01c7b6b205021d01da96513121</div><div class="line">Please enter all the source node IDs.</div><div class="line">  Type 'all' to use all the nodes as source nodes for the hash slots.</div><div class="line">  Type 'done' once you entered all the source nodes IDs.</div><div class="line">Source node #1:all</div><div class="line"></div><div class="line">Ready to move 5000 slots.</div><div class="line">  Source nodes:</div><div class="line">  Destination node:</div><div class="line">    M: 64a2d824459edc01c7b6b205021d01da96513121 127.0.0.1:6379</div><div class="line">   slots:0-16383 (16384 slots) master</div><div class="line">   2 additional replica(s)</div><div class="line">  Resharding plan:</div><div class="line">Do you want to proceed with the proposed reshard plan (yes/no)? yes</div></pre></td></tr></table></figure></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/redis%E9%9B%86%E7%BE%A4rehash.png" alt="image"></p>
<p>rehash只能在主节点进行，故247.152分配给了247.152，没有变化</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/01/RegionServer/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/01/RegionServer/" itemprop="url">
                  Hbase region处理
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-01T23:33:30+08:00">
                2016-12-01
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/01/RegionServer/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/01/RegionServer/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h3 id="减少RegionServer"><a href="#减少RegionServer" class="headerlink" title="减少RegionServer"></a>减少RegionServer</h3><p>节点退役：在制定节点上运行，停止的节点上的RegionServer<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/hbase-daemon.sh stop regionserver</div></pre></td></tr></table></figure></p>
<p>如果在关闭时Hbase存在正在运行的负载均衡，那么Maser针对Region的恢复操作和负载均衡之前可能会产生竞争，尤其是在生产环境中，碰到的几率更大，所以建议现金用负载均衡</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./hbase shell</div><div class="line">./hbase(main):<span class="number">001</span>:<span class="number">0</span>&gt;blance_switch <span class="literal">false</span></div></pre></td></tr></table></figure>
<h3 id="滚动重启"><a href="#滚动重启" class="headerlink" title="滚动重启"></a>滚动重启</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/graceful_stop slave2</div></pre></td></tr></table></figure>
<p>如果同时退役多个节点，为了防止正在退役的节点把region move到即将退役的节点上可以利用zookeeper，在<br>Hbase_root/training znode中创建要退役的RegionServer的entry</p>
<p>滚动重启三种方式：<br>使用rolling-restart.sh脚本<br>手工进行滚动重启<br>编写自定义的滚动重启脚本</p>
          <!--noindex-->
          <div class="post-more-link text-center">
            <a class="btn" href="/2016/12/01/RegionServer/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/27/docker-install/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/27/docker-install/" itemprop="url">
                  docker_install
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-27T21:33:03+08:00">
                2016-11-27
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/11/27/docker-install/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/11/27/docker-install/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h4 id="1-增加yum源"><a href="#1-增加yum源" class="headerlink" title="1.增加yum源"></a>1.增加yum源</h4><p>在命令行输入：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cat &gt;<span class="regexp">/etc/yum</span>.repos.d/docker.repo &lt;&lt;-EOF</div><div class="line">[dockerrepo]</div><div class="line">name=Docker Repository</div><div class="line">baseurl=https:<span class="comment">//yum.dockerproject.org/repo/main/centos/6</span></div><div class="line">enabled=<span class="number">1</span></div><div class="line">gpgcheck=<span class="number">1</span></div><div class="line">gpgkey=https:<span class="comment">//yum.dockerproject.org/gpg</span></div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<p>通过yum加载docker<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install docker-engine</div></pre></td></tr></table></figure></p>
<h4 id="2-禁用selinux"><a href="#2-禁用selinux" class="headerlink" title="2.禁用selinux"></a>2.禁用selinux</h4><p>#####2.1.查看selinux状态</p>
<p>[root@localhost ~]# cat /etc/selinux/config<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"># This file controls the state of SELinux on the system.</div><div class="line"># SELINUX= can take one of these three values:</div><div class="line">#     enforcing - SELinux security policy is enforced.</div><div class="line">#     permissive - SELinux prints warnings instead of enforcing.</div><div class="line">#     disabled - No SELinux policy is loaded.</div><div class="line">SELINUX=enforcing</div><div class="line"># SELINUXTYPE= can take one of these two values:</div><div class="line">#     targeted - Targeted processes are protected,</div><div class="line">#     mls - Multi Level Security protection.</div><div class="line">SELINUXTYPE=targeted</div><div class="line">``` </div><div class="line">2.2.修改该配置文件中将enforcing替换为disabled</div><div class="line"> </div><div class="line">[root@localhost ~]# cat /etc/selinux/config</div><div class="line">```js</div><div class="line"># This file controls the state of SELinux on the system.</div><div class="line"># SELINUX= can take one of these three values:</div><div class="line">#     enforcing - SELinux security policy is enforced.</div><div class="line">#     permissive - SELinux prints warnings instead of enforcing.</div><div class="line">#     disabled - No SELinux policy is loaded.</div><div class="line">SELINUX=disabled</div><div class="line"># SELINUXTYPE= can take one of these two values:</div><div class="line">#     targeted - Targeted processes are protected,</div><div class="line">#     mls - Multi Level Security protection.</div><div class="line">SELINUXTYPE=targeted</div></pre></td></tr></table></figure></p>
<p>2.3.Docker使用非root用户<br>将当前用户加入docker组<br>sudo gpasswd -a ${USER} docker</p>
<p>2.4.然后重启<br>restart</p>
          <!--noindex-->
          <div class="post-more-link text-center">
            <a class="btn" href="/2016/11/27/docker-install/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-tags " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/11/25/kafka_high_level_api/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Hexo">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Hexo" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/25/kafka_high_level_api/" itemprop="url">
                  Kafka_high_level_api测试
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-11-25T19:10:33+08:00">
                2016-11-25
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/11/25/kafka_high_level_api/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/11/25/kafka_high_level_api/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="1-查看官网API"><a href="#1-查看官网API" class="headerlink" title="1.查看官网API"></a>1.查看官网API</h1><p><a href="http://kafka.apache.org/0101/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html">Kafka API</a></p>
<h3 id="Automatic-Offset-Committing"><a href="#Automatic-Offset-Committing" class="headerlink" title="Automatic Offset Committing"></a>Automatic Offset Committing</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Properties props = <span class="keyword">new</span> Properties();</div><div class="line">     props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</div><div class="line">     props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</div><div class="line">     props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</div><div class="line">     props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</div><div class="line">     props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line">     props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line">     KafkaConsumer&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</div><div class="line">     consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</div><div class="line">     <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</div><div class="line">         ConsumerRecords&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; records = consumer.poll(<span class="number">100</span>);</div><div class="line">         <span class="keyword">for</span> (ConsumerRecord&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; record : records)</div><div class="line">             System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</div><div class="line">     &#125;</div></pre></td></tr></table></figure>
<h3 id="Manual-Offset-Control"><a href="#Manual-Offset-Control" class="headerlink" title="Manual Offset Control"></a>Manual Offset Control</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">Properties props = <span class="keyword">new</span> Properties();</div><div class="line">     props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</div><div class="line">     props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</div><div class="line">     props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</div><div class="line">     props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line">     props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line">     KafkaConsumer&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</div><div class="line">     consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</div><div class="line">     final int minBatchSize = <span class="number">200</span>;</div><div class="line">     List&lt;ConsumerRecord&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt;&gt; buffer = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">     <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</div><div class="line">         ConsumerRecords&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; records = consumer.poll(<span class="number">100</span>);</div><div class="line">         <span class="keyword">for</span> (ConsumerRecord&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; record : records) &#123;</div><div class="line">             buffer.add(record);</div><div class="line">         &#125;</div><div class="line">         <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</div><div class="line">             insertIntoDb(buffer);</div><div class="line">             consumer.commitSync();</div><div class="line">             buffer.clear();</div><div class="line">         &#125;</div><div class="line">     &#125;</div></pre></td></tr></table></figure>
          <!--noindex-->
          <div class="post-more-link text-center">
            <a class="btn" href="/2016/11/25/kafka_high_level_api/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="张洪铭" />
          <p class="site-author-name" itemprop="name">张洪铭</p>
          <p class="site-description motion-element" itemprop="description">人生在勤 勤则不匮</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">15</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张洪铭</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"zhm8"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  








  
  

  

  

  

  


</body>
</html>
