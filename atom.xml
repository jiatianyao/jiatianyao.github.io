<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张洪铭的个人博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-28T13:12:37.521Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>张洪铭</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>spark 机器学习入门(二)</title>
    <link href="http://yoursite.com/2017/10/18/spark%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2017/10/18/spark 性能优化/</id>
    <published>2017-10-18T06:30:00.000Z</published>
    <updated>2017-11-28T13:12:37.521Z</updated>
    
    <content type="html"><![CDATA[<h5 id="部署优化："><a href="#部署优化：" class="headerlink" title="部署优化："></a><strong>部署优化：</strong></h5><p>磁盘：<br>挂载磁盘时使用noatime和nodiratime选项减少写的开销</p>
<p>linux每个文件都会保留3个时间戳<br>用stat 文件名 来查看<br>Acess：文件访问时间<br>Modfiy：内容修改时间<br>Change：文件名修改时间</p>
<p>参数含义：<br>磁盘下的所有文件不更新访问时间<br><a id="more"></a><br>内存：<br>JVM 内存不建议每个executor 超过200G</p>
<p>CPU<br>每台机器的Vcore数不建议小于8</p>
<h2 id="JOB调度："><a href="#JOB调度：" class="headerlink" title="JOB调度："></a><strong>JOB调度：</strong></h2><p>Fail Schedule 可最大程度保证各个Job都有机会获取资源</p>
<h2 id="数据序列化："><a href="#数据序列化：" class="headerlink" title="数据序列化："></a><strong>数据序列化：</strong></h2><p>Kyro serialization序列化速度更快，结果更紧凑<br>为了更好的性能，需提前注册被序列化的类，否则会存在大量的空间浪费<br>通过spark.serializer指定</p>
<h2 id="减少内存消耗："><a href="#减少内存消耗：" class="headerlink" title="减少内存消耗："></a><strong>减少内存消耗：</strong></h2><p>尽量使用基本数据类型和数组，避免使用java集合类<br>尽量减少包含大量小对象的嵌套结构<br>Key尽量使用数值或枚举类型而不是字符串<br>RAM小于32GB时，使用-XX:+UseCompressedOops使用4字节（而非8字节）的指针</p>
<h2 id="调整并行度："><a href="#调整并行度：" class="headerlink" title="调整并行度："></a><strong>调整并行度：</strong></h2><p>调整Map侧并行度<br>对于kafka direct stream 可通过调整Topic的Patition个数调整Spark Map侧并行度<br>对于spark.textFile，通过参数调整</p>
<p>调整Reduce侧并行度<br>通过spark.default.parallelism设置shuffle时默认并行度</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;部署优化：&quot;&gt;&lt;a href=&quot;#部署优化：&quot; class=&quot;headerlink&quot; title=&quot;部署优化：&quot;&gt;&lt;/a&gt;&lt;strong&gt;部署优化：&lt;/strong&gt;&lt;/h5&gt;&lt;p&gt;磁盘：&lt;br&gt;挂载磁盘时使用noatime和nodiratime选项减少写的开销&lt;/p&gt;
&lt;p&gt;linux每个文件都会保留3个时间戳&lt;br&gt;用stat 文件名 来查看&lt;br&gt;Acess：文件访问时间&lt;br&gt;Modfiy：内容修改时间&lt;br&gt;Change：文件名修改时间&lt;/p&gt;
&lt;p&gt;参数含义：&lt;br&gt;磁盘下的所有文件不更新访问时间&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark 机器学习入门(一)</title>
    <link href="http://yoursite.com/2017/10/18/spark%20ml1/"/>
    <id>http://yoursite.com/2017/10/18/spark ml1/</id>
    <published>2017-10-18T03:30:00.000Z</published>
    <updated>2017-10-22T11:03:05.867Z</updated>
    
    <content type="html"><![CDATA[<p>目前基于RDD的MLlib已经进入维护莫斯。大概在spark2.3基于RDD的MLlib API将要被废弃。未来是基于DataFrame的API</p>
<p>1.基本统计<br>计算两组数据之间的相关性</p>
<h3 id="皮尔森相关系数（Pearson-correlation-coefficient）"><a href="#皮尔森相关系数（Pearson-correlation-coefficient）" class="headerlink" title="皮尔森相关系数（Pearson correlation coefficient）"></a>皮尔森相关系数（Pearson correlation coefficient）</h3><p>也称皮尔森积矩相关系数(Pearson product-moment correlation coefficient) ，是一种线性相关系数。皮尔森相关系数是用来反映两个变量线性相关程度的统计量。相关系数用r表示，其中n为样本量，分别为两个变量的观测值和均值。r描述的是两个变量间线性相关强弱的程度。r的绝对值越大表明相关性越强</p>
<p><img src="http://segmentfault.com/img/cGNupC" alt="此处输入图片的描述"><br><img src="http://segmentfault.com/img/eOV3Oj" alt="此处输入图片的描述"><br><img src="http://segmentfault.com/img/lJqNL" alt="此处输入图片的描述"><br><a id="more"></a><br>按照高中数学水平来理解, 它很简单, 可以看做将两组数据首先做Z分数处理之后, 然后两组数据的乘积和除以样本数</p>
<p>Z分数一般代表正态分布中, 数据偏离中心点的距离.等于变量减掉平均数再除以标准差.(就是高考的标准分类似的处理)</p>
<p>标准差则等于变量减掉平均数的平方和,再除以样本数,最后再开方.</p>
<p>所以, 根据这个最朴素的理解,我们可以将公式依次精简为:</p>
<p><img src="http://segmentfault.com/img/bKDASK" alt="此处输入图片的描述"></p>
<p>spearman相关系数：是衡量分级定序变量之间的相关程度的统计量，对不服从正态分布的资料、原始资料等级资料、一侧开口资料、总体分布类型未知的资料不符合使用积矩相关系数来描述关联性。此时可采用秩相关（rank correlation），也称等级相关，来描述两个变量之间的关联程度与方向。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.&#123;<span class="type">Matrix</span>, <span class="type">Vectors</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.stat.<span class="type">Correlation</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">ml_1</span> </span>&#123;</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></div><div class="line">      .builder</div><div class="line">      .master(<span class="string">"local"</span>)</div><div class="line">      .getOrCreate()</div><div class="line">    <span class="keyword">import</span> spark.implicits._</div><div class="line"></div><div class="line">    <span class="comment">// $example on$</span></div><div class="line">    <span class="keyword">val</span> data = <span class="type">Seq</span>(</div><div class="line">      <span class="type">Vectors</span>.sparse(<span class="number">4</span>, <span class="type">Seq</span>((<span class="number">0</span>, <span class="number">1.0</span>), (<span class="number">3</span>, <span class="number">-2.0</span>))),</div><div class="line">      <span class="type">Vectors</span>.dense(<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>),</div><div class="line">      <span class="type">Vectors</span>.dense(<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">0.0</span>, <span class="number">8.0</span>),</div><div class="line">      <span class="type">Vectors</span>.sparse(<span class="number">4</span>, <span class="type">Seq</span>((<span class="number">0</span>, <span class="number">9.0</span>), (<span class="number">3</span>, <span class="number">1.0</span>)))</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="keyword">val</span> df = data.map(<span class="type">Tuple1</span>.apply).toDF(<span class="string">"features"</span>)</div><div class="line">    <span class="keyword">val</span> <span class="type">Row</span>(coeff1: <span class="type">Matrix</span>) = <span class="type">Correlation</span>.corr(df, <span class="string">"features"</span>).head</div><div class="line">    println(<span class="string">"Pearson correlation matrix:\n"</span> + coeff1.toString)</div><div class="line">    </div><div class="line">    <span class="keyword">val</span> <span class="type">Row</span>(coeff2: <span class="type">Matrix</span>) = <span class="type">Correlation</span>.corr(df, <span class="string">"features"</span>, <span class="string">"spearman"</span>).head</div><div class="line">    println(<span class="string">"Spearman correlation matrix:\n"</span> + coeff2.toString)</div><div class="line">    <span class="comment">// $example off$</span></div><div class="line"></div><div class="line">    spark.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>1.0为第一列和第一列计算<br>0.055641488407465814为第一列和第二列计算<br>0.4004714203168137 为第三列和第四列计算</p>
<p>计算结果:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="type">Pearson</span> correlation matrix:</div><div class="line"><span class="number">1.0</span>                   <span class="number">0.055641488407465814</span>  <span class="type">NaN</span>  <span class="number">0.4004714203168137</span> </div><div class="line"><span class="number">0.055641488407465814</span>  <span class="number">1.0</span>                   <span class="type">NaN</span>  <span class="number">0.9135958615342522</span>  </div><div class="line"><span class="type">NaN</span>                   <span class="type">NaN</span>                   <span class="number">1.0</span>  <span class="type">NaN</span>                 </div><div class="line"><span class="number">0.4004714203168137</span>    <span class="number">0.9135958615342522</span>    <span class="type">NaN</span>  <span class="number">1.0</span>                 </div><div class="line"><span class="type">Spearman</span> correlation matrix:</div><div class="line"><span class="number">1.0</span>                  <span class="number">0.10540925533894532</span>  <span class="type">NaN</span>  <span class="number">0.40000000000000174</span>  </div><div class="line"><span class="number">0.10540925533894532</span>  <span class="number">1.0</span>                  <span class="type">NaN</span>  <span class="number">0.9486832980505141</span>   </div><div class="line"><span class="type">NaN</span>                  <span class="type">NaN</span>                  <span class="number">1.0</span>  <span class="type">NaN</span>                  </div><div class="line"><span class="number">0.40000000000000174</span>  <span class="number">0.9486832980505141</span>   <span class="type">NaN</span>  <span class="number">1.0</span></div></pre></td></tr></table></figure></p>
<h4 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a><strong>卡方检验</strong></h4><p>卡方检验是用途非常广的一种假设检验方法，它在分类资料统计推断中的应用，包括：两个率或两个构成比比较的卡方检验；多个率或多个构成比比较的卡方检验以及分类资料的相关分析等。<br>卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vectors</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.stat.<span class="type">ChiSquareTest</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by Administrator on 2017/10/18.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">ChiSquareTest</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></div><div class="line">      .builder</div><div class="line">      .master(<span class="string">"local"</span>)</div><div class="line">      .getOrCreate()</div><div class="line">    <span class="keyword">import</span> spark.implicits._</div><div class="line"></div><div class="line">    <span class="keyword">val</span> data = <span class="type">Seq</span>(</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">0.5</span>, <span class="number">10.0</span>)),</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">1.5</span>, <span class="number">20.0</span>)),</div><div class="line">      (<span class="number">1.0</span>, <span class="type">Vectors</span>.dense(<span class="number">1.5</span>, <span class="number">30.0</span>)),</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">3.5</span>, <span class="number">30.0</span>)),</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">3.5</span>, <span class="number">40.0</span>)),</div><div class="line">      (<span class="number">1.0</span>, <span class="type">Vectors</span>.dense(<span class="number">3.5</span>, <span class="number">40.0</span>))</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="keyword">val</span> df = data.toDF(<span class="string">"label"</span>, <span class="string">"features"</span>)</div><div class="line">    <span class="keyword">val</span> chi = <span class="type">ChiSquareTest</span>.test(df, <span class="string">"features"</span>, <span class="string">"label"</span>).head</div><div class="line">    println(<span class="string">"pValues = "</span> + chi.getAs[<span class="type">Vector</span>](<span class="number">0</span>))</div><div class="line">    println(<span class="string">"degreesOfFreedom = "</span> + chi.getSeq[<span class="type">Int</span>](<span class="number">1</span>).mkString(<span class="string">"["</span>, <span class="string">","</span>, <span class="string">"]"</span>))</div><div class="line">    println(<span class="string">"statistics = "</span> + chi.getAs[<span class="type">Vector</span>](<span class="number">2</span>))</div><div class="line"></div><div class="line">    spark.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>结果：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pValues = [<span class="number">0.6872892787909721</span>,<span class="number">0.6822703303362126</span>]</div><div class="line">degreesOfFreedom = [<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">statistics = [<span class="number">0.75</span>,<span class="number">1.5</span>]</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前基于RDD的MLlib已经进入维护莫斯。大概在spark2.3基于RDD的MLlib API将要被废弃。未来是基于DataFrame的API&lt;/p&gt;
&lt;p&gt;1.基本统计&lt;br&gt;计算两组数据之间的相关性&lt;/p&gt;
&lt;h3 id=&quot;皮尔森相关系数（Pearson-correlation-coefficient）&quot;&gt;&lt;a href=&quot;#皮尔森相关系数（Pearson-correlation-coefficient）&quot; class=&quot;headerlink&quot; title=&quot;皮尔森相关系数（Pearson correlation coefficient）&quot;&gt;&lt;/a&gt;皮尔森相关系数（Pearson correlation coefficient）&lt;/h3&gt;&lt;p&gt;也称皮尔森积矩相关系数(Pearson product-moment correlation coefficient) ，是一种线性相关系数。皮尔森相关系数是用来反映两个变量线性相关程度的统计量。相关系数用r表示，其中n为样本量，分别为两个变量的观测值和均值。r描述的是两个变量间线性相关强弱的程度。r的绝对值越大表明相关性越强&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://segmentfault.com/img/cGNupC&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://segmentfault.com/img/eOV3Oj&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://segmentfault.com/img/lJqNL&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark shuffle 调优</title>
    <link href="http://yoursite.com/2017/08/30/spark%20shuffle%20%E8%B0%83%E4%BC%98/"/>
    <id>http://yoursite.com/2017/08/30/spark shuffle 调优/</id>
    <published>2017-08-30T03:30:00.000Z</published>
    <updated>2017-09-17T12:04:23.244Z</updated>
    
    <content type="html"><![CDATA[<h2 id="shuffle实现的具体过程"><a href="#shuffle实现的具体过程" class="headerlink" title="shuffle实现的具体过程"></a>shuffle实现的具体过程</h2><p>1.Spark系统在运行含shuffle过程的应用时，Executor进程除了运行task，还要负责写shuffle 数据，给其他Executor提供shuffle数据。<br>    当Executor进程任务过重，导致GC而不能为其 他Executor提供shuffle数据时，会影响任务运行。<br>    这里实际上是利用External Shuffle Service 来提升性能，External shuffle Service是长期存在于NodeManager进程中的一个辅助服务。<br>    通过该服务 来抓取shuffle数据，减少了Executor的压力，在Executor GC的时候也不会影响其他 Executor的任务运行。</p>
<p>启用方法：</p>
<h3 id="一-在NodeManager中启动External-shuffle-Service。"><a href="#一-在NodeManager中启动External-shuffle-Service。" class="headerlink" title="一. 在NodeManager中启动External shuffle Service。"></a>一. 在NodeManager中启动External shuffle Service。</h3><pre><code>a. 在“yarn-site.xml”中添加如下配置项：
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;spark_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;spark.shuffle.service.port&lt;/name&gt;
&lt;value&gt;7337&lt;/value&gt;
&lt;/property&gt;
配置参数描述
yarn.nodemanager.aux-services  ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle计算性能。
yarn.nodemanager.auxservices.spark_shuffle.class ：NodeManager中辅助服务对应的类。
spark.shuffle.service.port ：Shuffle服务监听数据获取请求的端口。可选配置，默认值为“7337”。
b. 添加依赖的jar包
拷贝“${SPARK_HOME}/lib/spark-1.3.0-yarn-shuffle.jar”到“${HADOOP_HOME}/share/hadoop/yarn/lib/”目录下。
c. 重启NodeManager进程，也就启动了External shuffle Service。
</code></pre><a id="more"></a>
<h3 id="二-Spark应用使用External-shuffle-Service。"><a href="#二-Spark应用使用External-shuffle-Service。" class="headerlink" title="二. Spark应用使用External shuffle Service。"></a>二. Spark应用使用External shuffle Service。</h3><pre><code>在“spark-defaults.conf”中必须添加如下配置项： 
    spark.shuffle.service.enabled true 
    spark.shuffle.service.port 7337 
说明 
1.如果1.如果“yarn.nodemanager.aux-services”配置项已存在，则在value中添加 “spark_shuffle”，且用逗号和其他值分开。 
2.“spark.shuffle.service.port”的值需要和上面“yarn-site.xml”中的值一样。 
配置参数描述 
spark.shuffle.service.enabled   ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle 计算性能。默认为false，表示不启用该功能。 
spark.shuffle.service.port   ：Shuffle服务监听数据获取请求的端口。可选配置，默认值 为“7337”。
</code></pre><p>Hash Shuffle不足<br>map task会根据reduce的数量（partition） 生成相应的bucket 写shuffle blockFile</p>
<p>如果map 和reduce数量过多，会写很多blockFile，造成问题1：超过操作系统所能打开最大文件数，问题2：大量随机写随机读</p>
<p>解决方案：</p>
<p>1.shuffle 参数：<br>spark.shuffle.consolidateFiles  默认为false<br>如果设置为”true”，在shuffle期间，合并的中间文件将会被创建。创建更少的文件可以提供文件系统的shuffle的效 率。这些shuffle都伴随着大量递归任务。当用ext4和dfs文件系统时，推荐设置为”true”。在ext3中，因为文件系统的限制，这个选项可 能机器（大于8核）降低效率</p>
<p>2.sort shuffle<br>每个map只写到一个文件，和上面的写到reduce个数个文件不同</p>
<h2 id="不同算子影响shuffle表现形式不同"><a href="#不同算子影响shuffle表现形式不同" class="headerlink" title="不同算子影响shuffle表现形式不同"></a>不同算子影响shuffle表现形式不同</h2><p>reduceByKey(func, numPartitions=None)<br>也就是，reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。<br><img src="http://oh6ybr0jg.bkt.clouddn.com/reduceByKey.png" alt="此处输入图片的描述"></p>
<p>groupByKey(numPartitions=None)<br>也就是，groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。<br><img src="http://oh6ybr0jg.bkt.clouddn.com/groupByKey.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;shuffle实现的具体过程&quot;&gt;&lt;a href=&quot;#shuffle实现的具体过程&quot; class=&quot;headerlink&quot; title=&quot;shuffle实现的具体过程&quot;&gt;&lt;/a&gt;shuffle实现的具体过程&lt;/h2&gt;&lt;p&gt;1.Spark系统在运行含shuffle过程的应用时，Executor进程除了运行task，还要负责写shuffle 数据，给其他Executor提供shuffle数据。&lt;br&gt;    当Executor进程任务过重，导致GC而不能为其 他Executor提供shuffle数据时，会影响任务运行。&lt;br&gt;    这里实际上是利用External Shuffle Service 来提升性能，External shuffle Service是长期存在于NodeManager进程中的一个辅助服务。&lt;br&gt;    通过该服务 来抓取shuffle数据，减少了Executor的压力，在Executor GC的时候也不会影响其他 Executor的任务运行。&lt;/p&gt;
&lt;p&gt;启用方法：&lt;/p&gt;
&lt;h3 id=&quot;一-在NodeManager中启动External-shuffle-Service。&quot;&gt;&lt;a href=&quot;#一-在NodeManager中启动External-shuffle-Service。&quot; class=&quot;headerlink&quot; title=&quot;一. 在NodeManager中启动External shuffle Service。&quot;&gt;&lt;/a&gt;一. 在NodeManager中启动External shuffle Service。&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;a. 在“yarn-site.xml”中添加如下配置项：
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;spark_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;yarn.nodemanager.aux-services.spark_shuffle.class&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;org.apache.spark.network.yarn.YarnShuffleService&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;spark.shuffle.service.port&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;7337&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
配置参数描述
yarn.nodemanager.aux-services  ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle计算性能。
yarn.nodemanager.auxservices.spark_shuffle.class ：NodeManager中辅助服务对应的类。
spark.shuffle.service.port ：Shuffle服务监听数据获取请求的端口。可选配置，默认值为“7337”。
b. 添加依赖的jar包
拷贝“${SPARK_HOME}/lib/spark-1.3.0-yarn-shuffle.jar”到“${HADOOP_HOME}/share/hadoop/yarn/lib/”目录下。
c. 重启NodeManager进程，也就启动了External shuffle Service。
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CDH 安装spark2.2</title>
    <link href="http://yoursite.com/2017/08/05/CDH%20%E5%AE%89%E8%A3%85spark2.2/"/>
    <id>http://yoursite.com/2017/08/05/CDH 安装spark2.2/</id>
    <published>2017-08-05T03:30:00.000Z</published>
    <updated>2017-10-09T12:11:28.450Z</updated>
    
    <content type="html"><![CDATA[<h2 id="官方链接"><a href="#官方链接" class="headerlink" title="官方链接"></a>官方链接</h2><p><a href="https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html" target="_blank" rel="external">https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html</a></p>
<p>1.下载Spark2 CSD<br><a href="https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging" target="_blank" rel="external">https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging</a></p>
<p>1.1.1 CSD<br>笔者下载2.2版本<br><a href="http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar" target="_blank" rel="external">http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar</a><br><a id="more"></a><br>1.1.2 Parcel<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/</a></p>
<p>1.1.2.1 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel</a></p>
<p>1.1.2.2 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel.sha1" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel.sha1</a><br>然后将SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1改名为<br>SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha</p>
<p>1.1.2.3 manifest.json<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/manifest.json" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/manifest.json</a></p>
<p>停止服务<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server stop<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent stop</p>
<p>将csd文件放到<br>/soft/bigdata/clouderamanager/cloudera/csd</p>
<p>将parcel文件放到<br>/soft/bigdata/clouderamanager/cloudera/parcel-repo</p>
<p>修改权限<br>chgrp cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jar<br>chown cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jar </p>
<p>开启服务<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start</p>
<p>主机-&gt; Parcel就能看到spark2了</p>
<p>分配，激活。然后就可以添加服务了</p>
<p>如果添加服务不成功需要把jar文件放到/opt/cloudera/csd</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;官方链接&quot;&gt;&lt;a href=&quot;#官方链接&quot; class=&quot;headerlink&quot; title=&quot;官方链接&quot;&gt;&lt;/a&gt;官方链接&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html&quot;&gt;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.下载Spark2 CSD&lt;br&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging&quot;&gt;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.1.1 CSD&lt;br&gt;笔者下载2.2版本&lt;br&gt;&lt;a href=&quot;http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar&quot;&gt;http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>开源爬虫介绍</title>
    <link href="http://yoursite.com/2017/08/05/%E5%BC%80%E6%BA%90%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/08/05/开源爬虫介绍/</id>
    <published>2017-08-05T03:30:00.000Z</published>
    <updated>2017-10-17T11:10:37.532Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/Chyroc/WechatSogou" target="_blank" rel="external">https://github.com/Chyroc/WechatSogou</a></p>
<p> 微信公众号爬虫。基于搜狗微信搜索的微信公众号爬虫接口，可以扩展成基于搜狗搜索的爬虫，返回结果是列表，每一项均是公众号具体信息字典。</p>
<p> <a href="https://github.com/lanbing510/DouBanSpider" target="_blank" rel="external">https://github.com/lanbing510/DouBanSpider</a><br> 豆瓣读书爬虫。可以爬下豆瓣读书标签下的所有图书，按评分排名依次存储，存储到Excel中，可方便大家筛选搜罗，比如筛选评价人数&gt;1000的高分书籍；可依据不同的主题存储到Excel不同的Sheet ，采用User Agent伪装为浏览器进行爬取，并加入随机延时来更好的模仿浏览器行为，避免爬虫被封。</p>
<p><a href="https://github.com/LiuRoy/zhihu_spider" target="_blank" rel="external">https://github.com/LiuRoy/zhihu_spider</a><br>知乎爬虫。此项目的功能是爬取知乎用户信息以及人际拓扑关系，爬虫框架使用scrapy，数据存储使用mongo</p>
<p><a href="https://github.com/airingursb/bilibili-user" target="_blank" rel="external">https://github.com/airingursb/bilibili-user</a><br>Bilibili用户爬虫。总数据数：20119918，抓取字段：用户id，昵称，性别，头像，等级，经验值，粉丝数，生日，地址，注册时间，签名，等级与经验值等。抓取之后生成B站用户数据报告。<br><a id="more"></a></p>
<p><a href="https://github.com/LiuXingMing/SinaSpider" target="_blank" rel="external">https://github.com/LiuXingMing/SinaSpider</a><br>新浪微博爬虫。主要爬取新浪微博用户的个人信息、微博信息、粉丝和关注。代码获取新浪微博Cookie进行登录，可通过多账号登录来防止新浪的反扒。主要使用 scrapy 爬虫框架</p>
<p><a href="https://github.com/gnemoug/distribute_crawler" target="_blank" rel="external">https://github.com/gnemoug/distribute_crawler</a><br>小说下载分布式爬虫。使用scrapy,Redis, MongoDB,graphite实现的一个分布式网络爬虫,底层存储mongodb集群,分布式使用redis实现,爬虫状态显示使用graphite实现，主要针对一个小说站点。</p>
<p><a href="https://github.com/yanzhou/CnkiSpider" target="_blank" rel="external">https://github.com/yanzhou/CnkiSpider</a><br>中国知网爬虫。设置检索条件后，执行src/CnkiSpider.py抓取数据，抓取数据存储在/data目录下，每个数据文件的第一行为字段名称。</p>
<p><a href="https://github.com/lanbing510/LianJiaSpider" target="_blank" rel="external">https://github.com/lanbing510/LianJiaSpider</a><br>链家网爬虫。爬取北京地区链家历年二手房成交记录。涵盖链家爬虫一文的全部代码，包括链家模拟登录代码。</p>
<p><a href="https://github.com/taizilongxu/scrapy_jingdong" target="_blank" rel="external">https://github.com/taizilongxu/scrapy_jingdong</a><br>京东爬虫。基于scrapy的京东网站爬虫，保存格式为csv。</p>
<p><a href="https://github.com/caspartse/QQ-Groups-Spider" target="_blank" rel="external">https://github.com/caspartse/QQ-Groups-Spider</a><br>QQ 群爬虫。批量抓取 QQ 群信息，包括群名称、群号、群人数、群主、群简介等内容，最终生成 XLS(X) / CSV 结果文件。</p>
<p><a href="https://github.com/hanc00l/wooyun_public" target="_blank" rel="external">https://github.com/hanc00l/wooyun_public</a><br>乌云爬虫。 乌云公开漏洞、知识库爬虫和搜索。全部公开漏洞的列表和每个漏洞的文本内容存在mongodb中，大概约2G内容；如果整站爬全部文本和图片作为离线查询，大概需要10G空间、2小时（10M电信带宽）；爬取全部知识库，总共约500M空间。漏洞搜索使用了Flask作为web server，bootstrap作为前端。</p>
<p><a href="https://github.com/fankcoder/findtrip" target="_blank" rel="external">https://github.com/fankcoder/findtrip</a><br>机票爬虫（去哪儿和携程网）。Findtrip是一个基于Scrapy的机票爬虫，目前整合了国内两大机票网站（去哪儿 + 携程）。</p>
<p><a href="https://github.com/leyle/163spider" target="_blank" rel="external">https://github.com/leyle/163spider</a><br>基于requests、MySQLdb、torndb的网易客户端内容爬虫</p>
<p><a href="https://github.com/fanpei91/doubanspiders" target="_blank" rel="external">https://github.com/fanpei91/doubanspiders</a><br>豆瓣电影、书籍、小组、相册、东西等爬虫集</p>
<p><a href="https://github.com/LiuXingMing/QQSpider" target="_blank" rel="external">https://github.com/LiuXingMing/QQSpider</a><br>QQ空间爬虫，包括日志、说说、个人信息等，一天可抓取 400 万条数据。</p>
<p><a href="https://github.com/Shu-Ji/baidu-music-spider" target="_blank" rel="external">https://github.com/Shu-Ji/baidu-music-spider</a><br>百度mp3全站爬虫，使用redis支持断点续传。</p>
<p><a href="https://github.com/pakoo/tbcrawler" target="_blank" rel="external">https://github.com/pakoo/tbcrawler</a><br>淘宝和天猫的爬虫,可以根据搜索关键词,物品id来抓去页面的信息，数据存储在mongodb。</p>
<p><a href="https://github.com/benitoro/stockholm" target="_blank" rel="external">https://github.com/benitoro/stockholm</a><br>一个股票数据（沪深）爬虫和选股策略测试框架。根据选定的日期范围抓取所有沪深两市股票的行情数据。支持使用表达式定义选股策略。支持多线程处理。保存数据到JSON文件、CSV文件。</p>
<p><a href="https://github.com/k1995/BaiduyunSpider" target="_blank" rel="external">https://github.com/k1995/BaiduyunSpider</a><br>百度云盘爬虫</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/Chyroc/WechatSogou&quot;&gt;https://github.com/Chyroc/WechatSogou&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; 微信公众号爬虫。基于搜狗微信搜索的微信公众号爬虫接口，可以扩展成基于搜狗搜索的爬虫，返回结果是列表，每一项均是公众号具体信息字典。&lt;/p&gt;
&lt;p&gt; &lt;a href=&quot;https://github.com/lanbing510/DouBanSpider&quot;&gt;https://github.com/lanbing510/DouBanSpider&lt;/a&gt;&lt;br&gt; 豆瓣读书爬虫。可以爬下豆瓣读书标签下的所有图书，按评分排名依次存储，存储到Excel中，可方便大家筛选搜罗，比如筛选评价人数&amp;gt;1000的高分书籍；可依据不同的主题存储到Excel不同的Sheet ，采用User Agent伪装为浏览器进行爬取，并加入随机延时来更好的模仿浏览器行为，避免爬虫被封。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/LiuRoy/zhihu_spider&quot;&gt;https://github.com/LiuRoy/zhihu_spider&lt;/a&gt;&lt;br&gt;知乎爬虫。此项目的功能是爬取知乎用户信息以及人际拓扑关系，爬虫框架使用scrapy，数据存储使用mongo&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/airingursb/bilibili-user&quot;&gt;https://github.com/airingursb/bilibili-user&lt;/a&gt;&lt;br&gt;Bilibili用户爬虫。总数据数：20119918，抓取字段：用户id，昵称，性别，头像，等级，经验值，粉丝数，生日，地址，注册时间，签名，等级与经验值等。抓取之后生成B站用户数据报告。&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>spark 2.2.0源码编译</title>
    <link href="http://yoursite.com/2017/08/02/spark%202.2.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
    <id>http://yoursite.com/2017/08/02/spark 2.2.0源码编译/</id>
    <published>2017-08-02T03:30:00.000Z</published>
    <updated>2017-08-13T05:14:38.067Z</updated>
    
    <content type="html"><![CDATA[<p>官网下载spark源码<br><a href="https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz" target="_blank" rel="external">https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz</a><br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-source-download.png" alt="此处输入图片的描述"></p>
<p>然后在idea中导入spark源码项目(idea maven配置正确)，然后对spark项目build。Build成功后在进行编译。<br>Build过程中遇到问题：</p>
<p>找不到org.apache.spark.streaming.flume.sink.SparkFlumeProtocol<br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem2.png" alt="此处输入图片的描述"><br>找不到org.apache.spark.sql.catalyst.parser.SqlBaseParser<br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem3.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem4.png" alt="此处输入图片的描述"></p>
<p>设置maven的参数，否则一直出现outofMemory<br>在apache-maven-3.3.9-bin\bin下面的mvn.cmd文件里的：<br>@REM set MAVEN_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000<br>下面添加<br>set MAVEN_OPTS= -Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m</p>
<p>在git bash 里编译<br>进入spark源码目录<br>mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-success.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;官网下载spark源码&lt;br&gt;&lt;a href=&quot;https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://d3kbcqa49mib13.clou
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Ambari安装部署Spark集群</title>
    <link href="http://yoursite.com/2017/06/30/%E4%BD%BF%E7%94%A8Ambari%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Spark%E9%9B%86%E7%BE%A4/"/>
    <id>http://yoursite.com/2017/06/30/使用Ambari安装部署Spark集群/</id>
    <published>2017-06-30T03:30:00.000Z</published>
    <updated>2017-09-03T09:42:14.380Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>进入官网，选择产品里面的下载页面或者直接登录<br><a href="https://hortonworks.com/downloads/" target="_blank" rel="external">https://hortonworks.com/downloads/</a><br>选择<br>HDP® 2.6: Ready for the enterprise<br>下面的<br>Automated Install Guide</p>
<p>因为博主是CENTOS 7<br>在这个页面直接选择：<br>RHEL/CentOS/Oracle Linux 7</p>
<p>wget  <a href="http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo" target="_blank" rel="external">http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo</a> -O /etc/yum.repos.d/ambari.repo<br>yum repolist</p>
<p>yum install ambari-server</p>
<p>进入​Setup Options<br>ambari-server setup –j /usr/java/default<br>提示参数只能是一个，看来jdk要后设置</p>
<p>ambari-server setup<br>各种回车，jdk的时候选择安装1.8的<br>如果输入自定义的jdk要注意权限问题</p>
<h2 id="启动："><a href="#启动：" class="headerlink" title="启动："></a>启动：</h2><p>ambari-server start</p>
<p>查看：<br>[root@storm01 storm]# lsof -i:8080<br>COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME<br>java    7648 storm 1438u  IPv6 124470      0t0  TCP *:webcache (LISTEN)</p>
<p>关闭防火墙<br>systemctl disable firewalld.service<br>systemctl stop firewalld.service    </p>
<a id="more"></a>
<h2 id="在浏览器输入http-storm01-8080即可进入UI"><a href="#在浏览器输入http-storm01-8080即可进入UI" class="headerlink" title="在浏览器输入http://storm01:8080即可进入UI"></a>在浏览器输入<a href="http://storm01:8080即可进入UI" target="_blank" rel="external">http://storm01:8080即可进入UI</a></h2><p>用户密码都为admin</p>
<p>在每个机器上安装agent<br>yum install -y ambari-agent</p>
<p>修改配置文件<br> vi /etc/ambari-agent/conf/ambari-agent.ini<br> hostname=localhost更改为<br> hostname=storm01</p>
<p> 启动agent<br> service ambari-agent start</p>
<h2 id="设置hive"><a href="#设置hive" class="headerlink" title="设置hive"></a>设置hive</h2><p>用户密码都为hive</p>
<p>需要PostgreSQL支持远程连接<br>find / -name pg_hba.conf</p>
<p>vi /var/lib/pgsql/data/pg_hba.conf<br>将<br>local   all   postgres                                     peer改成<br>local   all   postgres                                     trust<br>host    all   postgres             127.0.0.1/32            ident改成<br>host    all   postgres             127.0.0.1/32            trust<br>local  all  ambari,mapred md5改成<br>local  all  ambari,mapred,hive trust<br>host  all   ambari,mapred 0.0.0.0/0  md5改成<br>host  all   ambari,mapred,hive 0.0.0.0/0  md5<br>host  all   ambari,mapred ::/0 md5改成<br>host  all   ambari,mapred,hive ::/0 md5</p>
<p>重启数据库<br>service postgresql restart</p>
<h2 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h2><p>lsof -i:5432</p>
<p>去仓库下载postgresql</p>
<p><a href="http://mvnrepository.com/" target="_blank" rel="external">http://mvnrepository.com/</a></p>
<p>下载</p>
<p>PostgreSQL JDBC Driver JDBC 4.2<br>的<br>9.2-1002-jdbc4</p>
<p>或者直接输入：<br>wget <a href="http://central.maven.org/maven2/org/postgresql/postgresql/9.2-1002-jdbc4/postgresql-9.2-1002-jdbc4.jar" target="_blank" rel="external">http://central.maven.org/maven2/org/postgresql/postgresql/9.2-1002-jdbc4/postgresql-9.2-1002-jdbc4.jar</a></p>
<p>ambari-server setup –jdbc-db=postgres –jdbc-driver=/root/postgresql-9.2-1002-jdbc4.jar</p>
<p>创建用户<br>psql -U postgres -d postgres<br>create user hive;<br>alter user hive password ‘hive’;<br>create database hive;<br>grant all on database hive to hive;<br>alter database hive owner to hive;</p>
<p>此时test ConnectionConnection 就可以测试成功</p>
<p>设置Ambari Metrics和Smart Sense用户密码都为admin</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariInstallOptions.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariConfirmHostsing.png" alt="此处输入图片的描述"><br>如果发现失败，需要设置/etc/hosts文件<br>192.168.247.180    spark01<br>192.168.247.181    spark02<br>192.168.247.182    spark03</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariConfirmHosts.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariChooseServices1.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariChooseServices2.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariAssignMasters.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariAssignSlavesandClients.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariCustomize1.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariCustomize2.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariCustomize3.png" alt="此处输入图片的描述"></p>
<p>安装完成<br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariInstallSuccess.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerlink&quot; title=&quot;下载&quot;&gt;&lt;/a&gt;下载&lt;/h2&gt;&lt;p&gt;进入官网，选择产品里面的下载页面或者直接登录&lt;br&gt;&lt;a href=&quot;https://hortonworks.com/downloads/&quot;&gt;https://hortonworks.com/downloads/&lt;/a&gt;&lt;br&gt;选择&lt;br&gt;HDP® 2.6: Ready for the enterprise&lt;br&gt;下面的&lt;br&gt;Automated Install Guide&lt;/p&gt;
&lt;p&gt;因为博主是CENTOS 7&lt;br&gt;在这个页面直接选择：&lt;br&gt;RHEL/CentOS/Oracle Linux 7&lt;/p&gt;
&lt;p&gt;wget  &lt;a href=&quot;http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo&quot;&gt;http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo&lt;/a&gt; -O /etc/yum.repos.d/ambari.repo&lt;br&gt;yum repolist&lt;/p&gt;
&lt;p&gt;yum install ambari-server&lt;/p&gt;
&lt;p&gt;进入​Setup Options&lt;br&gt;ambari-server setup –j /usr/java/default&lt;br&gt;提示参数只能是一个，看来jdk要后设置&lt;/p&gt;
&lt;p&gt;ambari-server setup&lt;br&gt;各种回车，jdk的时候选择安装1.8的&lt;br&gt;如果输入自定义的jdk要注意权限问题&lt;/p&gt;
&lt;h2 id=&quot;启动：&quot;&gt;&lt;a href=&quot;#启动：&quot; class=&quot;headerlink&quot; title=&quot;启动：&quot;&gt;&lt;/a&gt;启动：&lt;/h2&gt;&lt;p&gt;ambari-server start&lt;/p&gt;
&lt;p&gt;查看：&lt;br&gt;[root@storm01 storm]# lsof -i:8080&lt;br&gt;COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME&lt;br&gt;java    7648 storm 1438u  IPv6 124470      0t0  TCP *:webcache (LISTEN)&lt;/p&gt;
&lt;p&gt;关闭防火墙&lt;br&gt;systemctl disable firewalld.service&lt;br&gt;systemctl stop firewalld.service    &lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CDH oozie</title>
    <link href="http://yoursite.com/2017/05/18/CDH%20oozie%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/05/18/CDH oozie安装/</id>
    <published>2017-05-18T03:30:00.000Z</published>
    <updated>2017-06-23T00:27:18.872Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装完oozie，打开UI"><a href="#安装完oozie，打开UI" class="headerlink" title="安装完oozie，打开UI:"></a>安装完oozie，打开UI:</h2><p><a href="http://master1:11000/oozie/" target="_blank" rel="external">http://master1:11000/oozie/</a></p>
<p>显示Oozie web console is disabled.<br>解决方案;<br>原因是oozie的/var/lib/oozie目录里缺少EXT的包</p>
<p>点击Documentation链接里quickstart给出了解决方案<br>Download ExtJS library (it must be version 2.2)<br><a id="more"></a></p>
<h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><p><a href="http://dev.sencha.com/deploy/ext-2.2.zip" target="_blank" rel="external">http://dev.sencha.com/deploy/ext-2.2.zip</a></p>
<p>如果下载不下来可以试试CSDN<br><a href="http://download.csdn.net/download/start_baby/6280675" target="_blank" rel="external">http://download.csdn.net/download/start_baby/6280675</a><br>或者：<br><a href="http://archive.cloudera.com/gplextras/misc/ext-2.2.zip" target="_blank" rel="external">http://archive.cloudera.com/gplextras/misc/ext-2.2.zip</a></p>
<h2 id="unzip解压"><a href="#unzip解压" class="headerlink" title="unzip解压"></a>unzip解压</h2><p>然后刷新页面成功进入oozie的web界面</p>
<h2 id="编写job-properties（nameNode要当时active的）"><a href="#编写job-properties（nameNode要当时active的）" class="headerlink" title="编写job.properties（nameNode要当时active的）"></a>编写job.properties（nameNode要当时active的）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">nameNode=hdfs://master2:8020</div><div class="line">jobTracker=master2:8032</div><div class="line">queueName=default</div><div class="line">examplesRoot=user/oozie/my-apps/shell</div><div class="line">oozie.wf.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/workflow.xml</div><div class="line">EXEC=emp-join-demp.sh</div></pre></td></tr></table></figure>
<h2 id="编写workflow-xml"><a href="#编写workflow-xml" class="headerlink" title="编写workflow.xml"></a>编写workflow.xml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt;</div><div class="line"> &lt;start to=&quot;shell-node&quot;/&gt;</div><div class="line"> &lt;action name=&quot;shell-node&quot;&gt;</div><div class="line">     &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt;</div><div class="line">         &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</div><div class="line">         &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</div><div class="line">         &lt;configuration&gt;</div><div class="line">             &lt;property&gt;</div><div class="line">                 &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</div><div class="line">                 &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</div><div class="line">             &lt;/property&gt;</div><div class="line">         &lt;/configuration&gt;</div><div class="line">         &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt;</div><div class="line">         &lt;file&gt;$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;!--Copy the executable to compute node&apos;s current working directory --&gt;</div><div class="line">     &lt;/shell&gt;</div><div class="line">     &lt;ok to=&quot;end&quot;/&gt;</div><div class="line">     &lt;error to=&quot;fail&quot;/&gt;</div><div class="line"> &lt;/action&gt;</div><div class="line"> &lt;kill name=&quot;fail&quot;&gt;</div><div class="line">     &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;</div><div class="line"> &lt;/kill&gt;</div><div class="line"> &lt;end name=&quot;end&quot;/&gt;</div><div class="line">&lt;/workflow-app&gt;</div></pre></td></tr></table></figure>
<h2 id="编写emp-join-demp-sh"><a href="#编写emp-join-demp-sh" class="headerlink" title="编写emp-join-demp.sh"></a>编写emp-join-demp.sh</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line">java -cp PhoenixAPI-1.0-SNAPSHOT.jar com.zwjf.Month</div></pre></td></tr></table></figure>
<p>[hdfs@master1 data]$ hadoop fs -mkdir -p /user/oozie/my-apps/shell<br>[hdfs@master1 data]$ hadoop fs -put workflow.xml /user/oozie/my-apps/shell<br>[hdfs@master1 data]$ hadoop fs -put emp-join-demp.sh /user/oozie/my-apps/shell</p>
<h2 id="上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作"><a href="#上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作" class="headerlink" title="上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作"></a>上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作</h2><p>hadoop fs -put PhoenixAPI-1.0-SNAPSHOT.jar /user/oozie/my-apps/shell</p>
<p>[hdfs@master1 data]$ /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie  job -oozie <a href="http://master1:11000/oozie" target="_blank" rel="external">http://master1:11000/oozie</a>  -config /soft/data/job.properties -run </p>
<h2 id="提交出错："><a href="#提交出错：" class="headerlink" title="提交出错："></a>提交出错：</h2><p>去历史服务器查看问题<br><a href="http://master2:19888/jobhistory" target="_blank" rel="external">http://master2:19888/jobhistory</a></p>
<h2 id="报错："><a href="#报错：" class="headerlink" title="报错："></a>报错：</h2><p>Launcher ERROR, reason: Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]</p>
<p>是因为分发的时候找不到用户的jar包，在workflow.xml的</p>
<file>${nameNode}/${examplesRoot}/${EXEC}#${EXEC}</file><br>添加你的jar包并上传hdfs就可以<br>## 例如：<br><file>${nameNode}/${examplesRoot}/PhoenixAPI-1.0-SNAPSHOT.jar#PhoenixAPI-1.0-SNAPSHOT.jar</file>


<h2 id="可参考如下文章"><a href="#可参考如下文章" class="headerlink" title="可参考如下文章"></a>可参考如下文章</h2><p><a href="http://blog.sina.com.cn/s/blog_e699b42b0102xh3o.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_e699b42b0102xh3o.html</a></p>
<h2 id="增加定时任务"><a href="#增加定时任务" class="headerlink" title="增加定时任务"></a>增加定时任务</h2><p>job.properties增加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">oozie.coord.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/coordinator.xml</div><div class="line">start=2017-05-18T16:30Z</div><div class="line">end=2019-07-30T16:00Z</div></pre></td></tr></table></figure></p>
<p>oozie.wf.application.path=${nameNode}/${examplesRoot}/workflow.xml<br>改成<br>workflowAppUri=${nameNode}/${examplesRoot}/workflow.xml</p>
<h2 id="新建coordinator-xml"><a href="#新建coordinator-xml" class="headerlink" title="新建coordinator.xml"></a>新建coordinator.xml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&lt;coordinator-app name=&quot;coordinator&quot; frequency=&quot;$&#123;coord:minutes(10)&#125;&quot; start=&quot;$&#123;start&#125;&quot; end=&quot;$&#123;end&#125;&quot; timezone=&quot;Asia/Shanghai&quot; xmlns=&quot;uri:oozie:coordinator:0.2&quot;&gt;</div><div class="line">  &lt;action&gt;</div><div class="line">    &lt;workflow&gt;</div><div class="line">      &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt;</div><div class="line">      &lt;configuration&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;jobTracker&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;EXEC&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;EXEC&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;nameNode&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;queueName&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">      &lt;/configuration&gt;</div><div class="line">    &lt;/workflow&gt;</div><div class="line">  &lt;/action&gt;</div><div class="line">&lt;/coordinator-app&gt;</div></pre></td></tr></table></figure>
<h2 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h2><p>hadoop fs -put coordinator.xml /user/oozie/my-apps/shell</p>
<h2 id="停止任务："><a href="#停止任务：" class="headerlink" title="停止任务："></a>停止任务：</h2><p>/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie  job -oozie <a href="http://master1:11000/oozie" target="_blank" rel="external">http://master1:11000/oozie</a>  -kill 0000005-170518154227460-oozie-oozi-C</p>
<p>注：设置的时间不能小于当前时间，否则会把之前没执行的都执行</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装完oozie，打开UI&quot;&gt;&lt;a href=&quot;#安装完oozie，打开UI&quot; class=&quot;headerlink&quot; title=&quot;安装完oozie，打开UI:&quot;&gt;&lt;/a&gt;安装完oozie，打开UI:&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://master1:11000/oozie/&quot;&gt;http://master1:11000/oozie/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;显示Oozie web console is disabled.&lt;br&gt;解决方案;&lt;br&gt;原因是oozie的/var/lib/oozie目录里缺少EXT的包&lt;/p&gt;
&lt;p&gt;点击Documentation链接里quickstart给出了解决方案&lt;br&gt;Download ExtJS library (it must be version 2.2)&lt;br&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>kafka源码阅读2</title>
    <link href="http://yoursite.com/2017/05/08/kafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB2/"/>
    <id>http://yoursite.com/2017/05/08/kafka源码阅读2/</id>
    <published>2017-05-08T03:30:00.000Z</published>
    <updated>2017-08-13T06:04:53.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导入IDEA即可看kafka源码："><a href="#导入IDEA即可看kafka源码：" class="headerlink" title="导入IDEA即可看kafka源码："></a>导入IDEA即可看kafka源码：</h2><h2 id="启动之前需要安装zookeeper"><a href="#启动之前需要安装zookeeper" class="headerlink" title="启动之前需要安装zookeeper"></a>启动之前需要安装zookeeper</h2><p>地址：<br><a href="http://apache.fayea.com/zookeeper/" target="_blank" rel="external">http://apache.fayea.com/zookeeper/</a><br><a href="http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz" target="_blank" rel="external">http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz</a></p>
<h2 id="解压后再当前目录增加"><a href="#解压后再当前目录增加" class="headerlink" title="解压后再当前目录增加"></a>解压后再当前目录增加</h2><p>dataLogDir和data目录<br>复制一份配置文件<br>改名为zoo.cfg<br>修改配置文件：<br>zoo.cfg<br>修改并增加<br>dataDir=D:\tool\zookeeper-3.4.6\data<br>dataLogDir=D:\tool\zookeeper-3.4.6\dataLogDir</p>
<h2 id="启动zkServer-cmd"><a href="#启动zkServer-cmd" class="headerlink" title="启动zkServer.cmd"></a>启动zkServer.cmd</h2><a id="more"></a>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8zk.jpg" alt="此处输入图片的描述"></p>
<p>启动ZkCli.cmd<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8zkCli.jpg" alt="此处输入图片的描述"></p>
<p>kafka启动。在配置文件修改<br>Program arguments：config/server.properties</p>
<p>然后修改server.properties里面的参数即可。</p>
<p>启动前：<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8kafka%E5%89%8D.jpg" alt="此处输入图片的描述"></p>
<p>启动后：<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8kafka%E5%90%8E.jpg" alt="此处输入图片的描述"></p>
<p>源码阅读（一）<br>从启动入口分析：Kafka.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">/*从配置文件读取kafka服务器启动参数--将传入的参数转换成Properties 文件，如果参数为空将提示：</span></div><div class="line">       * USAGE: java [options] KafkaServer server.properties [--override property=value]*</div><div class="line">       * Option               Description</div><div class="line">       * ------               -----------</div><div class="line">       * --override &lt;String&gt;  Optional property that should override values set in</div><div class="line">       *                server.properties file</div><div class="line">       *</div><div class="line">       *  判断参数是否大于1，将后面的参数放到Properties里</div><div class="line">       * */</div><div class="line">      <span class="keyword">val</span> serverProps = getPropsFromArgs(args)</div><div class="line"></div><div class="line">      <span class="comment">//创建KafkaServerStartable对象</span></div><div class="line">      <span class="keyword">val</span> kafkaServerStartable = <span class="type">KafkaServerStartable</span>.fromProps(serverProps)</div><div class="line"></div><div class="line"></div><div class="line">      <span class="comment">// attach shutdown handler to catch control-c</span></div><div class="line">      <span class="type">Runtime</span>.getRuntime().addShutdownHook(<span class="keyword">new</span> <span class="type">Thread</span>() &#123;</div><div class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() = &#123;</div><div class="line">          kafkaServerStartable.shutdown</div><div class="line">        &#125;</div><div class="line">      &#125;)</div><div class="line"></div><div class="line">      kafkaServerStartable.startup</div><div class="line">      kafkaServerStartable.awaitShutdown</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        fatal(e)</div><div class="line">        <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="type">System</span>.exit(<span class="number">0</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h5 id="这上面有个小知识点：-告诉编译器你希望将某个参数当作参数序列处理"><a href="#这上面有个小知识点：-告诉编译器你希望将某个参数当作参数序列处理" class="headerlink" title="这上面有个小知识点：_* 告诉编译器你希望将某个参数当作参数序列处理"></a>这上面有个小知识点：_* 告诉编译器你希望将某个参数当作参数序列处理</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">echo</span></span>(args: <span class="type">String</span>*) = <span class="keyword">for</span> (arg &lt;- args) println(arg)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) = &#123;</div><div class="line">  <span class="keyword">var</span> args = <span class="type">Array</span>(<span class="string">"config/server.properties"</span>,<span class="string">"canshu1"</span>,<span class="string">"canshu2"</span>)</div><div class="line">  echo(args.slice(<span class="number">1</span>, args.length): _*)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>输出是<br>canshu1<br>canshu2</p>
<p>kafkaServerStartable封装了KafkaServer</p>
<h3 id="1-具体的启动类在：KafkaServerStartable的startup方法"><a href="#1-具体的启动类在：KafkaServerStartable的startup方法" class="headerlink" title="1.具体的启动类在：KafkaServerStartable的startup方法"></a>1.具体的启动类在：KafkaServerStartable的startup方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      info(<span class="string">"starting"</span>)</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(isShuttingDown.get)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(startupComplete.get)</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">      <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">      <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">        brokerState.newState(<span class="type">Starting</span>)</div><div class="line"></div><div class="line">        <span class="comment">/* start scheduler */</span></div><div class="line">        kafkaScheduler.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* setup zookeeper */</span></div><div class="line">        zkUtils = initZk()</div><div class="line"></div><div class="line">        <span class="comment">/* Get or create cluster_id */</span></div><div class="line">        _clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">        info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div><div class="line"></div><div class="line">        <span class="comment">/* generate brokerId */</span></div><div class="line">        config.brokerId =  getBrokerId</div><div class="line">        <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">        <span class="comment">/* create and configure metrics */</span></div><div class="line">        <span class="keyword">val</span> reporters = config.getConfiguredInstances(<span class="type">KafkaConfig</span>.<span class="type">MetricReporterClassesProp</span>, classOf[<span class="type">MetricsReporter</span>],</div><div class="line">            <span class="type">Map</span>[<span class="type">String</span>, <span class="type">AnyRef</span>](<span class="type">KafkaConfig</span>.<span class="type">BrokerIdProp</span> -&gt; (config.brokerId.toString)).asJava)</div><div class="line">        reporters.add(<span class="keyword">new</span> <span class="type">JmxReporter</span>(jmxPrefix))</div><div class="line">        <span class="keyword">val</span> metricConfig = <span class="type">KafkaServer</span>.metricConfig(config)</div><div class="line">        metrics = <span class="keyword">new</span> <span class="type">Metrics</span>(metricConfig, reporters, time, <span class="literal">true</span>)</div><div class="line"></div><div class="line">        quotaManagers = <span class="type">QuotaFactory</span>.instantiate(config, metrics, time)</div><div class="line">        notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala)</div><div class="line"></div><div class="line">        <span class="comment">/* start log manager */</span></div><div class="line">        logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">        logManager.startup()</div><div class="line"></div><div class="line">        metadataCache = <span class="keyword">new</span> <span class="type">MetadataCache</span>(config.brokerId)</div><div class="line">        credentialProvider = <span class="keyword">new</span> <span class="type">CredentialProvider</span>(config.saslEnabledMechanisms)</div><div class="line"></div><div class="line">        socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</div><div class="line">        socketServer.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* start replica manager */</span></div><div class="line">        replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">          isShuttingDown, quotaManagers.follower)</div><div class="line">        replicaManager.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* start kafka controller */</span></div><div class="line">        kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</div><div class="line">        kafkaController.startup()</div><div class="line"></div><div class="line">        adminManager = <span class="keyword">new</span> <span class="type">AdminManager</span>(config, metrics, metadataCache, zkUtils)</div><div class="line"></div><div class="line">        <span class="comment">/* start group coordinator */</span></div><div class="line">        <span class="comment">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</span></div><div class="line">        groupCoordinator = <span class="type">GroupCoordinator</span>(config, zkUtils, replicaManager, <span class="type">Time</span>.<span class="type">SYSTEM</span>)</div><div class="line">        groupCoordinator.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* Get the authorizer and initialize it if one is specified.*/</span></div><div class="line">        authorizer = <span class="type">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</div><div class="line">          <span class="keyword">val</span> authZ = <span class="type">CoreUtils</span>.createObject[<span class="type">Authorizer</span>](authorizerClassName)</div><div class="line">          authZ.configure(config.originals())</div><div class="line">          authZ</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">/* start processing requests */</span></div><div class="line">        apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</div><div class="line">          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</div><div class="line">          clusterId, time)</div><div class="line"></div><div class="line">        requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</div><div class="line">          config.numIoThreads)</div><div class="line"></div><div class="line">        <span class="type">Mx4jLoader</span>.maybeLoad()</div><div class="line"></div><div class="line">        <span class="comment">/* start dynamic config manager */</span></div><div class="line">        dynamicConfigHandlers = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ConfigHandler</span>](<span class="type">ConfigType</span>.<span class="type">Topic</span> -&gt; <span class="keyword">new</span> <span class="type">TopicConfigHandler</span>(logManager, config, quotaManagers),</div><div class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Client</span> -&gt; <span class="keyword">new</span> <span class="type">ClientIdConfigHandler</span>(quotaManagers),</div><div class="line">                                                           <span class="type">ConfigType</span>.<span class="type">User</span> -&gt; <span class="keyword">new</span> <span class="type">UserConfigHandler</span>(quotaManagers, credentialProvider),</div><div class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Broker</span> -&gt; <span class="keyword">new</span> <span class="type">BrokerConfigHandler</span>(config, quotaManagers))</div><div class="line"></div><div class="line">        <span class="comment">// Create the config manager. start listening to notifications</span></div><div class="line">        dynamicConfigManager = <span class="keyword">new</span> <span class="type">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</div><div class="line">        dynamicConfigManager.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* tell everyone we are alive */</span></div><div class="line">        <span class="keyword">val</span> listeners = config.advertisedListeners.map &#123; endpoint =&gt;</div><div class="line">          <span class="keyword">if</span> (endpoint.port == <span class="number">0</span>)</div><div class="line">            endpoint.copy(port = socketServer.boundPort(endpoint.listenerName))</div><div class="line">          <span class="keyword">else</span></div><div class="line">            endpoint</div><div class="line">        &#125;</div><div class="line">        kafkaHealthcheck = <span class="keyword">new</span> <span class="type">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</div><div class="line">          config.interBrokerProtocolVersion)</div><div class="line">        kafkaHealthcheck.startup()</div><div class="line"></div><div class="line">        <span class="comment">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></div><div class="line">        checkpointBrokerId(config.brokerId)</div><div class="line"></div><div class="line">        <span class="comment">/* register broker metrics */</span></div><div class="line">        registerStats()</div><div class="line"></div><div class="line">        brokerState.newState(<span class="type">RunningAsBroker</span>)</div><div class="line">        shutdownLatch = <span class="keyword">new</span> <span class="type">CountDownLatch</span>(<span class="number">1</span>)</div><div class="line">        startupComplete.set(<span class="literal">true</span>)</div><div class="line">        isStartingUp.set(<span class="literal">false</span>)</div><div class="line">        <span class="type">AppInfoParser</span>.registerAppInfo(jmxPrefix, config.brokerId.toString)</div><div class="line">        info(<span class="string">"started"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">        isStartingUp.set(<span class="literal">false</span>)</div><div class="line">        shutdown()</div><div class="line">        <span class="keyword">throw</span> e</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h5 id="Starting-继承BrokerStates，BrokerStates是一个sealed-trait"><a href="#Starting-继承BrokerStates，BrokerStates是一个sealed-trait" class="headerlink" title="Starting 继承BrokerStates，BrokerStates是一个sealed trait"></a>Starting 继承BrokerStates，BrokerStates是一个sealed trait</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">sealed</span> <span class="class"><span class="keyword">trait</span> <span class="title">BrokerStates</span> </span>&#123; <span class="function"><span class="keyword">def</span> <span class="title">state</span></span>: <span class="type">Byte</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">NotRunning</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">0</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">Starting</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">1</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">RecoveringFromUncleanShutdown</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">2</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">RunningAsBroker</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">3</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">PendingControlledShutdown</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">6</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">BrokerShuttingDown</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">7</span> &#125;</div></pre></td></tr></table></figure>
<p>trait定义为sealed 有两层含义<br>1.其修饰的trait class只能在当前文件里面被继承<br>2.用sealed修饰这样做的目的是告诉scala编译器在检查模式匹配的时候，让scala知道这些case的所有情况，scala就能够在编译的时候进行检查，看你写的代码是否有没有漏掉什么没case到，减少编程的错误。</p>
<h3 id="2-start-scheduler"><a href="#2-start-scheduler" class="headerlink" title="2.start scheduler:"></a>2.start scheduler:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kafkaScheduler.startup()</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaScheduler</span>(<span class="params">val threads: <span class="type">Int</span>, </span></span></div><div class="line">                     val threadNamePrefix: <span class="type">String</span> = "kafka-scheduler-", </div><div class="line">                     daemon: <span class="type">Boolean</span> = true) <span class="keyword">extends</span> <span class="title">Scheduler</span> <span class="keyword">with</span> <span class="title">Logging</span> &#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> executor: <span class="type">ScheduledThreadPoolExecutor</span> = <span class="literal">null</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> schedulerThreadId = <span class="keyword">new</span> <span class="type">AtomicInteger</span>(<span class="number">0</span>)</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">    debug(<span class="string">"Initializing task scheduler."</span>)</div><div class="line">    <span class="keyword">this</span> synchronized &#123;</div><div class="line">      <span class="keyword">if</span>(isStarted)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"This scheduler has already been started!"</span>)</div><div class="line">      executor = <span class="keyword">new</span> <span class="type">ScheduledThreadPoolExecutor</span>(threads)</div><div class="line">      executor.setContinueExistingPeriodicTasksAfterShutdownPolicy(<span class="literal">false</span>)</div><div class="line">      executor.setExecuteExistingDelayedTasksAfterShutdownPolicy(<span class="literal">false</span>)</div><div class="line">      executor.setThreadFactory(<span class="keyword">new</span> <span class="type">ThreadFactory</span>() &#123;</div><div class="line">                                  <span class="function"><span class="keyword">def</span> <span class="title">newThread</span></span>(runnable: <span class="type">Runnable</span>): <span class="type">Thread</span> = </div><div class="line">                                    <span class="type">Utils</span>.newThread(threadNamePrefix + schedulerThreadId.getAndIncrement(), runnable, daemon)</div><div class="line">                                &#125;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>1.此处使用了同步锁，如果已经启动，直接抛个IllegalStateException异常，由外面通用异常Throwable捕获。<br>2.根据配置文件的background.threads 创建一个ScheduledThreadPoolExecutor(threads)【java.util.concurrent包下的】</p>
<h3 id="3-setup-zookeeper"><a href="#3-setup-zookeeper" class="headerlink" title="3.setup zookeeper"></a>3.setup zookeeper</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">zkUtils = initZk()</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initZk</span></span>(): <span class="type">ZkUtils</span> = &#123;</div><div class="line">   info(<span class="string">s"Connecting to zookeeper on <span class="subst">$&#123;config.zkConnect&#125;</span>"</span>)</div><div class="line"></div><div class="line">   <span class="keyword">val</span> chrootIndex = config.zkConnect.indexOf(<span class="string">"/"</span>)</div><div class="line">   <span class="keyword">val</span> chrootOption = &#123;</div><div class="line">     <span class="keyword">if</span> (chrootIndex &gt; <span class="number">0</span>) <span class="type">Some</span>(config.zkConnect.substring(chrootIndex))</div><div class="line">     <span class="keyword">else</span> <span class="type">None</span></div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">val</span> secureAclsEnabled = config.zkEnableSecureAcls</div><div class="line">   <span class="keyword">val</span> isZkSecurityEnabled = <span class="type">JaasUtils</span>.isZkSecurityEnabled()</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (secureAclsEnabled &amp;&amp; !isZkSecurityEnabled)</div><div class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.<span class="type">SecurityException</span>(<span class="string">s"<span class="subst">$&#123;KafkaConfig.ZkEnableSecureAclsProp&#125;</span> is true, but the verification of the JAAS login file failed."</span>)</div><div class="line"></div><div class="line">   chrootOption.foreach &#123; chroot =&gt;</div><div class="line">     <span class="keyword">val</span> zkConnForChrootCreation = config.zkConnect.substring(<span class="number">0</span>, chrootIndex)</div><div class="line">     <span class="keyword">val</span> zkClientForChrootCreation = <span class="type">ZkUtils</span>(zkConnForChrootCreation,</div><div class="line">                                             sessionTimeout = config.zkSessionTimeoutMs,</div><div class="line">                                             connectionTimeout = config.zkConnectionTimeoutMs,</div><div class="line">                                             secureAclsEnabled)</div><div class="line">     zkClientForChrootCreation.makeSurePersistentPathExists(chroot)</div><div class="line">     info(<span class="string">s"Created zookeeper path <span class="subst">$chroot</span>"</span>)</div><div class="line">     zkClientForChrootCreation.zkClient.close()</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">val</span> zkUtils = <span class="type">ZkUtils</span>(config.zkConnect,</div><div class="line">                         sessionTimeout = config.zkSessionTimeoutMs,</div><div class="line">                         connectionTimeout = config.zkConnectionTimeoutMs,</div><div class="line">                         secureAclsEnabled)</div><div class="line">   zkUtils.setupCommonPaths()</div><div class="line">   zkUtils</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>1.创建连接<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> persistentZkPaths = <span class="type">Seq</span>(<span class="type">ConsumersPath</span>,</div><div class="line">                              <span class="type">BrokerIdsPath</span>,</div><div class="line">                              <span class="type">BrokerTopicsPath</span>,</div><div class="line">                              <span class="type">ConfigChangesPath</span>,</div><div class="line">                              getEntityConfigRootPath(<span class="type">ConfigType</span>.<span class="type">Topic</span>),</div><div class="line">                              getEntityConfigRootPath(<span class="type">ConfigType</span>.<span class="type">Client</span>),</div><div class="line">                              <span class="type">DeleteTopicsPath</span>,</div><div class="line">                              <span class="type">BrokerSequenceIdPath</span>,</div><div class="line">                              <span class="type">IsrChangeNotificationPath</span>)</div></pre></td></tr></table></figure></p>
<p>2.设置通用路径<br>/consumers<br>/brokers/ids<br>/brokers/topics<br>/config/changes<br>/config/topics<br>/config/clients<br>/admin/delete_topics<br>/brokers/seqid<br>/isr_change_notification</p>
<p>ISR：Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas） set，这个set里的所有replica都跟上了leader，只有ISR里的成员才有被选为leader的可能</p>
<h3 id="4-Get-or-create-cluster-id"><a href="#4-Get-or-create-cluster-id" class="headerlink" title="4.Get or create cluster_id"></a>4.Get or create cluster_id</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">_clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getOrGenerateClusterId</span></span>(zkUtils: <span class="type">ZkUtils</span>): <span class="type">String</span> = &#123;</div><div class="line">    zkUtils.getClusterId.getOrElse(zkUtils.createOrGetClusterId(<span class="type">CoreUtils</span>.generateUuidAsBase64))</div><div class="line">  &#125;</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createOrGetClusterId</span></span>(proposedClusterId: <span class="type">String</span>): <span class="type">String</span> = &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      createPersistentPath(<span class="type">ClusterIdPath</span>, <span class="type">ClusterId</span>.toJson(proposedClusterId))</div><div class="line">      proposedClusterId</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</div><div class="line">        getClusterId.getOrElse(<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Failed to get cluster id from Zookeeper. This can only happen if /cluster/id is deleted from Zookeeper."</span>))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"><span class="comment">/**</span></div><div class="line">   * Create an persistent node with the given path and data. Create parents if necessary.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createPersistentPath</span></span>(path: <span class="type">String</span>, data: <span class="type">String</span> = <span class="string">""</span>, acls: java.util.<span class="type">List</span>[<span class="type">ACL</span>] = <span class="type">UseDefaultAcls</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> acl = <span class="keyword">if</span> (acls eq <span class="type">UseDefaultAcls</span>) <span class="type">ZkUtils</span>.defaultAcls(isSecure, path) <span class="keyword">else</span> acls</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="type">ZkPath</span>.createPersistent(zkClient, path, data, acl)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">ZkNoNodeException</span> =&gt;</div><div class="line">        createParentPath(path)</div><div class="line">        <span class="type">ZkPath</span>.createPersistent(zkClient, path, data, acl)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>此处会创建一个persistent节点/cluster/id 如果节点已经存在，则刨除异常，上次获取异常，然后去节点下获取_clusterId，如果不存在，将创建的proposedClusterId返回</p>
<h5 id="此处需要注意zookeeper的节点类型分为："><a href="#此处需要注意zookeeper的节点类型分为：" class="headerlink" title="此处需要注意zookeeper的节点类型分为："></a>此处需要注意zookeeper的节点类型分为：</h5><p>持久节点（PERSISTENT）<br>持久顺序节点（PERSISTENT_SEQUENTIAL）<br>临时节点（EPHEMERAL）<br>临时顺序节点（EPHEMERAL_SEQUENTIAL）<br>顺序节点即创建有序的节点，节点名加上一个数字后缀。临时节点和客户端绑定，会话失效（非连接断开）则自动清楚</p>
<p>临时顺序节点可用来实现分布式锁<br>1.客户端调用create()方法创建名为“<em>locknode</em>/guid-lock-”的节点，需要注意的是，这里节点的创建类型需要设置为EPHEMERAL_SEQUENTIAL。<br>2.客户端调用getChildren(“<em>locknode</em>”)方法来获取所有已经创建的子节点，注意，这里不注册任何Watcher。<br>3.客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点序号最小，那么就认为这个客户端获得了锁。<br>4.如果在步骤3中发现自己并非所有子节点中最小的，说明自己还没有获取到锁。此时客户端需要找到比自己小的那个节点，然后对其调用exist()方法，同时注册事件监听。<br>5.之后当这个被关注的节点被移除了，客户端会收到相应的通知。这个时候客户端需要再次调用getChildren(“<em>locknode</em>”)方法来获取所有已经创建的子节点，确保自己确实是最小的节点了，然后进入步骤3。</p>
<h3 id="5-generate-brokerId"><a href="#5-generate-brokerId" class="headerlink" title="5.generate brokerId"></a>5.generate brokerId</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">config.brokerId =  getBrokerId</div><div class="line"><span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div></pre></td></tr></table></figure>
<p>略</p>
<h3 id="6-create-and-configure-metrics"><a href="#6-create-and-configure-metrics" class="headerlink" title="6.create and configure metrics"></a>6.create and configure metrics</h3><p>内部状态的监控模块<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> reporters = config.getConfiguredInstances(<span class="type">KafkaConfig</span>.<span class="type">MetricReporterClassesProp</span>, classOf[<span class="type">MetricsReporter</span>],</div><div class="line">            <span class="type">Map</span>[<span class="type">String</span>, <span class="type">AnyRef</span>](<span class="type">KafkaConfig</span>.<span class="type">BrokerIdProp</span> -&gt; (config.brokerId.toString)).asJava)</div><div class="line">reporters.add(<span class="keyword">new</span> <span class="type">JmxReporter</span>(jmxPrefix))</div><div class="line"><span class="keyword">val</span> metricConfig = <span class="type">KafkaServer</span>.metricConfig(config)</div><div class="line">metrics = <span class="keyword">new</span> <span class="type">Metrics</span>(metricConfig, reporters, time, <span class="literal">true</span>)</div><div class="line"></div><div class="line">quotaManagers = <span class="type">QuotaFactory</span>.instantiate(config, metrics, time)</div><div class="line">notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala)</div></pre></td></tr></table></figure></p>
<p>从配置参数metric.reporters 获取监控类</p>
<p>此处有个小概念;<br>集合允许使用asScala和asJava方法来做scala和java之间的转换</p>
<p>metricConfig里封装了<br>metrics.num.samples（     用于维护metrics的样本数）<br>metrics.recording.level<br>metrics.sample.window.ms（metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口推出后，我们会擦除并重写最老的窗口）</p>
<p>小概念：<br>4种操作符的区别和联系<br>:: 该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。用法为        x::list,其中x为加入到头部的元素，无论x是列表与否，它都只将成为新生成列表的第一个元素，也就是说新生成的列表长度为list的长度＋1(BTW ， x::list等价于list.::(x))<br>:+和+: 两者的区别在于:+方法用于在尾部追加元素，+:方法用于在头部追加元素，和::很类似，但是::可以用于pattern match ，而+:则不行. 关于+:和:+,只要记住冒号永远靠近集合类型就OK了。<br>++ 该方法用于连接两个集合，list1++list2<br>::: 该方法只能用于连接两个List类型的集合</p>
<h3 id="7-start-log-manager"><a href="#7-start-log-manager" class="headerlink" title="7.start log manager"></a>7.start log manager</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">logManager.startup()</div><div class="line"></div><div class="line">metadataCache = <span class="keyword">new</span> <span class="type">MetadataCache</span>(config.brokerId)</div><div class="line">credentialProvider = <span class="keyword">new</span> <span class="type">CredentialProvider</span>(config.saslEnabledMechanisms)</div><div class="line"></div><div class="line">socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</div><div class="line">socketServer.startup()</div></pre></td></tr></table></figure>
<p>根据一系列配置参数，启动LogManager。详情见kafka.log.CleanerConfig和kafka.log.LogManager</p>
<p>startup创建了4个线程，分别负责创建日志，写日志，检索日志，清理日志<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">    <span class="comment">/* Schedule the cleanup task to delete old logs */</span></div><div class="line">    <span class="keyword">if</span>(scheduler != <span class="literal">null</span>) &#123;</div><div class="line">      info(<span class="string">"Starting log cleanup with a period of %d ms."</span>.format(retentionCheckMs))</div><div class="line">      scheduler.schedule(<span class="string">"kafka-log-retention"</span>,</div><div class="line">                         cleanupLogs,</div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                         period = retentionCheckMs,</div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">      info(<span class="string">"Starting log flusher with a default period of %d ms."</span>.format(flushCheckMs))</div><div class="line">      scheduler.schedule(<span class="string">"kafka-log-flusher"</span>, </div><div class="line">                         flushDirtyLogs, </div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>, </div><div class="line">                         period = flushCheckMs, </div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">      scheduler.schedule(<span class="string">"kafka-recovery-point-checkpoint"</span>,</div><div class="line">                         checkpointRecoveryPointOffsets,</div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                         period = flushCheckpointMs,</div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">      scheduler.schedule(<span class="string">"kafka-delete-logs"</span>,</div><div class="line">                         deleteLogs,</div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                         period = defaultConfig.fileDeleteDelayMs,</div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span>(cleanerConfig.enableCleaner)</div><div class="line">      cleaner.startup()</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<h3 id="8-start-replica-manager"><a href="#8-start-replica-manager" class="headerlink" title="8.start replica manager"></a>8.start replica manager</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">          isShuttingDown, quotaManagers.follower)</div><div class="line">replicaManager.startup()</div></pre></td></tr></table></figure>
<p>启动isr-expiration线程<br>启动isr-change-propagation线程<br>在/controller下建了一个监听</p>
<p>此处有个小技巧<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inLock</span></span>[<span class="type">T</span>](lock: <span class="type">Lock</span>)(fun: =&gt; <span class="type">T</span>): <span class="type">T</span> = &#123;</div><div class="line">    lock.lock()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      fun</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      lock.unlock()</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<blockquote>
<p> : =&gt;注意:后面要有空格，此处标明调用的时候才执行，否则在用inlock的函数时候fun已经在锁外面执行了.</p>
</blockquote>
<p>参考：<a href="http://www.jianshu.com/p/f53e0b54a44a" target="_blank" rel="external">http://www.jianshu.com/p/f53e0b54a44a</a></p>
<h3 id="9-start-kafka-controller"><a href="#9-start-kafka-controller" class="headerlink" title="9.start kafka controller"></a>9.start kafka controller</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</div><div class="line">kafkaController.startup()</div><div class="line"></div><div class="line">adminManager = <span class="keyword">new</span> <span class="type">AdminManager</span>(config, metrics, metadataCache, zkUtils)</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() = &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      info(<span class="string">"Controller starting up"</span>)</div><div class="line">      registerSessionExpirationListener()</div><div class="line">      isRunning = <span class="literal">true</span></div><div class="line">      controllerElector.startup</div><div class="line">      info(<span class="string">"Controller startup complete"</span>)</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分支1.registerSessionExpirationListener-&gt;SessionExpirationListener-&gt;handleNewSession</p>
<p>当会话超时，重新连接上的时候，调用之前注册在ZookeeperLeaderElector的onControllerResignation函数<br>controllerElector.elect 重新选举</p>
<p>分支2.ZookeeperLeaderElector-&gt;（onControllerFailover，onControllerResignation）-&gt;LeaderChangeListener<br>controllerElector就是ZookeeperLeaderElector 是kafka的选举机制<br>ZookeeperLeaderElector：通过zk创建Ephemeral Node（临时节点）的方式来进行选举，即如果存在并发情况下向zk的同一个路径创建node的话，有且只有1个客户端会创建成功，其它客户端创建失败，但是当创建成功的客户端和zk的链接断开之后，这个node也会消失，其它的客户端从而继续竞争  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span> </span>&#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)</div><div class="line">      elect</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>1.监听electionPath（/controller）<br>2.elect选举</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> <span class="type">ControllerPath</span> = <span class="string">"/controller"</span></div><div class="line"><span class="keyword">val</span> electString = <span class="type">Json</span>.encode(<span class="type">Map</span>(<span class="string">"version"</span> -&gt; <span class="number">1</span>, <span class="string">"brokerid"</span> -&gt; brokerId, <span class="string">"timestamp"</span> -&gt; timestamp))</div><div class="line"><span class="keyword">val</span> zkCheckedEphemeral = <span class="keyword">new</span> <span class="type">ZKCheckedEphemeral</span>(electionPath,lectString, controllerContext.zkUtils.zkConnection.getZookeeper,  <span class="type">JaasUtils</span>.isZkSecurityEnabled())</div><div class="line">leaderId = getControllerID</div></pre></td></tr></table></figure>
<p>此处会在/controller 下面写一个类似如下的内容：<br>{“version”:1,”brokerid”:102,”timestamp”:”1495880001272”}<br>通过getControllerID获取当前的leaderId<br>然后通过amILeader看自己是否是leader</p>
<p>ZookeeperLeaderElecto<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeaderChangeListener</span> <span class="keyword">extends</span> <span class="title">IZkDataListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">    <span class="comment">/**</span></div><div class="line">     * Called when the leader information stored in zookeeper has changed. Record the new leader in memory</div><div class="line">     * @throws Exception On any error.</div><div class="line">     */</div><div class="line">    <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">Object</span>) &#123;</div><div class="line">      <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">        <span class="keyword">val</span> amILeaderBeforeDataChange = amILeader</div><div class="line">        leaderId = <span class="type">KafkaController</span>.parseControllerId(data.toString)</div><div class="line">        info(<span class="string">"New leader is %d"</span>.format(leaderId))</div><div class="line">        <span class="comment">// The old leader needs to resign leadership if it is no longer the leader</span></div><div class="line">        amILeaderBeforeDataChange &amp;&amp; !amILeader</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (shouldResign)</div><div class="line">        onResigningAsLeader()</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader</div><div class="line">     * @throws Exception</div><div class="line">     *             On any error.</div><div class="line">     */</div><div class="line">    <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123; </div><div class="line">      <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">        debug(<span class="string">"%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader"</span></div><div class="line">          .format(brokerId, dataPath))</div><div class="line">        amILeader</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (shouldResign)</div><div class="line">        onResigningAsLeader()</div><div class="line"></div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        elect</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>如果节点下线会调用handleDataDeleted。看自己是否是leader，如果是需要先退休onResigningAsLeader。<br>然后选举<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">val</span> zkCheckedEphemeral = <span class="keyword">new</span> <span class="type">ZKCheckedEphemeral</span>(electionPath,</div><div class="line">                                                      electString,</div><div class="line">                                                      controllerContext.zkUtils.zkConnection.getZookeeper,</div><div class="line">                                                      <span class="type">JaasUtils</span>.isZkSecurityEnabled())</div><div class="line">      zkCheckedEphemeral.create()</div><div class="line">      info(brokerId + <span class="string">" successfully elected as leader"</span>)</div><div class="line">      leaderId = brokerId</div><div class="line">      onBecomingLeader()</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</div><div class="line">        <span class="comment">// If someone else has written the path, then</span></div><div class="line">        leaderId = getControllerID </div><div class="line"></div><div class="line">        <span class="keyword">if</span> (leaderId != <span class="number">-1</span>)</div><div class="line">          debug(<span class="string">"Broker %d was elected as leader instead of broker %d"</span>.format(leaderId, brokerId))</div><div class="line">        <span class="keyword">else</span></div><div class="line">          warn(<span class="string">"A leader has been elected but just resigned, this will result in another round of election"</span>)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> e2: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Error while electing or becoming leader on broker %d"</span>.format(brokerId), e2)</div><div class="line">        resign()</div><div class="line">    &#125;</div><div class="line">    amILeader</div></pre></td></tr></table></figure></p>
<p>创建临时节点</p>
<p>onControllerFailover:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onControllerFailover</span></span>() &#123;</div><div class="line">    <span class="keyword">if</span>(isRunning) &#123;</div><div class="line">      info(<span class="string">"Broker %d starting become controller state transition"</span>.format(config.brokerId))</div><div class="line">      readControllerEpochFromZookeeper()</div><div class="line">      incrementControllerEpoch(zkUtils.zkClient)</div><div class="line"></div><div class="line">      <span class="comment">// before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks</span></div><div class="line">      registerReassignedPartitionsListener()</div><div class="line">      registerIsrChangeNotificationListener()</div><div class="line">      registerPreferredReplicaElectionListener()</div><div class="line">      partitionStateMachine.registerListeners()</div><div class="line">      replicaStateMachine.registerListeners()</div><div class="line"></div><div class="line">      initializeControllerContext()</div><div class="line"></div><div class="line">      <span class="comment">// We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines</span></div><div class="line">      <span class="comment">// are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before</span></div><div class="line">      <span class="comment">// they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and</span></div><div class="line">      <span class="comment">// partitionStateMachine.startup().</span></div><div class="line">      sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line"></div><div class="line">      replicaStateMachine.startup()</div><div class="line">      partitionStateMachine.startup()</div><div class="line"></div><div class="line">      <span class="comment">// register the partition change listeners for all existing topics on failover</span></div><div class="line">      controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">      info(<span class="string">"Broker %d is ready to serve as the new controller with epoch %d"</span>.format(config.brokerId, epoch))</div><div class="line">      maybeTriggerPartitionReassignment()</div><div class="line">      maybeTriggerPreferredReplicaElection()</div><div class="line">      <span class="keyword">if</span> (config.autoLeaderRebalanceEnable) &#123;</div><div class="line">        info(<span class="string">"starting the partition rebalance scheduler"</span>)</div><div class="line">        autoRebalanceScheduler.startup()</div><div class="line">        autoRebalanceScheduler.schedule(<span class="string">"partition-rebalance-thread"</span>, checkAndTriggerPartitionRebalance,</div><div class="line">          <span class="number">5</span>, config.leaderImbalanceCheckIntervalSeconds.toLong, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)</div><div class="line">      &#125;</div><div class="line">      deleteTopicManager.start()</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span></div><div class="line">      info(<span class="string">"Controller has been shut down, aborting startup/failover"</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>在/admin/reassign_partitions目录注册PartitionsReassignedListener监听函数<br>在/isr_change_notification目录注册IsrChangeNotificationListener监听函数<br>在/admin/preferred_replica_election目录注册PreferredReplicaElectionListener监听函数<br>在/brokers/topics目录注册TopicChangeListener监听函数<br>在/admin/delete_topics目录注册DeleteTopicsListener监听函数<br>在/brokers/ids目录注册BrokerChangeListener监听函数</p>
<blockquote>
<p>监听是调用zk的<br>zkUtils.zkClient.subscribeChildChanges函数，参数是路径和监听函数<br>监听函数实现IZkChildListener接口实现handleChildChange方法</p>
</blockquote>
<p>初始化ControllerContext上下文,里面包含存活的broker，所有主题，分区副本，分区的leader和已经下线的broker。更新leader和isr缓存。启动ControllerChannelManager<br>初始化所有的replica状态<br>初始化所有的partition状态<br>如果auto.leader.rebalance.enable 为true会启动Rebalance调度<br>最后删除主题</p>
<p>通过replicaStateMachine初始化所有的replica状态<br>replicaStateMachine的handleStateChanges<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleStateChanges</span></span>(replicas: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>], targetState: <span class="type">ReplicaState</span>,</div><div class="line">                         callbacks: <span class="type">Callbacks</span> = (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build) &#123;</div><div class="line">    <span class="keyword">if</span>(replicas.nonEmpty) &#123;</div><div class="line">      info(<span class="string">"Invoking state change to %s for replicas %s"</span>.format(targetState, replicas.mkString(<span class="string">","</span>)))</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        brokerRequestBatch.newBatch()</div><div class="line">        replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks))</div><div class="line">        brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">      &#125;<span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some replicas to %s state"</span>.format(targetState), e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>通过partitionStateMachine初始化所有的partition状态<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">handleStateChange</span></span>(topic: <span class="type">String</span>, partition: <span class="type">Int</span>, targetState: <span class="type">PartitionState</span>,</div><div class="line">                                leaderSelector: <span class="type">PartitionLeaderSelector</span>,</div><div class="line">                                callbacks: <span class="type">Callbacks</span>) &#123;</div><div class="line">    <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</div><div class="line">    <span class="keyword">if</span> (!hasStarted.get)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>((<span class="string">"Controller %d epoch %d initiated state change for partition %s to %s failed because "</span> +</div><div class="line">                                            <span class="string">"the partition state machine has not started"</span>)</div><div class="line">                                              .format(controllerId, controller.epoch, topicAndPartition, targetState))</div><div class="line">    <span class="keyword">val</span> currState = partitionState.getOrElseUpdate(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      targetState <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt;</div><div class="line">          <span class="comment">// pre: partition did not exist before this</span></div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NonExistentPartition</span>), <span class="type">NewPartition</span>)</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">NewPartition</span>)</div><div class="line">          <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(<span class="string">","</span>)</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,</div><div class="line">                                            assignedReplicas))</div><div class="line">          <span class="comment">// post: partition has been assigned replicas</span></div><div class="line">        <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt;</div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OnlinePartition</span>)</div><div class="line">          partitionState(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt;</div><div class="line">              <span class="comment">// initialize leader and isr path for new partition</span></div><div class="line">              initializeLeaderAndIsrForPartition(topicAndPartition)</div><div class="line">            <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt;</div><div class="line">              electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">            <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt; <span class="comment">// invoked when the leader needs to be re-elected</span></div><div class="line">              electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">            <span class="keyword">case</span> _ =&gt; <span class="comment">// should never come here since illegal previous states are checked above</span></div><div class="line">          &#125;</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">OnlinePartition</span>)</div><div class="line">          <span class="keyword">val</span> leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s from %s to %s with leader %d"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader))</div><div class="line">           <span class="comment">// post: partition has a leader</span></div><div class="line">        <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt;</div><div class="line">          <span class="comment">// pre: partition should be in New or Online state</span></div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OfflinePartition</span>)</div><div class="line">          <span class="comment">// should be called when the leader for a partition is no longer alive</span></div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">OfflinePartition</span>)</div><div class="line">          <span class="comment">// post: partition has no alive leader</span></div><div class="line">        <span class="keyword">case</span> <span class="type">NonExistentPartition</span> =&gt;</div><div class="line">          <span class="comment">// pre: partition should be in Offline state</span></div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">OfflinePartition</span>), <span class="type">NonExistentPartition</span>)</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line">          <span class="comment">// post: partition state is deleted from all brokers and zookeeper</span></div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">        stateChangeLogger.error(<span class="string">"Controller %d epoch %d initiated state change for partition %s from %s to %s failed"</span></div><div class="line">          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState), t)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>PartitionStateMachine实现了topic的分区状态切换功能，Partition存在的状态如下：<br>NewPartition  分区之前不存在，创建后被分配了replicas，但是还没有leader/isr<br>OnlinePartition  partition在replicas中选举某个成为leader之后<br>OfflinePartition  partition的replicas中的leader下线之后，没有重新选举新的leader之前  或 partition创建之后直接被下线<br>NonExistentPartition  partition重来没有被创建 或 partition创建之后被删除</p>
<p>scala小知识：<br>mkString<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">scala &gt; <span class="keyword">val</span> a = <span class="type">Array</span>(<span class="string">"apple"</span>, <span class="string">"banana"</span>, <span class="string">"cherry"</span>)</div><div class="line">a: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(apple, banana, cherry)</div><div class="line">scala &gt; a.mkString(<span class="string">","</span>)</div><div class="line">res2: <span class="type">String</span> = apple,banana,cherry</div><div class="line"></div><div class="line">scala&gt; a.mkString(<span class="string">"["</span>, <span class="string">", "</span>, <span class="string">"]"</span>)</div><div class="line">res3: <span class="type">String</span> = [apple, banana, cherry]</div><div class="line"></div><div class="line">如果是数组需要先展开数组</div><div class="line">scala&gt; <span class="keyword">val</span> b = <span class="type">Array</span>(<span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>), <span class="type">Array</span>(<span class="string">"c"</span>, <span class="string">"d"</span>))</div><div class="line">b: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(a, b), <span class="type">Array</span>(c, d))</div><div class="line">错误的</div><div class="line">scala&gt; b.mkString(<span class="string">","</span>)</div><div class="line">res4: <span class="type">String</span> = [<span class="type">Ljava</span>.lang.<span class="type">String</span>;@<span class="number">64</span>a9fca7,[<span class="type">Ljava</span>.lang.<span class="type">String</span>;@<span class="number">22</span>f756c5</div><div class="line">正确的</div><div class="line">scala&gt; b.flatten.mkString(<span class="string">","</span>)</div><div class="line">res5: <span class="type">String</span> = a,b,c,d</div></pre></td></tr></table></figure></p>
<p>OnlinePartition ：检查前置状态是否为NewPartition, OnlinePartition, OfflinePartition中的一种，<br>1.如果是NewPartition：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeLeaderAndIsrForPartition</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>) &#123;</div><div class="line">    <span class="keyword">val</span> replicaAssignment = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">val</span> liveAssignedReplicas = replicaAssignment.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">    liveAssignedReplicas.size <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="number">0</span> =&gt;</div><div class="line">        <span class="keyword">val</span> failMsg = (<span class="string">"encountered error during state change of partition %s from New to Online, assigned replicas are [%s], "</span> +</div><div class="line">                       <span class="string">"live brokers are [%s]. No assigned replica is alive."</span>)</div><div class="line">                         .format(topicAndPartition, replicaAssignment.mkString(<span class="string">","</span>), controllerContext.liveBrokerIds)</div><div class="line">        stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">      <span class="keyword">case</span> _ =&gt;</div><div class="line">        debug(<span class="string">"Live assigned replicas for partition %s are: [%s]"</span>.format(topicAndPartition, liveAssignedReplicas))</div><div class="line">        <span class="comment">// make the first replica in the list of assigned replicas, the leader</span></div><div class="line">        <span class="comment">//根据partitionReplicaAssignment中信息选择第一个live的replica为leader,其余为isr</span></div><div class="line">        <span class="keyword">val</span> leader = liveAssignedReplicas.head</div><div class="line">        <span class="keyword">val</span> leaderIsrAndControllerEpoch = <span class="keyword">new</span> <span class="type">LeaderIsrAndControllerEpoch</span>(<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(leader, liveAssignedReplicas.toList),</div><div class="line">          controller.epoch)</div><div class="line">        debug(<span class="string">"Initializing leader and isr for partition %s to %s"</span>.format(topicAndPartition, leaderIsrAndControllerEpoch))</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="comment">//将leader和isr持久化到zookeeper</span></div><div class="line">          zkUtils.createPersistentPath(</div><div class="line">            getTopicPartitionLeaderAndIsrPath(topicAndPartition.topic, topicAndPartition.partition),</div><div class="line">            zkUtils.leaderAndIsrZkData(leaderIsrAndControllerEpoch.leaderAndIsr, controller.epoch))</div><div class="line">          <span class="comment">// <span class="doctag">NOTE:</span> the above write can fail only if the current controller lost its zk session and the new controller</span></div><div class="line">          <span class="comment">// took over and initialized this partition. This can happen if the current controller went into a long</span></div><div class="line">          <span class="comment">// GC pause</span></div><div class="line">          <span class="comment">//更新controllerContext中的partitionLeadershipInfo</span></div><div class="line">          controllerContext.partitionLeadershipInfo.put(topicAndPartition, leaderIsrAndControllerEpoch)</div><div class="line">          <span class="comment">//封装发送给这些replica所在的broker的LeaderAndIsrRequest请求，交由ControllerBrokerRequestBatch(brokerRequestBatch)处理</span></div><div class="line">          brokerRequestBatch.addLeaderAndIsrRequestForBrokers(liveAssignedReplicas, topicAndPartition.topic,</div><div class="line">            topicAndPartition.partition, leaderIsrAndControllerEpoch, replicaAssignment)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</div><div class="line">            <span class="comment">// read the controller epoch</span></div><div class="line">            <span class="keyword">val</span> leaderIsrAndEpoch = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topicAndPartition.topic,</div><div class="line">              topicAndPartition.partition).get</div><div class="line">            <span class="keyword">val</span> failMsg = (<span class="string">"encountered error while changing partition %s's state from New to Online since LeaderAndIsr path already "</span> +</div><div class="line">                           <span class="string">"exists with value %s and controller epoch %d"</span>)</div><div class="line">                             .format(topicAndPartition, leaderIsrAndEpoch.leaderAndIsr.toString(), leaderIsrAndEpoch.controllerEpoch)</div><div class="line">            stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>2.如果是OfflinePartition，OnlinePartition<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">electLeaderForPartition</span></span>(topic: <span class="type">String</span>, partition: <span class="type">Int</span>, leaderSelector: <span class="type">PartitionLeaderSelector</span>) &#123;</div><div class="line">    <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</div><div class="line">    <span class="comment">// handle leader election for the partitions whose leader is no longer alive</span></div><div class="line">    stateChangeLogger.trace(<span class="string">"Controller %d epoch %d started leader election for partition %s"</span></div><div class="line">                              .format(controllerId, controller.epoch, topicAndPartition))</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">var</span> zookeeperPathUpdateSucceeded: <span class="type">Boolean</span> = <span class="literal">false</span></div><div class="line">      <span class="keyword">var</span> newLeaderAndIsr: <span class="type">LeaderAndIsr</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">var</span> replicasForThisPartition: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">Seq</span>.empty[<span class="type">Int</span>]</div><div class="line">      <span class="keyword">while</span>(!zookeeperPathUpdateSucceeded) &#123;</div><div class="line">        <span class="keyword">val</span> currentLeaderIsrAndEpoch = getLeaderIsrAndEpochOrThrowException(topic, partition)</div><div class="line">        <span class="keyword">val</span> currentLeaderAndIsr = currentLeaderIsrAndEpoch.leaderAndIsr</div><div class="line">        <span class="keyword">val</span> controllerEpoch = currentLeaderIsrAndEpoch.controllerEpoch</div><div class="line">        <span class="keyword">if</span> (controllerEpoch &gt; controller.epoch) &#123;</div><div class="line">          <span class="keyword">val</span> failMsg = (<span class="string">"aborted leader election for partition [%s,%d] since the LeaderAndIsr path was "</span> +</div><div class="line">                         <span class="string">"already written by another controller. This probably means that the current controller %d went through "</span> +</div><div class="line">                         <span class="string">"a soft failure and another controller was elected with epoch %d."</span>)</div><div class="line">                           .format(topic, partition, controllerId, controllerEpoch)</div><div class="line">          stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// elect new leader or throw exception</span></div><div class="line">        <span class="keyword">val</span> (leaderAndIsr, replicas) = leaderSelector.selectLeader(topicAndPartition, currentLeaderAndIsr)</div><div class="line">        <span class="keyword">val</span> (updateSucceeded, newVersion) = <span class="type">ReplicationUtils</span>.updateLeaderAndIsr(zkUtils, topic, partition,</div><div class="line">          leaderAndIsr, controller.epoch, currentLeaderAndIsr.zkVersion)</div><div class="line">        <span class="comment">//根据不同的leaderSelector选举新的leader，这里一般调用的是OfflinePartitionLeaderSelector</span></div><div class="line">        newLeaderAndIsr = leaderAndIsr</div><div class="line">        newLeaderAndIsr.zkVersion = newVersion</div><div class="line">        <span class="comment">//将leader和isr持久化到zookeeper</span></div><div class="line">        zookeeperPathUpdateSucceeded = updateSucceeded</div><div class="line">        replicasForThisPartition = replicas</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> newLeaderIsrAndControllerEpoch = <span class="keyword">new</span> <span class="type">LeaderIsrAndControllerEpoch</span>(newLeaderAndIsr, controller.epoch)</div><div class="line">      <span class="comment">// update the leader cache</span></div><div class="line">      <span class="comment">//更新controllerContext中的partitionLeadershipInfo</span></div><div class="line">      controllerContext.partitionLeadershipInfo.put(<span class="type">TopicAndPartition</span>(topic, partition), newLeaderIsrAndControllerEpoch)</div><div class="line">      stateChangeLogger.trace(<span class="string">"Controller %d epoch %d elected leader %d for Offline partition %s"</span></div><div class="line">                                .format(controllerId, controller.epoch, newLeaderAndIsr.leader, topicAndPartition))</div><div class="line">      <span class="keyword">val</span> replicas = controllerContext.partitionReplicaAssignment(<span class="type">TopicAndPartition</span>(topic, partition))</div><div class="line">      <span class="comment">// store new leader and isr info in cache</span></div><div class="line">      <span class="comment">//封装发送给这些replica所在的broker的LeaderAndIsrRequest请求，交由ControllerBrokerRequestBatch(brokerRequestBatch)处理</span></div><div class="line">      brokerRequestBatch.addLeaderAndIsrRequestForBrokers(replicasForThisPartition, topic, partition,</div><div class="line">        newLeaderIsrAndControllerEpoch, replicas)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">LeaderElectionNotNeededException</span> =&gt; <span class="comment">// swallow</span></div><div class="line">      <span class="keyword">case</span> nroe: <span class="type">NoReplicaOnlineException</span> =&gt; <span class="keyword">throw</span> nroe</div><div class="line">      <span class="keyword">case</span> sce: <span class="type">Throwable</span> =&gt;</div><div class="line">        <span class="keyword">val</span> failMsg = <span class="string">"encountered error while electing leader for partition %s due to: %s."</span>.format(topicAndPartition, sce.getMessage)</div><div class="line">        stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg, sce)</div><div class="line">    &#125;</div><div class="line">    debug(<span class="string">"After leader election, leader cache is updated to %s"</span>.format(controllerContext.partitionLeadershipInfo.map(l =&gt; (l._1, l._2))))</div><div class="line">  &#125;</div><div class="line">···</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">在brokers/topics<span class="comment">/***(具体的topic名字)/目录下注册PartitionModificationsListener-&gt;AddPartitionsListener监听</span></div><div class="line">通过处理之前启动留下的partition重分配的情况</div><div class="line">处理之前启动留下的replica重新选举的情况</div><div class="line">向其它KafkaServer发送集群topic的元数据信息已进行数据的同步更新</div><div class="line">根据配置是否开启自动均衡</div><div class="line">开始删除topic</div><div class="line"></div><div class="line">### 10.start group coordinator</div><div class="line">```scala</div><div class="line">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</div><div class="line">groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM)</div><div class="line">groupCoordinator.startup()</div></pre></td></tr></table></figure></p>
<h3 id="11-Get-the-authorizer-and-initialize-it-if-one-is-specified"><a href="#11-Get-the-authorizer-and-initialize-it-if-one-is-specified" class="headerlink" title="11.Get the authorizer and initialize it if one is specified."></a>11.Get the authorizer and initialize it if one is specified.</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">authorizer = <span class="type">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</div><div class="line">          <span class="keyword">val</span> authZ = <span class="type">CoreUtils</span>.createObject[<span class="type">Authorizer</span>](authorizerClassName)</div><div class="line">          authZ.configure(config.originals())</div><div class="line">          authZ</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="12-start-processing-requests"><a href="#12-start-processing-requests" class="headerlink" title="12.start processing requests"></a>12.start processing requests</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</div><div class="line">  kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</div><div class="line">  clusterId, time)</div><div class="line">requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</div><div class="line">  config.numIoThreads)</div><div class="line"><span class="type">Mx4jLoader</span>.maybeLoad()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="13-start-dynamic-config-manager"><a href="#13-start-dynamic-config-manager" class="headerlink" title="13.start dynamic config manager"></a>13.start dynamic config manager</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> dynamicConfigHandlers = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ConfigHandler</span>](<span class="type">ConfigType</span>.<span class="type">Topic</span> -&gt; <span class="keyword">new</span> <span class="type">TopicConfigHandler</span>(logManager, config, quotaManagers),<span class="type">ConfigType</span>.<span class="type">Client</span> -&gt; <span class="keyword">new</span> <span class="type">ClientIdConfigHandler</span>(quotaManagers),<span class="type">ConfigType</span>.<span class="type">User</span> -&gt; <span class="keyword">new</span> <span class="type">UserConfigHandler</span>(quotaManagers, credentialProvider),<span class="type">ConfigType</span>.<span class="type">Broker</span> -&gt; <span class="keyword">new</span> <span class="type">BrokerConfigHandler</span>(config, quotaManagers))</div><div class="line"></div><div class="line"><span class="comment">// Create the config manager. start listening to notifications</span></div><div class="line">dynamicConfigManager = <span class="keyword">new</span> <span class="type">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</div><div class="line">dynamicConfigManager.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="14-tell-everyone-we-are-alive"><a href="#14-tell-everyone-we-are-alive" class="headerlink" title="14.tell everyone we are alive"></a>14.tell everyone we are alive</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> listeners = config.advertisedListeners.map &#123; endpoint =&gt;</div><div class="line">          <span class="keyword">if</span> (endpoint.port == <span class="number">0</span>)</div><div class="line">            endpoint.copy(port = socketServer.boundPort(endpoint.listenerName))</div><div class="line">          <span class="keyword">else</span></div><div class="line">            endpoint</div><div class="line">&#125;</div><div class="line">kafkaHealthcheck = <span class="keyword">new</span> <span class="type">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</div><div class="line">  config.interBrokerProtocolVersion)</div><div class="line">kafkaHealthcheck.startup()</div><div class="line"></div><div class="line"><span class="comment">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></div><div class="line">checkpointBrokerId(config.brokerId)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>kafkaHealthcheck.startup()<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">   zkUtils.zkClient.subscribeStateChanges(sessionExpireListener)</div><div class="line">   register()</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="comment">/**</span></div><div class="line">  * Register this broker as "alive" in zookeeper</div><div class="line">  */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">register</span></span>() &#123;</div><div class="line">   <span class="keyword">val</span> jmxPort = <span class="type">System</span>.getProperty(<span class="string">"com.sun.management.jmxremote.port"</span>, <span class="string">"-1"</span>).toInt</div><div class="line">   <span class="keyword">val</span> updatedEndpoints = advertisedEndpoints.map(endpoint =&gt;</div><div class="line">     <span class="keyword">if</span> (endpoint.host == <span class="literal">null</span> || endpoint.host.trim.isEmpty)</div><div class="line">       endpoint.copy(host = <span class="type">InetAddress</span>.getLocalHost.getCanonicalHostName)</div><div class="line">     <span class="keyword">else</span></div><div class="line">       endpoint</div><div class="line">   )</div><div class="line"></div><div class="line">   <span class="comment">// the default host and port are here for compatibility with older clients that only support PLAINTEXT</span></div><div class="line">   <span class="comment">// we choose the first plaintext port, if there is one</span></div><div class="line">   <span class="comment">// or we register an empty endpoint, which means that older clients will not be able to connect</span></div><div class="line">   <span class="keyword">val</span> plaintextEndpoint = updatedEndpoints.find(_.securityProtocol == <span class="type">SecurityProtocol</span>.<span class="type">PLAINTEXT</span>).getOrElse(</div><div class="line">     <span class="keyword">new</span> <span class="type">EndPoint</span>(<span class="literal">null</span>, <span class="number">-1</span>, <span class="literal">null</span>, <span class="literal">null</span>))</div><div class="line">   zkUtils.registerBrokerInZk(brokerId, plaintextEndpoint.host, plaintextEndpoint.port, updatedEndpoints, jmxPort, rack,</div><div class="line">     interBrokerProtocolVersion)</div><div class="line"> &#125;</div></pre></td></tr></table></figure></p>
<p>注册新的brokerid</p>
<h3 id="15-register-broker-metrics"><a href="#15-register-broker-metrics" class="headerlink" title="15.register broker metrics."></a>15.register broker metrics.</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> registerStats()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;导入IDEA即可看kafka源码：&quot;&gt;&lt;a href=&quot;#导入IDEA即可看kafka源码：&quot; class=&quot;headerlink&quot; title=&quot;导入IDEA即可看kafka源码：&quot;&gt;&lt;/a&gt;导入IDEA即可看kafka源码：&lt;/h2&gt;&lt;h2 id=&quot;启动之前需要安装zookeeper&quot;&gt;&lt;a href=&quot;#启动之前需要安装zookeeper&quot; class=&quot;headerlink&quot; title=&quot;启动之前需要安装zookeeper&quot;&gt;&lt;/a&gt;启动之前需要安装zookeeper&lt;/h2&gt;&lt;p&gt;地址：&lt;br&gt;&lt;a href=&quot;http://apache.fayea.com/zookeeper/&quot;&gt;http://apache.fayea.com/zookeeper/&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz&quot;&gt;http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;解压后再当前目录增加&quot;&gt;&lt;a href=&quot;#解压后再当前目录增加&quot; class=&quot;headerlink&quot; title=&quot;解压后再当前目录增加&quot;&gt;&lt;/a&gt;解压后再当前目录增加&lt;/h2&gt;&lt;p&gt;dataLogDir和data目录&lt;br&gt;复制一份配置文件&lt;br&gt;改名为zoo.cfg&lt;br&gt;修改配置文件：&lt;br&gt;zoo.cfg&lt;br&gt;修改并增加&lt;br&gt;dataDir=D:\tool\zookeeper-3.4.6\data&lt;br&gt;dataLogDir=D:\tool\zookeeper-3.4.6\dataLogDir&lt;/p&gt;
&lt;h2 id=&quot;启动zkServer-cmd&quot;&gt;&lt;a href=&quot;#启动zkServer-cmd&quot; class=&quot;headerlink&quot; title=&quot;启动zkServer.cmd&quot;&gt;&lt;/a&gt;启动zkServer.cmd&lt;/h2&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://yoursite.com/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka源码阅读</title>
    <link href="http://yoursite.com/2017/05/02/kafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2017/05/02/kafka源码阅读/</id>
    <published>2017-05-02T03:30:00.000Z</published>
    <updated>2017-05-18T00:58:23.538Z</updated>
    
    <content type="html"><![CDATA[<h3 id="下载kafka的源码包："><a href="#下载kafka的源码包：" class="headerlink" title="下载kafka的源码包："></a>下载kafka的源码包：</h3><p><a href="http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz" target="_blank" rel="external">http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz</a></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>gradle </p>
<p>1.官网下载<br><a href="https://gradle.org/releases" target="_blank" rel="external">https://gradle.org/releases</a><br>选择当时最新版本：<br><a href="https://downloads.gradle.org/distributions/gradle-3.5-all.zip" target="_blank" rel="external">https://downloads.gradle.org/distributions/gradle-3.5-all.zip</a></p>
<p>2.解压<br>解压到D:\tool\gradle-3.5</p>
<p>3.配置环境变量<br>GRADLE_HOME<br>D:\tool\gradle-3.5</p>
<p>Path追加;%GRADLE_HOME%\BIN;<br><a id="more"></a><br>4.测试：<br>gradle -v<br><img src="http://oh6ybr0jg.bkt.clouddn.com/gradle.jpg" alt="此处输入图片的描述"></p>
<p>安装参考:<br><a href="http://blog.csdn.net/lizhitao/article/details/26875463" target="_blank" rel="external">http://blog.csdn.net/lizhitao/article/details/26875463</a></p>
<h3 id="编译：（此处博主用某云主机编译的，很快）"><a href="#编译：（此处博主用某云主机编译的，很快）" class="headerlink" title="编译：（此处博主用某云主机编译的，很快）"></a>编译：（此处博主用某云主机编译的，很快）</h3><p>gradle idea</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170427134226.jpg" alt="此处输入图片的描述"></p>
<p>（编译完把/root/.gradle/caches/modules-2下下载的文件放到我们的环境中C:\Users\Administrator.gradle\caches\modules-2 再windows上编译 ）<br><img src="http://oh6ybr0jg.bkt.clouddn.com/windows%E7%BC%96%E8%AF%91kafka.jpg" alt="此处输入图片的描述"></p>
<p>用IDEA工具打开<br><img src="http://oh6ybr0jg.bkt.clouddn.com/kafka_src_emv.jpg" alt="此处输入图片的描述"></p>
<h3 id="目录介绍："><a href="#目录介绍：" class="headerlink" title="目录介绍："></a>目录介绍：</h3><table>
<thead>
<tr>
<th>模块名</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin</td>
<td>管理员模块，操作和管理topic，paritions相关，包含create/delete topic,扩展patitions</td>
</tr>
<tr>
<td>api</td>
<td>主要负责交互数据的组装，客户端与服务端交互数据编解码</td>
</tr>
<tr>
<td>client</td>
<td>Producer读取kafka broker元数据信息，topic和partitions，以及leader</td>
</tr>
<tr>
<td>cluster</td>
<td>包含几个实体类，Broker,Cluster,Partition,Replica,解释他们之间关系：          Cluster由多个broker组成，一个Broker包含多个partition，一个topic的所有partitions分布在不同broker的中，一个Replica包含多个Partition。</td>
</tr>
<tr>
<td>common</td>
<td>异常类和错误验证</td>
</tr>
<tr>
<td>consumer</td>
<td>负责所有客户端消费者数据和逻辑处理</td>
</tr>
<tr>
<td>controller</td>
<td>负责中央控制器选举，partition的leader选举，副本分配，副本重新分配，partition和replica扩容。</td>
</tr>
<tr>
<td>coordinator</td>
<td>partition分配机制</td>
</tr>
<tr>
<td>javaapi</td>
<td>提供java的producer和consumer接口api</td>
</tr>
<tr>
<td>log</td>
<td>Kafka文件存储模块，负责读写所有kafka的topic消息数据。</td>
</tr>
<tr>
<td>message</td>
<td>封装多个消息组成一个“消息集”或压缩消息集。</td>
</tr>
<tr>
<td>metrics</td>
<td>内部状态的监控模块</td>
</tr>
<tr>
<td>network</td>
<td>网络事件处理模块，负责处理和接收客户端连接</td>
</tr>
<tr>
<td>producer</td>
<td>producer实现模块，包括同步和异步发送消息。</td>
</tr>
<tr>
<td>security</td>
<td>安全</td>
</tr>
<tr>
<td>serializer</td>
<td>序列化或反序列化当前消息</td>
</tr>
<tr>
<td>server</td>
<td></td>
</tr>
<tr>
<td>tools</td>
<td>工具模块，    包含a.导出对应consumer的offset值.b.导出LogSegments信息，当前topic的log写的位置信息.c.导出zk上所有consumer的offset值.d.修改注册在zk的consumer的offset值.f.producer和consumer的使用例子.</td>
</tr>
<tr>
<td>utils</td>
<td>Json工具类，Zkutils工具类，Utils创建线程工具类，KafkaScheduler公共调度器类，公共日志类等等。</td>
</tr>
</tbody>
</table>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;下载kafka的源码包：&quot;&gt;&lt;a href=&quot;#下载kafka的源码包：&quot; class=&quot;headerlink&quot; title=&quot;下载kafka的源码包：&quot;&gt;&lt;/a&gt;下载kafka的源码包：&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz&quot;&gt;http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h3&gt;&lt;p&gt;gradle &lt;/p&gt;
&lt;p&gt;1.官网下载&lt;br&gt;&lt;a href=&quot;https://gradle.org/releases&quot;&gt;https://gradle.org/releases&lt;/a&gt;&lt;br&gt;选择当时最新版本：&lt;br&gt;&lt;a href=&quot;https://downloads.gradle.org/distributions/gradle-3.5-all.zip&quot;&gt;https://downloads.gradle.org/distributions/gradle-3.5-all.zip&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2.解压&lt;br&gt;解压到D:\tool\gradle-3.5&lt;/p&gt;
&lt;p&gt;3.配置环境变量&lt;br&gt;GRADLE_HOME&lt;br&gt;D:\tool\gradle-3.5&lt;/p&gt;
&lt;p&gt;Path追加;%GRADLE_HOME%\BIN;&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://yoursite.com/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>CDH phoenix安装</title>
    <link href="http://yoursite.com/2017/04/18/CDH%20phoenix%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/04/18/CDH phoenix安装/</id>
    <published>2017-04-18T03:30:00.000Z</published>
    <updated>2017-12-08T11:46:06.162Z</updated>
    
    <content type="html"><![CDATA[<h5 id="首先下载jdk-略"><a href="#首先下载jdk-略" class="headerlink" title="首先下载jdk 略"></a>首先下载jdk 略</h5><p>安装maven</p>
<h2 id="下载yum源"><a href="#下载yum源" class="headerlink" title="下载yum源"></a>下载yum源</h2><p>wget <a href="http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo" target="_blank" rel="external">http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo</a> -O /etc/yum.repos.d/epel-apache-maven.repo</p>
<h2 id="安装maven："><a href="#安装maven：" class="headerlink" title="安装maven："></a>安装maven：</h2><p>yum -y install apache-maven</p>
<a id="more"></a>
<h2 id="编译phoenix"><a href="#编译phoenix" class="headerlink" title="编译phoenix"></a>编译phoenix</h2><p>找到最新版本的phoenix<br><a href="https://github.com/chiastic-security/phoenix-for-cloudera/tree/4.8-HBase-1.2-cdh5.8" target="_blank" rel="external">https://github.com/chiastic-security/phoenix-for-cloudera/tree/4.8-HBase-1.2-cdh5.8</a></p>
<h2 id="下载（博主下载到-soft下）"><a href="#下载（博主下载到-soft下）" class="headerlink" title="下载（博主下载到/soft下）"></a>下载（博主下载到/soft下）</h2><p>并编译<br>mvn clean package -DskipTests -Dcdh.flume.version=1.6.0</p>
<h2 id="打开路径："><a href="#打开路径：" class="headerlink" title="打开路径："></a>打开路径：</h2><p>/soft/phoenix-for-cloudera-4.8-HBase-1.2-cdh5.8/phoenix-assembly/target</p>
<p>找到phoenix-4.8.0-cdh5.8.0.tar.gz<br>将phoenix-4.8.0-cdh5.8.0中的phoenix-4.8.0-cdh5.8.0-server.jar拷贝到每一个RegionServer下/opt/cloudera/parcels/CDH/lib/hbase/lib</p>
<h2 id="启动："><a href="#启动：" class="headerlink" title="启动："></a>启动：</h2><p>./sqlline.py slave1:2181/hbase</p>
<h3 id="如果报错："><a href="#如果报错：" class="headerlink" title="如果报错："></a>如果报错：</h3><p>Error: org.apache.hadoop.hbase.DoNotRetryIOException: Class org.apache.phoenix.coprocessor.MetaDataEndpointImpl cannot be loaded Set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks<br>        at org.apache.hadoop.hbase.master.HMaster.warnOrThrowExceptionForFailure(HMaster.java:1741)<br>        at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1602)<br>        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1531)<br>        at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:469)<br>        at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55682)<br>        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170)<br>        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)<br>        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:185)<br>        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:165) (state=08000,code=101)</p>
<p>需要点击<br>CDH-&gt;hbase -&gt;配置-&gt;高级<br>hbase-site.xml 的 HBase 服务高级配置代码段（安全阀）<br>添加<br>hbase.table.sanity.checks<br>false</p>
<h2 id="重启即可"><a href="#重启即可" class="headerlink" title="重启即可"></a>重启即可</h2><h2 id="如果想要安装phonenix"><a href="#如果想要安装phonenix" class="headerlink" title="如果想要安装phonenix"></a>如果想要安装phonenix</h2><p>先下载<br><a href="http://squirrel-sql.sourceforge.net/#installation" target="_blank" rel="external">http://squirrel-sql.sourceforge.net/#installation</a></p>
<h2 id="安装（安装的时候勾选imort-data和mysql）"><a href="#安装（安装的时候勾选imort-data和mysql）" class="headerlink" title="安装（安装的时候勾选imort-data和mysql）"></a>安装（安装的时候勾选imort-data和mysql）</h2><p>注：下载后直接安装jar包即可，不要解压缩<br>由于是CDH 我拷贝了如下jar防盗lib下面<br>phoenix-core-4.8.0-cdh5.8.0.jar<br>phoenix-4.8.0-cdh5.8.0-client.jar<br>phoenix-pherf-4.8.0-cdh5.8.0-minimal.jar</p>
<h2 id="链接参考"><a href="#链接参考" class="headerlink" title="链接参考;"></a>链接参考;</h2><p><a href="http://www.cnblogs.com/raphael5200/p/5260198.html" target="_blank" rel="external">http://www.cnblogs.com/raphael5200/p/5260198.html</a><br>有图</p>
<h2 id="phoenix-API"><a href="#phoenix-API" class="headerlink" title="phoenix API"></a>phoenix API</h2><p>　　<a href="http://phoenix.apache.org/language/functions.html" target="_blank" rel="external">http://phoenix.apache.org/language/functions.html</a><br>　　<a href="http://phoenix.apache.org/language/index.html" target="_blank" rel="external">http://phoenix.apache.org/language/index.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;首先下载jdk-略&quot;&gt;&lt;a href=&quot;#首先下载jdk-略&quot; class=&quot;headerlink&quot; title=&quot;首先下载jdk 略&quot;&gt;&lt;/a&gt;首先下载jdk 略&lt;/h5&gt;&lt;p&gt;安装maven&lt;/p&gt;
&lt;h2 id=&quot;下载yum源&quot;&gt;&lt;a href=&quot;#下载yum源&quot; class=&quot;headerlink&quot; title=&quot;下载yum源&quot;&gt;&lt;/a&gt;下载yum源&lt;/h2&gt;&lt;p&gt;wget &lt;a href=&quot;http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo&quot;&gt;http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo&lt;/a&gt; -O /etc/yum.repos.d/epel-apache-maven.repo&lt;/p&gt;
&lt;h2 id=&quot;安装maven：&quot;&gt;&lt;a href=&quot;#安装maven：&quot; class=&quot;headerlink&quot; title=&quot;安装maven：&quot;&gt;&lt;/a&gt;安装maven：&lt;/h2&gt;&lt;p&gt;yum -y install apache-maven&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH-spark</title>
    <link href="http://yoursite.com/2017/03/29/CDH-spark/"/>
    <id>http://yoursite.com/2017/03/29/CDH-spark/</id>
    <published>2017-03-29T03:30:00.000Z</published>
    <updated>2017-03-31T13:27:02.808Z</updated>
    
    <content type="html"><![CDATA[<p>添加服务选择spark on yarn<br>安装的时候会选择history server，博主选择的是slave1，后续可以通过这个地址看spark的UI</p>
<p>博主安装的是：<br>scala     version 2.10.5（后续再UI上看到的）<br>spark   version 1.6.0（执行spark-shell时看到的）</p>
<p>安装完后测试：</p>
<p>su hdfs<br>spark-submit –class org.apache.spark.examples.SparkPi –executor-memory 1G –total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100</p>
<p>查看UI：<br><a href="http://slave1:18088/" target="_blank" rel="external">http://slave1:18088/</a></p>
<a id="more"></a>
<p>编写代码测试离线功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</div><div class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</div><div class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</div><div class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</div><div class="line"></div><div class="line">    &lt;groupId&gt;cn.zwjf.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;SpakrZwujf&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</div><div class="line"></div><div class="line"></div><div class="line">    &lt;properties&gt;</div><div class="line">        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;</div><div class="line">        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;</div><div class="line">        &lt;encoding&gt;UTF-8&lt;/encoding&gt;</div><div class="line">        &lt;scala.version&gt;2.10.5&lt;/scala.version&gt;</div><div class="line">        &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt;</div><div class="line">    &lt;/properties&gt;</div><div class="line"></div><div class="line">    &lt;dependencies&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;1.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;1.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">    &lt;/dependencies&gt;</div><div class="line"></div><div class="line">    &lt;build&gt;</div><div class="line">        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;</div><div class="line">        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;</div><div class="line">        &lt;plugins&gt;</div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;3.2.0&lt;/version&gt;</div><div class="line">                &lt;executions&gt;</div><div class="line">                    &lt;execution&gt;</div><div class="line">                        &lt;goals&gt;</div><div class="line">                            &lt;goal&gt;compile&lt;/goal&gt;</div><div class="line">                            &lt;goal&gt;testCompile&lt;/goal&gt;</div><div class="line">                        &lt;/goals&gt;</div><div class="line">                        &lt;configuration&gt;</div><div class="line">                            &lt;args&gt;</div><div class="line">                                &lt;arg&gt;-make:transitive&lt;/arg&gt;</div><div class="line">                                &lt;arg&gt;-dependencyfile&lt;/arg&gt;</div><div class="line">                                &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt;</div><div class="line">                            &lt;/args&gt;</div><div class="line">                        &lt;/configuration&gt;</div><div class="line">                    &lt;/execution&gt;</div><div class="line">                &lt;/executions&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;2.18.1&lt;/version&gt;</div><div class="line">                &lt;configuration&gt;</div><div class="line">                    &lt;useFile&gt;false&lt;/useFile&gt;</div><div class="line">                    &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt;</div><div class="line">                    &lt;includes&gt;</div><div class="line">                        &lt;include&gt;**/*Test.*&lt;/include&gt;</div><div class="line">                        &lt;include&gt;**/*Suite.*&lt;/include&gt;</div><div class="line">                    &lt;/includes&gt;</div><div class="line">                &lt;/configuration&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line"></div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;2.3&lt;/version&gt;</div><div class="line">                &lt;executions&gt;</div><div class="line">                    &lt;execution&gt;</div><div class="line">                        &lt;phase&gt;package&lt;/phase&gt;</div><div class="line">                        &lt;goals&gt;</div><div class="line">                            &lt;goal&gt;shade&lt;/goal&gt;</div><div class="line">                        &lt;/goals&gt;</div><div class="line">                        &lt;configuration&gt;</div><div class="line">                            &lt;filters&gt;</div><div class="line">                                &lt;filter&gt;</div><div class="line">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</div><div class="line">                                    &lt;excludes&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</div><div class="line">                                    &lt;/excludes&gt;</div><div class="line">                                &lt;/filter&gt;</div><div class="line">                            &lt;/filters&gt;</div><div class="line">                            &lt;transformers&gt;</div><div class="line">                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</div><div class="line">                                    &lt;mainClass&gt;cn.zwjf.uidPhone&lt;/mainClass&gt;</div><div class="line">                                &lt;/transformer&gt;</div><div class="line">                            &lt;/transformers&gt;</div><div class="line">                        &lt;/configuration&gt;</div><div class="line">                    &lt;/execution&gt;</div><div class="line">                &lt;/executions&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line">        &lt;/plugins&gt;</div><div class="line">    &lt;/build&gt;</div><div class="line">&lt;/project&gt;</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.zwjf</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by Administrator on 2017/3/29.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">uidPhone</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"uid_phone_etl"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    sc.textFile(args(<span class="number">0</span>)).map(line =&gt; &#123;</div><div class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</div><div class="line">      <span class="keyword">if</span>(arr.length &gt; <span class="number">1</span> &amp;&amp; !arr(<span class="number">1</span>).equals(<span class="string">"null"</span>))&#123;</div><div class="line">        arr(<span class="number">0</span>)+<span class="string">","</span>+arr(<span class="number">1</span>)</div><div class="line">      &#125;<span class="keyword">else</span>&#123;</div><div class="line">        <span class="string">"null"</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">    &#125;).filter(!_.equals(<span class="string">"null"</span>)).saveAsTextFile(args(<span class="number">1</span>))</div><div class="line">    sc.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>打包提交到服务器</p>
<p>spark-submit \<br>–class cn.zwjf.uidPhone –executor-memory 2G –total-executor-cores 4 \<br>/var/lib/hadoop-hdfs/data/spark/SpakrZwujf-1.0-SNAPSHOT.jar \<br>/bigdata/data/hdfs/Baidu/uid_phone/* \<br>/bigdata/data/hdfs/Baidu/uid_phone3</p>
<p>/bigdata/data/hdfs/Baidu/uid_phone/* /bigdata/data/hdfs/Baidu/uid_phone2</p>
<p>本地调试方法：、<br><a href="http://www.jianshu.com/p/c801761ce088" target="_blank" rel="external">http://www.jianshu.com/p/c801761ce088</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;添加服务选择spark on yarn&lt;br&gt;安装的时候会选择history server，博主选择的是slave1，后续可以通过这个地址看spark的UI&lt;/p&gt;
&lt;p&gt;博主安装的是：&lt;br&gt;scala     version 2.10.5（后续再UI上看到的）&lt;br&gt;spark   version 1.6.0（执行spark-shell时看到的）&lt;/p&gt;
&lt;p&gt;安装完后测试：&lt;/p&gt;
&lt;p&gt;su hdfs&lt;br&gt;spark-submit –class org.apache.spark.examples.SparkPi –executor-memory 1G –total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100&lt;/p&gt;
&lt;p&gt;查看UI：&lt;br&gt;&lt;a href=&quot;http://slave1:18088/&quot;&gt;http://slave1:18088/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>strom 监控</title>
    <link href="http://yoursite.com/2017/03/26/strom-%E7%9B%91%E6%8E%A7/"/>
    <id>http://yoursite.com/2017/03/26/strom-监控/</id>
    <published>2017-03-26T01:30:00.000Z</published>
    <updated>2017-03-31T13:27:13.752Z</updated>
    
    <content type="html"><![CDATA[<p>看当前GC的情况<br>jstat -gcutil 端口号 1000</p>
<p>看wait的是否过高<br>top -p 端口号</p>
<p>把某个端口当前的堆栈信息dump到某个文件<br>jstack 端口号 &gt; 文件</p>
<p>看磁盘<br>iostat -x 1</p>
<p>如果发现分配不均匀，例如FiledGrouping userid=1特别多，导致下游nolt或execute搞死了，重新启动也很快就死掉。可以对userid 做hash或md5或者组成更多条件，让下游Bolt分布更均匀</p>
<p>complete latency(这里主要针对spout)是一个tuple从发出, 到经过bolt处理完成, 最终调用spout的ack这个完整的过程所花的时间.complete latency和吞吐量可以认为是互斥的, 二者不可兼得, 要提高吞吐量, 必然会增加complete latency, 反之亦然. 所以我们需要根据具体的场景, 在二者之间找到一个平衡.<br>complete latency受到两个因素的影响:<br>bolt的处理时间<br>spout的parallelism数量<br><a id="more"></a></p>
<p>官网对并行度等的设置方式：<br>storm.apache.org/releases/1.0.3/Understanding-the-parallelism-of-a-Storm-topology.html</p>
<p>如果executors总是挂就需要关注</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看当前GC的情况&lt;br&gt;jstat -gcutil 端口号 1000&lt;/p&gt;
&lt;p&gt;看wait的是否过高&lt;br&gt;top -p 端口号&lt;/p&gt;
&lt;p&gt;把某个端口当前的堆栈信息dump到某个文件&lt;br&gt;jstack 端口号 &amp;gt; 文件&lt;/p&gt;
&lt;p&gt;看磁盘&lt;br&gt;iostat -x 1&lt;/p&gt;
&lt;p&gt;如果发现分配不均匀，例如FiledGrouping userid=1特别多，导致下游nolt或execute搞死了，重新启动也很快就死掉。可以对userid 做hash或md5或者组成更多条件，让下游Bolt分布更均匀&lt;/p&gt;
&lt;p&gt;complete latency(这里主要针对spout)是一个tuple从发出, 到经过bolt处理完成, 最终调用spout的ack这个完整的过程所花的时间.complete latency和吞吐量可以认为是互斥的, 二者不可兼得, 要提高吞吐量, 必然会增加complete latency, 反之亦然. 所以我们需要根据具体的场景, 在二者之间找到一个平衡.&lt;br&gt;complete latency受到两个因素的影响:&lt;br&gt;bolt的处理时间&lt;br&gt;spout的parallelism数量&lt;br&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>strom-Metrics</title>
    <link href="http://yoursite.com/2017/03/25/strom-Metrics/"/>
    <id>http://yoursite.com/2017/03/25/strom-Metrics/</id>
    <published>2017-03-24T16:30:00.000Z</published>
    <updated>2017-03-31T13:27:19.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。"><a href="#Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。" class="headerlink" title="Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。"></a>Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。</h2><p>使用 Metrics 只需要实现一个接口方法：getValueAndReset，在方法中可以查找汇总值、并将该值复位为初始值。例如，在 MeanReducer 中就实现了通过运行总数除以对应的运行计数的方式来求取均值，然后将两个值都重新设置为 0。</p>
<p>Storm 提供了以下几种 metric 类型：</p>
<h4 id="AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。"><a href="#AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。" class="headerlink" title="AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。"></a>AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。</h4><h4 id="CombinedMetric-–-可以对-metric-进行关联更新的通用接口。"><a href="#CombinedMetric-–-可以对-metric-进行关联更新的通用接口。" class="headerlink" title="CombinedMetric – 可以对 metric 进行关联更新的通用接口。"></a>CombinedMetric – 可以对 metric 进行关联更新的通用接口。</h4><h4 id="CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。"><a href="#CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。" class="headerlink" title="CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。"></a>CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。</h4><h4 id="MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap"><a href="#MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap" class="headerlink" title="MultiCountMetric – 返回包含一组 CountMetric 的 HashMap"></a>MultiCountMetric – 返回包含一组 CountMetric 的 HashMap</h4><h4 id="ReducedMetric"><a href="#ReducedMetric" class="headerlink" title="ReducedMetric"></a>ReducedMetric</h4><h6 id="MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。"><a href="#MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。" class="headerlink" title="MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。"></a>MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。</h6><h6 id="MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap"><a href="#MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap" class="headerlink" title="MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap"></a>MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap</h6><a id="more"></a>
<p>自定义Metric：<br>代码注册：<br>conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);<br>或者修改配置文件<br>topology.metrics.consumer.register:</p>
<ul>
<li>class: “org.apache.storm.metric.LoggingMetricsConsumer”<br>parallelism.hint: 1</li>
<li>class: “org.apache.storm.metric.HttpForwardingMetricsConsumer”<br>parallelism.hint: 1<br>argument: “<a href="http://example.com:8080/metrics/my-topology/" target="_blank" rel="external">http://example.com:8080/metrics/my-topology/</a>“</li>
</ul>
<p>构建自己的Metric<br>定义不可被序列号类型transient<br>private transient CountMetric countMetric;</p>
<p>重写prepare<br>@Override<br>public void prepare(Map conf, TopologyContext context, OutputCollector collector) {<br>    // other intialization here.<br>    countMetric = new CountMetric();<br>    context.registerMetric(“execute_count”, countMetric, 60);<br>}</p>
<p>bolt的execute</p>
<p>public void execute(Tuple input) {<br>    countMetric.incr();<br>    // handle tuple here.<br>}</p>
<p>builtin_metrics.clj 为内部的 metrics 设置了数据结构，以及其他框架组件可以用于更新的虚拟方法。metrics 本身是在回调代码中实现计算的 – 请参考 clj/b/s/daemon/daemon/executor.clj 中的 ack-spout-msg 的例子。</p>
<p>LoggingMetricsConsumer,统计指标值将输出到metric.log日志文件中。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。&quot;&gt;&lt;a href=&quot;#Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。&quot; class=&quot;headerlink&quot; title=&quot;Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。&quot;&gt;&lt;/a&gt;Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。&lt;/h2&gt;&lt;p&gt;使用 Metrics 只需要实现一个接口方法：getValueAndReset，在方法中可以查找汇总值、并将该值复位为初始值。例如，在 MeanReducer 中就实现了通过运行总数除以对应的运行计数的方式来求取均值，然后将两个值都重新设置为 0。&lt;/p&gt;
&lt;p&gt;Storm 提供了以下几种 metric 类型：&lt;/p&gt;
&lt;h4 id=&quot;AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。&quot;&gt;&lt;a href=&quot;#AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。&quot; class=&quot;headerlink&quot; title=&quot;AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。&quot;&gt;&lt;/a&gt;AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。&lt;/h4&gt;&lt;h4 id=&quot;CombinedMetric-–-可以对-metric-进行关联更新的通用接口。&quot;&gt;&lt;a href=&quot;#CombinedMetric-–-可以对-metric-进行关联更新的通用接口。&quot; class=&quot;headerlink&quot; title=&quot;CombinedMetric – 可以对 metric 进行关联更新的通用接口。&quot;&gt;&lt;/a&gt;CombinedMetric – 可以对 metric 进行关联更新的通用接口。&lt;/h4&gt;&lt;h4 id=&quot;CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。&quot;&gt;&lt;a href=&quot;#CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。&quot; class=&quot;headerlink&quot; title=&quot;CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。&quot;&gt;&lt;/a&gt;CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。&lt;/h4&gt;&lt;h4 id=&quot;MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap&quot;&gt;&lt;a href=&quot;#MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap&quot; class=&quot;headerlink&quot; title=&quot;MultiCountMetric – 返回包含一组 CountMetric 的 HashMap&quot;&gt;&lt;/a&gt;MultiCountMetric – 返回包含一组 CountMetric 的 HashMap&lt;/h4&gt;&lt;h4 id=&quot;ReducedMetric&quot;&gt;&lt;a href=&quot;#ReducedMetric&quot; class=&quot;headerlink&quot; title=&quot;ReducedMetric&quot;&gt;&lt;/a&gt;ReducedMetric&lt;/h4&gt;&lt;h6 id=&quot;MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。&quot;&gt;&lt;a href=&quot;#MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。&quot; class=&quot;headerlink&quot; title=&quot;MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。&quot;&gt;&lt;/a&gt;MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。&lt;/h6&gt;&lt;h6 id=&quot;MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap&quot;&gt;&lt;a href=&quot;#MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap&quot; class=&quot;headerlink&quot; title=&quot;MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap&quot;&gt;&lt;/a&gt;MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap&lt;/h6&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>bash脚本实战</title>
    <link href="http://yoursite.com/2017/03/19/bash%E8%84%9A%E6%9C%AC%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/19/bash脚本实战/</id>
    <published>2017-03-19T15:30:00.000Z</published>
    <updated>2017-08-12T01:05:48.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="练习题目"><a href="#练习题目" class="headerlink" title="练习题目"></a>练习题目</h2><p>写一个脚本getinterface.sh，脚本可以接受参数(i,I,a)，完成以下任务：<br>   (1)使用以下形式：getinterface.sh [-i interface|-I IP|-a]<br>   (2)当用户使用-i选项时，显示其指定网卡的IP地址；<br>   (3)当用户使用-I选项时，显示其后面的IP地址所属的网络接口；（如 192.168.199.183：eth0）<br>   (4)当用户单独使用-a选项时，显示所有网络接口及其IP地址（lo除外）</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>参数1为param1 参数2位param2<br>（1）利用CAT &lt;&lt; EOF  <em>*</em> EOF打印信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cat &lt;&lt; EOF</div><div class="line">    getinterface.sh [-i interface|-I IP|<span class="_">-a</span>]</div><div class="line">    -i interface) show ip of the interface</div><div class="line">    -I IP) show interface of the IP and IP with :;</div><div class="line">    <span class="_">-a</span>) list all interfaces and their IPs except lo;</div><div class="line">    *) quit</div><div class="line">=================================================================</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>（2）校验接口是否存在（参数2）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ifconfig <span class="variable">$interface</span> &gt;/dev/null </div><div class="line">$? <span class="_">-ne</span> 1  </div><div class="line"><span class="comment">#如果存在打印网卡的IP</span></div><div class="line">ifconfig <span class="variable">$parma2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div></pre></td></tr></table></figure></p>
<p>（3）先打印所有接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;</span></div></pre></td></tr></table></figure></p>
<p>并存在一个变量中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">list=`netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;'</span>`</div></pre></td></tr></table></figure>
<p>遍历集合，查看和输入IP相同的打印IP和接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> inter <span class="keyword">in</span> <span class="variable">$list</span>;  </div><div class="line"><span class="keyword">do</span>  </div><div class="line">    ip_temp=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$IP</span> == <span class="variable">$ip_temp</span> ];<span class="keyword">then</span>  </div><div class="line">	<span class="built_in">echo</span> <span class="string">"<span class="variable">$IP</span> : <span class="variable">$inter</span>"</span>  </div><div class="line">	<span class="built_in">exit</span> 0  </div><div class="line">    <span class="keyword">fi</span>  </div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<p>（4）遍历集合，过滤接口为lo的，打印IP和接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> inter <span class="keyword">in</span> <span class="variable">$list</span>;  </div><div class="line"><span class="keyword">do</span>  </div><div class="line">    ip_temp=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$inter</span> != <span class="string">"lo"</span> ];<span class="keyword">then</span>  </div><div class="line">	<span class="built_in">echo</span> <span class="string">"<span class="variable">$IP</span> : <span class="variable">$inter</span>"</span>  </div><div class="line">	<span class="built_in">exit</span> 0  </div><div class="line">    <span class="keyword">fi</span>  </div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<h2 id="最终的脚本如下："><a href="#最终的脚本如下：" class="headerlink" title="最终的脚本如下："></a>最终的脚本如下：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">cat &lt;&lt; EOF</div><div class="line">    getinterface.sh [-i interface|-I IP|<span class="_">-a</span>]</div><div class="line">    -i interface) show ip of the interface</div><div class="line">    -I IP) show interface of the IP and IP with :;</div><div class="line">    <span class="_">-a</span>) list all interfaces and their IPs except lo;</div><div class="line">    *) quit</div><div class="line">=================================================================</div><div class="line">EOF</div><div class="line"></div><div class="line"><span class="built_in">read</span> -p <span class="string">"please choice "</span> param1 param2</div><div class="line">list=`netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;'</span>`</div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-i'</span> ]]; <span class="keyword">then</span></div><div class="line">    ifconfig <span class="variable">$interface</span> &gt;/dev/null</div><div class="line">    flag=$?</div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$flag</span> <span class="_">-ne</span> 1 ];<span class="keyword">then</span></div><div class="line">        ip=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>`</div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$param2</span> <span class="variable">$ip</span>"</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$parma2</span> is not exist"</span> </div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-I'</span> ]]; <span class="keyword">then</span></div><div class="line"><span class="keyword">for</span> lt <span class="keyword">in</span> <span class="variable">$list</span>;</div><div class="line">  <span class="keyword">do</span></div><div class="line">     ip =$(ifconfig <span class="variable">$lt</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>)</div><div class="line">     <span class="comment">#ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;')</span></div><div class="line">     <span class="keyword">if</span> [ <span class="variable">$param2</span> == <span class="variable">$ip</span> ];<span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$lt</span> : <span class="variable">$ip</span>"</span></div><div class="line">     <span class="keyword">fi</span></div><div class="line">  <span class="keyword">done</span></div><div class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-a'</span> ]]; <span class="keyword">then</span></div><div class="line"> <span class="keyword">for</span> lt <span class="keyword">in</span> <span class="variable">$list</span>;</div><div class="line">  <span class="keyword">do</span></div><div class="line">     ip=$(ifconfig <span class="variable">$lt</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>)</div><div class="line">     <span class="comment">#ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;')</span></div><div class="line">     <span class="keyword">if</span> [ <span class="variable">$lt</span> != <span class="string">"lo"</span> ];<span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$lt</span> : <span class="variable">$ip</span>"</span></div><div class="line">        <span class="built_in">exit</span> 0</div><div class="line">     <span class="keyword">fi</span></div><div class="line">  <span class="keyword">done</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"quit"</span></div><div class="line">    <span class="built_in">exit</span> 0</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<h3 id="注意如上内容根据你个人机器显示情况而定，如果是显示"><a href="#注意如上内容根据你个人机器显示情况而定，如果是显示" class="headerlink" title="注意如上内容根据你个人机器显示情况而定，如果是显示"></a>注意如上内容根据你个人机器显示情况而定，如果是显示</h3><p> inet addr:127.0.0.1  Mask:255.0.0.0<br> 就要把正则改成<br> ip=$(ifconfig $lt | awk -F” “ ‘/inet.*Mask/{print $2}’ | awk -F: ‘{print $2}’)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;练习题目&quot;&gt;&lt;a href=&quot;#练习题目&quot; class=&quot;headerlink&quot; title=&quot;练习题目&quot;&gt;&lt;/a&gt;练习题目&lt;/h2&gt;&lt;p&gt;写一个脚本getinterface.sh，脚本可以接受参数(i,I,a)，完成以下任务：&lt;br&gt;   (1)使用以下形式：getinterface.sh [-i interface|-I IP|-a]&lt;br&gt;   (2)当用户使用-i选项时，显示其指定网卡的IP地址；&lt;br&gt;   (3)当用户使用-I选项时，显示其后面的IP地址所属的网络接口；（如 192.168.199.183：eth0）&lt;br&gt;   (4)当用户单独使用-a选项时，显示所有网络接口及其IP地址（lo除外）&lt;/p&gt;
&lt;h3 id=&quot;分析&quot;&gt;&lt;a href=&quot;#分析&quot; class=&quot;headerlink&quot; title=&quot;分析&quot;&gt;&lt;/a&gt;分析&lt;/h3&gt;&lt;p&gt;参数1为param1 参数2位param2&lt;br&gt;（1）利用CAT &amp;lt;&amp;lt; EOF  &lt;em&gt;*&lt;/em&gt; EOF打印信息&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;cat &amp;lt;&amp;lt; EOF&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    getinterface.sh [-i interface|-I IP|&lt;span class=&quot;_&quot;&gt;-a&lt;/span&gt;]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    -i interface) show ip of the interface&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    -I IP) show interface of the IP and IP with :;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;_&quot;&gt;-a&lt;/span&gt;) list all interfaces and their IPs except lo;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    *) quit&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;=================================================================&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;EOF&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="bash" scheme="http://yoursite.com/categories/bash/"/>
    
    
      <category term="bash" scheme="http://yoursite.com/tags/bash/"/>
    
  </entry>
  
  <entry>
    <title>CDH hadoop实战</title>
    <link href="http://yoursite.com/2017/03/14/CDH%20hadoop%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/14/CDH hadoop实战/</id>
    <published>2017-03-14T06:30:00.000Z</published>
    <updated>2017-07-02T00:12:55.833Z</updated>
    
    <content type="html"><![CDATA[<h5 id="解决问题："><a href="#解决问题：" class="headerlink" title="解决问题："></a>解决问题：</h5><p>因为数据手机号一项包含为NULL的数据，需要清洗。</p>
<p>创建工程，添加如下依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&lt;dependencies&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;junit&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;junit&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;3.8.1&lt;/version&gt;</div><div class="line">            &lt;scope&gt;test&lt;/scope&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">    &lt;/dependencies&gt;</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h2 id="编写mapper类"><a href="#编写mapper类" class="headerlink" title="编写mapper类"></a>编写mapper类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> map;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by Administrator on 2017/3/14.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UidPhoneMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">        String arr[] = key.toString().split(<span class="string">","</span>);</div><div class="line">        <span class="keyword">if</span> (arr.length != <span class="number">2</span>) &#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(arr[<span class="number">1</span>] == <span class="keyword">null</span>)&#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        context.write(<span class="keyword">new</span> Text(arr[<span class="number">0</span>]), <span class="keyword">new</span> Text(arr[<span class="number">1</span>]));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> main;</div><div class="line"></div><div class="line"><span class="keyword">import</span> map.UidPhoneMapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by Administrator on 2017/3/14.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UidPhoneDropNull</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">        GenericOptionsParser parser = <span class="keyword">new</span> GenericOptionsParser(conf, args);</div><div class="line">        String[] otherArgs = parser.getRemainingArgs();</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage: NewlyJoin &lt;inpath&gt; &lt;output&gt;"</span>);</div><div class="line">            System.exit(<span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Job job = <span class="keyword">new</span> Job(conf, <span class="string">"UidPhoneDropNull"</span>);</div><div class="line">        <span class="comment">// 设置运行的job</span></div><div class="line">        job.setJarByClass(UidPhoneDropNull.class);</div><div class="line">        <span class="comment">// 设置Map相关内容</span></div><div class="line">        job.setMapperClass(UidPhoneMapper.class);</div><div class="line">        job.setMapOutputKeyClass(Text.class);</div><div class="line">        job.setMapOutputValueClass(Text.class);</div><div class="line">        job.setInputFormatClass(KeyValueTextInputFormat.class); <span class="comment">//设置文件输入格式</span></div><div class="line">        job.setNumReduceTasks(<span class="number">0</span>); <span class="comment">//设置Reduce个数为0</span></div><div class="line"></div><div class="line">        job.setOutputKeyClass(Text.class);</div><div class="line">        job.setOutputValueClass(Text.class);</div><div class="line">        <span class="comment">// 设置输入和输出的目录</span></div><div class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));</div><div class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));</div><div class="line">        <span class="comment">// 执行，直到结束就退出</span></div><div class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行上面的程序<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">[hdfs@slave3 jar]$ hadoop jar bigdataMR.jar main.UidPhoneDropNull /bigdata/data/hdfs/BD/UP/* /bigdata/data/hdfs/BD/UP2 </div><div class="line">17/03/14 17:37:54 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:8032</div><div class="line">17/03/14 17:37:55 INFO input.FileInputFormat: Total input paths to process : 3</div><div class="line">17/03/14 17:37:55 INFO mapreduce.JobSubmitter: number of splits:3</div><div class="line">17/03/14 17:37:55 INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_1488970234114_0044</div><div class="line">17/03/14 17:37:55 INFO impl.YarnClientImpl: Submitted application application_1488970234114_0044</div><div class="line">17/03/14 17:37:55 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0044/</div><div class="line">17/03/14 17:37:55 INFO mapreduce.Job: Running job: job_1488970234114_0044</div><div class="line">17/03/14 17:38:00 INFO mapreduce.Job: Job job_1488970234114_0044 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></div><div class="line">17/03/14 17:38:00 INFO mapreduce.Job:  map 0% reduce 0%</div><div class="line">17/03/14 17:38:04 INFO mapreduce.Job:  map 33% reduce 0%</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job:  map 100% reduce 0%</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job: Job job_1488970234114_0044 completed successfully</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job: Counters: 30</div><div class="line">        File System Counters</div><div class="line">                FILE: Number of bytes <span class="built_in">read</span>=0</div><div class="line">                FILE: Number of bytes written=378748</div><div class="line">                FILE: Number of <span class="built_in">read</span> operations=0</div><div class="line">                FILE: Number of large <span class="built_in">read</span> operations=0</div><div class="line">                FILE: Number of write operations=0</div><div class="line">                HDFS: Number of bytes <span class="built_in">read</span>=13514456</div><div class="line">                HDFS: Number of bytes written=10824901</div><div class="line">                HDFS: Number of <span class="built_in">read</span> operations=15</div><div class="line">                HDFS: Number of large <span class="built_in">read</span> operations=0</div><div class="line">                HDFS: Number of write operations=6</div><div class="line">        Job Counters </div><div class="line">                Launched map tasks=3</div><div class="line">                Data-local map tasks=3</div><div class="line">                Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=8240</div><div class="line">                Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</div><div class="line">                Total time spent by all map tasks (ms)=8240</div><div class="line">                Total vcore-seconds taken by all map tasks=8240</div><div class="line">                Total megabyte-seconds taken by all map tasks=8437760</div><div class="line">        Map-Reduce Framework</div><div class="line">                Map input records=661590</div><div class="line">                Map output records=485478</div><div class="line">                Input split bytes=393</div><div class="line">                Spilled Records=0</div><div class="line">                Failed Shuffles=0</div><div class="line">                Merged Map outputs=0</div><div class="line">                GC time elapsed (ms)=175</div><div class="line">                CPU time spent (ms)=3730</div><div class="line">                Physical memory (bytes) snapshot=712597504</div><div class="line">                Virtual memory (bytes) snapshot=8316772352</div><div class="line">                Total committed heap usage (bytes)=814743552</div><div class="line">        File Input Format Counters </div><div class="line">                Bytes Read=13514063</div><div class="line">        File Output Format Counters </div><div class="line">                Bytes Written=10824901</div><div class="line">[hdfs@slave3 jar]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP2</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7609438 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    3215441 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup         22 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00002</div></pre></td></tr></table></figure></p>
<p>最后将数据导入到hive<br>hive&gt; create EXTERNAL table IF NOT EXISTS UP2 (uid STRING,phone STRING) row format delimited fields terminated by ‘,’ location ‘/bigdata/data/hdfs/BD/UP2/‘;<br>OK<br>Time taken: 0.026 seconds<br>hive&gt; select count(1) from UP2;<br>Query ID = hdfs_20170314174141_1b60d560-2036-44e6-ab65-19f17efc5b1b<br>Total jobs = 1<br>Launching Job 1 out of 1<br>Number of reduce tasks determined at compile time: 1<br>In order to change the average load for a reducer (in bytes):<br>  set hive.exec.reducers.bytes.per.reducer=<number><br>In order to limit the maximum number of reducers:<br>  set hive.exec.reducers.max=<number><br>In order to set a constant number of reducers:<br>  set mapreduce.job.reduces=<number><br>Starting Job = job_1488970234114_0045, Tracking URL = <a href="http://master2:8088/proxy/application_1488970234114_0045/" target="_blank" rel="external">http://master2:8088/proxy/application_1488970234114_0045/</a><br>Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0045<br>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1<br>2017-03-14 17:41:33,200 Stage-1 map = 0%,  reduce = 0%<br>2017-03-14 17:41:38,371 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.54 sec<br>2017-03-14 17:41:43,486 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.38 sec<br>MapReduce Total cumulative CPU time: 4 seconds 380 msec<br>Ended Job = job_1488970234114_0045<br>MapReduce Jobs Launched:<br>Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.38 sec   HDFS Read: 10831709 HDFS Write: 7 SUCCESS<br>Total MapReduce CPU Time Spent: 4 seconds 380 msec<br>OK<br>485478<br>Time taken: 16.267 seconds, Fetched: 1 row(s)</number></number></number></p>
<p>比清洗前数据661590少了176112条</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;解决问题：&quot;&gt;&lt;a href=&quot;#解决问题：&quot; class=&quot;headerlink&quot; title=&quot;解决问题：&quot;&gt;&lt;/a&gt;解决问题：&lt;/h5&gt;&lt;p&gt;因为数据手机号一项包含为NULL的数据，需要清洗。&lt;/p&gt;
&lt;p&gt;创建工程，添加如下依赖&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;lt;dependencies&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-common&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-hdfs&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;3.8.1&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;lt;/dependencies&amp;gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH hive 实战</title>
    <link href="http://yoursite.com/2017/03/14/CDH%20hive%20%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/14/CDH hive 实战/</id>
    <published>2017-03-14T00:30:00.000Z</published>
    <updated>2017-03-31T16:13:09.590Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">hive&gt; create database BD;</div><div class="line">OK</div><div class="line">Time taken: 0.161 seconds</div><div class="line">hive&gt; use BD;</div><div class="line">OK</div><div class="line">Time taken: 0.013 seconds</div></pre></td></tr></table></figure>
<p>创建表<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hive&gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> ;</div><div class="line">OK</div><div class="line">Time taken: 0.211 seconds</div><div class="line">hive&gt; load data inpath <span class="string">'/bigdata/data/hdfs/shuju/SO/part-m-00000'</span> into table UP;</div><div class="line">Loading data to table BD.UP</div><div class="line">Table BD.UP stats: [numFiles=1, totalSize=898611236]</div><div class="line">OK</div><div class="line">Time taken: 0.317 seconds</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>查询表数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314111111_d29152a3-56b3-42f9-b17b-1ddaf7451117</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0025, Tracking URL = http://master2:8088/proxy/application_1488970234114_0025/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0025</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 4; number of reducers: 1</div><div class="line">2017-03-14 11:11:49,267 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 11:11:55,519 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 3.31 sec</div><div class="line">2017-03-14 11:11:56,542 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.57 sec</div><div class="line">2017-03-14 11:12:01,655 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 18.41 sec</div><div class="line">MapReduce Total cumulative CPU time: 18 seconds 410 msec</div><div class="line">Ended Job = job_1488970234114_0025</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 4  Reduce: 1   Cumulative CPU: 18.41 sec   HDFS Read: 899021344 HDFS Write: 8 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 18 seconds 410 msec</div><div class="line">OK</div><div class="line">5674200</div><div class="line">Time taken: 19.429 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>删除表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.088 seconds</div></pre></td></tr></table></figure></p>
<p>查看hdfs上的数据，发现数据被删除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/SO</span></div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-13 19:16 /bigdata/data/hdfs/shuju/SO/_SUCCESS</div></pre></td></tr></table></figure></p>
<h2 id="测试2："><a href="#测试2：" class="headerlink" title="测试2："></a>测试2：</h2><p>首先查询，发现biaoming数据存在<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/biaoming</span></div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup   13514063 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/part-m-00000</div></pre></td></tr></table></figure></p>
<p>创建外部表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> ;</div><div class="line">OK</div><div class="line">Time taken: 0.033 seconds</div></pre></td></tr></table></figure></p>
<p>加载数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hive&gt; load data inpath <span class="string">'/bigdata/data/hdfs/shuju/biaoming/part-m-00000'</span> into table UP;</div><div class="line">Loading data to table BD.UP</div><div class="line">Table BD.UP stats: [numFiles=1, totalSize=13514063]</div><div class="line">OK</div><div class="line">Time taken: 0.201 seconds</div></pre></td></tr></table></figure></p>
<p>查询数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314115959_6004ef81-913b-4d38-99dc-1199cc6e73f2</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0027, Tracking URL = http://master2:8088/proxy/application_1488970234114_0027/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0027</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</div><div class="line">2017-03-14 11:59:09,331 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 11:59:14,463 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.06 sec</div><div class="line">2017-03-14 11:59:19,587 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.9 sec</div><div class="line">MapReduce Total cumulative CPU time: 4 seconds 900 msec</div><div class="line">Ended Job = job_1488970234114_0027</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.9 sec   HDFS Read: 13520683 HDFS Write: 7 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 4 seconds 900 msec</div><div class="line">OK</div><div class="line">661590</div><div class="line">Time taken: 17.707 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>删除数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.037 seconds</div></pre></td></tr></table></figure></p>
<p>再查看元数据，发现依然被删除了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/biaoming</span></div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS</div></pre></td></tr></table></figure></p>
<h2 id="测试3："><a href="#测试3：" class="headerlink" title="测试3："></a>测试3：</h2><p>重新上传数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver  --connect <span class="string">'jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=BD'</span> --table=UP --target-dir /bigdata/data/hdfs/BD/UP --split-by uid -m 3</div></pre></td></tr></table></figure></p>
<p>查看数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup    3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002</div><div class="line">-rw-r--r--   3 hdfs supergroup         22 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00003</div></pre></td></tr></table></figure></p>
<p>创建外部表数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">FAILED: SemanticException [Error 10001]: Line 1:21 Table not found <span class="string">'UP'</span></div><div class="line">hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> location <span class="string">'/bigdata/data/hdfs/BD/UP/'</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.03 seconds</div><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314141111_5be2c701-ad6a-4717-8801-513f38d64928</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0031, Tracking URL = http://master2:8088/proxy/application_1488970234114_0031/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0031</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</div><div class="line">2017-03-14 14:11:48,608 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 14:11:54,743 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.26 sec</div><div class="line">2017-03-14 14:11:58,833 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.08 sec</div><div class="line">MapReduce Total cumulative CPU time: 5 seconds 80 msec</div><div class="line">Ended Job = job_1488970234114_0031</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.08 sec   HDFS Read: 13520952 HDFS Write: 7 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 5 seconds 80 msec</div><div class="line">OK</div><div class="line">661590</div><div class="line">Time taken: 16.253 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>此时再删除表后数据还存在。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.037 seconds</div><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup    3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002</div></pre></td></tr></table></figure></p>
<p>创建普通表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; create table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> location <span class="string">'/bigdata/data/hdfs/BD/UP/'</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.029 seconds</div></pre></td></tr></table></figure></p>
<p>删除表后数据不存在<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.046 seconds</div><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">ls: `/bigdata/data/hdfs/BD/UP<span class="string">': No such file or directory</span></div></pre></td></tr></table></figure></p>
<p>总结<br>用load方式无论外部表还是内部表数据都会删除，用location方式，外部表不会删除数据，内部表会删除数据</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;创建数据库&quot;&gt;&lt;a href=&quot;#创建数据库&quot; class=&quot;headerlink&quot; title=&quot;创建数据库&quot;&gt;&lt;/a&gt;创建数据库&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; create database BD;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.161 seconds&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; use BD;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.013 seconds&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;创建表&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by &lt;span class=&quot;string&quot;&gt;&#39;,&#39;&lt;/span&gt; ;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.211 seconds&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; load data inpath &lt;span class=&quot;string&quot;&gt;&#39;/bigdata/data/hdfs/shuju/SO/part-m-00000&#39;&lt;/span&gt; into table UP;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Loading data to table BD.UP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Table BD.UP stats: [numFiles=1, totalSize=898611236]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.317 seconds&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH sqoop1 实战-带条件的导入</title>
    <link href="http://yoursite.com/2017/03/13/CDH%20sqoop1%20%E5%AE%9E%E6%88%98-%E5%B8%A6%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5/"/>
    <id>http://yoursite.com/2017/03/13/CDH sqoop1 实战-带条件的导入/</id>
    <published>2017-03-13T01:30:00.000Z</published>
    <updated>2017-06-25T00:50:06.600Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h2><p>[hdfs@slave3 lib]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query “SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1”  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming<br>17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0<br>17/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br>17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br>17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation<br>17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‘$CONDITIONS’ in WHERE clause.<br>        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332)<br>        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858)<br>        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658)<br>        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)<br>        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)<br>        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)<br>        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)<br>        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)<br>        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)</p>
<p>修改为：<br>sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming</p>
<a id="more"></a>
<p>在执行，发现报错说我database不存在，并在目录下生成了一个metastore_db，原来是执行sqoop的机器不对，换了一台机器执行没有问题，很奇怪明明是分布式的为什么会出现这个问题呢</p>
<p>最终的具体日志：</p>
<p>[hdfs@master1 root]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming –hive-database shuju<br>Warning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.<br>Please set $ACCUMULO_HOME to the root of your Accumulo installation.<br>17/03/15 14:32:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0<br>17/03/15 14:32:53 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br>17/03/15 14:32:53 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br>17/03/15 14:32:53 INFO tool.CodeGenTool: Beginning code generation<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../hadoop-mapreduce<br>Note: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.java uses or overrides a deprecated API.<br>Note: Recompile with -Xlint:deprecation for details.<br>17/03/15 14:33:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.jar<br>17/03/15 14:33:00 INFO mapreduce.ImportJobBase: Beginning query import.<br>17/03/15 14:33:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar<br>17/03/15 14:33:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps<br>17/03/15 14:33:01 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:8032<br>17/03/15 14:33:02 INFO db.DBInputFormat: Using read commited transaction isolation<br>17/03/15 14:33:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM (SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 1) ) AS t1<br>17/03/15 14:33:03 INFO db.IntegerSplitter: Split size: 7508318; Num splits: 4 from: 20079 to: 30053354<br>17/03/15 14:33:03 INFO mapreduce.JobSubmitter: number of splits:4<br>17/03/15 14:33:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_0057<br>17/03/15 14:33:03 INFO impl.YarnClientImpl: Submitted application application_1488970234114_0057<br>17/03/15 14:33:03 INFO mapreduce.Job: The url to track the job: <a href="http://master2:8088/proxy/application_1488970234114_0057/" target="_blank" rel="external">http://master2:8088/proxy/application_1488970234114_0057/</a><br>17/03/15 14:33:03 INFO mapreduce.Job: Running job: job_1488970234114_0057<br>17/03/15 14:33:08 INFO mapreduce.Job: Job job_1488970234114_0057 running in uber mode : false<br>17/03/15 14:33:08 INFO mapreduce.Job:  map 0% reduce 0%<br>17/03/15 14:33:15 INFO mapreduce.Job:  map 75% reduce 0%<br>17/03/15 14:33:16 INFO mapreduce.Job:  map 100% reduce 0%<br>17/03/15 14:33:16 INFO mapreduce.Job: Job job_1488970234114_0057 completed successfully<br>17/03/15 14:33:16 INFO mapreduce.Job: Counters: 30<br>        File System Counters<br>                FILE: Number of bytes read=0<br>                FILE: Number of bytes written=608160<br>                FILE: Number of read operations=0<br>                FILE: Number of large read operations=0<br>                FILE: Number of write operations=0<br>                HDFS: Number of bytes read=428<br>                HDFS: Number of bytes written=17645206<br>                HDFS: Number of read operations=16<br>                HDFS: Number of large read operations=0<br>                HDFS: Number of write operations=8<br>        Job Counters<br>                Launched map tasks=4<br>                Other local map tasks=4<br>                Total time spent by all maps in occupied slots (ms)=17968<br>                Total time spent by all reduces in occupied slots (ms)=0<br>                Total time spent by all map tasks (ms)=17968<br>                Total vcore-seconds taken by all map tasks=17968<br>                Total megabyte-seconds taken by all map tasks=18399232<br>        Map-Reduce Framework<br>                Map input records=283659<br>                Map output records=283659<br>                Input split bytes=428<br>                Spilled Records=0<br>                Failed Shuffles=0<br>                Merged Map outputs=0<br>                GC time elapsed (ms)=268<br>                CPU time spent (ms)=12960<br>                Physical memory (bytes) snapshot=1151381504<br>                Virtual memory (bytes) snapshot=11157348352<br>                Total committed heap usage (bytes)=1045954560<br>        File Input Format Counters<br>                Bytes Read=0<br>        File Output Format Counters<br>                Bytes Written=17645206<br>17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Transferred 16.8278 MB in 15.7965 seconds (1.0653 MB/sec)<br>17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Retrieved 283659 records.<br>17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:33:16 WARN hive.TableDefWriter: Column update_time had to be cast to a less precise type in Hive<br>17/03/15 14:33:16 INFO hive.HiveImport: Loading uploaded data into Hive</p>
<p>Logging initialized using configuration in jar:file:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.properties<br>OK<br>Time taken: 1.874 seconds<br>Loading data to table shuju.card_record<br>Table shuju.card_record stats: [numFiles=4, totalSize=17645206]<br>OK<br>Time taken: 0.367 seconds</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;导入&quot;&gt;&lt;a href=&quot;#导入&quot; class=&quot;headerlink&quot; title=&quot;导入&quot;&gt;&lt;/a&gt;导入&lt;/h2&gt;&lt;p&gt;[hdfs@slave3 lib]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query “SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1”  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming&lt;br&gt;17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0&lt;br&gt;17/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.&lt;br&gt;17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.&lt;br&gt;17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation&lt;br&gt;17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‘$CONDITIONS’ in WHERE clause.&lt;br&gt;        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332)&lt;br&gt;        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858)&lt;br&gt;        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658)&lt;br&gt;        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)&lt;br&gt;        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)&lt;br&gt;        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)&lt;br&gt;        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)&lt;br&gt;        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)&lt;br&gt;        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)&lt;br&gt;        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)&lt;br&gt;        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)&lt;br&gt;        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)&lt;/p&gt;
&lt;p&gt;修改为：&lt;br&gt;sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH sqoop1 实战</title>
    <link href="http://yoursite.com/2017/03/13/CDH%20sqoop1%20%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/13/CDH sqoop1 实战/</id>
    <published>2017-03-13T01:30:00.000Z</published>
    <updated>2017-07-23T08:05:17.973Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载sqlserver的jdbc驱动包"><a href="#下载sqlserver的jdbc驱动包" class="headerlink" title="下载sqlserver的jdbc驱动包"></a>下载sqlserver的jdbc驱动包</h2><p><a href="https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599" target="_blank" rel="external">https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599</a><br>解压<br>将sqljdbc4.jar放在：<br>/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/lib</p>
<p>下载SQL Server-Hadoop Connector：sqoop-sqlserver-1.0.tar.gz<br>             <a href="http://www.microsoft.com/en-us/download/details.aspx?id=27584" target="_blank" rel="external">http://www.microsoft.com/en-us/download/details.aspx?id=27584</a></p>
<a id="more"></a>
<h2 id="导入数据："><a href="#导入数据：" class="headerlink" title="导入数据："></a>导入数据：</h2><p>[root@slave3 sqoop]# ./bin/sqoop import –connect ‘jdbc: server://10.105.32.246 username=sa password=123456 database=databaseName –table=tableName –target-dir /bigdata/data/hdfs<br>Warning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/bin/../../accumulo does not exist! Accumulo imports will fail.<br>Please set $ACCUMULO_HOME to the root of your Accumulo installation.<br>+======================================================================+<br>|                    Error: JAVA_HOME is not set                       |<br>+———————————————————————-+<br>| Please download the latest Sun JDK from the Sun Java web site        |<br>|     &gt; <a href="http://www.oracle.com/technetwork/java/javase/downloads" target="_blank" rel="external">http://www.oracle.com/technetwork/java/javase/downloads</a>        |<br>|                                                                      |<br>| HBase requires Java 1.7 or later.                                    |<br>+======================================================================+<br>Error: JAVA_HOME is not set and could not be found.</p>
<p>报错解决方法：<br>修改bin下的<br>configure-sqoop</p>
<p>注释以下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then</div><div class="line">#  if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then</div><div class="line">#    HCAT_HOME=/usr/lib/hive-hcatalog</div><div class="line">#  elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then</div><div class="line">#    HCAT_HOME=/usr/lib/hcatalog</div><div class="line">#  else</div><div class="line">#    HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog</div><div class="line">#    if [ ! -d $&#123;HCAT_HOME&#125; ]; then</div><div class="line">#       HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog</div><div class="line">#    fi</div><div class="line">#  fi</div><div class="line">#fi</div><div class="line">#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then</div><div class="line">#  if [ -d &quot;/usr/lib/accumulo&quot; ]; then</div><div class="line">#    ACCUMULO_HOME=/usr/lib/accumulo</div><div class="line">#  else</div><div class="line">#    ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo</div><div class="line">#  fi</div><div class="line">#fi</div><div class="line"></div><div class="line">#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then</div><div class="line">#  if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then</div><div class="line">#    HCAT_HOME=/usr/lib/hive-hcatalog</div><div class="line">#  elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then</div><div class="line">#    HCAT_HOME=/usr/lib/hcatalog</div><div class="line">#  else</div><div class="line">#    HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog</div><div class="line">#    if [ ! -d $&#123;HCAT_HOME&#125; ]; then</div><div class="line">#       HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog</div><div class="line">#    fi</div><div class="line">#  fi</div><div class="line">#fi</div><div class="line">#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then</div><div class="line">#  if [ -d &quot;/usr/lib/accumulo&quot; ]; then</div><div class="line">#    ACCUMULO_HOME=/usr/lib/accumulo</div><div class="line">#  else</div><div class="line">#    ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo</div><div class="line">#  fi</div><div class="line">#fi</div></pre></td></tr></table></figure></p>
<p>在执行如果提示JAVA_HOME不存在，手动执行一下export JAVA_HOME=/usr/java/jdk1.8.0_121</p>
<p>再执行提示：<br>ERROR tool.BaseSqoopTool: Error parsing arguments for import:</p>
<p>./bin/sqoop import –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’ –table=tableName –target-dir /bigdata/data/hdfs –split-by order_id –fields-terminated-by ‘\t’  –m 3</p>
<p>需要下载sqoop-sqlserver<br>下载地址已经失效，在CSDN上找到：<br><a href="https://passport.csdn.net/account/login?from=http://download.csdn.net/download/nma_123456/9405343" target="_blank" rel="external">https://passport.csdn.net/account/login?from=http://download.csdn.net/download/nma_123456/9405343</a></p>
<p>解压进入目录执行：<br>[root@slave3 sqoop-sqlserver-1.0]# export SQOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop<br>[root@slave3 sqoop-sqlserver-1.0]# ./install.sh </p>
<p>再次执行：报如下错误<br>Exception in thread “main” java.lang.NoClassDefFoundError: org/json/JSONObject<br>        at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:43)<br>        at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:767)<br>        at org.apache.sqoop.mapreduce.JobBase.putSqoopOptionsToConfiguration(JobBase.java:388)<br>        at org.apache.sqoop.mapreduce.JobBase.createJob(JobBase.java:374)<br>        at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:256)<br>        at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692)<br>        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507)<br>        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)<br>        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)<br>        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)<br>        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)<br>Caused by: java.lang.ClassNotFoundException: org.json.JSONObject<br>        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)<br>        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)<br>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)<br>        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</p>
<p>分析原因缺少org/json/JSONObject，上网下载一个json.jar<br>地址：<a href="http://download.csdn.net/download/haixia_12/8462933" target="_blank" rel="external">http://download.csdn.net/download/haixia_12/8462933</a></p>
<p>扔进去重新执行就OK了</p>
<p>最终命令：<br>sqoop import –driver com.microsoft.sqlserver.jdbc.SQLServerDriver  –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’ –table=tableName –target-dir /bigdata/data/hdfs/cards –split-by order_id -m 3</p>
<p>查看sqlserver上有什么数据：<br> sqoop list-tables –driver com.microsoft.sqlserver.jdbc.SQLServerDriver  –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’</p>
<p> sqlserver如果连接失败，要看是否开启了远程访问和TCP/UDP端口映射</p>
<p>报错<br>17/03/13 17:20:01 ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.<br>17/03/13 17:20:01 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</p>
<p>增加：<br>export HADOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop<br>export HIVE_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive</p>
<p>export HIVE_CONF_DIR=/etc/hive/conf<br>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib/*</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;下载sqlserver的jdbc驱动包&quot;&gt;&lt;a href=&quot;#下载sqlserver的jdbc驱动包&quot; class=&quot;headerlink&quot; title=&quot;下载sqlserver的jdbc驱动包&quot;&gt;&lt;/a&gt;下载sqlserver的jdbc驱动包&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599&quot;&gt;https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599&lt;/a&gt;&lt;br&gt;解压&lt;br&gt;将sqljdbc4.jar放在：&lt;br&gt;/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/lib&lt;/p&gt;
&lt;p&gt;下载SQL Server-Hadoop Connector：sqoop-sqlserver-1.0.tar.gz&lt;br&gt;             &lt;a href=&quot;http://www.microsoft.com/en-us/download/details.aspx?id=27584&quot;&gt;http://www.microsoft.com/en-us/download/details.aspx?id=27584&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH 安装</title>
    <link href="http://yoursite.com/2017/03/08/CDH%20%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/03/08/CDH 安装/</id>
    <published>2017-03-08T01:30:00.000Z</published>
    <updated>2017-07-06T01:17:23.929Z</updated>
    
    <content type="html"><![CDATA[<h2 id="运行环境："><a href="#运行环境：" class="headerlink" title="运行环境："></a>运行环境：</h2><table>
<thead>
<tr>
<th>主机IP</th>
<th>主机名</th>
<th>内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.147</td>
<td>po-master1</td>
<td>16G</td>
</tr>
<tr>
<td>1.1.1.127</td>
<td>po-master2</td>
<td>8G</td>
</tr>
<tr>
<td>1.1.1.118</td>
<td>po-slave1</td>
<td>8G</td>
</tr>
<tr>
<td>1.1.1.92</td>
<td>po-slave2</td>
<td>8G</td>
</tr>
<tr>
<td>1.1.1.230</td>
<td>po-slave3</td>
<td>8G</td>
</tr>
</tbody>
</table>
<h2 id="配置主机名-分别在五台机器上执行"><a href="#配置主机名-分别在五台机器上执行" class="headerlink" title="配置主机名(分别在五台机器上执行)"></a>配置主机名(分别在五台机器上执行)</h2><p>vi /etc/sysconfig/network<br>hostname +主机名<br>例如： hostname po-master1</p>
<h2 id="配置映射关系-把以下五条命令在五台机器上执行"><a href="#配置映射关系-把以下五条命令在五台机器上执行" class="headerlink" title="配置映射关系(把以下五条命令在五台机器上执行)"></a>配置映射关系(把以下五条命令在五台机器上执行)</h2><p>echo “1.1.1.147   po-master1”&gt;&gt;/etc/hosts<br>echo “1.1.1.127   po-master2”&gt;&gt;/etc/hosts<br>echo “1.1.1.118  po-slave1”&gt;&gt;/etc/hosts<br>echo “1.1.1.92   po-slave2”&gt;&gt;/etc/hosts<br>echo “1.1.1.230  po-slave3”&gt;&gt;/etc/hosts<br><a id="more"></a></p>
<h2 id="安装JDK（在po-master1上执行）"><a href="#安装JDK（在po-master1上执行）" class="headerlink" title="安装JDK（在po-master1上执行）"></a>安装JDK（在po-master1上执行）</h2><p>1.下载JDK安装包：jdk-8u102-linux-x64.tar.gz<br>注：作者放到/soft/java具体位置可自行安排</p>
<ol>
<li><p>安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /soft/java</div><div class="line">mkdir jdk1.8.0_121</div><div class="line">rpm -ivh jdk-7u76-linux-x64.rpm --prefix=/soft/java</div></pre></td></tr></table></figure>
</li>
<li><p>创建连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ln <span class="_">-s</span> <span class="_">-f</span> jdk1.8.0_121/ jdk</div></pre></td></tr></table></figure>
</li>
</ol>
<!-- more -->
<h2 id="开放端口（五台机器上都需要配置）"><a href="#开放端口（五台机器上都需要配置）" class="headerlink" title="开放端口（五台机器上都需要配置）"></a>开放端口（五台机器上都需要配置）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">/sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEP</div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> 0.0.0.0/0 -p tcp --dport 22	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 22	  -j ACCEPT</div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50010	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 1004	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50075	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 1006	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50070	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50470	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50090	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50495	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8485	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8480	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8032	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8030	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8031	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8033	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8088	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8040	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8042	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8041	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 10020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 13562	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 19888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60010	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60030	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 3888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8080	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8085	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9090	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9095	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9090	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9083	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 10000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 16000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 3888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 3181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 4181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8019	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9010	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 11000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 11001	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 14000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 14001	  -j ACCEPT </div><div class="line">/etc/rc.d/init.d/iptables save</div></pre></td></tr></table></figure>
<p>关闭端口详解-参考CDH官网<br><a href="https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_ports_cdh5.html" target="_blank" rel="external">https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_ports_cdh5.html</a></p>
<h2 id="测试（可忽略）"><a href="#测试（可忽略）" class="headerlink" title="测试（可忽略）"></a>测试（可忽略）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/etc/init.d/iptables status</div></pre></td></tr></table></figure>
<h2 id="五台机器配置互相免秘钥登陆"><a href="#五台机器配置互相免秘钥登陆" class="headerlink" title="五台机器配置互相免秘钥登陆"></a>五台机器配置互相免秘钥登陆</h2><p>1.创建ssh文件<br>如果已经创建不要覆盖<br>cat ~/.ssh/id_rsa.pub &gt;&gt;~/.ssh/authorized_keys<br>分别把五台机器的公钥加载到authorized_keys</p>
<p>2.<br>vi /etc/ssh/sshd_config<br>打开如下内容<br>HostKey /etc/ssh/ssh_host_rsa_key<br>RSAAuthentication yes<br>PubkeyAuthentication yes<br>AuthorizedKeysFile      .ssh/authorized_keys</p>
<p>3.重启<br>/etc/init.d/sshd restart</p>
<p>4.测试ssh<br>ssh po-master1<br>ssh po-master2<br>ssh po-slave1<br>ssh po-slave2<br>ssh po-slave3</p>
<h2 id="向其他机器分发jdk"><a href="#向其他机器分发jdk" class="headerlink" title="向其他机器分发jdk"></a>向其他机器分发jdk</h2><p>scp -rp /soft/java/ root@po-master2:/soft/java<br>scp -rp /soft/java/ root@po-salve1:/soft/java<br>scp -rp /soft/java/ root@po-salve2:/soft/java<br>scp -rp /soft/java/ root@po-salve3:/soft/java</p>
<h2 id="配置环境变量-分别在五台机器上执行"><a href="#配置环境变量-分别在五台机器上执行" class="headerlink" title="配置环境变量(分别在五台机器上执行)"></a>配置环境变量(分别在五台机器上执行)</h2><p>执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">"export JAVA_HOME=/soft/java/jdk"</span>  &gt;&gt; /etc/profile</div><div class="line"><span class="built_in">echo</span> <span class="string">"export PATH=<span class="variable">$PATH</span>:<span class="variable">$HOME</span>/bin:<span class="variable">$JAVA_HOME</span>:<span class="variable">$JAVA_HOME</span>/bin:/usr/bin/"</span>  &gt;&gt; /etc/profile</div><div class="line"><span class="built_in">echo</span> <span class="string">"export CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib"</span>  &gt;&gt; /etc/profile</div><div class="line">. /etc/profile</div></pre></td></tr></table></figure></p>
<h2 id="测试（可忽略）-1"><a href="#测试（可忽略）-1" class="headerlink" title="测试（可忽略）"></a>测试（可忽略）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 java]<span class="comment"># java -version</span></div><div class="line">java version <span class="string">"1.8.0_121"</span></div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_121-b13)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)</div></pre></td></tr></table></figure>
<h2 id="配置NTP服务器和客户端（因为使用阿里云此处省略）"><a href="#配置NTP服务器和客户端（因为使用阿里云此处省略）" class="headerlink" title="配置NTP服务器和客户端（因为使用阿里云此处省略）"></a>配置NTP服务器和客户端（因为使用阿里云此处省略）</h2><h2 id="配置mysql"><a href="#配置mysql" class="headerlink" title="配置mysql"></a>配置mysql</h2><p>1.上传mysql文件（博主放到/soft/mysql目录下）<br>2.解压<br>cd /soft/mysql<br>tar -zxvf mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz -C /usr/local<br>3.将目录重命名<br>cd /usr/local<br>mv mysql-5.7.17-linux-glibc2.5-x86_64/ mysql<br>4.创建data目录</p>
<p>mkdir /usr/local/mysql/data</p>
<p>5.安装插件（网上有人说不安装提示libiao错误，博主用阿里云libaio已经是最新版本，所以不用安装，也不知道没安装有什么坏处）<br> yum install libaio</p>
<p>6.安装mysql<br>cd /usr/local/mysql/bin<br>./mysql_install_db –user=root –basedir=/usr/local/mysql –datadir=//usr/local/mysql/data</p>
<p>1.官网下载yum源<br><a href="https://dev.mysql.com/downloads/repo/yum/" target="_blank" rel="external">https://dev.mysql.com/downloads/repo/yum/</a><br>2.安装yum源<br>yum localinstall  mysql57-community-release-el6-9.noarch.rpm<br>3.安装mysql<br>yum install mysql-community-server</p>
<p>4.创建组和用户<br>groupadd mysql<br>useradd mysql -g mysql</p>
<p>5.修改配置文件开启二进制日志<br>vi /etc/my.cnf  （在[mysqld]下面添加如下内容）<br>server-id=1<br>log-bin=/home/mysql/log/logbin.log</p>
<p>6.开启服务<br>service mysqld start</p>
<p>7.查看mysql默认的密码<br>grep ‘temporary password’ /var/log/mysqld.log</p>
<p>8.根据密码进入mysql<br>mysql -u root -p<br>ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘MyNewPass4!’;</p>
<p>例如：<br>ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘password’;<br>Query OK, 0 rows affected (0.01 sec)</p>
<p>注：MySQL’s validate_password plugin is installed by default. This will require that passwords contain at least one upper case letter, one lower case letter, one digit, and one special character, and that the total password length is at least 8 characters. </p>
<p>9.授权（给其他四台机器授权）<br>grant all privileges on oozie.<em> to ‘oozie’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.</em> to ‘oozie’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.<em> to ‘oozie’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.</em> to ‘oozie’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.* to ‘oozie’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>GRANT all privileges on <em>.</em> to ‘root’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’182.48.105.23’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>grant all privileges on hive.<em> to ‘hive’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.</em> to ‘hive’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.<em> to ‘hive’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.</em> to ‘hive’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.* to ‘hive’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>flush privileges;</p>
<p>关于新版本的账户说明：<br><a href="https://dev.mysql.com/doc/refman/5.7/en/adding-users.html" target="_blank" rel="external">https://dev.mysql.com/doc/refman/5.7/en/adding-users.html</a></p>
<p>10.创建数据库<br>create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</p>
<p>##安装cloudera manager<br>1.下载<br>地址：<a href="http://archive-primary.cloudera.com/cm5/cm/5/" target="_blank" rel="external">http://archive-primary.cloudera.com/cm5/cm/5/</a><br>（博主下载的是cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz 放在/soft/bigdata/clouderamanager下）<br>cd /soft/bigdata/clouderamanager<br>tar -xvf cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz</p>
<p>测试：（可选）<br>cat /etc/passwd</p>
<h2 id="创建用户（所有节点）"><a href="#创建用户（所有节点）" class="headerlink" title="创建用户（所有节点）"></a>创建用户（所有节点）</h2><p>useradd –system –home=/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server –no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm</p>
<p>测试（可选）<br>[root@master1 cloudera-scm-server]# cat /etc/passwd<br>….<br>cloudera-scm:x:498:498:Cloudera SCM User:/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server:/bin/false</p>
<h2 id="修改主机名和端口号"><a href="#修改主机名和端口号" class="headerlink" title="修改主机名和端口号"></a>修改主机名和端口号</h2><p>cd /soft/bigdata/clouderamanager/cm-5.10.0/etc/cloudera-scm-agent<br>vi config.ini </p>
<h1 id="Hostname-of-the-CM-server"><a href="#Hostname-of-the-CM-server" class="headerlink" title="Hostname of the CM server."></a>Hostname of the CM server.</h1><p>server_host=po-master1</p>
<h1 id="Port-that-the-CM-server-is-listening-on"><a href="#Port-that-the-CM-server-is-listening-on" class="headerlink" title="Port that the CM server is listening on."></a>Port that the CM server is listening on.</h1><p>server_port=7182</p>
<h2 id="下载驱动包"><a href="#下载驱动包" class="headerlink" title="下载驱动包"></a>下载驱动包</h2><p>下载mysql-connector-java-*.jar（博主下载的mysql-connector-java-5.1.7-bin.jar）放到<br>/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib  目录下</p>
<h2 id="为Cloudera-Manager-5建立数据库："><a href="#为Cloudera-Manager-5建立数据库：" class="headerlink" title="为Cloudera Manager 5建立数据库："></a>为Cloudera Manager 5建立数据库：</h2><p>/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/schema/scm_prepare_database.sh mysql scm -hlocalhost -uroot -ppassword –scm-host localhost scm password scm<br>格式是:scm_prepare_database.sh 数据库类型  数据库 服务器 用户名 密码  –scm-host  Cloudera_Manager_Server所在的机器，后面那三个不知道代表什么，直接照抄官网的了。 </p>
<p>开启Cloudera Manager 5 Server端：</p>
<p>注意scm见面是两个-，因为博客关系不能全部显示</p>
<h2 id="向其他机器分发CDH"><a href="#向其他机器分发CDH" class="headerlink" title="向其他机器分发CDH"></a>向其他机器分发CDH</h2><p>scp -rp /soft/bigdata/clouderamanager root@po-master2:/soft/bigdata<br>scp -rp /soft/bigdata/clouderamanager root@po-slave1:/soft/bigdata<br>scp -rp /soft/bigdata/clouderamanager root@po-slave2:/soft/bigdata<br>scp -rp /soft/bigdata/clouderamanager root@po-slave3:/soft/bigdata</p>
<h2 id="准备Parcels，用以安装CDH5-（博主放在-soft-bigdata-clouderamanager-cloudera-parcel-repo，路径必须包含cloudera-parcel-repo）"><a href="#准备Parcels，用以安装CDH5-（博主放在-soft-bigdata-clouderamanager-cloudera-parcel-repo，路径必须包含cloudera-parcel-repo）" class="headerlink" title="准备Parcels，用以安装CDH5 （博主放在:/soft/bigdata/clouderamanager/cloudera/parcel-repo，路径必须包含cloudera/parcel-repo）"></a>准备Parcels，用以安装CDH5 （博主放在:/soft/bigdata/clouderamanager/cloudera/parcel-repo，路径必须包含cloudera/parcel-repo）</h2><p>官方地址：<br><a href="http://archive.cloudera.com/cdh5/parcels" target="_blank" rel="external">http://archive.cloudera.com/cdh5/parcels</a><br>博主选择的<br><a href="http://archive.cloudera.com/cdh5/parcels/latest/" target="_blank" rel="external">http://archive.cloudera.com/cdh5/parcels/latest/</a></p>
<p>需要下载以下两个文件<br>•    CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel<br>•    manifest.json</p>
<p>打开 manifest.json找到CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel的hash值里的内容<br>“hash”: “52f95da433f203a05c2fd33eb0f144e6a5c9d558”<br>echo ‘52f95da433f203a05c2fd33eb0f144e6a5c9d558’ &gt;&gt; CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha</p>
<h2 id="测试（可选）"><a href="#测试（可选）" class="headerlink" title="测试（可选）"></a>测试（可选）</h2><p>[root@master1 parcel-repo]# ll<br>total 1466572<br>-rw-r–r– 1 root root 1501694035 Mar  6 14:24 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel<br>-rw-r–r– 1 root root         41 Mar 20 15:26 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha<br>-rw-r–r– 1 root root      64807 Mar 17 17:07 manifest.json</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start（主节点启动）<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start（所有节点上启动）  </p>
<p>测试<br>netstat -an | grep 7182<br>netstat -an | grep 7180 </p>
<h2 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h2><p><a href="http://po-master1:7180" target="_blank" rel="external">http://po-master1:7180</a><br>默认用户密码都是admin</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CDH%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%99%BB%E9%99%86%E9%A1%B5%E9%9D%A2.png" alt="此处输入图片的描述"><br>点击继续<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%862.png" alt="此处输入图片的描述"><br>选择免费的点击继续<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%863.png" alt="此处输入图片的描述"></p>
<p>勾选机器<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%864.png" alt="此处输入图片的描述"></p>
<p>点击更多选项修改parcel路径<br>/soft/bigdata/clouderamanager/cloudera/parcel-repo<br>插入图5<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%865.png" alt="此处输入图片的描述"></p>
<p>需要重启所有节点的服务<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server restart（主节点启动）<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent restart（所有节点上启动） </p>
<p>选择如下内容点击继续<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%866.png" alt="此处输入图片的描述"></p>
<p>等待安装…<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%867.png" alt="此处输入图片的描述"></p>
<p>安装完成，点击继续</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%868.png" alt="此处输入图片的描述"><br>安装过程有个小提示：<br>已启用透明大页面压缩，可能会导致重大性能问题。请运行“echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag”以禁用此设置，然后将同一命令添加到 /etc/rc.local 等初始脚本中，以便在系统重启时予以设置。以下主机将受到影响： </p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%869.png" alt="此处输入图片的描述"><br>选择自定义服务，选择自己需要的服务<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8610.png" alt="此处输入图片的描述"></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8611.png" alt="节点设置"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8612.png" alt="节点设置"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8613.png" alt="数据库设置设置"></p>
<p>等待安装<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8614.png" alt="等待安装"></p>
<p>安装过程中会遇到错误：</p>
<p>是缺少jdbc驱动把文件考入到lib下即可<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8615.png" alt="此处输入图片的描述"></p>
<p>配置NameNode HA<br>进入HDFS界面，点击“启用High Availability”<br><img src="http://oh6ybr0jg.bkt.clouddn.com/HDFS%E5%90%AF%E7%94%A8HA.png" alt="此处输入图片的描述"><br>输入NameService名称，这里设置为：nameservice1，点击继续按钮。<br><img src="http://oh6ybr0jg.bkt.clouddn.com/HDFS%E5%90%AF%E7%94%A8HA2.png" alt="此处输入图片的描述"><br>配置JourNode的路径，(博主修改为/opt/dfs/jn)</p>
<p>错误整理;<br>Fatal error during KafkaServer startup. Prepare to shutdown<br>kafka.common.InconsistentBrokerIdException: Configured broker.id 52 doesn’t match stored broker.id 102 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs).<br>    at kafka.server.KafkaServer.getBrokerId(KafkaServer.scala:648)<br>    at kafka.server.KafkaServer.startup(KafkaServer.scala:187)<br>    at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:37)<br>    at kafka.Kafka$.main(Kafka.scala:67)<br>    at com.cloudera.kafka.wrap.Kafka$.main(Kafka.scala:76)<br>    at com.cloudera.kafka.wrap.Kafka.main(Kafka.scala)</p>
<p>进入到/var/local/kafka/data目录查看meta.propertie里面的kakfa 的broker id是什么</p>
<p>[main]: Metastore Thrift Server threw an exception…<br>javax.jdo.JDOFatalInternalException: Error creating transactional connection factory<br>    at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)<br>    at java.security.AccessController.doPrivileged(Native Method)<br>    at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)<br>    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291)<br>    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)<br>    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)<br>NestedThrowablesStackTrace:<br>java.lang.reflect.InvocationTargetException<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)<br>    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)<br>    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)<br>    at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)<br>    at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)<br>    at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)<br>    at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)<br>    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)<br>    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)<br>    at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)<br>    at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)<br>    at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)<br>    at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)<br>    at java.security.AccessController.doPrivileged(Native Method)<br>    at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)<br>    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291)<br>    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)<br>    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)<br>Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the “BONECP” plugin to create a ConnectionPool gave an error : The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)<br>    … 54 more<br>Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.<br>    at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)<br>    at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)<br>    … 56 more</init></init></init></init></init></init></init></p>
<p>把驱动程序放在<br>/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib</p>
<p>SERVER[po-master1] E0103: Could not load service classes, Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>    at org.apache.oozie.service.Services.loadServices(Services.java:309)<br>    at org.apache.oozie.service.Services.init(Services.java:213)<br>    at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)<br>    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236)<br>    at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739)<br>    at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803)<br>    at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780)<br>    at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)<br>    at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944)<br>    at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779)<br>    at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505)<br>    at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322)<br>    at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325)<br>    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069)<br>    at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061)<br>    at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)<br>    at org.apache.catalina.core.StandardService.start(StandardService.java:525)<br>    at org.apache.catalina.core.StandardServer.start(StandardServer.java:759)<br>    at org.apache.catalina.startup.Catalina.start(Catalina.java:595)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)<br>    at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)<br>Caused by: <openjpa-2.2.2-r422266:1468616 fatal="" general="" error=""> org.apache.openjpa.persistence.PersistenceException: Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>    at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:102)<br>    at org.apache.openjpa.jdbc.conf.JDBCConfigurationImpl.getDBDictionaryInstance(JDBCConfigurationImpl.java:603)<br>    at org.apache.openjpa.jdbc.meta.MappingRepository.endConfiguration(MappingRepository.java:1518)<br>    at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:531)<br>    at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:456)<br>    at org.apache.openjpa.lib.conf.PluginValue.instantiate(PluginValue.java:120)<br>    at org.apache.openjpa.conf.MetaDataRepositoryValue.instantiate(MetaDataRepositoryValue.java:68)<br>    at org.apache.openjpa.lib.conf.ObjectValue.instantiate(ObjectValue.java:83)<br>    at org.apache.openjpa.conf.OpenJPAConfigurationImpl.newMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:967)<br>    at org.apache.openjpa.conf.OpenJPAConfigurationImpl.getMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:958)<br>    at org.apache.openjpa.kernel.AbstractBrokerFactory.makeReadOnly(AbstractBrokerFactory.java:644)<br>    at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:203)<br>    at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:156)<br>    at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:227)<br>    at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:154)<br>    at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:60)<br>    at org.apache.oozie.service.JPAService.getEntityManager(JPAService.java:514)<br>    at org.apache.oozie.service.JPAService.init(JPAService.java:215)<br>    at org.apache.oozie.service.Services.setServiceInternal(Services.java:386)<br>    at org.apache.oozie.service.Services.setService(Services.java:372)<br>    at org.apache.oozie.service.Services.loadServices(Services.java:305)<br>    … 26 more<br>Caused by: org.apache.commons.dbcp.SQLNestedException: Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>    at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1429)<br>    at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1371)<br>    at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044)<br>    at org.apache.openjpa.lib.jdbc.DelegatingDataSource.getConnection(DelegatingDataSource.java:110)<br>    at org.apache.openjpa.lib.jdbc.DecoratingDataSource.getConnection(DecoratingDataSource.java:87)<br>    at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:91)<br>    … 46 more<br>Caused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver<br>    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680)<br>    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526)<br>    at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1420)<br>    … 51 more</openjpa-2.2.2-r422266:1468616></p>
<p>把mysql-connector-java.jar，mysql-connector-java-5.1.39.jar驱动程序放在：<br>/var/lib/oozie</p>
<p>[main]: Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidates<br>Required table missing : “<code>DBS</code>“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables”<br>org.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : “<code>DBS</code>“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables”<br>    at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:485)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3380)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)<br>    at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)<br>    at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)<br>    at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)<br>    at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)<br>    at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)<br>    at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)<br>    at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)<br>    at org.datanucleus.store.query.Query.execute(Query.java:1654)<br>    at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)<br>    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:185)<br>    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:136)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:340)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291)<br>    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)<br>    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</init></init></init></p>
<pre><code>SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table &apos;oozie.VALIDATE_CONN&apos; doesn&apos;t exist)
</code></pre><p>org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist)<br>    at org.apache.oozie.service.Services.loadServices(Services.java:309)<br>    at org.apache.oozie.service.Services.init(Services.java:213)<br>    at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)<br>    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236)<br>    at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739)<br>    at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803)<br>    at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780)<br>    at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)<br>    at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944)<br>    at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779)<br>    at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505)<br>    at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322)<br>    at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325)<br>    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069)<br>    at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061)<br>    at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)<br>    at org.apache.catalina.core.StandardService.start(StandardService.java:525)<br>    at org.apache.catalina.core.StandardServer.start(StandardServer.java:759)<br>    at org.apache.catalina.startup.Catalina.start(Catalina.java:595)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)<br>    at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)</p>
<p>报这个错误需要修改hive的配置。搜索autoCreateSchema 改为true</p>
<p>SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist)<br>org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist)<br>    at org.apache.oozie.service.Services.loadServices(Services.java:309)<br>    at org.apache.oozie.service.Services.init(Services.java:213)<br>    at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)<br>    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236)<br>    at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739)<br>    at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803)<br>    at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780)<br>    at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)<br>    at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944)<br>    at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779)<br>    at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505)<br>    at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322)<br>    at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325)<br>    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069)<br>    at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061)<br>    at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)<br>    at org.apache.catalina.core.StandardService.start(StandardService.java:525)<br>    at org.apache.catalina.core.StandardServer.start(StandardServer.java:759)<br>    at org.apache.catalina.startup.Catalina.start(Catalina.java:595)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)<br>    at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)</p>
<p>点击界面上的Oozie 点击操作，创建Oozie数据库表</p>
<p>最后导入环境变量就可以测试了<br>export ZK_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/<br>export HBASE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hbase/<br>export HADOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/<br>export HIVE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/<br>export SQOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/<br>export OOZIE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/oozie/<br>export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$OOZIE_HOME/bin:$ZK_HOME/bin:$HBASE_HOME/bin</p>
<h2 id="最后测试阶段，可忽略，本文完。"><a href="#最后测试阶段，可忽略，本文完。" class="headerlink" title="最后测试阶段，可忽略，本文完。"></a>最后测试阶段，可忽略，本文完。</h2><h4 id="测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）："><a href="#测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）：" class="headerlink" title="测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）："></a>测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-slave1 data]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: leader</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-slave2 data]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-slave3 dfs]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">[root@po-slave3 dfs]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div><div class="line">[root@po-slave3 dfs]<span class="comment"># zkCli.sh </span></div><div class="line">Connecting to localhost:2181</div><div class="line">2017-03-21 16:05:56,829 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.10.0--1, built on 01/20/2017 20:10 GMT</div><div class="line">2017-03-21 16:05:56,832 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=po-slave3</div><div class="line">2017-03-21 16:05:56,832 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_121</div><div class="line">2017-03-21 16:05:56,834 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2017-03-21 16:05:56,834 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/soft/java/jdk/jre</div><div class="line">2017-03-21 16:05:56,834 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/classes:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/<span class="built_in">log</span>4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../src/java/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf:.:/soft/java/jdk/lib</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-642.13.1.el6.x86_64</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=root</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/root</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/opt/dfs</div><div class="line">2017-03-21 16:05:56,836 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain<span class="variable">$MyWatcher</span>@506c589e</div><div class="line">Welcome to ZooKeeper!</div><div class="line">2017-03-21 16:05:56,873 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn<span class="variable">$SendThread</span>@975] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)</div><div class="line">JLine support is enabled</div><div class="line">2017-03-21 16:05:56,935 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn<span class="variable">$SendThread</span>@852] - Socket connection established, initiating session, client: /127.0.0.1:42694, server: localhost/127.0.0.1:2181</div><div class="line">2017-03-21 16:05:56,941 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn<span class="variable">$SendThread</span>@1235] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15aeb7f0edb054c, negotiated timeout = 30000</div><div class="line"></div><div class="line">WATCHER::</div><div class="line"></div><div class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</div><div class="line">[zk: localhost:2181(CONNECTED) 0] ls /</div><div class="line">[controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, config, hbase]</div><div class="line">[zk: localhost:2181(CONNECTED) 1]</div></pre></td></tr></table></figure>
<h4 id="测试hdfs"><a href="#测试hdfs" class="headerlink" title="测试hdfs"></a>测试hdfs</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 ~]<span class="comment"># hadoop dfs -ls /</span></div><div class="line">DEPRECATED: Use of this script to execute hdfs <span class="built_in">command</span> is deprecated.</div><div class="line">Instead use the hdfs <span class="built_in">command</span> <span class="keyword">for</span> it.</div><div class="line"></div><div class="line">Found 3 items</div><div class="line">drwxr-xr-x   - hbase hbase               0 2017-03-21 10:30 /hbase</div><div class="line">drwxrwxrwt   - hdfs  supergroup          0 2017-03-20 17:06 /tmp</div><div class="line">drwxr-xr-x   - hdfs  supergroup          0 2017-03-20 17:06 /user</div><div class="line"></div><div class="line">[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -mkdir /data2</div><div class="line">[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -put hdfs-audit.log /data2/hdfs-audit.log</div><div class="line">[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -ls /data2</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup    2908825 2017-03-21 17:28 /data2/hdfs-audit.log</div></pre></td></tr></table></figure>
<p>测试网页</p>
<h4 id="测试hadoop页面"><a href="#测试hadoop页面" class="headerlink" title="测试hadoop页面"></a>测试hadoop页面</h4><p><a href="http://po-master1:50030/jobtracker.jsp" target="_blank" rel="external">http://po-master1:50030/jobtracker.jsp</a><br><img src="http://oh6ybr0jg.bkt.clouddn.com/hadoop%E9%A1%B5%E9%9D%A2.png" alt="此处输入图片的描述"></p>
<h4 id="测试hive"><a href="#测试hive" class="headerlink" title="测试hive"></a>测试hive</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 ~]<span class="comment"># hive</span></div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed <span class="keyword">in</span> 8.0</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed <span class="keyword">in</span> a future release</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed <span class="keyword">in</span> 8.0</div><div class="line"></div><div class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.properties</div><div class="line">WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</div><div class="line">hive&gt; show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">Time taken: 1.836 seconds, Fetched: 1 row(s)</div><div class="line">hive&gt; create database <span class="built_in">test</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.06 seconds</div><div class="line">hive&gt; drop database <span class="built_in">test</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.184 seconds</div></pre></td></tr></table></figure>
<h4 id="测试hbase"><a href="#测试hbase" class="headerlink" title="测试hbase"></a>测试hbase</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 ~]<span class="comment"># hbase shell</span></div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed <span class="keyword">in</span> a future release</div><div class="line">17/03/21 16:20:23 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available</div><div class="line">HBase Shell; enter <span class="string">'help&lt;RETURN&gt;'</span> <span class="keyword">for</span> list of supported commands.</div><div class="line">Type <span class="string">"exit&lt;RETURN&gt;"</span> to leave the HBase Shell</div><div class="line">Version 1.2.0-cdh5.10.0, rUnknown, Fri Jan 20 12:13:18 PST 2017</div><div class="line">hbase(main):001:0&gt; list</div><div class="line">TABLE                                                                                                           0 row(s) <span class="keyword">in</span> 0.2020 seconds</div><div class="line">=&gt; []</div><div class="line">hbase(main):002:0&gt; create <span class="string">'t1'</span>,<span class="string">'id'</span>,<span class="string">'name'</span></div><div class="line">0 row(s) <span class="keyword">in</span> 2.3540 seconds</div><div class="line">=&gt; Hbase::Table - t1</div><div class="line">hbase(main):003:0&gt; list</div><div class="line">TABLE                                                                                                           t1                                                                                                              1 row(s) <span class="keyword">in</span> 0.0100 seconds</div><div class="line">=&gt; [<span class="string">"t1"</span>]</div><div class="line">hbase(main):004:0&gt;</div></pre></td></tr></table></figure>
<h4 id="卸载安装："><a href="#卸载安装：" class="headerlink" title="卸载安装："></a>卸载安装：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">umount /soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-agent/process</div></pre></td></tr></table></figure>
<h2 id="问题汇总："><a href="#问题汇总：" class="headerlink" title="问题汇总："></a>问题汇总：</h2><p>如果启动CDH后无法点击HDFS 的WEB UI，查看端口又是被监听<br>是因为需要修改配置：<br>![此处输入图片的描述][19]</p>
<p>同理job 的web UI也需要修改<br>![此处输入图片的描述][20]</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;运行环境：&quot;&gt;&lt;a href=&quot;#运行环境：&quot; class=&quot;headerlink&quot; title=&quot;运行环境：&quot;&gt;&lt;/a&gt;运行环境：&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机IP&lt;/th&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;内存&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.147&lt;/td&gt;
&lt;td&gt;po-master1&lt;/td&gt;
&lt;td&gt;16G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.127&lt;/td&gt;
&lt;td&gt;po-master2&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.118&lt;/td&gt;
&lt;td&gt;po-slave1&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.92&lt;/td&gt;
&lt;td&gt;po-slave2&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.230&lt;/td&gt;
&lt;td&gt;po-slave3&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;配置主机名-分别在五台机器上执行&quot;&gt;&lt;a href=&quot;#配置主机名-分别在五台机器上执行&quot; class=&quot;headerlink&quot; title=&quot;配置主机名(分别在五台机器上执行)&quot;&gt;&lt;/a&gt;配置主机名(分别在五台机器上执行)&lt;/h2&gt;&lt;p&gt;vi /etc/sysconfig/network&lt;br&gt;hostname +主机名&lt;br&gt;例如： hostname po-master1&lt;/p&gt;
&lt;h2 id=&quot;配置映射关系-把以下五条命令在五台机器上执行&quot;&gt;&lt;a href=&quot;#配置映射关系-把以下五条命令在五台机器上执行&quot; class=&quot;headerlink&quot; title=&quot;配置映射关系(把以下五条命令在五台机器上执行)&quot;&gt;&lt;/a&gt;配置映射关系(把以下五条命令在五台机器上执行)&lt;/h2&gt;&lt;p&gt;echo “1.1.1.147   po-master1”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.127   po-master2”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.118  po-slave1”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.92   po-slave2”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.230  po-slave3”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
</feed>
