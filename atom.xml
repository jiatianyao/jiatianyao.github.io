<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张洪铭的个人博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-21T15:51:58.692Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>张洪铭</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python numpy</title>
    <link href="http://yoursite.com/2018/01/10/python%20numpy/"/>
    <id>http://yoursite.com/2018/01/10/python numpy/</id>
    <published>2018-01-10T03:30:00.000Z</published>
    <updated>2018-01-21T15:51:58.692Z</updated>
    
    <content type="html"><![CDATA[<p>执行：jupyter notebook</p>
<p>1.arange<br>生成数组<br>np.arange(10)<br>array([0,1,2,3,4,5,6,7,8,9])</p>
<p>2.等差数列是指从第二项起，每一项与它的前一项的差等于同一个常数的一种数列，常用A、P表示<br>等差数列 linspace<br>np.linspace(1,10,5)<br>array([1. , 3.25 ,5.5,5.75,10])<br><a id="more"></a><br>np.linspace(1,10,5,endpoint=False)  相当于生成6个数只显示前五个<br>array([1. , 2.8 ,4.6,6.5,8.2])</p>
<p>3.等比数列是指从第二项起，每一项与它的前一项的比值等于同一个常数的一种数列<br>等比数列logspace<br>np.logspace(1,10,5)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;执行：jupyter notebook&lt;/p&gt;
&lt;p&gt;1.arange&lt;br&gt;生成数组&lt;br&gt;np.arange(10)&lt;br&gt;array([0,1,2,3,4,5,6,7,8,9])&lt;/p&gt;
&lt;p&gt;2.等差数列是指从第二项起，每一项与它的前一项的差等于同一个常数的一种数列，常用A、P表示&lt;br&gt;等差数列 linspace&lt;br&gt;np.linspace(1,10,5)&lt;br&gt;array([1. , 3.25 ,5.5,5.75,10])&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python—pandas</title>
    <link href="http://yoursite.com/2018/01/08/python%E2%80%94pandas/"/>
    <id>http://yoursite.com/2018/01/08/python—pandas/</id>
    <published>2018-01-08T01:30:00.000Z</published>
    <updated>2018-01-19T16:02:42.664Z</updated>
    
    <content type="html"><![CDATA[<p>from pandas import Series,DataFrame<br>import pandas as pd</p>
<h3 id="Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。-索引可重复"><a href="#Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。-索引可重复" class="headerlink" title="Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。  索引可重复"></a>Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。  索引可重复</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> Series,DataFrame</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">arr = np.array([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,np.NaN,<span class="number">10</span>])</div><div class="line">serises0=Series(arr)</div><div class="line">serises0</div></pre></td></tr></table></figure>
<p>0     1.0<br>1     3.0<br>2     5.0<br>3     NaN<br>4    10.0<br>dtype: float64<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">serises0.dtype</div></pre></td></tr></table></figure></p>
<p>dtype(‘float64’)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">serises0.index</div></pre></td></tr></table></figure>
<p>RangeIndex(start=0, stop=5, step=1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">serises0.values</div></pre></td></tr></table></figure>
<p>array([  1.,   3.,   5.,  nan,  10.])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">serises1=Series(data=[<span class="number">91</span>,<span class="number">92</span>,<span class="number">93</span>],dtype=np.float64,index=[<span class="string">u'数学'</span>,<span class="string">u'语文'</span>,<span class="string">u'外语'</span>])</div><div class="line">serises1</div></pre></td></tr></table></figure>
<p>数学    91.0<br>语文    92.0<br>外语    93.0<br>dtype: float64</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dict0=&#123;<span class="string">u'数学'</span>:<span class="number">91.0</span>,<span class="string">u'语文'</span>:<span class="number">92</span>,<span class="string">u'外语'</span>:<span class="number">93</span>&#125;</div><div class="line">dict0</div></pre></td></tr></table></figure>
<p>{u’\u5916\u8bed’: 93, u’\u6570\u5b66’: 91.0, u’\u8bed\u6587’: 92}</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">serises2=Series(dict0)</div><div class="line">serises2</div></pre></td></tr></table></figure>
<p>外语    93.0<br>数学    91.0<br>语文    92.0<br>dtype: float64</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">serises2[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>93.0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">serises2[<span class="string">u'外语'</span>]</div></pre></td></tr></table></figure>
<p>93.0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">serises2[<span class="string">'外语'</span>:<span class="string">'语文'</span>]</div></pre></td></tr></table></figure>
<p>外语    93.0<br>数学    91.0<br>语文    92.0<br>dtype: float64</p>
<p>Series运算，自动对齐索引<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">serises2=Series(data=[<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>],dtype=np.float64,index=[<span class="string">'p1'</span>,<span class="string">'p2'</span>,<span class="string">'p3'</span>])</div><div class="line">serises3=Series(data=[<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>,<span class="number">25</span>],dtype=np.float64,index=[<span class="string">'p2'</span>,<span class="string">'p3'</span>,<span class="string">'p4'</span>,<span class="string">'p5'</span>])</div><div class="line">serises2 +serises3</div></pre></td></tr></table></figure></p>
<p>p1     NaN<br>p2    34.0<br>p3    36.0<br>p4     NaN<br>p5     NaN<br>dtype: float64</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">serises1.name=<span class="string">'name'</span></div><div class="line">serises1.index.name=<span class="string">'考试科目'</span></div><div class="line">serises1</div></pre></td></tr></table></figure>
<p>考试科目<br>数学    91.0<br>语文    92.0<br>外语    93.0<br>Name: name, dtype: float64</p>
<h3 id="DataFrame-：-表格形式的数据结构，包含一组有序的列，每列可以是不同的值类型，DataFrame既有行索引又有列索引，可以看做是由Series组成的字典"><a href="#DataFrame-：-表格形式的数据结构，包含一组有序的列，每列可以是不同的值类型，DataFrame既有行索引又有列索引，可以看做是由Series组成的字典" class="headerlink" title="DataFrame ： 表格形式的数据结构，包含一组有序的列，每列可以是不同的值类型，DataFrame既有行索引又有列索引，可以看做是由Series组成的字典"></a>DataFrame ： 表格形式的数据结构，包含一组有序的列，每列可以是不同的值类型，DataFrame既有行索引又有列索引，可以看做是由Series组成的字典</h3><p>通过二维数组创建<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df01=DataFrame([[<span class="string">'Tom'</span>,<span class="string">'John'</span>],[<span class="number">88</span>,<span class="number">90</span>]])</div><div class="line">df01</div></pre></td></tr></table></figure></p>
<pre><code>0    1
</code></pre><p>0    Tom    John<br>1    88    90</p>
<p>通过字典创建<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">data=&#123;<span class="string">'Tom'</span>:[<span class="number">88</span>,<span class="number">55</span>],<span class="string">'John'</span>:[<span class="number">90</span>,<span class="number">22</span>]&#125;</div><div class="line">df02=DataFrame(data)</div><div class="line">df02</div></pre></td></tr></table></figure></p>
<p>   John    Tom<br>0    90    88<br>1    22    55</p>
<p>DataFrame 可以增加数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">df02.ix[<span class="string">'2'</span>]=np.NaN</div></pre></td></tr></table></figure></p>
<pre><code>John    Tom
</code></pre><p>0    90.0    88.0<br>1    22.0    55.0<br>2    NaN        NaN</p>
<p>数据删除<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df02=df02.dropna()</div><div class="line">df02</div></pre></td></tr></table></figure></p>
<pre><code>John    Tom
</code></pre><p>0    90.0    88.0<br>1    22.0    55.0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">arr1=np.random.randint(<span class="number">5</span>,<span class="number">10</span>,(<span class="number">4</span>,<span class="number">4</span>))</div><div class="line">df1=pd.DataFrame(arr1)</div><div class="line">df1</div><div class="line">df1.ix[:<span class="number">2</span>,<span class="number">1</span>]=np.NAN</div><div class="line">df1.ix[:<span class="number">1</span>,<span class="number">2</span>]=np.NAN</div><div class="line">df1</div></pre></td></tr></table></figure>
<pre><code>0    1    2    3
</code></pre><p>0    8    NaN    NaN    7<br>1    5    NaN    NaN    9<br>2    5    NaN    5.0    5<br>3    9    8.0    7.0    5</p>
<h3 id="loc-iloc"><a href="#loc-iloc" class="headerlink" title="loc iloc"></a>loc iloc</h3><p>iloc 对于下标进行操作<br>loc 对于索引值进行操作</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;from pandas import Series,DataFrame&lt;br&gt;import pandas as pd&lt;/p&gt;
&lt;h3 id=&quot;Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。-索引可重复&quot;&gt;&lt;a href=&quot;#Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。-索引可重复&quot; class=&quot;headerlink&quot; title=&quot;Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。  索引可重复&quot;&gt;&lt;/a&gt;Series：一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。  索引可重复&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Series,DataFrame&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;arr = np.array([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;,np.NaN,&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;])&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;serises0=Series(arr)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;serises0&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;0     1.0&lt;br&gt;1     3.0&lt;br&gt;2     5.0&lt;br&gt;3     NaN&lt;br&gt;4    10.0&lt;br&gt;dtype: float64&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>spark 机器学习入门(二)</title>
    <link href="http://yoursite.com/2017/10/18/spark%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2017/10/18/spark 性能优化/</id>
    <published>2017-10-18T06:30:00.000Z</published>
    <updated>2017-11-28T13:12:37.521Z</updated>
    
    <content type="html"><![CDATA[<h5 id="部署优化："><a href="#部署优化：" class="headerlink" title="部署优化："></a><strong>部署优化：</strong></h5><p>磁盘：<br>挂载磁盘时使用noatime和nodiratime选项减少写的开销</p>
<p>linux每个文件都会保留3个时间戳<br>用stat 文件名 来查看<br>Acess：文件访问时间<br>Modfiy：内容修改时间<br>Change：文件名修改时间</p>
<p>参数含义：<br>磁盘下的所有文件不更新访问时间<br><a id="more"></a><br>内存：<br>JVM 内存不建议每个executor 超过200G</p>
<p>CPU<br>每台机器的Vcore数不建议小于8</p>
<h2 id="JOB调度："><a href="#JOB调度：" class="headerlink" title="JOB调度："></a><strong>JOB调度：</strong></h2><p>Fail Schedule 可最大程度保证各个Job都有机会获取资源</p>
<h2 id="数据序列化："><a href="#数据序列化：" class="headerlink" title="数据序列化："></a><strong>数据序列化：</strong></h2><p>Kyro serialization序列化速度更快，结果更紧凑<br>为了更好的性能，需提前注册被序列化的类，否则会存在大量的空间浪费<br>通过spark.serializer指定</p>
<h2 id="减少内存消耗："><a href="#减少内存消耗：" class="headerlink" title="减少内存消耗："></a><strong>减少内存消耗：</strong></h2><p>尽量使用基本数据类型和数组，避免使用java集合类<br>尽量减少包含大量小对象的嵌套结构<br>Key尽量使用数值或枚举类型而不是字符串<br>RAM小于32GB时，使用-XX:+UseCompressedOops使用4字节（而非8字节）的指针</p>
<h2 id="调整并行度："><a href="#调整并行度：" class="headerlink" title="调整并行度："></a><strong>调整并行度：</strong></h2><p>调整Map侧并行度<br>对于kafka direct stream 可通过调整Topic的Patition个数调整Spark Map侧并行度<br>对于spark.textFile，通过参数调整</p>
<p>调整Reduce侧并行度<br>通过spark.default.parallelism设置shuffle时默认并行度</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;部署优化：&quot;&gt;&lt;a href=&quot;#部署优化：&quot; class=&quot;headerlink&quot; title=&quot;部署优化：&quot;&gt;&lt;/a&gt;&lt;strong&gt;部署优化：&lt;/strong&gt;&lt;/h5&gt;&lt;p&gt;磁盘：&lt;br&gt;挂载磁盘时使用noatime和nodiratime选项减少写的开销&lt;/p&gt;
&lt;p&gt;linux每个文件都会保留3个时间戳&lt;br&gt;用stat 文件名 来查看&lt;br&gt;Acess：文件访问时间&lt;br&gt;Modfiy：内容修改时间&lt;br&gt;Change：文件名修改时间&lt;/p&gt;
&lt;p&gt;参数含义：&lt;br&gt;磁盘下的所有文件不更新访问时间&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark 机器学习入门(一)</title>
    <link href="http://yoursite.com/2017/10/18/spark%20ml1/"/>
    <id>http://yoursite.com/2017/10/18/spark ml1/</id>
    <published>2017-10-18T03:30:00.000Z</published>
    <updated>2017-10-22T11:03:05.867Z</updated>
    
    <content type="html"><![CDATA[<p>目前基于RDD的MLlib已经进入维护莫斯。大概在spark2.3基于RDD的MLlib API将要被废弃。未来是基于DataFrame的API</p>
<p>1.基本统计<br>计算两组数据之间的相关性</p>
<h3 id="皮尔森相关系数（Pearson-correlation-coefficient）"><a href="#皮尔森相关系数（Pearson-correlation-coefficient）" class="headerlink" title="皮尔森相关系数（Pearson correlation coefficient）"></a>皮尔森相关系数（Pearson correlation coefficient）</h3><p>也称皮尔森积矩相关系数(Pearson product-moment correlation coefficient) ，是一种线性相关系数。皮尔森相关系数是用来反映两个变量线性相关程度的统计量。相关系数用r表示，其中n为样本量，分别为两个变量的观测值和均值。r描述的是两个变量间线性相关强弱的程度。r的绝对值越大表明相关性越强</p>
<p><img src="http://segmentfault.com/img/cGNupC" alt="此处输入图片的描述"><br><img src="http://segmentfault.com/img/eOV3Oj" alt="此处输入图片的描述"><br><img src="http://segmentfault.com/img/lJqNL" alt="此处输入图片的描述"><br><a id="more"></a><br>按照高中数学水平来理解, 它很简单, 可以看做将两组数据首先做Z分数处理之后, 然后两组数据的乘积和除以样本数</p>
<p>Z分数一般代表正态分布中, 数据偏离中心点的距离.等于变量减掉平均数再除以标准差.(就是高考的标准分类似的处理)</p>
<p>标准差则等于变量减掉平均数的平方和,再除以样本数,最后再开方.</p>
<p>所以, 根据这个最朴素的理解,我们可以将公式依次精简为:</p>
<p><img src="http://segmentfault.com/img/bKDASK" alt="此处输入图片的描述"></p>
<p>spearman相关系数：是衡量分级定序变量之间的相关程度的统计量，对不服从正态分布的资料、原始资料等级资料、一侧开口资料、总体分布类型未知的资料不符合使用积矩相关系数来描述关联性。此时可采用秩相关（rank correlation），也称等级相关，来描述两个变量之间的关联程度与方向。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.&#123;<span class="type">Matrix</span>, <span class="type">Vectors</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.stat.<span class="type">Correlation</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">ml_1</span> </span>&#123;</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></div><div class="line">      .builder</div><div class="line">      .master(<span class="string">"local"</span>)</div><div class="line">      .getOrCreate()</div><div class="line">    <span class="keyword">import</span> spark.implicits._</div><div class="line"></div><div class="line">    <span class="comment">// $example on$</span></div><div class="line">    <span class="keyword">val</span> data = <span class="type">Seq</span>(</div><div class="line">      <span class="type">Vectors</span>.sparse(<span class="number">4</span>, <span class="type">Seq</span>((<span class="number">0</span>, <span class="number">1.0</span>), (<span class="number">3</span>, <span class="number">-2.0</span>))),</div><div class="line">      <span class="type">Vectors</span>.dense(<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>),</div><div class="line">      <span class="type">Vectors</span>.dense(<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">0.0</span>, <span class="number">8.0</span>),</div><div class="line">      <span class="type">Vectors</span>.sparse(<span class="number">4</span>, <span class="type">Seq</span>((<span class="number">0</span>, <span class="number">9.0</span>), (<span class="number">3</span>, <span class="number">1.0</span>)))</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="keyword">val</span> df = data.map(<span class="type">Tuple1</span>.apply).toDF(<span class="string">"features"</span>)</div><div class="line">    <span class="keyword">val</span> <span class="type">Row</span>(coeff1: <span class="type">Matrix</span>) = <span class="type">Correlation</span>.corr(df, <span class="string">"features"</span>).head</div><div class="line">    println(<span class="string">"Pearson correlation matrix:\n"</span> + coeff1.toString)</div><div class="line">    </div><div class="line">    <span class="keyword">val</span> <span class="type">Row</span>(coeff2: <span class="type">Matrix</span>) = <span class="type">Correlation</span>.corr(df, <span class="string">"features"</span>, <span class="string">"spearman"</span>).head</div><div class="line">    println(<span class="string">"Spearman correlation matrix:\n"</span> + coeff2.toString)</div><div class="line">    <span class="comment">// $example off$</span></div><div class="line"></div><div class="line">    spark.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>1.0为第一列和第一列计算<br>0.055641488407465814为第一列和第二列计算<br>0.4004714203168137 为第三列和第四列计算</p>
<p>计算结果:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="type">Pearson</span> correlation matrix:</div><div class="line"><span class="number">1.0</span>                   <span class="number">0.055641488407465814</span>  <span class="type">NaN</span>  <span class="number">0.4004714203168137</span> </div><div class="line"><span class="number">0.055641488407465814</span>  <span class="number">1.0</span>                   <span class="type">NaN</span>  <span class="number">0.9135958615342522</span>  </div><div class="line"><span class="type">NaN</span>                   <span class="type">NaN</span>                   <span class="number">1.0</span>  <span class="type">NaN</span>                 </div><div class="line"><span class="number">0.4004714203168137</span>    <span class="number">0.9135958615342522</span>    <span class="type">NaN</span>  <span class="number">1.0</span>                 </div><div class="line"><span class="type">Spearman</span> correlation matrix:</div><div class="line"><span class="number">1.0</span>                  <span class="number">0.10540925533894532</span>  <span class="type">NaN</span>  <span class="number">0.40000000000000174</span>  </div><div class="line"><span class="number">0.10540925533894532</span>  <span class="number">1.0</span>                  <span class="type">NaN</span>  <span class="number">0.9486832980505141</span>   </div><div class="line"><span class="type">NaN</span>                  <span class="type">NaN</span>                  <span class="number">1.0</span>  <span class="type">NaN</span>                  </div><div class="line"><span class="number">0.40000000000000174</span>  <span class="number">0.9486832980505141</span>   <span class="type">NaN</span>  <span class="number">1.0</span></div></pre></td></tr></table></figure></p>
<h4 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a><strong>卡方检验</strong></h4><p>卡方检验是用途非常广的一种假设检验方法，它在分类资料统计推断中的应用，包括：两个率或两个构成比比较的卡方检验；多个率或多个构成比比较的卡方检验以及分类资料的相关分析等。<br>卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vectors</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.stat.<span class="type">ChiSquareTest</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by Administrator on 2017/10/18.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">ChiSquareTest</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></div><div class="line">      .builder</div><div class="line">      .master(<span class="string">"local"</span>)</div><div class="line">      .getOrCreate()</div><div class="line">    <span class="keyword">import</span> spark.implicits._</div><div class="line"></div><div class="line">    <span class="keyword">val</span> data = <span class="type">Seq</span>(</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">0.5</span>, <span class="number">10.0</span>)),</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">1.5</span>, <span class="number">20.0</span>)),</div><div class="line">      (<span class="number">1.0</span>, <span class="type">Vectors</span>.dense(<span class="number">1.5</span>, <span class="number">30.0</span>)),</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">3.5</span>, <span class="number">30.0</span>)),</div><div class="line">      (<span class="number">0.0</span>, <span class="type">Vectors</span>.dense(<span class="number">3.5</span>, <span class="number">40.0</span>)),</div><div class="line">      (<span class="number">1.0</span>, <span class="type">Vectors</span>.dense(<span class="number">3.5</span>, <span class="number">40.0</span>))</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="keyword">val</span> df = data.toDF(<span class="string">"label"</span>, <span class="string">"features"</span>)</div><div class="line">    <span class="keyword">val</span> chi = <span class="type">ChiSquareTest</span>.test(df, <span class="string">"features"</span>, <span class="string">"label"</span>).head</div><div class="line">    println(<span class="string">"pValues = "</span> + chi.getAs[<span class="type">Vector</span>](<span class="number">0</span>))</div><div class="line">    println(<span class="string">"degreesOfFreedom = "</span> + chi.getSeq[<span class="type">Int</span>](<span class="number">1</span>).mkString(<span class="string">"["</span>, <span class="string">","</span>, <span class="string">"]"</span>))</div><div class="line">    println(<span class="string">"statistics = "</span> + chi.getAs[<span class="type">Vector</span>](<span class="number">2</span>))</div><div class="line"></div><div class="line">    spark.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>结果：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pValues = [<span class="number">0.6872892787909721</span>,<span class="number">0.6822703303362126</span>]</div><div class="line">degreesOfFreedom = [<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">statistics = [<span class="number">0.75</span>,<span class="number">1.5</span>]</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前基于RDD的MLlib已经进入维护莫斯。大概在spark2.3基于RDD的MLlib API将要被废弃。未来是基于DataFrame的API&lt;/p&gt;
&lt;p&gt;1.基本统计&lt;br&gt;计算两组数据之间的相关性&lt;/p&gt;
&lt;h3 id=&quot;皮尔森相关系数（Pearson-correlation-coefficient）&quot;&gt;&lt;a href=&quot;#皮尔森相关系数（Pearson-correlation-coefficient）&quot; class=&quot;headerlink&quot; title=&quot;皮尔森相关系数（Pearson correlation coefficient）&quot;&gt;&lt;/a&gt;皮尔森相关系数（Pearson correlation coefficient）&lt;/h3&gt;&lt;p&gt;也称皮尔森积矩相关系数(Pearson product-moment correlation coefficient) ，是一种线性相关系数。皮尔森相关系数是用来反映两个变量线性相关程度的统计量。相关系数用r表示，其中n为样本量，分别为两个变量的观测值和均值。r描述的是两个变量间线性相关强弱的程度。r的绝对值越大表明相关性越强&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://segmentfault.com/img/cGNupC&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://segmentfault.com/img/eOV3Oj&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://segmentfault.com/img/lJqNL&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark shuffle 调优</title>
    <link href="http://yoursite.com/2017/08/30/spark%20shuffle%20%E8%B0%83%E4%BC%98/"/>
    <id>http://yoursite.com/2017/08/30/spark shuffle 调优/</id>
    <published>2017-08-30T03:30:00.000Z</published>
    <updated>2017-09-17T12:04:23.244Z</updated>
    
    <content type="html"><![CDATA[<h2 id="shuffle实现的具体过程"><a href="#shuffle实现的具体过程" class="headerlink" title="shuffle实现的具体过程"></a>shuffle实现的具体过程</h2><p>1.Spark系统在运行含shuffle过程的应用时，Executor进程除了运行task，还要负责写shuffle 数据，给其他Executor提供shuffle数据。<br>    当Executor进程任务过重，导致GC而不能为其 他Executor提供shuffle数据时，会影响任务运行。<br>    这里实际上是利用External Shuffle Service 来提升性能，External shuffle Service是长期存在于NodeManager进程中的一个辅助服务。<br>    通过该服务 来抓取shuffle数据，减少了Executor的压力，在Executor GC的时候也不会影响其他 Executor的任务运行。</p>
<p>启用方法：</p>
<h3 id="一-在NodeManager中启动External-shuffle-Service。"><a href="#一-在NodeManager中启动External-shuffle-Service。" class="headerlink" title="一. 在NodeManager中启动External shuffle Service。"></a>一. 在NodeManager中启动External shuffle Service。</h3><pre><code>a. 在“yarn-site.xml”中添加如下配置项：
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;spark_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;spark.shuffle.service.port&lt;/name&gt;
&lt;value&gt;7337&lt;/value&gt;
&lt;/property&gt;
配置参数描述
yarn.nodemanager.aux-services  ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle计算性能。
yarn.nodemanager.auxservices.spark_shuffle.class ：NodeManager中辅助服务对应的类。
spark.shuffle.service.port ：Shuffle服务监听数据获取请求的端口。可选配置，默认值为“7337”。
b. 添加依赖的jar包
拷贝“${SPARK_HOME}/lib/spark-1.3.0-yarn-shuffle.jar”到“${HADOOP_HOME}/share/hadoop/yarn/lib/”目录下。
c. 重启NodeManager进程，也就启动了External shuffle Service。
</code></pre><a id="more"></a>
<h3 id="二-Spark应用使用External-shuffle-Service。"><a href="#二-Spark应用使用External-shuffle-Service。" class="headerlink" title="二. Spark应用使用External shuffle Service。"></a>二. Spark应用使用External shuffle Service。</h3><pre><code>在“spark-defaults.conf”中必须添加如下配置项： 
    spark.shuffle.service.enabled true 
    spark.shuffle.service.port 7337 
说明 
1.如果1.如果“yarn.nodemanager.aux-services”配置项已存在，则在value中添加 “spark_shuffle”，且用逗号和其他值分开。 
2.“spark.shuffle.service.port”的值需要和上面“yarn-site.xml”中的值一样。 
配置参数描述 
spark.shuffle.service.enabled   ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle 计算性能。默认为false，表示不启用该功能。 
spark.shuffle.service.port   ：Shuffle服务监听数据获取请求的端口。可选配置，默认值 为“7337”。
</code></pre><p>Hash Shuffle不足<br>map task会根据reduce的数量（partition） 生成相应的bucket 写shuffle blockFile</p>
<p>如果map 和reduce数量过多，会写很多blockFile，造成问题1：超过操作系统所能打开最大文件数，问题2：大量随机写随机读</p>
<p>解决方案：</p>
<p>1.shuffle 参数：<br>spark.shuffle.consolidateFiles  默认为false<br>如果设置为”true”，在shuffle期间，合并的中间文件将会被创建。创建更少的文件可以提供文件系统的shuffle的效 率。这些shuffle都伴随着大量递归任务。当用ext4和dfs文件系统时，推荐设置为”true”。在ext3中，因为文件系统的限制，这个选项可 能机器（大于8核）降低效率</p>
<p>2.sort shuffle<br>每个map只写到一个文件，和上面的写到reduce个数个文件不同</p>
<h2 id="不同算子影响shuffle表现形式不同"><a href="#不同算子影响shuffle表现形式不同" class="headerlink" title="不同算子影响shuffle表现形式不同"></a>不同算子影响shuffle表现形式不同</h2><p>reduceByKey(func, numPartitions=None)<br>也就是，reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。<br><img src="http://oh6ybr0jg.bkt.clouddn.com/reduceByKey.png" alt="此处输入图片的描述"></p>
<p>groupByKey(numPartitions=None)<br>也就是，groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。<br><img src="http://oh6ybr0jg.bkt.clouddn.com/groupByKey.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;shuffle实现的具体过程&quot;&gt;&lt;a href=&quot;#shuffle实现的具体过程&quot; class=&quot;headerlink&quot; title=&quot;shuffle实现的具体过程&quot;&gt;&lt;/a&gt;shuffle实现的具体过程&lt;/h2&gt;&lt;p&gt;1.Spark系统在运行含shuffle过程的应用时，Executor进程除了运行task，还要负责写shuffle 数据，给其他Executor提供shuffle数据。&lt;br&gt;    当Executor进程任务过重，导致GC而不能为其 他Executor提供shuffle数据时，会影响任务运行。&lt;br&gt;    这里实际上是利用External Shuffle Service 来提升性能，External shuffle Service是长期存在于NodeManager进程中的一个辅助服务。&lt;br&gt;    通过该服务 来抓取shuffle数据，减少了Executor的压力，在Executor GC的时候也不会影响其他 Executor的任务运行。&lt;/p&gt;
&lt;p&gt;启用方法：&lt;/p&gt;
&lt;h3 id=&quot;一-在NodeManager中启动External-shuffle-Service。&quot;&gt;&lt;a href=&quot;#一-在NodeManager中启动External-shuffle-Service。&quot; class=&quot;headerlink&quot; title=&quot;一. 在NodeManager中启动External shuffle Service。&quot;&gt;&lt;/a&gt;一. 在NodeManager中启动External shuffle Service。&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;a. 在“yarn-site.xml”中添加如下配置项：
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;spark_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;yarn.nodemanager.aux-services.spark_shuffle.class&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;org.apache.spark.network.yarn.YarnShuffleService&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
&amp;lt;name&amp;gt;spark.shuffle.service.port&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;7337&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
配置参数描述
yarn.nodemanager.aux-services  ：NodeManager中一个长期运行的辅助服务，用于提升Shuffle计算性能。
yarn.nodemanager.auxservices.spark_shuffle.class ：NodeManager中辅助服务对应的类。
spark.shuffle.service.port ：Shuffle服务监听数据获取请求的端口。可选配置，默认值为“7337”。
b. 添加依赖的jar包
拷贝“${SPARK_HOME}/lib/spark-1.3.0-yarn-shuffle.jar”到“${HADOOP_HOME}/share/hadoop/yarn/lib/”目录下。
c. 重启NodeManager进程，也就启动了External shuffle Service。
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>开源爬虫介绍</title>
    <link href="http://yoursite.com/2017/08/05/%E5%BC%80%E6%BA%90%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/08/05/开源爬虫介绍/</id>
    <published>2017-08-05T03:30:00.000Z</published>
    <updated>2017-10-17T11:10:37.532Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/Chyroc/WechatSogou" target="_blank" rel="external">https://github.com/Chyroc/WechatSogou</a></p>
<p> 微信公众号爬虫。基于搜狗微信搜索的微信公众号爬虫接口，可以扩展成基于搜狗搜索的爬虫，返回结果是列表，每一项均是公众号具体信息字典。</p>
<p> <a href="https://github.com/lanbing510/DouBanSpider" target="_blank" rel="external">https://github.com/lanbing510/DouBanSpider</a><br> 豆瓣读书爬虫。可以爬下豆瓣读书标签下的所有图书，按评分排名依次存储，存储到Excel中，可方便大家筛选搜罗，比如筛选评价人数&gt;1000的高分书籍；可依据不同的主题存储到Excel不同的Sheet ，采用User Agent伪装为浏览器进行爬取，并加入随机延时来更好的模仿浏览器行为，避免爬虫被封。</p>
<p><a href="https://github.com/LiuRoy/zhihu_spider" target="_blank" rel="external">https://github.com/LiuRoy/zhihu_spider</a><br>知乎爬虫。此项目的功能是爬取知乎用户信息以及人际拓扑关系，爬虫框架使用scrapy，数据存储使用mongo</p>
<p><a href="https://github.com/airingursb/bilibili-user" target="_blank" rel="external">https://github.com/airingursb/bilibili-user</a><br>Bilibili用户爬虫。总数据数：20119918，抓取字段：用户id，昵称，性别，头像，等级，经验值，粉丝数，生日，地址，注册时间，签名，等级与经验值等。抓取之后生成B站用户数据报告。<br><a id="more"></a></p>
<p><a href="https://github.com/LiuXingMing/SinaSpider" target="_blank" rel="external">https://github.com/LiuXingMing/SinaSpider</a><br>新浪微博爬虫。主要爬取新浪微博用户的个人信息、微博信息、粉丝和关注。代码获取新浪微博Cookie进行登录，可通过多账号登录来防止新浪的反扒。主要使用 scrapy 爬虫框架</p>
<p><a href="https://github.com/gnemoug/distribute_crawler" target="_blank" rel="external">https://github.com/gnemoug/distribute_crawler</a><br>小说下载分布式爬虫。使用scrapy,Redis, MongoDB,graphite实现的一个分布式网络爬虫,底层存储mongodb集群,分布式使用redis实现,爬虫状态显示使用graphite实现，主要针对一个小说站点。</p>
<p><a href="https://github.com/yanzhou/CnkiSpider" target="_blank" rel="external">https://github.com/yanzhou/CnkiSpider</a><br>中国知网爬虫。设置检索条件后，执行src/CnkiSpider.py抓取数据，抓取数据存储在/data目录下，每个数据文件的第一行为字段名称。</p>
<p><a href="https://github.com/lanbing510/LianJiaSpider" target="_blank" rel="external">https://github.com/lanbing510/LianJiaSpider</a><br>链家网爬虫。爬取北京地区链家历年二手房成交记录。涵盖链家爬虫一文的全部代码，包括链家模拟登录代码。</p>
<p><a href="https://github.com/taizilongxu/scrapy_jingdong" target="_blank" rel="external">https://github.com/taizilongxu/scrapy_jingdong</a><br>京东爬虫。基于scrapy的京东网站爬虫，保存格式为csv。</p>
<p><a href="https://github.com/caspartse/QQ-Groups-Spider" target="_blank" rel="external">https://github.com/caspartse/QQ-Groups-Spider</a><br>QQ 群爬虫。批量抓取 QQ 群信息，包括群名称、群号、群人数、群主、群简介等内容，最终生成 XLS(X) / CSV 结果文件。</p>
<p><a href="https://github.com/hanc00l/wooyun_public" target="_blank" rel="external">https://github.com/hanc00l/wooyun_public</a><br>乌云爬虫。 乌云公开漏洞、知识库爬虫和搜索。全部公开漏洞的列表和每个漏洞的文本内容存在mongodb中，大概约2G内容；如果整站爬全部文本和图片作为离线查询，大概需要10G空间、2小时（10M电信带宽）；爬取全部知识库，总共约500M空间。漏洞搜索使用了Flask作为web server，bootstrap作为前端。</p>
<p><a href="https://github.com/fankcoder/findtrip" target="_blank" rel="external">https://github.com/fankcoder/findtrip</a><br>机票爬虫（去哪儿和携程网）。Findtrip是一个基于Scrapy的机票爬虫，目前整合了国内两大机票网站（去哪儿 + 携程）。</p>
<p><a href="https://github.com/leyle/163spider" target="_blank" rel="external">https://github.com/leyle/163spider</a><br>基于requests、MySQLdb、torndb的网易客户端内容爬虫</p>
<p><a href="https://github.com/fanpei91/doubanspiders" target="_blank" rel="external">https://github.com/fanpei91/doubanspiders</a><br>豆瓣电影、书籍、小组、相册、东西等爬虫集</p>
<p><a href="https://github.com/LiuXingMing/QQSpider" target="_blank" rel="external">https://github.com/LiuXingMing/QQSpider</a><br>QQ空间爬虫，包括日志、说说、个人信息等，一天可抓取 400 万条数据。</p>
<p><a href="https://github.com/Shu-Ji/baidu-music-spider" target="_blank" rel="external">https://github.com/Shu-Ji/baidu-music-spider</a><br>百度mp3全站爬虫，使用redis支持断点续传。</p>
<p><a href="https://github.com/pakoo/tbcrawler" target="_blank" rel="external">https://github.com/pakoo/tbcrawler</a><br>淘宝和天猫的爬虫,可以根据搜索关键词,物品id来抓去页面的信息，数据存储在mongodb。</p>
<p><a href="https://github.com/benitoro/stockholm" target="_blank" rel="external">https://github.com/benitoro/stockholm</a><br>一个股票数据（沪深）爬虫和选股策略测试框架。根据选定的日期范围抓取所有沪深两市股票的行情数据。支持使用表达式定义选股策略。支持多线程处理。保存数据到JSON文件、CSV文件。</p>
<p><a href="https://github.com/k1995/BaiduyunSpider" target="_blank" rel="external">https://github.com/k1995/BaiduyunSpider</a><br>百度云盘爬虫</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/Chyroc/WechatSogou&quot;&gt;https://github.com/Chyroc/WechatSogou&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; 微信公众号爬虫。基于搜狗微信搜索的微信公众号爬虫接口，可以扩展成基于搜狗搜索的爬虫，返回结果是列表，每一项均是公众号具体信息字典。&lt;/p&gt;
&lt;p&gt; &lt;a href=&quot;https://github.com/lanbing510/DouBanSpider&quot;&gt;https://github.com/lanbing510/DouBanSpider&lt;/a&gt;&lt;br&gt; 豆瓣读书爬虫。可以爬下豆瓣读书标签下的所有图书，按评分排名依次存储，存储到Excel中，可方便大家筛选搜罗，比如筛选评价人数&amp;gt;1000的高分书籍；可依据不同的主题存储到Excel不同的Sheet ，采用User Agent伪装为浏览器进行爬取，并加入随机延时来更好的模仿浏览器行为，避免爬虫被封。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/LiuRoy/zhihu_spider&quot;&gt;https://github.com/LiuRoy/zhihu_spider&lt;/a&gt;&lt;br&gt;知乎爬虫。此项目的功能是爬取知乎用户信息以及人际拓扑关系，爬虫框架使用scrapy，数据存储使用mongo&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/airingursb/bilibili-user&quot;&gt;https://github.com/airingursb/bilibili-user&lt;/a&gt;&lt;br&gt;Bilibili用户爬虫。总数据数：20119918，抓取字段：用户id，昵称，性别，头像，等级，经验值，粉丝数，生日，地址，注册时间，签名，等级与经验值等。抓取之后生成B站用户数据报告。&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>CDH 安装spark2.2</title>
    <link href="http://yoursite.com/2017/08/05/CDH%20%E5%AE%89%E8%A3%85spark2.2/"/>
    <id>http://yoursite.com/2017/08/05/CDH 安装spark2.2/</id>
    <published>2017-08-05T03:30:00.000Z</published>
    <updated>2017-10-09T12:11:28.450Z</updated>
    
    <content type="html"><![CDATA[<h2 id="官方链接"><a href="#官方链接" class="headerlink" title="官方链接"></a>官方链接</h2><p><a href="https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html" target="_blank" rel="external">https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html</a></p>
<p>1.下载Spark2 CSD<br><a href="https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging" target="_blank" rel="external">https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging</a></p>
<p>1.1.1 CSD<br>笔者下载2.2版本<br><a href="http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar" target="_blank" rel="external">http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar</a><br><a id="more"></a><br>1.1.2 Parcel<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/</a></p>
<p>1.1.2.1 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel</a></p>
<p>1.1.2.2 SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel.sha1" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el6.parcel.sha1</a><br>然后将SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha1改名为<br>SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354-el7.parcel.sha</p>
<p>1.1.2.3 manifest.json<br><a href="http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/manifest.json" target="_blank" rel="external">http://archive.cloudera.com/spark2/parcels/2.2.0.cloudera1/manifest.json</a></p>
<p>停止服务<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server stop<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent stop</p>
<p>将csd文件放到<br>/soft/bigdata/clouderamanager/cloudera/csd</p>
<p>将parcel文件放到<br>/soft/bigdata/clouderamanager/cloudera/parcel-repo</p>
<p>修改权限<br>chgrp cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jar<br>chown cloudera-scm SPARK2_ON_YARN-2.2.0.cloudera1.jar </p>
<p>开启服务<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start</p>
<p>主机-&gt; Parcel就能看到spark2了</p>
<p>分配，激活。然后就可以添加服务了</p>
<p>如果添加服务不成功需要把jar文件放到/opt/cloudera/csd</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;官方链接&quot;&gt;&lt;a href=&quot;#官方链接&quot; class=&quot;headerlink&quot; title=&quot;官方链接&quot;&gt;&lt;/a&gt;官方链接&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html&quot;&gt;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.下载Spark2 CSD&lt;br&gt;&lt;a href=&quot;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging&quot;&gt;https://www.cloudera.com/documentation/spark2/latest/topics/spark2_packaging.html#packaging&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.1.1 CSD&lt;br&gt;笔者下载2.2版本&lt;br&gt;&lt;a href=&quot;http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar&quot;&gt;http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>spark 2.2.0源码编译</title>
    <link href="http://yoursite.com/2017/08/02/spark%202.2.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"/>
    <id>http://yoursite.com/2017/08/02/spark 2.2.0源码编译/</id>
    <published>2017-08-02T03:30:00.000Z</published>
    <updated>2017-08-13T05:14:38.067Z</updated>
    
    <content type="html"><![CDATA[<p>官网下载spark源码<br><a href="https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz" target="_blank" rel="external">https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz</a><br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-source-download.png" alt="此处输入图片的描述"></p>
<p>然后在idea中导入spark源码项目(idea maven配置正确)，然后对spark项目build。Build成功后在进行编译。<br>Build过程中遇到问题：</p>
<p>找不到org.apache.spark.streaming.flume.sink.SparkFlumeProtocol<br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem2.png" alt="此处输入图片的描述"><br>找不到org.apache.spark.sql.catalyst.parser.SqlBaseParser<br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem3.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-problem4.png" alt="此处输入图片的描述"></p>
<p>设置maven的参数，否则一直出现outofMemory<br>在apache-maven-3.3.9-bin\bin下面的mvn.cmd文件里的：<br>@REM set MAVEN_OPTS=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000<br>下面添加<br>set MAVEN_OPTS= -Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m</p>
<p>在git bash 里编译<br>进入spark源码目录<br>mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.3 -DskipTests clean package</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/spark-idea-build-success.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;官网下载spark源码&lt;br&gt;&lt;a href=&quot;https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0.tgz&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://d3kbcqa49mib13.clou
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>使用Ambari安装部署Spark集群</title>
    <link href="http://yoursite.com/2017/06/30/%E4%BD%BF%E7%94%A8Ambari%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Spark%E9%9B%86%E7%BE%A4/"/>
    <id>http://yoursite.com/2017/06/30/使用Ambari安装部署Spark集群/</id>
    <published>2017-06-30T03:30:00.000Z</published>
    <updated>2017-09-03T09:42:14.380Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>进入官网，选择产品里面的下载页面或者直接登录<br><a href="https://hortonworks.com/downloads/" target="_blank" rel="external">https://hortonworks.com/downloads/</a><br>选择<br>HDP® 2.6: Ready for the enterprise<br>下面的<br>Automated Install Guide</p>
<p>因为博主是CENTOS 7<br>在这个页面直接选择：<br>RHEL/CentOS/Oracle Linux 7</p>
<p>wget  <a href="http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo" target="_blank" rel="external">http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo</a> -O /etc/yum.repos.d/ambari.repo<br>yum repolist</p>
<p>yum install ambari-server</p>
<p>进入​Setup Options<br>ambari-server setup –j /usr/java/default<br>提示参数只能是一个，看来jdk要后设置</p>
<p>ambari-server setup<br>各种回车，jdk的时候选择安装1.8的<br>如果输入自定义的jdk要注意权限问题</p>
<h2 id="启动："><a href="#启动：" class="headerlink" title="启动："></a>启动：</h2><p>ambari-server start</p>
<p>查看：<br>[root@storm01 storm]# lsof -i:8080<br>COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME<br>java    7648 storm 1438u  IPv6 124470      0t0  TCP *:webcache (LISTEN)</p>
<p>关闭防火墙<br>systemctl disable firewalld.service<br>systemctl stop firewalld.service    </p>
<a id="more"></a>
<h2 id="在浏览器输入http-storm01-8080即可进入UI"><a href="#在浏览器输入http-storm01-8080即可进入UI" class="headerlink" title="在浏览器输入http://storm01:8080即可进入UI"></a>在浏览器输入<a href="http://storm01:8080即可进入UI" target="_blank" rel="external">http://storm01:8080即可进入UI</a></h2><p>用户密码都为admin</p>
<p>在每个机器上安装agent<br>yum install -y ambari-agent</p>
<p>修改配置文件<br> vi /etc/ambari-agent/conf/ambari-agent.ini<br> hostname=localhost更改为<br> hostname=storm01</p>
<p> 启动agent<br> service ambari-agent start</p>
<h2 id="设置hive"><a href="#设置hive" class="headerlink" title="设置hive"></a>设置hive</h2><p>用户密码都为hive</p>
<p>需要PostgreSQL支持远程连接<br>find / -name pg_hba.conf</p>
<p>vi /var/lib/pgsql/data/pg_hba.conf<br>将<br>local   all   postgres                                     peer改成<br>local   all   postgres                                     trust<br>host    all   postgres             127.0.0.1/32            ident改成<br>host    all   postgres             127.0.0.1/32            trust<br>local  all  ambari,mapred md5改成<br>local  all  ambari,mapred,hive trust<br>host  all   ambari,mapred 0.0.0.0/0  md5改成<br>host  all   ambari,mapred,hive 0.0.0.0/0  md5<br>host  all   ambari,mapred ::/0 md5改成<br>host  all   ambari,mapred,hive ::/0 md5</p>
<p>重启数据库<br>service postgresql restart</p>
<h2 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h2><p>lsof -i:5432</p>
<p>去仓库下载postgresql</p>
<p><a href="http://mvnrepository.com/" target="_blank" rel="external">http://mvnrepository.com/</a></p>
<p>下载</p>
<p>PostgreSQL JDBC Driver JDBC 4.2<br>的<br>9.2-1002-jdbc4</p>
<p>或者直接输入：<br>wget <a href="http://central.maven.org/maven2/org/postgresql/postgresql/9.2-1002-jdbc4/postgresql-9.2-1002-jdbc4.jar" target="_blank" rel="external">http://central.maven.org/maven2/org/postgresql/postgresql/9.2-1002-jdbc4/postgresql-9.2-1002-jdbc4.jar</a></p>
<p>ambari-server setup –jdbc-db=postgres –jdbc-driver=/root/postgresql-9.2-1002-jdbc4.jar</p>
<p>创建用户<br>psql -U postgres -d postgres<br>create user hive;<br>alter user hive password ‘hive’;<br>create database hive;<br>grant all on database hive to hive;<br>alter database hive owner to hive;</p>
<p>此时test ConnectionConnection 就可以测试成功</p>
<p>设置Ambari Metrics和Smart Sense用户密码都为admin</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariInstallOptions.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariConfirmHostsing.png" alt="此处输入图片的描述"><br>如果发现失败，需要设置/etc/hosts文件<br>192.168.247.180    spark01<br>192.168.247.181    spark02<br>192.168.247.182    spark03</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariConfirmHosts.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariChooseServices1.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariChooseServices2.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariAssignMasters.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariAssignSlavesandClients.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariCustomize1.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariCustomize2.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariCustomize3.png" alt="此处输入图片的描述"></p>
<p>安装完成<br><img src="http://oh6ybr0jg.bkt.clouddn.com/AmbariInstallSuccess.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerlink&quot; title=&quot;下载&quot;&gt;&lt;/a&gt;下载&lt;/h2&gt;&lt;p&gt;进入官网，选择产品里面的下载页面或者直接登录&lt;br&gt;&lt;a href=&quot;https://hortonworks.com/downloads/&quot;&gt;https://hortonworks.com/downloads/&lt;/a&gt;&lt;br&gt;选择&lt;br&gt;HDP® 2.6: Ready for the enterprise&lt;br&gt;下面的&lt;br&gt;Automated Install Guide&lt;/p&gt;
&lt;p&gt;因为博主是CENTOS 7&lt;br&gt;在这个页面直接选择：&lt;br&gt;RHEL/CentOS/Oracle Linux 7&lt;/p&gt;
&lt;p&gt;wget  &lt;a href=&quot;http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo&quot;&gt;http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari.repo&lt;/a&gt; -O /etc/yum.repos.d/ambari.repo&lt;br&gt;yum repolist&lt;/p&gt;
&lt;p&gt;yum install ambari-server&lt;/p&gt;
&lt;p&gt;进入​Setup Options&lt;br&gt;ambari-server setup –j /usr/java/default&lt;br&gt;提示参数只能是一个，看来jdk要后设置&lt;/p&gt;
&lt;p&gt;ambari-server setup&lt;br&gt;各种回车，jdk的时候选择安装1.8的&lt;br&gt;如果输入自定义的jdk要注意权限问题&lt;/p&gt;
&lt;h2 id=&quot;启动：&quot;&gt;&lt;a href=&quot;#启动：&quot; class=&quot;headerlink&quot; title=&quot;启动：&quot;&gt;&lt;/a&gt;启动：&lt;/h2&gt;&lt;p&gt;ambari-server start&lt;/p&gt;
&lt;p&gt;查看：&lt;br&gt;[root@storm01 storm]# lsof -i:8080&lt;br&gt;COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME&lt;br&gt;java    7648 storm 1438u  IPv6 124470      0t0  TCP *:webcache (LISTEN)&lt;/p&gt;
&lt;p&gt;关闭防火墙&lt;br&gt;systemctl disable firewalld.service&lt;br&gt;systemctl stop firewalld.service    &lt;/p&gt;
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CDH oozie</title>
    <link href="http://yoursite.com/2017/05/18/CDH%20oozie%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/05/18/CDH oozie安装/</id>
    <published>2017-05-18T03:30:00.000Z</published>
    <updated>2017-06-23T00:27:18.872Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装完oozie，打开UI"><a href="#安装完oozie，打开UI" class="headerlink" title="安装完oozie，打开UI:"></a>安装完oozie，打开UI:</h2><p><a href="http://master1:11000/oozie/" target="_blank" rel="external">http://master1:11000/oozie/</a></p>
<p>显示Oozie web console is disabled.<br>解决方案;<br>原因是oozie的/var/lib/oozie目录里缺少EXT的包</p>
<p>点击Documentation链接里quickstart给出了解决方案<br>Download ExtJS library (it must be version 2.2)<br><a id="more"></a></p>
<h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><p><a href="http://dev.sencha.com/deploy/ext-2.2.zip" target="_blank" rel="external">http://dev.sencha.com/deploy/ext-2.2.zip</a></p>
<p>如果下载不下来可以试试CSDN<br><a href="http://download.csdn.net/download/start_baby/6280675" target="_blank" rel="external">http://download.csdn.net/download/start_baby/6280675</a><br>或者：<br><a href="http://archive.cloudera.com/gplextras/misc/ext-2.2.zip" target="_blank" rel="external">http://archive.cloudera.com/gplextras/misc/ext-2.2.zip</a></p>
<h2 id="unzip解压"><a href="#unzip解压" class="headerlink" title="unzip解压"></a>unzip解压</h2><p>然后刷新页面成功进入oozie的web界面</p>
<h2 id="编写job-properties（nameNode要当时active的）"><a href="#编写job-properties（nameNode要当时active的）" class="headerlink" title="编写job.properties（nameNode要当时active的）"></a>编写job.properties（nameNode要当时active的）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">nameNode=hdfs://master2:8020</div><div class="line">jobTracker=master2:8032</div><div class="line">queueName=default</div><div class="line">examplesRoot=user/oozie/my-apps/shell</div><div class="line">oozie.wf.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/workflow.xml</div><div class="line">EXEC=emp-join-demp.sh</div></pre></td></tr></table></figure>
<h2 id="编写workflow-xml"><a href="#编写workflow-xml" class="headerlink" title="编写workflow.xml"></a>编写workflow.xml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt;</div><div class="line"> &lt;start to=&quot;shell-node&quot;/&gt;</div><div class="line"> &lt;action name=&quot;shell-node&quot;&gt;</div><div class="line">     &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt;</div><div class="line">         &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</div><div class="line">         &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</div><div class="line">         &lt;configuration&gt;</div><div class="line">             &lt;property&gt;</div><div class="line">                 &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</div><div class="line">                 &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</div><div class="line">             &lt;/property&gt;</div><div class="line">         &lt;/configuration&gt;</div><div class="line">         &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt;</div><div class="line">         &lt;file&gt;$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt; &lt;!--Copy the executable to compute node&apos;s current working directory --&gt;</div><div class="line">     &lt;/shell&gt;</div><div class="line">     &lt;ok to=&quot;end&quot;/&gt;</div><div class="line">     &lt;error to=&quot;fail&quot;/&gt;</div><div class="line"> &lt;/action&gt;</div><div class="line"> &lt;kill name=&quot;fail&quot;&gt;</div><div class="line">     &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;</div><div class="line"> &lt;/kill&gt;</div><div class="line"> &lt;end name=&quot;end&quot;/&gt;</div><div class="line">&lt;/workflow-app&gt;</div></pre></td></tr></table></figure>
<h2 id="编写emp-join-demp-sh"><a href="#编写emp-join-demp-sh" class="headerlink" title="编写emp-join-demp.sh"></a>编写emp-join-demp.sh</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line">java -cp PhoenixAPI-1.0-SNAPSHOT.jar com.zwjf.Month</div></pre></td></tr></table></figure>
<p>[hdfs@master1 data]$ hadoop fs -mkdir -p /user/oozie/my-apps/shell<br>[hdfs@master1 data]$ hadoop fs -put workflow.xml /user/oozie/my-apps/shell<br>[hdfs@master1 data]$ hadoop fs -put emp-join-demp.sh /user/oozie/my-apps/shell</p>
<h2 id="上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作"><a href="#上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作" class="headerlink" title="上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作"></a>上传我shell脚本里执行的jar包，你们根据自己的shell决定如何操作</h2><p>hadoop fs -put PhoenixAPI-1.0-SNAPSHOT.jar /user/oozie/my-apps/shell</p>
<p>[hdfs@master1 data]$ /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie  job -oozie <a href="http://master1:11000/oozie" target="_blank" rel="external">http://master1:11000/oozie</a>  -config /soft/data/job.properties -run </p>
<h2 id="提交出错："><a href="#提交出错：" class="headerlink" title="提交出错："></a>提交出错：</h2><p>去历史服务器查看问题<br><a href="http://master2:19888/jobhistory" target="_blank" rel="external">http://master2:19888/jobhistory</a></p>
<h2 id="报错："><a href="#报错：" class="headerlink" title="报错："></a>报错：</h2><p>Launcher ERROR, reason: Main class [org.apache.oozie.action.hadoop.ShellMain], exit code [1]</p>
<p>是因为分发的时候找不到用户的jar包，在workflow.xml的</p>
<file>${nameNode}/${examplesRoot}/${EXEC}#${EXEC}</file><br>添加你的jar包并上传hdfs就可以<br>## 例如：<br><file>${nameNode}/${examplesRoot}/PhoenixAPI-1.0-SNAPSHOT.jar#PhoenixAPI-1.0-SNAPSHOT.jar</file>


<h2 id="可参考如下文章"><a href="#可参考如下文章" class="headerlink" title="可参考如下文章"></a>可参考如下文章</h2><p><a href="http://blog.sina.com.cn/s/blog_e699b42b0102xh3o.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_e699b42b0102xh3o.html</a></p>
<h2 id="增加定时任务"><a href="#增加定时任务" class="headerlink" title="增加定时任务"></a>增加定时任务</h2><p>job.properties增加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">oozie.coord.application.path=$&#123;nameNode&#125;/$&#123;examplesRoot&#125;/coordinator.xml</div><div class="line">start=2017-05-18T16:30Z</div><div class="line">end=2019-07-30T16:00Z</div></pre></td></tr></table></figure></p>
<p>oozie.wf.application.path=${nameNode}/${examplesRoot}/workflow.xml<br>改成<br>workflowAppUri=${nameNode}/${examplesRoot}/workflow.xml</p>
<h2 id="新建coordinator-xml"><a href="#新建coordinator-xml" class="headerlink" title="新建coordinator.xml"></a>新建coordinator.xml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&lt;coordinator-app name=&quot;coordinator&quot; frequency=&quot;$&#123;coord:minutes(10)&#125;&quot; start=&quot;$&#123;start&#125;&quot; end=&quot;$&#123;end&#125;&quot; timezone=&quot;Asia/Shanghai&quot; xmlns=&quot;uri:oozie:coordinator:0.2&quot;&gt;</div><div class="line">  &lt;action&gt;</div><div class="line">    &lt;workflow&gt;</div><div class="line">      &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt;</div><div class="line">      &lt;configuration&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;jobTracker&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;EXEC&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;EXEC&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;nameNode&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">        &lt;property&gt;</div><div class="line">          &lt;name&gt;queueName&lt;/name&gt;</div><div class="line">          &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</div><div class="line">        &lt;/property&gt;</div><div class="line">      &lt;/configuration&gt;</div><div class="line">    &lt;/workflow&gt;</div><div class="line">  &lt;/action&gt;</div><div class="line">&lt;/coordinator-app&gt;</div></pre></td></tr></table></figure>
<h2 id="上传"><a href="#上传" class="headerlink" title="上传"></a>上传</h2><p>hadoop fs -put coordinator.xml /user/oozie/my-apps/shell</p>
<h2 id="停止任务："><a href="#停止任务：" class="headerlink" title="停止任务："></a>停止任务：</h2><p>/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/oozie  job -oozie <a href="http://master1:11000/oozie" target="_blank" rel="external">http://master1:11000/oozie</a>  -kill 0000005-170518154227460-oozie-oozi-C</p>
<p>注：设置的时间不能小于当前时间，否则会把之前没执行的都执行</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装完oozie，打开UI&quot;&gt;&lt;a href=&quot;#安装完oozie，打开UI&quot; class=&quot;headerlink&quot; title=&quot;安装完oozie，打开UI:&quot;&gt;&lt;/a&gt;安装完oozie，打开UI:&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://master1:11000/oozie/&quot;&gt;http://master1:11000/oozie/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;显示Oozie web console is disabled.&lt;br&gt;解决方案;&lt;br&gt;原因是oozie的/var/lib/oozie目录里缺少EXT的包&lt;/p&gt;
&lt;p&gt;点击Documentation链接里quickstart给出了解决方案&lt;br&gt;Download ExtJS library (it must be version 2.2)&lt;br&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>kafka源码阅读2</title>
    <link href="http://yoursite.com/2017/05/08/kafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB2/"/>
    <id>http://yoursite.com/2017/05/08/kafka源码阅读2/</id>
    <published>2017-05-08T03:30:00.000Z</published>
    <updated>2017-08-13T06:04:53.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导入IDEA即可看kafka源码："><a href="#导入IDEA即可看kafka源码：" class="headerlink" title="导入IDEA即可看kafka源码："></a>导入IDEA即可看kafka源码：</h2><h2 id="启动之前需要安装zookeeper"><a href="#启动之前需要安装zookeeper" class="headerlink" title="启动之前需要安装zookeeper"></a>启动之前需要安装zookeeper</h2><p>地址：<br><a href="http://apache.fayea.com/zookeeper/" target="_blank" rel="external">http://apache.fayea.com/zookeeper/</a><br><a href="http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz" target="_blank" rel="external">http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz</a></p>
<h2 id="解压后再当前目录增加"><a href="#解压后再当前目录增加" class="headerlink" title="解压后再当前目录增加"></a>解压后再当前目录增加</h2><p>dataLogDir和data目录<br>复制一份配置文件<br>改名为zoo.cfg<br>修改配置文件：<br>zoo.cfg<br>修改并增加<br>dataDir=D:\tool\zookeeper-3.4.6\data<br>dataLogDir=D:\tool\zookeeper-3.4.6\dataLogDir</p>
<h2 id="启动zkServer-cmd"><a href="#启动zkServer-cmd" class="headerlink" title="启动zkServer.cmd"></a>启动zkServer.cmd</h2><a id="more"></a>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8zk.jpg" alt="此处输入图片的描述"></p>
<p>启动ZkCli.cmd<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8zkCli.jpg" alt="此处输入图片的描述"></p>
<p>kafka启动。在配置文件修改<br>Program arguments：config/server.properties</p>
<p>然后修改server.properties里面的参数即可。</p>
<p>启动前：<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8kafka%E5%89%8D.jpg" alt="此处输入图片的描述"></p>
<p>启动后：<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8kafka%E5%90%8E.jpg" alt="此处输入图片的描述"></p>
<p>源码阅读（一）<br>从启动入口分析：Kafka.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">/*从配置文件读取kafka服务器启动参数--将传入的参数转换成Properties 文件，如果参数为空将提示：</span></div><div class="line">       * USAGE: java [options] KafkaServer server.properties [--override property=value]*</div><div class="line">       * Option               Description</div><div class="line">       * ------               -----------</div><div class="line">       * --override &lt;String&gt;  Optional property that should override values set in</div><div class="line">       *                server.properties file</div><div class="line">       *</div><div class="line">       *  判断参数是否大于1，将后面的参数放到Properties里</div><div class="line">       * */</div><div class="line">      <span class="keyword">val</span> serverProps = getPropsFromArgs(args)</div><div class="line"></div><div class="line">      <span class="comment">//创建KafkaServerStartable对象</span></div><div class="line">      <span class="keyword">val</span> kafkaServerStartable = <span class="type">KafkaServerStartable</span>.fromProps(serverProps)</div><div class="line"></div><div class="line"></div><div class="line">      <span class="comment">// attach shutdown handler to catch control-c</span></div><div class="line">      <span class="type">Runtime</span>.getRuntime().addShutdownHook(<span class="keyword">new</span> <span class="type">Thread</span>() &#123;</div><div class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() = &#123;</div><div class="line">          kafkaServerStartable.shutdown</div><div class="line">        &#125;</div><div class="line">      &#125;)</div><div class="line"></div><div class="line">      kafkaServerStartable.startup</div><div class="line">      kafkaServerStartable.awaitShutdown</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        fatal(e)</div><div class="line">        <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="type">System</span>.exit(<span class="number">0</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h5 id="这上面有个小知识点：-告诉编译器你希望将某个参数当作参数序列处理"><a href="#这上面有个小知识点：-告诉编译器你希望将某个参数当作参数序列处理" class="headerlink" title="这上面有个小知识点：_* 告诉编译器你希望将某个参数当作参数序列处理"></a>这上面有个小知识点：_* 告诉编译器你希望将某个参数当作参数序列处理</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">echo</span></span>(args: <span class="type">String</span>*) = <span class="keyword">for</span> (arg &lt;- args) println(arg)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) = &#123;</div><div class="line">  <span class="keyword">var</span> args = <span class="type">Array</span>(<span class="string">"config/server.properties"</span>,<span class="string">"canshu1"</span>,<span class="string">"canshu2"</span>)</div><div class="line">  echo(args.slice(<span class="number">1</span>, args.length): _*)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>输出是<br>canshu1<br>canshu2</p>
<p>kafkaServerStartable封装了KafkaServer</p>
<h3 id="1-具体的启动类在：KafkaServerStartable的startup方法"><a href="#1-具体的启动类在：KafkaServerStartable的startup方法" class="headerlink" title="1.具体的启动类在：KafkaServerStartable的startup方法"></a>1.具体的启动类在：KafkaServerStartable的startup方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      info(<span class="string">"starting"</span>)</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(isShuttingDown.get)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(startupComplete.get)</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">      <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">      <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">        brokerState.newState(<span class="type">Starting</span>)</div><div class="line"></div><div class="line">        <span class="comment">/* start scheduler */</span></div><div class="line">        kafkaScheduler.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* setup zookeeper */</span></div><div class="line">        zkUtils = initZk()</div><div class="line"></div><div class="line">        <span class="comment">/* Get or create cluster_id */</span></div><div class="line">        _clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">        info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div><div class="line"></div><div class="line">        <span class="comment">/* generate brokerId */</span></div><div class="line">        config.brokerId =  getBrokerId</div><div class="line">        <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">        <span class="comment">/* create and configure metrics */</span></div><div class="line">        <span class="keyword">val</span> reporters = config.getConfiguredInstances(<span class="type">KafkaConfig</span>.<span class="type">MetricReporterClassesProp</span>, classOf[<span class="type">MetricsReporter</span>],</div><div class="line">            <span class="type">Map</span>[<span class="type">String</span>, <span class="type">AnyRef</span>](<span class="type">KafkaConfig</span>.<span class="type">BrokerIdProp</span> -&gt; (config.brokerId.toString)).asJava)</div><div class="line">        reporters.add(<span class="keyword">new</span> <span class="type">JmxReporter</span>(jmxPrefix))</div><div class="line">        <span class="keyword">val</span> metricConfig = <span class="type">KafkaServer</span>.metricConfig(config)</div><div class="line">        metrics = <span class="keyword">new</span> <span class="type">Metrics</span>(metricConfig, reporters, time, <span class="literal">true</span>)</div><div class="line"></div><div class="line">        quotaManagers = <span class="type">QuotaFactory</span>.instantiate(config, metrics, time)</div><div class="line">        notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala)</div><div class="line"></div><div class="line">        <span class="comment">/* start log manager */</span></div><div class="line">        logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">        logManager.startup()</div><div class="line"></div><div class="line">        metadataCache = <span class="keyword">new</span> <span class="type">MetadataCache</span>(config.brokerId)</div><div class="line">        credentialProvider = <span class="keyword">new</span> <span class="type">CredentialProvider</span>(config.saslEnabledMechanisms)</div><div class="line"></div><div class="line">        socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</div><div class="line">        socketServer.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* start replica manager */</span></div><div class="line">        replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">          isShuttingDown, quotaManagers.follower)</div><div class="line">        replicaManager.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* start kafka controller */</span></div><div class="line">        kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</div><div class="line">        kafkaController.startup()</div><div class="line"></div><div class="line">        adminManager = <span class="keyword">new</span> <span class="type">AdminManager</span>(config, metrics, metadataCache, zkUtils)</div><div class="line"></div><div class="line">        <span class="comment">/* start group coordinator */</span></div><div class="line">        <span class="comment">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</span></div><div class="line">        groupCoordinator = <span class="type">GroupCoordinator</span>(config, zkUtils, replicaManager, <span class="type">Time</span>.<span class="type">SYSTEM</span>)</div><div class="line">        groupCoordinator.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* Get the authorizer and initialize it if one is specified.*/</span></div><div class="line">        authorizer = <span class="type">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</div><div class="line">          <span class="keyword">val</span> authZ = <span class="type">CoreUtils</span>.createObject[<span class="type">Authorizer</span>](authorizerClassName)</div><div class="line">          authZ.configure(config.originals())</div><div class="line">          authZ</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">/* start processing requests */</span></div><div class="line">        apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</div><div class="line">          kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</div><div class="line">          clusterId, time)</div><div class="line"></div><div class="line">        requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</div><div class="line">          config.numIoThreads)</div><div class="line"></div><div class="line">        <span class="type">Mx4jLoader</span>.maybeLoad()</div><div class="line"></div><div class="line">        <span class="comment">/* start dynamic config manager */</span></div><div class="line">        dynamicConfigHandlers = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ConfigHandler</span>](<span class="type">ConfigType</span>.<span class="type">Topic</span> -&gt; <span class="keyword">new</span> <span class="type">TopicConfigHandler</span>(logManager, config, quotaManagers),</div><div class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Client</span> -&gt; <span class="keyword">new</span> <span class="type">ClientIdConfigHandler</span>(quotaManagers),</div><div class="line">                                                           <span class="type">ConfigType</span>.<span class="type">User</span> -&gt; <span class="keyword">new</span> <span class="type">UserConfigHandler</span>(quotaManagers, credentialProvider),</div><div class="line">                                                           <span class="type">ConfigType</span>.<span class="type">Broker</span> -&gt; <span class="keyword">new</span> <span class="type">BrokerConfigHandler</span>(config, quotaManagers))</div><div class="line"></div><div class="line">        <span class="comment">// Create the config manager. start listening to notifications</span></div><div class="line">        dynamicConfigManager = <span class="keyword">new</span> <span class="type">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</div><div class="line">        dynamicConfigManager.startup()</div><div class="line"></div><div class="line">        <span class="comment">/* tell everyone we are alive */</span></div><div class="line">        <span class="keyword">val</span> listeners = config.advertisedListeners.map &#123; endpoint =&gt;</div><div class="line">          <span class="keyword">if</span> (endpoint.port == <span class="number">0</span>)</div><div class="line">            endpoint.copy(port = socketServer.boundPort(endpoint.listenerName))</div><div class="line">          <span class="keyword">else</span></div><div class="line">            endpoint</div><div class="line">        &#125;</div><div class="line">        kafkaHealthcheck = <span class="keyword">new</span> <span class="type">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</div><div class="line">          config.interBrokerProtocolVersion)</div><div class="line">        kafkaHealthcheck.startup()</div><div class="line"></div><div class="line">        <span class="comment">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></div><div class="line">        checkpointBrokerId(config.brokerId)</div><div class="line"></div><div class="line">        <span class="comment">/* register broker metrics */</span></div><div class="line">        registerStats()</div><div class="line"></div><div class="line">        brokerState.newState(<span class="type">RunningAsBroker</span>)</div><div class="line">        shutdownLatch = <span class="keyword">new</span> <span class="type">CountDownLatch</span>(<span class="number">1</span>)</div><div class="line">        startupComplete.set(<span class="literal">true</span>)</div><div class="line">        isStartingUp.set(<span class="literal">false</span>)</div><div class="line">        <span class="type">AppInfoParser</span>.registerAppInfo(jmxPrefix, config.brokerId.toString)</div><div class="line">        info(<span class="string">"started"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">        isStartingUp.set(<span class="literal">false</span>)</div><div class="line">        shutdown()</div><div class="line">        <span class="keyword">throw</span> e</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h5 id="Starting-继承BrokerStates，BrokerStates是一个sealed-trait"><a href="#Starting-继承BrokerStates，BrokerStates是一个sealed-trait" class="headerlink" title="Starting 继承BrokerStates，BrokerStates是一个sealed trait"></a>Starting 继承BrokerStates，BrokerStates是一个sealed trait</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">sealed</span> <span class="class"><span class="keyword">trait</span> <span class="title">BrokerStates</span> </span>&#123; <span class="function"><span class="keyword">def</span> <span class="title">state</span></span>: <span class="type">Byte</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">NotRunning</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">0</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">Starting</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">1</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">RecoveringFromUncleanShutdown</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">2</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">RunningAsBroker</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">3</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">PendingControlledShutdown</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">6</span> &#125;</div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">BrokerShuttingDown</span> <span class="keyword">extends</span> <span class="title">BrokerStates</span> </span>&#123; <span class="keyword">val</span> state: <span class="type">Byte</span> = <span class="number">7</span> &#125;</div></pre></td></tr></table></figure>
<p>trait定义为sealed 有两层含义<br>1.其修饰的trait class只能在当前文件里面被继承<br>2.用sealed修饰这样做的目的是告诉scala编译器在检查模式匹配的时候，让scala知道这些case的所有情况，scala就能够在编译的时候进行检查，看你写的代码是否有没有漏掉什么没case到，减少编程的错误。</p>
<h3 id="2-start-scheduler"><a href="#2-start-scheduler" class="headerlink" title="2.start scheduler:"></a>2.start scheduler:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kafkaScheduler.startup()</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaScheduler</span>(<span class="params">val threads: <span class="type">Int</span>, </span></span></div><div class="line">                     val threadNamePrefix: <span class="type">String</span> = "kafka-scheduler-", </div><div class="line">                     daemon: <span class="type">Boolean</span> = true) <span class="keyword">extends</span> <span class="title">Scheduler</span> <span class="keyword">with</span> <span class="title">Logging</span> &#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> executor: <span class="type">ScheduledThreadPoolExecutor</span> = <span class="literal">null</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> schedulerThreadId = <span class="keyword">new</span> <span class="type">AtomicInteger</span>(<span class="number">0</span>)</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">    debug(<span class="string">"Initializing task scheduler."</span>)</div><div class="line">    <span class="keyword">this</span> synchronized &#123;</div><div class="line">      <span class="keyword">if</span>(isStarted)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"This scheduler has already been started!"</span>)</div><div class="line">      executor = <span class="keyword">new</span> <span class="type">ScheduledThreadPoolExecutor</span>(threads)</div><div class="line">      executor.setContinueExistingPeriodicTasksAfterShutdownPolicy(<span class="literal">false</span>)</div><div class="line">      executor.setExecuteExistingDelayedTasksAfterShutdownPolicy(<span class="literal">false</span>)</div><div class="line">      executor.setThreadFactory(<span class="keyword">new</span> <span class="type">ThreadFactory</span>() &#123;</div><div class="line">                                  <span class="function"><span class="keyword">def</span> <span class="title">newThread</span></span>(runnable: <span class="type">Runnable</span>): <span class="type">Thread</span> = </div><div class="line">                                    <span class="type">Utils</span>.newThread(threadNamePrefix + schedulerThreadId.getAndIncrement(), runnable, daemon)</div><div class="line">                                &#125;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>1.此处使用了同步锁，如果已经启动，直接抛个IllegalStateException异常，由外面通用异常Throwable捕获。<br>2.根据配置文件的background.threads 创建一个ScheduledThreadPoolExecutor(threads)【java.util.concurrent包下的】</p>
<h3 id="3-setup-zookeeper"><a href="#3-setup-zookeeper" class="headerlink" title="3.setup zookeeper"></a>3.setup zookeeper</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">zkUtils = initZk()</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initZk</span></span>(): <span class="type">ZkUtils</span> = &#123;</div><div class="line">   info(<span class="string">s"Connecting to zookeeper on <span class="subst">$&#123;config.zkConnect&#125;</span>"</span>)</div><div class="line"></div><div class="line">   <span class="keyword">val</span> chrootIndex = config.zkConnect.indexOf(<span class="string">"/"</span>)</div><div class="line">   <span class="keyword">val</span> chrootOption = &#123;</div><div class="line">     <span class="keyword">if</span> (chrootIndex &gt; <span class="number">0</span>) <span class="type">Some</span>(config.zkConnect.substring(chrootIndex))</div><div class="line">     <span class="keyword">else</span> <span class="type">None</span></div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">val</span> secureAclsEnabled = config.zkEnableSecureAcls</div><div class="line">   <span class="keyword">val</span> isZkSecurityEnabled = <span class="type">JaasUtils</span>.isZkSecurityEnabled()</div><div class="line"></div><div class="line">   <span class="keyword">if</span> (secureAclsEnabled &amp;&amp; !isZkSecurityEnabled)</div><div class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.<span class="type">SecurityException</span>(<span class="string">s"<span class="subst">$&#123;KafkaConfig.ZkEnableSecureAclsProp&#125;</span> is true, but the verification of the JAAS login file failed."</span>)</div><div class="line"></div><div class="line">   chrootOption.foreach &#123; chroot =&gt;</div><div class="line">     <span class="keyword">val</span> zkConnForChrootCreation = config.zkConnect.substring(<span class="number">0</span>, chrootIndex)</div><div class="line">     <span class="keyword">val</span> zkClientForChrootCreation = <span class="type">ZkUtils</span>(zkConnForChrootCreation,</div><div class="line">                                             sessionTimeout = config.zkSessionTimeoutMs,</div><div class="line">                                             connectionTimeout = config.zkConnectionTimeoutMs,</div><div class="line">                                             secureAclsEnabled)</div><div class="line">     zkClientForChrootCreation.makeSurePersistentPathExists(chroot)</div><div class="line">     info(<span class="string">s"Created zookeeper path <span class="subst">$chroot</span>"</span>)</div><div class="line">     zkClientForChrootCreation.zkClient.close()</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">val</span> zkUtils = <span class="type">ZkUtils</span>(config.zkConnect,</div><div class="line">                         sessionTimeout = config.zkSessionTimeoutMs,</div><div class="line">                         connectionTimeout = config.zkConnectionTimeoutMs,</div><div class="line">                         secureAclsEnabled)</div><div class="line">   zkUtils.setupCommonPaths()</div><div class="line">   zkUtils</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>1.创建连接<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> persistentZkPaths = <span class="type">Seq</span>(<span class="type">ConsumersPath</span>,</div><div class="line">                              <span class="type">BrokerIdsPath</span>,</div><div class="line">                              <span class="type">BrokerTopicsPath</span>,</div><div class="line">                              <span class="type">ConfigChangesPath</span>,</div><div class="line">                              getEntityConfigRootPath(<span class="type">ConfigType</span>.<span class="type">Topic</span>),</div><div class="line">                              getEntityConfigRootPath(<span class="type">ConfigType</span>.<span class="type">Client</span>),</div><div class="line">                              <span class="type">DeleteTopicsPath</span>,</div><div class="line">                              <span class="type">BrokerSequenceIdPath</span>,</div><div class="line">                              <span class="type">IsrChangeNotificationPath</span>)</div></pre></td></tr></table></figure></p>
<p>2.设置通用路径<br>/consumers<br>/brokers/ids<br>/brokers/topics<br>/config/changes<br>/config/topics<br>/config/clients<br>/admin/delete_topics<br>/brokers/seqid<br>/isr_change_notification</p>
<p>ISR：Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas） set，这个set里的所有replica都跟上了leader，只有ISR里的成员才有被选为leader的可能</p>
<h3 id="4-Get-or-create-cluster-id"><a href="#4-Get-or-create-cluster-id" class="headerlink" title="4.Get or create cluster_id"></a>4.Get or create cluster_id</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">_clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getOrGenerateClusterId</span></span>(zkUtils: <span class="type">ZkUtils</span>): <span class="type">String</span> = &#123;</div><div class="line">    zkUtils.getClusterId.getOrElse(zkUtils.createOrGetClusterId(<span class="type">CoreUtils</span>.generateUuidAsBase64))</div><div class="line">  &#125;</div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createOrGetClusterId</span></span>(proposedClusterId: <span class="type">String</span>): <span class="type">String</span> = &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      createPersistentPath(<span class="type">ClusterIdPath</span>, <span class="type">ClusterId</span>.toJson(proposedClusterId))</div><div class="line">      proposedClusterId</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</div><div class="line">        getClusterId.getOrElse(<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Failed to get cluster id from Zookeeper. This can only happen if /cluster/id is deleted from Zookeeper."</span>))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"><span class="comment">/**</span></div><div class="line">   * Create an persistent node with the given path and data. Create parents if necessary.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createPersistentPath</span></span>(path: <span class="type">String</span>, data: <span class="type">String</span> = <span class="string">""</span>, acls: java.util.<span class="type">List</span>[<span class="type">ACL</span>] = <span class="type">UseDefaultAcls</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> acl = <span class="keyword">if</span> (acls eq <span class="type">UseDefaultAcls</span>) <span class="type">ZkUtils</span>.defaultAcls(isSecure, path) <span class="keyword">else</span> acls</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="type">ZkPath</span>.createPersistent(zkClient, path, data, acl)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">ZkNoNodeException</span> =&gt;</div><div class="line">        createParentPath(path)</div><div class="line">        <span class="type">ZkPath</span>.createPersistent(zkClient, path, data, acl)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>此处会创建一个persistent节点/cluster/id 如果节点已经存在，则刨除异常，上次获取异常，然后去节点下获取_clusterId，如果不存在，将创建的proposedClusterId返回</p>
<h5 id="此处需要注意zookeeper的节点类型分为："><a href="#此处需要注意zookeeper的节点类型分为：" class="headerlink" title="此处需要注意zookeeper的节点类型分为："></a>此处需要注意zookeeper的节点类型分为：</h5><p>持久节点（PERSISTENT）<br>持久顺序节点（PERSISTENT_SEQUENTIAL）<br>临时节点（EPHEMERAL）<br>临时顺序节点（EPHEMERAL_SEQUENTIAL）<br>顺序节点即创建有序的节点，节点名加上一个数字后缀。临时节点和客户端绑定，会话失效（非连接断开）则自动清楚</p>
<p>临时顺序节点可用来实现分布式锁<br>1.客户端调用create()方法创建名为“<em>locknode</em>/guid-lock-”的节点，需要注意的是，这里节点的创建类型需要设置为EPHEMERAL_SEQUENTIAL。<br>2.客户端调用getChildren(“<em>locknode</em>”)方法来获取所有已经创建的子节点，注意，这里不注册任何Watcher。<br>3.客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点序号最小，那么就认为这个客户端获得了锁。<br>4.如果在步骤3中发现自己并非所有子节点中最小的，说明自己还没有获取到锁。此时客户端需要找到比自己小的那个节点，然后对其调用exist()方法，同时注册事件监听。<br>5.之后当这个被关注的节点被移除了，客户端会收到相应的通知。这个时候客户端需要再次调用getChildren(“<em>locknode</em>”)方法来获取所有已经创建的子节点，确保自己确实是最小的节点了，然后进入步骤3。</p>
<h3 id="5-generate-brokerId"><a href="#5-generate-brokerId" class="headerlink" title="5.generate brokerId"></a>5.generate brokerId</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">config.brokerId =  getBrokerId</div><div class="line"><span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div></pre></td></tr></table></figure>
<p>略</p>
<h3 id="6-create-and-configure-metrics"><a href="#6-create-and-configure-metrics" class="headerlink" title="6.create and configure metrics"></a>6.create and configure metrics</h3><p>内部状态的监控模块<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> reporters = config.getConfiguredInstances(<span class="type">KafkaConfig</span>.<span class="type">MetricReporterClassesProp</span>, classOf[<span class="type">MetricsReporter</span>],</div><div class="line">            <span class="type">Map</span>[<span class="type">String</span>, <span class="type">AnyRef</span>](<span class="type">KafkaConfig</span>.<span class="type">BrokerIdProp</span> -&gt; (config.brokerId.toString)).asJava)</div><div class="line">reporters.add(<span class="keyword">new</span> <span class="type">JmxReporter</span>(jmxPrefix))</div><div class="line"><span class="keyword">val</span> metricConfig = <span class="type">KafkaServer</span>.metricConfig(config)</div><div class="line">metrics = <span class="keyword">new</span> <span class="type">Metrics</span>(metricConfig, reporters, time, <span class="literal">true</span>)</div><div class="line"></div><div class="line">quotaManagers = <span class="type">QuotaFactory</span>.instantiate(config, metrics, time)</div><div class="line">notifyClusterListeners(kafkaMetricsReporters ++ reporters.asScala)</div></pre></td></tr></table></figure></p>
<p>从配置参数metric.reporters 获取监控类</p>
<p>此处有个小概念;<br>集合允许使用asScala和asJava方法来做scala和java之间的转换</p>
<p>metricConfig里封装了<br>metrics.num.samples（     用于维护metrics的样本数）<br>metrics.recording.level<br>metrics.sample.window.ms（metrics系统维护可配置的样本数量，在一个可修正的window size。这项配置配置了窗口大小，例如。我们可能在30s的期间维护两个样本。当一个窗口推出后，我们会擦除并重写最老的窗口）</p>
<p>小概念：<br>4种操作符的区别和联系<br>:: 该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。用法为        x::list,其中x为加入到头部的元素，无论x是列表与否，它都只将成为新生成列表的第一个元素，也就是说新生成的列表长度为list的长度＋1(BTW ， x::list等价于list.::(x))<br>:+和+: 两者的区别在于:+方法用于在尾部追加元素，+:方法用于在头部追加元素，和::很类似，但是::可以用于pattern match ，而+:则不行. 关于+:和:+,只要记住冒号永远靠近集合类型就OK了。<br>++ 该方法用于连接两个集合，list1++list2<br>::: 该方法只能用于连接两个List类型的集合</p>
<h3 id="7-start-log-manager"><a href="#7-start-log-manager" class="headerlink" title="7.start log manager"></a>7.start log manager</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">logManager.startup()</div><div class="line"></div><div class="line">metadataCache = <span class="keyword">new</span> <span class="type">MetadataCache</span>(config.brokerId)</div><div class="line">credentialProvider = <span class="keyword">new</span> <span class="type">CredentialProvider</span>(config.saslEnabledMechanisms)</div><div class="line"></div><div class="line">socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</div><div class="line">socketServer.startup()</div></pre></td></tr></table></figure>
<p>根据一系列配置参数，启动LogManager。详情见kafka.log.CleanerConfig和kafka.log.LogManager</p>
<p>startup创建了4个线程，分别负责创建日志，写日志，检索日志，清理日志<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">    <span class="comment">/* Schedule the cleanup task to delete old logs */</span></div><div class="line">    <span class="keyword">if</span>(scheduler != <span class="literal">null</span>) &#123;</div><div class="line">      info(<span class="string">"Starting log cleanup with a period of %d ms."</span>.format(retentionCheckMs))</div><div class="line">      scheduler.schedule(<span class="string">"kafka-log-retention"</span>,</div><div class="line">                         cleanupLogs,</div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                         period = retentionCheckMs,</div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">      info(<span class="string">"Starting log flusher with a default period of %d ms."</span>.format(flushCheckMs))</div><div class="line">      scheduler.schedule(<span class="string">"kafka-log-flusher"</span>, </div><div class="line">                         flushDirtyLogs, </div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>, </div><div class="line">                         period = flushCheckMs, </div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">      scheduler.schedule(<span class="string">"kafka-recovery-point-checkpoint"</span>,</div><div class="line">                         checkpointRecoveryPointOffsets,</div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                         period = flushCheckpointMs,</div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">      scheduler.schedule(<span class="string">"kafka-delete-logs"</span>,</div><div class="line">                         deleteLogs,</div><div class="line">                         delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                         period = defaultConfig.fileDeleteDelayMs,</div><div class="line">                         <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span>(cleanerConfig.enableCleaner)</div><div class="line">      cleaner.startup()</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<h3 id="8-start-replica-manager"><a href="#8-start-replica-manager" class="headerlink" title="8.start replica manager"></a>8.start replica manager</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">          isShuttingDown, quotaManagers.follower)</div><div class="line">replicaManager.startup()</div></pre></td></tr></table></figure>
<p>启动isr-expiration线程<br>启动isr-change-propagation线程<br>在/controller下建了一个监听</p>
<p>此处有个小技巧<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inLock</span></span>[<span class="type">T</span>](lock: <span class="type">Lock</span>)(fun: =&gt; <span class="type">T</span>): <span class="type">T</span> = &#123;</div><div class="line">    lock.lock()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      fun</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      lock.unlock()</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<blockquote>
<p> : =&gt;注意:后面要有空格，此处标明调用的时候才执行，否则在用inlock的函数时候fun已经在锁外面执行了.</p>
</blockquote>
<p>参考：<a href="http://www.jianshu.com/p/f53e0b54a44a" target="_blank" rel="external">http://www.jianshu.com/p/f53e0b54a44a</a></p>
<h3 id="9-start-kafka-controller"><a href="#9-start-kafka-controller" class="headerlink" title="9.start kafka controller"></a>9.start kafka controller</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</div><div class="line">kafkaController.startup()</div><div class="line"></div><div class="line">adminManager = <span class="keyword">new</span> <span class="type">AdminManager</span>(config, metrics, metadataCache, zkUtils)</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() = &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      info(<span class="string">"Controller starting up"</span>)</div><div class="line">      registerSessionExpirationListener()</div><div class="line">      isRunning = <span class="literal">true</span></div><div class="line">      controllerElector.startup</div><div class="line">      info(<span class="string">"Controller startup complete"</span>)</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分支1.registerSessionExpirationListener-&gt;SessionExpirationListener-&gt;handleNewSession</p>
<p>当会话超时，重新连接上的时候，调用之前注册在ZookeeperLeaderElector的onControllerResignation函数<br>controllerElector.elect 重新选举</p>
<p>分支2.ZookeeperLeaderElector-&gt;（onControllerFailover，onControllerResignation）-&gt;LeaderChangeListener<br>controllerElector就是ZookeeperLeaderElector 是kafka的选举机制<br>ZookeeperLeaderElector：通过zk创建Ephemeral Node（临时节点）的方式来进行选举，即如果存在并发情况下向zk的同一个路径创建node的话，有且只有1个客户端会创建成功，其它客户端创建失败，但是当创建成功的客户端和zk的链接断开之后，这个node也会消失，其它的客户端从而继续竞争  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span> </span>&#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)</div><div class="line">      elect</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>1.监听electionPath（/controller）<br>2.elect选举</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> <span class="type">ControllerPath</span> = <span class="string">"/controller"</span></div><div class="line"><span class="keyword">val</span> electString = <span class="type">Json</span>.encode(<span class="type">Map</span>(<span class="string">"version"</span> -&gt; <span class="number">1</span>, <span class="string">"brokerid"</span> -&gt; brokerId, <span class="string">"timestamp"</span> -&gt; timestamp))</div><div class="line"><span class="keyword">val</span> zkCheckedEphemeral = <span class="keyword">new</span> <span class="type">ZKCheckedEphemeral</span>(electionPath,lectString, controllerContext.zkUtils.zkConnection.getZookeeper,  <span class="type">JaasUtils</span>.isZkSecurityEnabled())</div><div class="line">leaderId = getControllerID</div></pre></td></tr></table></figure>
<p>此处会在/controller 下面写一个类似如下的内容：<br>{“version”:1,”brokerid”:102,”timestamp”:”1495880001272”}<br>通过getControllerID获取当前的leaderId<br>然后通过amILeader看自己是否是leader</p>
<p>ZookeeperLeaderElecto<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeaderChangeListener</span> <span class="keyword">extends</span> <span class="title">IZkDataListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">    <span class="comment">/**</span></div><div class="line">     * Called when the leader information stored in zookeeper has changed. Record the new leader in memory</div><div class="line">     * @throws Exception On any error.</div><div class="line">     */</div><div class="line">    <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">Object</span>) &#123;</div><div class="line">      <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">        <span class="keyword">val</span> amILeaderBeforeDataChange = amILeader</div><div class="line">        leaderId = <span class="type">KafkaController</span>.parseControllerId(data.toString)</div><div class="line">        info(<span class="string">"New leader is %d"</span>.format(leaderId))</div><div class="line">        <span class="comment">// The old leader needs to resign leadership if it is no longer the leader</span></div><div class="line">        amILeaderBeforeDataChange &amp;&amp; !amILeader</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (shouldResign)</div><div class="line">        onResigningAsLeader()</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader</div><div class="line">     * @throws Exception</div><div class="line">     *             On any error.</div><div class="line">     */</div><div class="line">    <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123; </div><div class="line">      <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">        debug(<span class="string">"%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader"</span></div><div class="line">          .format(brokerId, dataPath))</div><div class="line">        amILeader</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (shouldResign)</div><div class="line">        onResigningAsLeader()</div><div class="line"></div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        elect</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>如果节点下线会调用handleDataDeleted。看自己是否是leader，如果是需要先退休onResigningAsLeader。<br>然后选举<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">val</span> zkCheckedEphemeral = <span class="keyword">new</span> <span class="type">ZKCheckedEphemeral</span>(electionPath,</div><div class="line">                                                      electString,</div><div class="line">                                                      controllerContext.zkUtils.zkConnection.getZookeeper,</div><div class="line">                                                      <span class="type">JaasUtils</span>.isZkSecurityEnabled())</div><div class="line">      zkCheckedEphemeral.create()</div><div class="line">      info(brokerId + <span class="string">" successfully elected as leader"</span>)</div><div class="line">      leaderId = brokerId</div><div class="line">      onBecomingLeader()</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</div><div class="line">        <span class="comment">// If someone else has written the path, then</span></div><div class="line">        leaderId = getControllerID </div><div class="line"></div><div class="line">        <span class="keyword">if</span> (leaderId != <span class="number">-1</span>)</div><div class="line">          debug(<span class="string">"Broker %d was elected as leader instead of broker %d"</span>.format(leaderId, brokerId))</div><div class="line">        <span class="keyword">else</span></div><div class="line">          warn(<span class="string">"A leader has been elected but just resigned, this will result in another round of election"</span>)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> e2: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Error while electing or becoming leader on broker %d"</span>.format(brokerId), e2)</div><div class="line">        resign()</div><div class="line">    &#125;</div><div class="line">    amILeader</div></pre></td></tr></table></figure></p>
<p>创建临时节点</p>
<p>onControllerFailover:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onControllerFailover</span></span>() &#123;</div><div class="line">    <span class="keyword">if</span>(isRunning) &#123;</div><div class="line">      info(<span class="string">"Broker %d starting become controller state transition"</span>.format(config.brokerId))</div><div class="line">      readControllerEpochFromZookeeper()</div><div class="line">      incrementControllerEpoch(zkUtils.zkClient)</div><div class="line"></div><div class="line">      <span class="comment">// before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks</span></div><div class="line">      registerReassignedPartitionsListener()</div><div class="line">      registerIsrChangeNotificationListener()</div><div class="line">      registerPreferredReplicaElectionListener()</div><div class="line">      partitionStateMachine.registerListeners()</div><div class="line">      replicaStateMachine.registerListeners()</div><div class="line"></div><div class="line">      initializeControllerContext()</div><div class="line"></div><div class="line">      <span class="comment">// We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines</span></div><div class="line">      <span class="comment">// are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before</span></div><div class="line">      <span class="comment">// they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and</span></div><div class="line">      <span class="comment">// partitionStateMachine.startup().</span></div><div class="line">      sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line"></div><div class="line">      replicaStateMachine.startup()</div><div class="line">      partitionStateMachine.startup()</div><div class="line"></div><div class="line">      <span class="comment">// register the partition change listeners for all existing topics on failover</span></div><div class="line">      controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">      info(<span class="string">"Broker %d is ready to serve as the new controller with epoch %d"</span>.format(config.brokerId, epoch))</div><div class="line">      maybeTriggerPartitionReassignment()</div><div class="line">      maybeTriggerPreferredReplicaElection()</div><div class="line">      <span class="keyword">if</span> (config.autoLeaderRebalanceEnable) &#123;</div><div class="line">        info(<span class="string">"starting the partition rebalance scheduler"</span>)</div><div class="line">        autoRebalanceScheduler.startup()</div><div class="line">        autoRebalanceScheduler.schedule(<span class="string">"partition-rebalance-thread"</span>, checkAndTriggerPartitionRebalance,</div><div class="line">          <span class="number">5</span>, config.leaderImbalanceCheckIntervalSeconds.toLong, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>)</div><div class="line">      &#125;</div><div class="line">      deleteTopicManager.start()</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span></div><div class="line">      info(<span class="string">"Controller has been shut down, aborting startup/failover"</span>)</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>在/admin/reassign_partitions目录注册PartitionsReassignedListener监听函数<br>在/isr_change_notification目录注册IsrChangeNotificationListener监听函数<br>在/admin/preferred_replica_election目录注册PreferredReplicaElectionListener监听函数<br>在/brokers/topics目录注册TopicChangeListener监听函数<br>在/admin/delete_topics目录注册DeleteTopicsListener监听函数<br>在/brokers/ids目录注册BrokerChangeListener监听函数</p>
<blockquote>
<p>监听是调用zk的<br>zkUtils.zkClient.subscribeChildChanges函数，参数是路径和监听函数<br>监听函数实现IZkChildListener接口实现handleChildChange方法</p>
</blockquote>
<p>初始化ControllerContext上下文,里面包含存活的broker，所有主题，分区副本，分区的leader和已经下线的broker。更新leader和isr缓存。启动ControllerChannelManager<br>初始化所有的replica状态<br>初始化所有的partition状态<br>如果auto.leader.rebalance.enable 为true会启动Rebalance调度<br>最后删除主题</p>
<p>通过replicaStateMachine初始化所有的replica状态<br>replicaStateMachine的handleStateChanges<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleStateChanges</span></span>(replicas: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>], targetState: <span class="type">ReplicaState</span>,</div><div class="line">                         callbacks: <span class="type">Callbacks</span> = (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build) &#123;</div><div class="line">    <span class="keyword">if</span>(replicas.nonEmpty) &#123;</div><div class="line">      info(<span class="string">"Invoking state change to %s for replicas %s"</span>.format(targetState, replicas.mkString(<span class="string">","</span>)))</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        brokerRequestBatch.newBatch()</div><div class="line">        replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks))</div><div class="line">        brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">      &#125;<span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some replicas to %s state"</span>.format(targetState), e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>通过partitionStateMachine初始化所有的partition状态<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">handleStateChange</span></span>(topic: <span class="type">String</span>, partition: <span class="type">Int</span>, targetState: <span class="type">PartitionState</span>,</div><div class="line">                                leaderSelector: <span class="type">PartitionLeaderSelector</span>,</div><div class="line">                                callbacks: <span class="type">Callbacks</span>) &#123;</div><div class="line">    <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</div><div class="line">    <span class="keyword">if</span> (!hasStarted.get)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>((<span class="string">"Controller %d epoch %d initiated state change for partition %s to %s failed because "</span> +</div><div class="line">                                            <span class="string">"the partition state machine has not started"</span>)</div><div class="line">                                              .format(controllerId, controller.epoch, topicAndPartition, targetState))</div><div class="line">    <span class="keyword">val</span> currState = partitionState.getOrElseUpdate(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      targetState <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt;</div><div class="line">          <span class="comment">// pre: partition did not exist before this</span></div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NonExistentPartition</span>), <span class="type">NewPartition</span>)</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">NewPartition</span>)</div><div class="line">          <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(<span class="string">","</span>)</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,</div><div class="line">                                            assignedReplicas))</div><div class="line">          <span class="comment">// post: partition has been assigned replicas</span></div><div class="line">        <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt;</div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OnlinePartition</span>)</div><div class="line">          partitionState(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt;</div><div class="line">              <span class="comment">// initialize leader and isr path for new partition</span></div><div class="line">              initializeLeaderAndIsrForPartition(topicAndPartition)</div><div class="line">            <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt;</div><div class="line">              electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">            <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt; <span class="comment">// invoked when the leader needs to be re-elected</span></div><div class="line">              electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">            <span class="keyword">case</span> _ =&gt; <span class="comment">// should never come here since illegal previous states are checked above</span></div><div class="line">          &#125;</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">OnlinePartition</span>)</div><div class="line">          <span class="keyword">val</span> leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s from %s to %s with leader %d"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader))</div><div class="line">           <span class="comment">// post: partition has a leader</span></div><div class="line">        <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt;</div><div class="line">          <span class="comment">// pre: partition should be in New or Online state</span></div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OfflinePartition</span>)</div><div class="line">          <span class="comment">// should be called when the leader for a partition is no longer alive</span></div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">OfflinePartition</span>)</div><div class="line">          <span class="comment">// post: partition has no alive leader</span></div><div class="line">        <span class="keyword">case</span> <span class="type">NonExistentPartition</span> =&gt;</div><div class="line">          <span class="comment">// pre: partition should be in Offline state</span></div><div class="line">          assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">OfflinePartition</span>), <span class="type">NonExistentPartition</span>)</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">          partitionState.put(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line">          <span class="comment">// post: partition state is deleted from all brokers and zookeeper</span></div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">        stateChangeLogger.error(<span class="string">"Controller %d epoch %d initiated state change for partition %s from %s to %s failed"</span></div><div class="line">          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState), t)</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>PartitionStateMachine实现了topic的分区状态切换功能，Partition存在的状态如下：<br>NewPartition  分区之前不存在，创建后被分配了replicas，但是还没有leader/isr<br>OnlinePartition  partition在replicas中选举某个成为leader之后<br>OfflinePartition  partition的replicas中的leader下线之后，没有重新选举新的leader之前  或 partition创建之后直接被下线<br>NonExistentPartition  partition重来没有被创建 或 partition创建之后被删除</p>
<p>scala小知识：<br>mkString<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">scala &gt; <span class="keyword">val</span> a = <span class="type">Array</span>(<span class="string">"apple"</span>, <span class="string">"banana"</span>, <span class="string">"cherry"</span>)</div><div class="line">a: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(apple, banana, cherry)</div><div class="line">scala &gt; a.mkString(<span class="string">","</span>)</div><div class="line">res2: <span class="type">String</span> = apple,banana,cherry</div><div class="line"></div><div class="line">scala&gt; a.mkString(<span class="string">"["</span>, <span class="string">", "</span>, <span class="string">"]"</span>)</div><div class="line">res3: <span class="type">String</span> = [apple, banana, cherry]</div><div class="line"></div><div class="line">如果是数组需要先展开数组</div><div class="line">scala&gt; <span class="keyword">val</span> b = <span class="type">Array</span>(<span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>), <span class="type">Array</span>(<span class="string">"c"</span>, <span class="string">"d"</span>))</div><div class="line">b: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(a, b), <span class="type">Array</span>(c, d))</div><div class="line">错误的</div><div class="line">scala&gt; b.mkString(<span class="string">","</span>)</div><div class="line">res4: <span class="type">String</span> = [<span class="type">Ljava</span>.lang.<span class="type">String</span>;@<span class="number">64</span>a9fca7,[<span class="type">Ljava</span>.lang.<span class="type">String</span>;@<span class="number">22</span>f756c5</div><div class="line">正确的</div><div class="line">scala&gt; b.flatten.mkString(<span class="string">","</span>)</div><div class="line">res5: <span class="type">String</span> = a,b,c,d</div></pre></td></tr></table></figure></p>
<p>OnlinePartition ：检查前置状态是否为NewPartition, OnlinePartition, OfflinePartition中的一种，<br>1.如果是NewPartition：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeLeaderAndIsrForPartition</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>) &#123;</div><div class="line">    <span class="keyword">val</span> replicaAssignment = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">val</span> liveAssignedReplicas = replicaAssignment.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">    liveAssignedReplicas.size <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="number">0</span> =&gt;</div><div class="line">        <span class="keyword">val</span> failMsg = (<span class="string">"encountered error during state change of partition %s from New to Online, assigned replicas are [%s], "</span> +</div><div class="line">                       <span class="string">"live brokers are [%s]. No assigned replica is alive."</span>)</div><div class="line">                         .format(topicAndPartition, replicaAssignment.mkString(<span class="string">","</span>), controllerContext.liveBrokerIds)</div><div class="line">        stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">      <span class="keyword">case</span> _ =&gt;</div><div class="line">        debug(<span class="string">"Live assigned replicas for partition %s are: [%s]"</span>.format(topicAndPartition, liveAssignedReplicas))</div><div class="line">        <span class="comment">// make the first replica in the list of assigned replicas, the leader</span></div><div class="line">        <span class="comment">//根据partitionReplicaAssignment中信息选择第一个live的replica为leader,其余为isr</span></div><div class="line">        <span class="keyword">val</span> leader = liveAssignedReplicas.head</div><div class="line">        <span class="keyword">val</span> leaderIsrAndControllerEpoch = <span class="keyword">new</span> <span class="type">LeaderIsrAndControllerEpoch</span>(<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(leader, liveAssignedReplicas.toList),</div><div class="line">          controller.epoch)</div><div class="line">        debug(<span class="string">"Initializing leader and isr for partition %s to %s"</span>.format(topicAndPartition, leaderIsrAndControllerEpoch))</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="comment">//将leader和isr持久化到zookeeper</span></div><div class="line">          zkUtils.createPersistentPath(</div><div class="line">            getTopicPartitionLeaderAndIsrPath(topicAndPartition.topic, topicAndPartition.partition),</div><div class="line">            zkUtils.leaderAndIsrZkData(leaderIsrAndControllerEpoch.leaderAndIsr, controller.epoch))</div><div class="line">          <span class="comment">// <span class="doctag">NOTE:</span> the above write can fail only if the current controller lost its zk session and the new controller</span></div><div class="line">          <span class="comment">// took over and initialized this partition. This can happen if the current controller went into a long</span></div><div class="line">          <span class="comment">// GC pause</span></div><div class="line">          <span class="comment">//更新controllerContext中的partitionLeadershipInfo</span></div><div class="line">          controllerContext.partitionLeadershipInfo.put(topicAndPartition, leaderIsrAndControllerEpoch)</div><div class="line">          <span class="comment">//封装发送给这些replica所在的broker的LeaderAndIsrRequest请求，交由ControllerBrokerRequestBatch(brokerRequestBatch)处理</span></div><div class="line">          brokerRequestBatch.addLeaderAndIsrRequestForBrokers(liveAssignedReplicas, topicAndPartition.topic,</div><div class="line">            topicAndPartition.partition, leaderIsrAndControllerEpoch, replicaAssignment)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</div><div class="line">            <span class="comment">// read the controller epoch</span></div><div class="line">            <span class="keyword">val</span> leaderIsrAndEpoch = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topicAndPartition.topic,</div><div class="line">              topicAndPartition.partition).get</div><div class="line">            <span class="keyword">val</span> failMsg = (<span class="string">"encountered error while changing partition %s's state from New to Online since LeaderAndIsr path already "</span> +</div><div class="line">                           <span class="string">"exists with value %s and controller epoch %d"</span>)</div><div class="line">                             .format(topicAndPartition, leaderIsrAndEpoch.leaderAndIsr.toString(), leaderIsrAndEpoch.controllerEpoch)</div><div class="line">            stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>2.如果是OfflinePartition，OnlinePartition<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">electLeaderForPartition</span></span>(topic: <span class="type">String</span>, partition: <span class="type">Int</span>, leaderSelector: <span class="type">PartitionLeaderSelector</span>) &#123;</div><div class="line">    <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</div><div class="line">    <span class="comment">// handle leader election for the partitions whose leader is no longer alive</span></div><div class="line">    stateChangeLogger.trace(<span class="string">"Controller %d epoch %d started leader election for partition %s"</span></div><div class="line">                              .format(controllerId, controller.epoch, topicAndPartition))</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">var</span> zookeeperPathUpdateSucceeded: <span class="type">Boolean</span> = <span class="literal">false</span></div><div class="line">      <span class="keyword">var</span> newLeaderAndIsr: <span class="type">LeaderAndIsr</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">var</span> replicasForThisPartition: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">Seq</span>.empty[<span class="type">Int</span>]</div><div class="line">      <span class="keyword">while</span>(!zookeeperPathUpdateSucceeded) &#123;</div><div class="line">        <span class="keyword">val</span> currentLeaderIsrAndEpoch = getLeaderIsrAndEpochOrThrowException(topic, partition)</div><div class="line">        <span class="keyword">val</span> currentLeaderAndIsr = currentLeaderIsrAndEpoch.leaderAndIsr</div><div class="line">        <span class="keyword">val</span> controllerEpoch = currentLeaderIsrAndEpoch.controllerEpoch</div><div class="line">        <span class="keyword">if</span> (controllerEpoch &gt; controller.epoch) &#123;</div><div class="line">          <span class="keyword">val</span> failMsg = (<span class="string">"aborted leader election for partition [%s,%d] since the LeaderAndIsr path was "</span> +</div><div class="line">                         <span class="string">"already written by another controller. This probably means that the current controller %d went through "</span> +</div><div class="line">                         <span class="string">"a soft failure and another controller was elected with epoch %d."</span>)</div><div class="line">                           .format(topic, partition, controllerId, controllerEpoch)</div><div class="line">          stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// elect new leader or throw exception</span></div><div class="line">        <span class="keyword">val</span> (leaderAndIsr, replicas) = leaderSelector.selectLeader(topicAndPartition, currentLeaderAndIsr)</div><div class="line">        <span class="keyword">val</span> (updateSucceeded, newVersion) = <span class="type">ReplicationUtils</span>.updateLeaderAndIsr(zkUtils, topic, partition,</div><div class="line">          leaderAndIsr, controller.epoch, currentLeaderAndIsr.zkVersion)</div><div class="line">        <span class="comment">//根据不同的leaderSelector选举新的leader，这里一般调用的是OfflinePartitionLeaderSelector</span></div><div class="line">        newLeaderAndIsr = leaderAndIsr</div><div class="line">        newLeaderAndIsr.zkVersion = newVersion</div><div class="line">        <span class="comment">//将leader和isr持久化到zookeeper</span></div><div class="line">        zookeeperPathUpdateSucceeded = updateSucceeded</div><div class="line">        replicasForThisPartition = replicas</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> newLeaderIsrAndControllerEpoch = <span class="keyword">new</span> <span class="type">LeaderIsrAndControllerEpoch</span>(newLeaderAndIsr, controller.epoch)</div><div class="line">      <span class="comment">// update the leader cache</span></div><div class="line">      <span class="comment">//更新controllerContext中的partitionLeadershipInfo</span></div><div class="line">      controllerContext.partitionLeadershipInfo.put(<span class="type">TopicAndPartition</span>(topic, partition), newLeaderIsrAndControllerEpoch)</div><div class="line">      stateChangeLogger.trace(<span class="string">"Controller %d epoch %d elected leader %d for Offline partition %s"</span></div><div class="line">                                .format(controllerId, controller.epoch, newLeaderAndIsr.leader, topicAndPartition))</div><div class="line">      <span class="keyword">val</span> replicas = controllerContext.partitionReplicaAssignment(<span class="type">TopicAndPartition</span>(topic, partition))</div><div class="line">      <span class="comment">// store new leader and isr info in cache</span></div><div class="line">      <span class="comment">//封装发送给这些replica所在的broker的LeaderAndIsrRequest请求，交由ControllerBrokerRequestBatch(brokerRequestBatch)处理</span></div><div class="line">      brokerRequestBatch.addLeaderAndIsrRequestForBrokers(replicasForThisPartition, topic, partition,</div><div class="line">        newLeaderIsrAndControllerEpoch, replicas)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> _: <span class="type">LeaderElectionNotNeededException</span> =&gt; <span class="comment">// swallow</span></div><div class="line">      <span class="keyword">case</span> nroe: <span class="type">NoReplicaOnlineException</span> =&gt; <span class="keyword">throw</span> nroe</div><div class="line">      <span class="keyword">case</span> sce: <span class="type">Throwable</span> =&gt;</div><div class="line">        <span class="keyword">val</span> failMsg = <span class="string">"encountered error while electing leader for partition %s due to: %s."</span>.format(topicAndPartition, sce.getMessage)</div><div class="line">        stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg, sce)</div><div class="line">    &#125;</div><div class="line">    debug(<span class="string">"After leader election, leader cache is updated to %s"</span>.format(controllerContext.partitionLeadershipInfo.map(l =&gt; (l._1, l._2))))</div><div class="line">  &#125;</div><div class="line">···</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">在brokers/topics<span class="comment">/***(具体的topic名字)/目录下注册PartitionModificationsListener-&gt;AddPartitionsListener监听</span></div><div class="line">通过处理之前启动留下的partition重分配的情况</div><div class="line">处理之前启动留下的replica重新选举的情况</div><div class="line">向其它KafkaServer发送集群topic的元数据信息已进行数据的同步更新</div><div class="line">根据配置是否开启自动均衡</div><div class="line">开始删除topic</div><div class="line"></div><div class="line">### 10.start group coordinator</div><div class="line">```scala</div><div class="line">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</div><div class="line">groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, Time.SYSTEM)</div><div class="line">groupCoordinator.startup()</div></pre></td></tr></table></figure></p>
<h3 id="11-Get-the-authorizer-and-initialize-it-if-one-is-specified"><a href="#11-Get-the-authorizer-and-initialize-it-if-one-is-specified" class="headerlink" title="11.Get the authorizer and initialize it if one is specified."></a>11.Get the authorizer and initialize it if one is specified.</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">authorizer = <span class="type">Option</span>(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt;</div><div class="line">          <span class="keyword">val</span> authZ = <span class="type">CoreUtils</span>.createObject[<span class="type">Authorizer</span>](authorizerClassName)</div><div class="line">          authZ.configure(config.originals())</div><div class="line">          authZ</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="12-start-processing-requests"><a href="#12-start-processing-requests" class="headerlink" title="12.start processing requests"></a>12.start processing requests</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</div><div class="line">  kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</div><div class="line">  clusterId, time)</div><div class="line">requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</div><div class="line">  config.numIoThreads)</div><div class="line"><span class="type">Mx4jLoader</span>.maybeLoad()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="13-start-dynamic-config-manager"><a href="#13-start-dynamic-config-manager" class="headerlink" title="13.start dynamic config manager"></a>13.start dynamic config manager</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> dynamicConfigHandlers = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ConfigHandler</span>](<span class="type">ConfigType</span>.<span class="type">Topic</span> -&gt; <span class="keyword">new</span> <span class="type">TopicConfigHandler</span>(logManager, config, quotaManagers),<span class="type">ConfigType</span>.<span class="type">Client</span> -&gt; <span class="keyword">new</span> <span class="type">ClientIdConfigHandler</span>(quotaManagers),<span class="type">ConfigType</span>.<span class="type">User</span> -&gt; <span class="keyword">new</span> <span class="type">UserConfigHandler</span>(quotaManagers, credentialProvider),<span class="type">ConfigType</span>.<span class="type">Broker</span> -&gt; <span class="keyword">new</span> <span class="type">BrokerConfigHandler</span>(config, quotaManagers))</div><div class="line"></div><div class="line"><span class="comment">// Create the config manager. start listening to notifications</span></div><div class="line">dynamicConfigManager = <span class="keyword">new</span> <span class="type">DynamicConfigManager</span>(zkUtils, dynamicConfigHandlers)</div><div class="line">dynamicConfigManager.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="14-tell-everyone-we-are-alive"><a href="#14-tell-everyone-we-are-alive" class="headerlink" title="14.tell everyone we are alive"></a>14.tell everyone we are alive</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> listeners = config.advertisedListeners.map &#123; endpoint =&gt;</div><div class="line">          <span class="keyword">if</span> (endpoint.port == <span class="number">0</span>)</div><div class="line">            endpoint.copy(port = socketServer.boundPort(endpoint.listenerName))</div><div class="line">          <span class="keyword">else</span></div><div class="line">            endpoint</div><div class="line">&#125;</div><div class="line">kafkaHealthcheck = <span class="keyword">new</span> <span class="type">KafkaHealthcheck</span>(config.brokerId, listeners, zkUtils, config.rack,</div><div class="line">  config.interBrokerProtocolVersion)</div><div class="line">kafkaHealthcheck.startup()</div><div class="line"></div><div class="line"><span class="comment">// Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it</span></div><div class="line">checkpointBrokerId(config.brokerId)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>kafkaHealthcheck.startup()<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">   zkUtils.zkClient.subscribeStateChanges(sessionExpireListener)</div><div class="line">   register()</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="comment">/**</span></div><div class="line">  * Register this broker as "alive" in zookeeper</div><div class="line">  */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">register</span></span>() &#123;</div><div class="line">   <span class="keyword">val</span> jmxPort = <span class="type">System</span>.getProperty(<span class="string">"com.sun.management.jmxremote.port"</span>, <span class="string">"-1"</span>).toInt</div><div class="line">   <span class="keyword">val</span> updatedEndpoints = advertisedEndpoints.map(endpoint =&gt;</div><div class="line">     <span class="keyword">if</span> (endpoint.host == <span class="literal">null</span> || endpoint.host.trim.isEmpty)</div><div class="line">       endpoint.copy(host = <span class="type">InetAddress</span>.getLocalHost.getCanonicalHostName)</div><div class="line">     <span class="keyword">else</span></div><div class="line">       endpoint</div><div class="line">   )</div><div class="line"></div><div class="line">   <span class="comment">// the default host and port are here for compatibility with older clients that only support PLAINTEXT</span></div><div class="line">   <span class="comment">// we choose the first plaintext port, if there is one</span></div><div class="line">   <span class="comment">// or we register an empty endpoint, which means that older clients will not be able to connect</span></div><div class="line">   <span class="keyword">val</span> plaintextEndpoint = updatedEndpoints.find(_.securityProtocol == <span class="type">SecurityProtocol</span>.<span class="type">PLAINTEXT</span>).getOrElse(</div><div class="line">     <span class="keyword">new</span> <span class="type">EndPoint</span>(<span class="literal">null</span>, <span class="number">-1</span>, <span class="literal">null</span>, <span class="literal">null</span>))</div><div class="line">   zkUtils.registerBrokerInZk(brokerId, plaintextEndpoint.host, plaintextEndpoint.port, updatedEndpoints, jmxPort, rack,</div><div class="line">     interBrokerProtocolVersion)</div><div class="line"> &#125;</div></pre></td></tr></table></figure></p>
<p>注册新的brokerid</p>
<h3 id="15-register-broker-metrics"><a href="#15-register-broker-metrics" class="headerlink" title="15.register broker metrics."></a>15.register broker metrics.</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> registerStats()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;导入IDEA即可看kafka源码：&quot;&gt;&lt;a href=&quot;#导入IDEA即可看kafka源码：&quot; class=&quot;headerlink&quot; title=&quot;导入IDEA即可看kafka源码：&quot;&gt;&lt;/a&gt;导入IDEA即可看kafka源码：&lt;/h2&gt;&lt;h2 id=&quot;启动之前需要安装zookeeper&quot;&gt;&lt;a href=&quot;#启动之前需要安装zookeeper&quot; class=&quot;headerlink&quot; title=&quot;启动之前需要安装zookeeper&quot;&gt;&lt;/a&gt;启动之前需要安装zookeeper&lt;/h2&gt;&lt;p&gt;地址：&lt;br&gt;&lt;a href=&quot;http://apache.fayea.com/zookeeper/&quot;&gt;http://apache.fayea.com/zookeeper/&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz&quot;&gt;http://apache.fayea.com/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;解压后再当前目录增加&quot;&gt;&lt;a href=&quot;#解压后再当前目录增加&quot; class=&quot;headerlink&quot; title=&quot;解压后再当前目录增加&quot;&gt;&lt;/a&gt;解压后再当前目录增加&lt;/h2&gt;&lt;p&gt;dataLogDir和data目录&lt;br&gt;复制一份配置文件&lt;br&gt;改名为zoo.cfg&lt;br&gt;修改配置文件：&lt;br&gt;zoo.cfg&lt;br&gt;修改并增加&lt;br&gt;dataDir=D:\tool\zookeeper-3.4.6\data&lt;br&gt;dataLogDir=D:\tool\zookeeper-3.4.6\dataLogDir&lt;/p&gt;
&lt;h2 id=&quot;启动zkServer-cmd&quot;&gt;&lt;a href=&quot;#启动zkServer-cmd&quot; class=&quot;headerlink&quot; title=&quot;启动zkServer.cmd&quot;&gt;&lt;/a&gt;启动zkServer.cmd&lt;/h2&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://yoursite.com/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka源码阅读</title>
    <link href="http://yoursite.com/2017/05/02/kafka%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2017/05/02/kafka源码阅读/</id>
    <published>2017-05-02T03:30:00.000Z</published>
    <updated>2017-05-18T00:58:23.538Z</updated>
    
    <content type="html"><![CDATA[<h3 id="下载kafka的源码包："><a href="#下载kafka的源码包：" class="headerlink" title="下载kafka的源码包："></a>下载kafka的源码包：</h3><p><a href="http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz" target="_blank" rel="external">http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz</a></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>gradle </p>
<p>1.官网下载<br><a href="https://gradle.org/releases" target="_blank" rel="external">https://gradle.org/releases</a><br>选择当时最新版本：<br><a href="https://downloads.gradle.org/distributions/gradle-3.5-all.zip" target="_blank" rel="external">https://downloads.gradle.org/distributions/gradle-3.5-all.zip</a></p>
<p>2.解压<br>解压到D:\tool\gradle-3.5</p>
<p>3.配置环境变量<br>GRADLE_HOME<br>D:\tool\gradle-3.5</p>
<p>Path追加;%GRADLE_HOME%\BIN;<br><a id="more"></a><br>4.测试：<br>gradle -v<br><img src="http://oh6ybr0jg.bkt.clouddn.com/gradle.jpg" alt="此处输入图片的描述"></p>
<p>安装参考:<br><a href="http://blog.csdn.net/lizhitao/article/details/26875463" target="_blank" rel="external">http://blog.csdn.net/lizhitao/article/details/26875463</a></p>
<h3 id="编译：（此处博主用某云主机编译的，很快）"><a href="#编译：（此处博主用某云主机编译的，很快）" class="headerlink" title="编译：（此处博主用某云主机编译的，很快）"></a>编译：（此处博主用某云主机编译的，很快）</h3><p>gradle idea</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20170427134226.jpg" alt="此处输入图片的描述"></p>
<p>（编译完把/root/.gradle/caches/modules-2下下载的文件放到我们的环境中C:\Users\Administrator.gradle\caches\modules-2 再windows上编译 ）<br><img src="http://oh6ybr0jg.bkt.clouddn.com/windows%E7%BC%96%E8%AF%91kafka.jpg" alt="此处输入图片的描述"></p>
<p>用IDEA工具打开<br><img src="http://oh6ybr0jg.bkt.clouddn.com/kafka_src_emv.jpg" alt="此处输入图片的描述"></p>
<h3 id="目录介绍："><a href="#目录介绍：" class="headerlink" title="目录介绍："></a>目录介绍：</h3><table>
<thead>
<tr>
<th>模块名</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>admin</td>
<td>管理员模块，操作和管理topic，paritions相关，包含create/delete topic,扩展patitions</td>
</tr>
<tr>
<td>api</td>
<td>主要负责交互数据的组装，客户端与服务端交互数据编解码</td>
</tr>
<tr>
<td>client</td>
<td>Producer读取kafka broker元数据信息，topic和partitions，以及leader</td>
</tr>
<tr>
<td>cluster</td>
<td>包含几个实体类，Broker,Cluster,Partition,Replica,解释他们之间关系：          Cluster由多个broker组成，一个Broker包含多个partition，一个topic的所有partitions分布在不同broker的中，一个Replica包含多个Partition。</td>
</tr>
<tr>
<td>common</td>
<td>异常类和错误验证</td>
</tr>
<tr>
<td>consumer</td>
<td>负责所有客户端消费者数据和逻辑处理</td>
</tr>
<tr>
<td>controller</td>
<td>负责中央控制器选举，partition的leader选举，副本分配，副本重新分配，partition和replica扩容。</td>
</tr>
<tr>
<td>coordinator</td>
<td>partition分配机制</td>
</tr>
<tr>
<td>javaapi</td>
<td>提供java的producer和consumer接口api</td>
</tr>
<tr>
<td>log</td>
<td>Kafka文件存储模块，负责读写所有kafka的topic消息数据。</td>
</tr>
<tr>
<td>message</td>
<td>封装多个消息组成一个“消息集”或压缩消息集。</td>
</tr>
<tr>
<td>metrics</td>
<td>内部状态的监控模块</td>
</tr>
<tr>
<td>network</td>
<td>网络事件处理模块，负责处理和接收客户端连接</td>
</tr>
<tr>
<td>producer</td>
<td>producer实现模块，包括同步和异步发送消息。</td>
</tr>
<tr>
<td>security</td>
<td>安全</td>
</tr>
<tr>
<td>serializer</td>
<td>序列化或反序列化当前消息</td>
</tr>
<tr>
<td>server</td>
<td></td>
</tr>
<tr>
<td>tools</td>
<td>工具模块，    包含a.导出对应consumer的offset值.b.导出LogSegments信息，当前topic的log写的位置信息.c.导出zk上所有consumer的offset值.d.修改注册在zk的consumer的offset值.f.producer和consumer的使用例子.</td>
</tr>
<tr>
<td>utils</td>
<td>Json工具类，Zkutils工具类，Utils创建线程工具类，KafkaScheduler公共调度器类，公共日志类等等。</td>
</tr>
</tbody>
</table>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;下载kafka的源码包：&quot;&gt;&lt;a href=&quot;#下载kafka的源码包：&quot; class=&quot;headerlink&quot; title=&quot;下载kafka的源码包：&quot;&gt;&lt;/a&gt;下载kafka的源码包：&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz&quot;&gt;http://archive.apache.org/dist/kafka/0.10.2.1/kafka-0.10.2.1-src.tgz&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h3&gt;&lt;p&gt;gradle &lt;/p&gt;
&lt;p&gt;1.官网下载&lt;br&gt;&lt;a href=&quot;https://gradle.org/releases&quot;&gt;https://gradle.org/releases&lt;/a&gt;&lt;br&gt;选择当时最新版本：&lt;br&gt;&lt;a href=&quot;https://downloads.gradle.org/distributions/gradle-3.5-all.zip&quot;&gt;https://downloads.gradle.org/distributions/gradle-3.5-all.zip&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2.解压&lt;br&gt;解压到D:\tool\gradle-3.5&lt;/p&gt;
&lt;p&gt;3.配置环境变量&lt;br&gt;GRADLE_HOME&lt;br&gt;D:\tool\gradle-3.5&lt;/p&gt;
&lt;p&gt;Path追加;%GRADLE_HOME%\BIN;&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://yoursite.com/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>CDH phoenix安装</title>
    <link href="http://yoursite.com/2017/04/18/CDH%20phoenix%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/04/18/CDH phoenix安装/</id>
    <published>2017-04-18T03:30:00.000Z</published>
    <updated>2018-01-15T16:45:56.209Z</updated>
    
    <content type="html"><![CDATA[<h3 id="首先下载jdk-略"><a href="#首先下载jdk-略" class="headerlink" title="首先下载jdk 略"></a>首先下载jdk 略</h3><p>安装maven</p>
<h2 id="下载yum源"><a href="#下载yum源" class="headerlink" title="下载yum源"></a>下载yum源</h2><p>wget <a href="http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo" target="_blank" rel="external">http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo</a> -O /etc/yum.repos.d/epel-apache-maven.repo</p>
<h2 id="安装maven："><a href="#安装maven：" class="headerlink" title="安装maven："></a>安装maven：</h2><p>yum -y install apache-maven</p>
<a id="more"></a>
<h2 id="编译phoenix"><a href="#编译phoenix" class="headerlink" title="编译phoenix"></a>编译phoenix</h2><p>找到最新版本的phoenix<br><a href="https://github.com/chiastic-security/phoenix-for-cloudera/tree/4.8-HBase-1.2-cdh5.8" target="_blank" rel="external">https://github.com/chiastic-security/phoenix-for-cloudera/tree/4.8-HBase-1.2-cdh5.8</a></p>
<h2 id="下载（博主下载到-soft下）"><a href="#下载（博主下载到-soft下）" class="headerlink" title="下载（博主下载到/soft下）"></a>下载（博主下载到/soft下）</h2><p>并编译<br>mvn clean package -DskipTests -Dcdh.flume.version=1.6.0</p>
<h2 id="打开路径："><a href="#打开路径：" class="headerlink" title="打开路径："></a>打开路径：</h2><p>/soft/phoenix-for-cloudera-4.8-HBase-1.2-cdh5.8/phoenix-assembly/target</p>
<p>找到phoenix-4.8.0-cdh5.8.0.tar.gz<br>将phoenix-4.8.0-cdh5.8.0中的phoenix-4.8.0-cdh5.8.0-server.jar拷贝到每一个RegionServer下/opt/cloudera/parcels/CDH/lib/hbase/lib</p>
<h2 id="启动："><a href="#启动：" class="headerlink" title="启动："></a>启动：</h2><p>./sqlline.py slave1:2181/hbase</p>
<h3 id="如果报错："><a href="#如果报错：" class="headerlink" title="如果报错："></a>如果报错：</h3><p>Error: org.apache.hadoop.hbase.DoNotRetryIOException: Class org.apache.phoenix.coprocessor.MetaDataEndpointImpl cannot be loaded Set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks<br>        at org.apache.hadoop.hbase.master.HMaster.warnOrThrowExceptionForFailure(HMaster.java:1741)<br>        at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1602)<br>        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1531)<br>        at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:469)<br>        at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55682)<br>        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170)<br>        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)<br>        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:185)<br>        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:165) (state=08000,code=101)</p>
<p>需要点击<br>CDH-&gt;hbase -&gt;配置-&gt;高级<br>hbase-site.xml 的 HBase 服务高级配置代码段（安全阀）<br>添加<br>hbase.table.sanity.checks<br>false</p>
<h2 id="重启即可"><a href="#重启即可" class="headerlink" title="重启即可"></a>重启即可</h2><h2 id="如果想要安装phonenix"><a href="#如果想要安装phonenix" class="headerlink" title="如果想要安装phonenix"></a>如果想要安装phonenix</h2><p>先下载<br><a href="http://squirrel-sql.sourceforge.net/#installation" target="_blank" rel="external">http://squirrel-sql.sourceforge.net/#installation</a></p>
<h2 id="安装（安装的时候勾选imort-data和mysql）"><a href="#安装（安装的时候勾选imort-data和mysql）" class="headerlink" title="安装（安装的时候勾选imort-data和mysql）"></a>安装（安装的时候勾选imort-data和mysql）</h2><p>注：下载后直接安装jar包即可，不要解压缩<br>由于是CDH 我拷贝了如下jar防盗lib下面<br>phoenix-core-4.8.0-cdh5.8.0.jar<br>phoenix-4.8.0-cdh5.8.0-client.jar<br>phoenix-pherf-4.8.0-cdh5.8.0-minimal.jar</p>
<h2 id="链接参考"><a href="#链接参考" class="headerlink" title="链接参考;"></a>链接参考;</h2><p><a href="http://www.cnblogs.com/raphael5200/p/5260198.html" target="_blank" rel="external">http://www.cnblogs.com/raphael5200/p/5260198.html</a><br>有图</p>
<h2 id="phoenix-API"><a href="#phoenix-API" class="headerlink" title="phoenix API"></a>phoenix API</h2><p>　　<a href="http://phoenix.apache.org/language/functions.html" target="_blank" rel="external">http://phoenix.apache.org/language/functions.html</a><br>　　<a href="http://phoenix.apache.org/language/index.html" target="_blank" rel="external">http://phoenix.apache.org/language/index.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;首先下载jdk-略&quot;&gt;&lt;a href=&quot;#首先下载jdk-略&quot; class=&quot;headerlink&quot; title=&quot;首先下载jdk 略&quot;&gt;&lt;/a&gt;首先下载jdk 略&lt;/h3&gt;&lt;p&gt;安装maven&lt;/p&gt;
&lt;h2 id=&quot;下载yum源&quot;&gt;&lt;a href=&quot;#下载yum源&quot; class=&quot;headerlink&quot; title=&quot;下载yum源&quot;&gt;&lt;/a&gt;下载yum源&lt;/h2&gt;&lt;p&gt;wget &lt;a href=&quot;http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo&quot;&gt;http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo&lt;/a&gt; -O /etc/yum.repos.d/epel-apache-maven.repo&lt;/p&gt;
&lt;h2 id=&quot;安装maven：&quot;&gt;&lt;a href=&quot;#安装maven：&quot; class=&quot;headerlink&quot; title=&quot;安装maven：&quot;&gt;&lt;/a&gt;安装maven：&lt;/h2&gt;&lt;p&gt;yum -y install apache-maven&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH-spark</title>
    <link href="http://yoursite.com/2017/03/29/CDH-spark/"/>
    <id>http://yoursite.com/2017/03/29/CDH-spark/</id>
    <published>2017-03-29T03:30:00.000Z</published>
    <updated>2017-03-31T13:27:02.808Z</updated>
    
    <content type="html"><![CDATA[<p>添加服务选择spark on yarn<br>安装的时候会选择history server，博主选择的是slave1，后续可以通过这个地址看spark的UI</p>
<p>博主安装的是：<br>scala     version 2.10.5（后续再UI上看到的）<br>spark   version 1.6.0（执行spark-shell时看到的）</p>
<p>安装完后测试：</p>
<p>su hdfs<br>spark-submit –class org.apache.spark.examples.SparkPi –executor-memory 1G –total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100</p>
<p>查看UI：<br><a href="http://slave1:18088/" target="_blank" rel="external">http://slave1:18088/</a></p>
<a id="more"></a>
<p>编写代码测试离线功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</div><div class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</div><div class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</div><div class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</div><div class="line"></div><div class="line">    &lt;groupId&gt;cn.zwjf.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;SpakrZwujf&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</div><div class="line"></div><div class="line"></div><div class="line">    &lt;properties&gt;</div><div class="line">        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;</div><div class="line">        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;</div><div class="line">        &lt;encoding&gt;UTF-8&lt;/encoding&gt;</div><div class="line">        &lt;scala.version&gt;2.10.5&lt;/scala.version&gt;</div><div class="line">        &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt;</div><div class="line">    &lt;/properties&gt;</div><div class="line"></div><div class="line">    &lt;dependencies&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;1.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;1.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">    &lt;/dependencies&gt;</div><div class="line"></div><div class="line">    &lt;build&gt;</div><div class="line">        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;</div><div class="line">        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;</div><div class="line">        &lt;plugins&gt;</div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;3.2.0&lt;/version&gt;</div><div class="line">                &lt;executions&gt;</div><div class="line">                    &lt;execution&gt;</div><div class="line">                        &lt;goals&gt;</div><div class="line">                            &lt;goal&gt;compile&lt;/goal&gt;</div><div class="line">                            &lt;goal&gt;testCompile&lt;/goal&gt;</div><div class="line">                        &lt;/goals&gt;</div><div class="line">                        &lt;configuration&gt;</div><div class="line">                            &lt;args&gt;</div><div class="line">                                &lt;arg&gt;-make:transitive&lt;/arg&gt;</div><div class="line">                                &lt;arg&gt;-dependencyfile&lt;/arg&gt;</div><div class="line">                                &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt;</div><div class="line">                            &lt;/args&gt;</div><div class="line">                        &lt;/configuration&gt;</div><div class="line">                    &lt;/execution&gt;</div><div class="line">                &lt;/executions&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;2.18.1&lt;/version&gt;</div><div class="line">                &lt;configuration&gt;</div><div class="line">                    &lt;useFile&gt;false&lt;/useFile&gt;</div><div class="line">                    &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt;</div><div class="line">                    &lt;includes&gt;</div><div class="line">                        &lt;include&gt;**/*Test.*&lt;/include&gt;</div><div class="line">                        &lt;include&gt;**/*Suite.*&lt;/include&gt;</div><div class="line">                    &lt;/includes&gt;</div><div class="line">                &lt;/configuration&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line"></div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;2.3&lt;/version&gt;</div><div class="line">                &lt;executions&gt;</div><div class="line">                    &lt;execution&gt;</div><div class="line">                        &lt;phase&gt;package&lt;/phase&gt;</div><div class="line">                        &lt;goals&gt;</div><div class="line">                            &lt;goal&gt;shade&lt;/goal&gt;</div><div class="line">                        &lt;/goals&gt;</div><div class="line">                        &lt;configuration&gt;</div><div class="line">                            &lt;filters&gt;</div><div class="line">                                &lt;filter&gt;</div><div class="line">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</div><div class="line">                                    &lt;excludes&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</div><div class="line">                                    &lt;/excludes&gt;</div><div class="line">                                &lt;/filter&gt;</div><div class="line">                            &lt;/filters&gt;</div><div class="line">                            &lt;transformers&gt;</div><div class="line">                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</div><div class="line">                                    &lt;mainClass&gt;cn.zwjf.uidPhone&lt;/mainClass&gt;</div><div class="line">                                &lt;/transformer&gt;</div><div class="line">                            &lt;/transformers&gt;</div><div class="line">                        &lt;/configuration&gt;</div><div class="line">                    &lt;/execution&gt;</div><div class="line">                &lt;/executions&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line">        &lt;/plugins&gt;</div><div class="line">    &lt;/build&gt;</div><div class="line">&lt;/project&gt;</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.zwjf</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by Administrator on 2017/3/29.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">uidPhone</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"uid_phone_etl"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    sc.textFile(args(<span class="number">0</span>)).map(line =&gt; &#123;</div><div class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</div><div class="line">      <span class="keyword">if</span>(arr.length &gt; <span class="number">1</span> &amp;&amp; !arr(<span class="number">1</span>).equals(<span class="string">"null"</span>))&#123;</div><div class="line">        arr(<span class="number">0</span>)+<span class="string">","</span>+arr(<span class="number">1</span>)</div><div class="line">      &#125;<span class="keyword">else</span>&#123;</div><div class="line">        <span class="string">"null"</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">    &#125;).filter(!_.equals(<span class="string">"null"</span>)).saveAsTextFile(args(<span class="number">1</span>))</div><div class="line">    sc.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>打包提交到服务器</p>
<p>spark-submit \<br>–class cn.zwjf.uidPhone –executor-memory 2G –total-executor-cores 4 \<br>/var/lib/hadoop-hdfs/data/spark/SpakrZwujf-1.0-SNAPSHOT.jar \<br>/bigdata/data/hdfs/Baidu/uid_phone/* \<br>/bigdata/data/hdfs/Baidu/uid_phone3</p>
<p>/bigdata/data/hdfs/Baidu/uid_phone/* /bigdata/data/hdfs/Baidu/uid_phone2</p>
<p>本地调试方法：、<br><a href="http://www.jianshu.com/p/c801761ce088" target="_blank" rel="external">http://www.jianshu.com/p/c801761ce088</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;添加服务选择spark on yarn&lt;br&gt;安装的时候会选择history server，博主选择的是slave1，后续可以通过这个地址看spark的UI&lt;/p&gt;
&lt;p&gt;博主安装的是：&lt;br&gt;scala     version 2.10.5（后续再UI上看到的）&lt;br&gt;spark   version 1.6.0（执行spark-shell时看到的）&lt;/p&gt;
&lt;p&gt;安装完后测试：&lt;/p&gt;
&lt;p&gt;su hdfs&lt;br&gt;spark-submit –class org.apache.spark.examples.SparkPi –executor-memory 1G –total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100&lt;/p&gt;
&lt;p&gt;查看UI：&lt;br&gt;&lt;a href=&quot;http://slave1:18088/&quot;&gt;http://slave1:18088/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>strom 监控</title>
    <link href="http://yoursite.com/2017/03/26/strom-%E7%9B%91%E6%8E%A7/"/>
    <id>http://yoursite.com/2017/03/26/strom-监控/</id>
    <published>2017-03-26T01:30:00.000Z</published>
    <updated>2017-03-31T13:27:13.752Z</updated>
    
    <content type="html"><![CDATA[<p>看当前GC的情况<br>jstat -gcutil 端口号 1000</p>
<p>看wait的是否过高<br>top -p 端口号</p>
<p>把某个端口当前的堆栈信息dump到某个文件<br>jstack 端口号 &gt; 文件</p>
<p>看磁盘<br>iostat -x 1</p>
<p>如果发现分配不均匀，例如FiledGrouping userid=1特别多，导致下游nolt或execute搞死了，重新启动也很快就死掉。可以对userid 做hash或md5或者组成更多条件，让下游Bolt分布更均匀</p>
<p>complete latency(这里主要针对spout)是一个tuple从发出, 到经过bolt处理完成, 最终调用spout的ack这个完整的过程所花的时间.complete latency和吞吐量可以认为是互斥的, 二者不可兼得, 要提高吞吐量, 必然会增加complete latency, 反之亦然. 所以我们需要根据具体的场景, 在二者之间找到一个平衡.<br>complete latency受到两个因素的影响:<br>bolt的处理时间<br>spout的parallelism数量<br><a id="more"></a></p>
<p>官网对并行度等的设置方式：<br>storm.apache.org/releases/1.0.3/Understanding-the-parallelism-of-a-Storm-topology.html</p>
<p>如果executors总是挂就需要关注</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看当前GC的情况&lt;br&gt;jstat -gcutil 端口号 1000&lt;/p&gt;
&lt;p&gt;看wait的是否过高&lt;br&gt;top -p 端口号&lt;/p&gt;
&lt;p&gt;把某个端口当前的堆栈信息dump到某个文件&lt;br&gt;jstack 端口号 &amp;gt; 文件&lt;/p&gt;
&lt;p&gt;看磁盘&lt;br&gt;iostat -x 1&lt;/p&gt;
&lt;p&gt;如果发现分配不均匀，例如FiledGrouping userid=1特别多，导致下游nolt或execute搞死了，重新启动也很快就死掉。可以对userid 做hash或md5或者组成更多条件，让下游Bolt分布更均匀&lt;/p&gt;
&lt;p&gt;complete latency(这里主要针对spout)是一个tuple从发出, 到经过bolt处理完成, 最终调用spout的ack这个完整的过程所花的时间.complete latency和吞吐量可以认为是互斥的, 二者不可兼得, 要提高吞吐量, 必然会增加complete latency, 反之亦然. 所以我们需要根据具体的场景, 在二者之间找到一个平衡.&lt;br&gt;complete latency受到两个因素的影响:&lt;br&gt;bolt的处理时间&lt;br&gt;spout的parallelism数量&lt;br&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>strom-Metrics</title>
    <link href="http://yoursite.com/2017/03/25/strom-Metrics/"/>
    <id>http://yoursite.com/2017/03/25/strom-Metrics/</id>
    <published>2017-03-24T16:30:00.000Z</published>
    <updated>2017-03-31T13:27:19.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。"><a href="#Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。" class="headerlink" title="Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。"></a>Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。</h2><p>使用 Metrics 只需要实现一个接口方法：getValueAndReset，在方法中可以查找汇总值、并将该值复位为初始值。例如，在 MeanReducer 中就实现了通过运行总数除以对应的运行计数的方式来求取均值，然后将两个值都重新设置为 0。</p>
<p>Storm 提供了以下几种 metric 类型：</p>
<h4 id="AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。"><a href="#AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。" class="headerlink" title="AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。"></a>AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。</h4><h4 id="CombinedMetric-–-可以对-metric-进行关联更新的通用接口。"><a href="#CombinedMetric-–-可以对-metric-进行关联更新的通用接口。" class="headerlink" title="CombinedMetric – 可以对 metric 进行关联更新的通用接口。"></a>CombinedMetric – 可以对 metric 进行关联更新的通用接口。</h4><h4 id="CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。"><a href="#CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。" class="headerlink" title="CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。"></a>CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。</h4><h4 id="MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap"><a href="#MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap" class="headerlink" title="MultiCountMetric – 返回包含一组 CountMetric 的 HashMap"></a>MultiCountMetric – 返回包含一组 CountMetric 的 HashMap</h4><h4 id="ReducedMetric"><a href="#ReducedMetric" class="headerlink" title="ReducedMetric"></a>ReducedMetric</h4><h6 id="MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。"><a href="#MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。" class="headerlink" title="MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。"></a>MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。</h6><h6 id="MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap"><a href="#MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap" class="headerlink" title="MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap"></a>MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap</h6><a id="more"></a>
<p>自定义Metric：<br>代码注册：<br>conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);<br>或者修改配置文件<br>topology.metrics.consumer.register:</p>
<ul>
<li>class: “org.apache.storm.metric.LoggingMetricsConsumer”<br>parallelism.hint: 1</li>
<li>class: “org.apache.storm.metric.HttpForwardingMetricsConsumer”<br>parallelism.hint: 1<br>argument: “<a href="http://example.com:8080/metrics/my-topology/" target="_blank" rel="external">http://example.com:8080/metrics/my-topology/</a>“</li>
</ul>
<p>构建自己的Metric<br>定义不可被序列号类型transient<br>private transient CountMetric countMetric;</p>
<p>重写prepare<br>@Override<br>public void prepare(Map conf, TopologyContext context, OutputCollector collector) {<br>    // other intialization here.<br>    countMetric = new CountMetric();<br>    context.registerMetric(“execute_count”, countMetric, 60);<br>}</p>
<p>bolt的execute</p>
<p>public void execute(Tuple input) {<br>    countMetric.incr();<br>    // handle tuple here.<br>}</p>
<p>builtin_metrics.clj 为内部的 metrics 设置了数据结构，以及其他框架组件可以用于更新的虚拟方法。metrics 本身是在回调代码中实现计算的 – 请参考 clj/b/s/daemon/daemon/executor.clj 中的 ack-spout-msg 的例子。</p>
<p>LoggingMetricsConsumer,统计指标值将输出到metric.log日志文件中。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。&quot;&gt;&lt;a href=&quot;#Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。&quot; class=&quot;headerlink&quot; title=&quot;Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。&quot;&gt;&lt;/a&gt;Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。&lt;/h2&gt;&lt;p&gt;使用 Metrics 只需要实现一个接口方法：getValueAndReset，在方法中可以查找汇总值、并将该值复位为初始值。例如，在 MeanReducer 中就实现了通过运行总数除以对应的运行计数的方式来求取均值，然后将两个值都重新设置为 0。&lt;/p&gt;
&lt;p&gt;Storm 提供了以下几种 metric 类型：&lt;/p&gt;
&lt;h4 id=&quot;AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。&quot;&gt;&lt;a href=&quot;#AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。&quot; class=&quot;headerlink&quot; title=&quot;AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。&quot;&gt;&lt;/a&gt;AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。&lt;/h4&gt;&lt;h4 id=&quot;CombinedMetric-–-可以对-metric-进行关联更新的通用接口。&quot;&gt;&lt;a href=&quot;#CombinedMetric-–-可以对-metric-进行关联更新的通用接口。&quot; class=&quot;headerlink&quot; title=&quot;CombinedMetric – 可以对 metric 进行关联更新的通用接口。&quot;&gt;&lt;/a&gt;CombinedMetric – 可以对 metric 进行关联更新的通用接口。&lt;/h4&gt;&lt;h4 id=&quot;CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。&quot;&gt;&lt;a href=&quot;#CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。&quot; class=&quot;headerlink&quot; title=&quot;CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。&quot;&gt;&lt;/a&gt;CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。&lt;/h4&gt;&lt;h4 id=&quot;MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap&quot;&gt;&lt;a href=&quot;#MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap&quot; class=&quot;headerlink&quot; title=&quot;MultiCountMetric – 返回包含一组 CountMetric 的 HashMap&quot;&gt;&lt;/a&gt;MultiCountMetric – 返回包含一组 CountMetric 的 HashMap&lt;/h4&gt;&lt;h4 id=&quot;ReducedMetric&quot;&gt;&lt;a href=&quot;#ReducedMetric&quot; class=&quot;headerlink&quot; title=&quot;ReducedMetric&quot;&gt;&lt;/a&gt;ReducedMetric&lt;/h4&gt;&lt;h6 id=&quot;MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。&quot;&gt;&lt;a href=&quot;#MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。&quot; class=&quot;headerlink&quot; title=&quot;MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。&quot;&gt;&lt;/a&gt;MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。&lt;/h6&gt;&lt;h6 id=&quot;MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap&quot;&gt;&lt;a href=&quot;#MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap&quot; class=&quot;headerlink&quot; title=&quot;MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap&quot;&gt;&lt;/a&gt;MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap&lt;/h6&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>bash脚本实战</title>
    <link href="http://yoursite.com/2017/03/19/bash%E8%84%9A%E6%9C%AC%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/19/bash脚本实战/</id>
    <published>2017-03-19T15:30:00.000Z</published>
    <updated>2017-08-12T01:05:48.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="练习题目"><a href="#练习题目" class="headerlink" title="练习题目"></a>练习题目</h2><p>写一个脚本getinterface.sh，脚本可以接受参数(i,I,a)，完成以下任务：<br>   (1)使用以下形式：getinterface.sh [-i interface|-I IP|-a]<br>   (2)当用户使用-i选项时，显示其指定网卡的IP地址；<br>   (3)当用户使用-I选项时，显示其后面的IP地址所属的网络接口；（如 192.168.199.183：eth0）<br>   (4)当用户单独使用-a选项时，显示所有网络接口及其IP地址（lo除外）</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>参数1为param1 参数2位param2<br>（1）利用CAT &lt;&lt; EOF  <em>*</em> EOF打印信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cat &lt;&lt; EOF</div><div class="line">    getinterface.sh [-i interface|-I IP|<span class="_">-a</span>]</div><div class="line">    -i interface) show ip of the interface</div><div class="line">    -I IP) show interface of the IP and IP with :;</div><div class="line">    <span class="_">-a</span>) list all interfaces and their IPs except lo;</div><div class="line">    *) quit</div><div class="line">=================================================================</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>（2）校验接口是否存在（参数2）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ifconfig <span class="variable">$interface</span> &gt;/dev/null </div><div class="line">$? <span class="_">-ne</span> 1  </div><div class="line"><span class="comment">#如果存在打印网卡的IP</span></div><div class="line">ifconfig <span class="variable">$parma2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div></pre></td></tr></table></figure></p>
<p>（3）先打印所有接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;</span></div></pre></td></tr></table></figure></p>
<p>并存在一个变量中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">list=`netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;'</span>`</div></pre></td></tr></table></figure>
<p>遍历集合，查看和输入IP相同的打印IP和接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> inter <span class="keyword">in</span> <span class="variable">$list</span>;  </div><div class="line"><span class="keyword">do</span>  </div><div class="line">    ip_temp=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$IP</span> == <span class="variable">$ip_temp</span> ];<span class="keyword">then</span>  </div><div class="line">	<span class="built_in">echo</span> <span class="string">"<span class="variable">$IP</span> : <span class="variable">$inter</span>"</span>  </div><div class="line">	<span class="built_in">exit</span> 0  </div><div class="line">    <span class="keyword">fi</span>  </div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<p>（4）遍历集合，过滤接口为lo的，打印IP和接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> inter <span class="keyword">in</span> <span class="variable">$list</span>;  </div><div class="line"><span class="keyword">do</span>  </div><div class="line">    ip_temp=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$inter</span> != <span class="string">"lo"</span> ];<span class="keyword">then</span>  </div><div class="line">	<span class="built_in">echo</span> <span class="string">"<span class="variable">$IP</span> : <span class="variable">$inter</span>"</span>  </div><div class="line">	<span class="built_in">exit</span> 0  </div><div class="line">    <span class="keyword">fi</span>  </div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<h2 id="最终的脚本如下："><a href="#最终的脚本如下：" class="headerlink" title="最终的脚本如下："></a>最终的脚本如下：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">cat &lt;&lt; EOF</div><div class="line">    getinterface.sh [-i interface|-I IP|<span class="_">-a</span>]</div><div class="line">    -i interface) show ip of the interface</div><div class="line">    -I IP) show interface of the IP and IP with :;</div><div class="line">    <span class="_">-a</span>) list all interfaces and their IPs except lo;</div><div class="line">    *) quit</div><div class="line">=================================================================</div><div class="line">EOF</div><div class="line"></div><div class="line"><span class="built_in">read</span> -p <span class="string">"please choice "</span> param1 param2</div><div class="line">list=`netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;'</span>`</div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-i'</span> ]]; <span class="keyword">then</span></div><div class="line">    ifconfig <span class="variable">$interface</span> &gt;/dev/null</div><div class="line">    flag=$?</div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$flag</span> <span class="_">-ne</span> 1 ];<span class="keyword">then</span></div><div class="line">        ip=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>`</div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$param2</span> <span class="variable">$ip</span>"</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$parma2</span> is not exist"</span> </div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-I'</span> ]]; <span class="keyword">then</span></div><div class="line"><span class="keyword">for</span> lt <span class="keyword">in</span> <span class="variable">$list</span>;</div><div class="line">  <span class="keyword">do</span></div><div class="line">     ip =$(ifconfig <span class="variable">$lt</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>)</div><div class="line">     <span class="comment">#ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;')</span></div><div class="line">     <span class="keyword">if</span> [ <span class="variable">$param2</span> == <span class="variable">$ip</span> ];<span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$lt</span> : <span class="variable">$ip</span>"</span></div><div class="line">     <span class="keyword">fi</span></div><div class="line">  <span class="keyword">done</span></div><div class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-a'</span> ]]; <span class="keyword">then</span></div><div class="line"> <span class="keyword">for</span> lt <span class="keyword">in</span> <span class="variable">$list</span>;</div><div class="line">  <span class="keyword">do</span></div><div class="line">     ip=$(ifconfig <span class="variable">$lt</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>)</div><div class="line">     <span class="comment">#ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;')</span></div><div class="line">     <span class="keyword">if</span> [ <span class="variable">$lt</span> != <span class="string">"lo"</span> ];<span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$lt</span> : <span class="variable">$ip</span>"</span></div><div class="line">        <span class="built_in">exit</span> 0</div><div class="line">     <span class="keyword">fi</span></div><div class="line">  <span class="keyword">done</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"quit"</span></div><div class="line">    <span class="built_in">exit</span> 0</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure>
<h3 id="注意如上内容根据你个人机器显示情况而定，如果是显示"><a href="#注意如上内容根据你个人机器显示情况而定，如果是显示" class="headerlink" title="注意如上内容根据你个人机器显示情况而定，如果是显示"></a>注意如上内容根据你个人机器显示情况而定，如果是显示</h3><p> inet addr:127.0.0.1  Mask:255.0.0.0<br> 就要把正则改成<br> ip=$(ifconfig $lt | awk -F” “ ‘/inet.*Mask/{print $2}’ | awk -F: ‘{print $2}’)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;练习题目&quot;&gt;&lt;a href=&quot;#练习题目&quot; class=&quot;headerlink&quot; title=&quot;练习题目&quot;&gt;&lt;/a&gt;练习题目&lt;/h2&gt;&lt;p&gt;写一个脚本getinterface.sh，脚本可以接受参数(i,I,a)，完成以下任务：&lt;br&gt;   (1)使用以下形式：getinterface.sh [-i interface|-I IP|-a]&lt;br&gt;   (2)当用户使用-i选项时，显示其指定网卡的IP地址；&lt;br&gt;   (3)当用户使用-I选项时，显示其后面的IP地址所属的网络接口；（如 192.168.199.183：eth0）&lt;br&gt;   (4)当用户单独使用-a选项时，显示所有网络接口及其IP地址（lo除外）&lt;/p&gt;
&lt;h3 id=&quot;分析&quot;&gt;&lt;a href=&quot;#分析&quot; class=&quot;headerlink&quot; title=&quot;分析&quot;&gt;&lt;/a&gt;分析&lt;/h3&gt;&lt;p&gt;参数1为param1 参数2位param2&lt;br&gt;（1）利用CAT &amp;lt;&amp;lt; EOF  &lt;em&gt;*&lt;/em&gt; EOF打印信息&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;cat &amp;lt;&amp;lt; EOF&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    getinterface.sh [-i interface|-I IP|&lt;span class=&quot;_&quot;&gt;-a&lt;/span&gt;]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    -i interface) show ip of the interface&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    -I IP) show interface of the IP and IP with :;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;_&quot;&gt;-a&lt;/span&gt;) list all interfaces and their IPs except lo;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    *) quit&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;=================================================================&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;EOF&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="bash" scheme="http://yoursite.com/categories/bash/"/>
    
    
      <category term="bash" scheme="http://yoursite.com/tags/bash/"/>
    
  </entry>
  
  <entry>
    <title>CDH hadoop实战</title>
    <link href="http://yoursite.com/2017/03/14/CDH%20hadoop%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/14/CDH hadoop实战/</id>
    <published>2017-03-14T06:30:00.000Z</published>
    <updated>2017-07-02T00:12:55.833Z</updated>
    
    <content type="html"><![CDATA[<h5 id="解决问题："><a href="#解决问题：" class="headerlink" title="解决问题："></a>解决问题：</h5><p>因为数据手机号一项包含为NULL的数据，需要清洗。</p>
<p>创建工程，添加如下依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&lt;dependencies&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;junit&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;junit&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;3.8.1&lt;/version&gt;</div><div class="line">            &lt;scope&gt;test&lt;/scope&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">    &lt;/dependencies&gt;</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h2 id="编写mapper类"><a href="#编写mapper类" class="headerlink" title="编写mapper类"></a>编写mapper类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> map;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by Administrator on 2017/3/14.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UidPhoneMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">        String arr[] = key.toString().split(<span class="string">","</span>);</div><div class="line">        <span class="keyword">if</span> (arr.length != <span class="number">2</span>) &#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(arr[<span class="number">1</span>] == <span class="keyword">null</span>)&#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        context.write(<span class="keyword">new</span> Text(arr[<span class="number">0</span>]), <span class="keyword">new</span> Text(arr[<span class="number">1</span>]));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> main;</div><div class="line"></div><div class="line"><span class="keyword">import</span> map.UidPhoneMapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by Administrator on 2017/3/14.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UidPhoneDropNull</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">        GenericOptionsParser parser = <span class="keyword">new</span> GenericOptionsParser(conf, args);</div><div class="line">        String[] otherArgs = parser.getRemainingArgs();</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage: NewlyJoin &lt;inpath&gt; &lt;output&gt;"</span>);</div><div class="line">            System.exit(<span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Job job = <span class="keyword">new</span> Job(conf, <span class="string">"UidPhoneDropNull"</span>);</div><div class="line">        <span class="comment">// 设置运行的job</span></div><div class="line">        job.setJarByClass(UidPhoneDropNull.class);</div><div class="line">        <span class="comment">// 设置Map相关内容</span></div><div class="line">        job.setMapperClass(UidPhoneMapper.class);</div><div class="line">        job.setMapOutputKeyClass(Text.class);</div><div class="line">        job.setMapOutputValueClass(Text.class);</div><div class="line">        job.setInputFormatClass(KeyValueTextInputFormat.class); <span class="comment">//设置文件输入格式</span></div><div class="line">        job.setNumReduceTasks(<span class="number">0</span>); <span class="comment">//设置Reduce个数为0</span></div><div class="line"></div><div class="line">        job.setOutputKeyClass(Text.class);</div><div class="line">        job.setOutputValueClass(Text.class);</div><div class="line">        <span class="comment">// 设置输入和输出的目录</span></div><div class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));</div><div class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));</div><div class="line">        <span class="comment">// 执行，直到结束就退出</span></div><div class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行上面的程序<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">[hdfs@slave3 jar]$ hadoop jar bigdataMR.jar main.UidPhoneDropNull /bigdata/data/hdfs/BD/UP/* /bigdata/data/hdfs/BD/UP2 </div><div class="line">17/03/14 17:37:54 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:8032</div><div class="line">17/03/14 17:37:55 INFO input.FileInputFormat: Total input paths to process : 3</div><div class="line">17/03/14 17:37:55 INFO mapreduce.JobSubmitter: number of splits:3</div><div class="line">17/03/14 17:37:55 INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_1488970234114_0044</div><div class="line">17/03/14 17:37:55 INFO impl.YarnClientImpl: Submitted application application_1488970234114_0044</div><div class="line">17/03/14 17:37:55 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0044/</div><div class="line">17/03/14 17:37:55 INFO mapreduce.Job: Running job: job_1488970234114_0044</div><div class="line">17/03/14 17:38:00 INFO mapreduce.Job: Job job_1488970234114_0044 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></div><div class="line">17/03/14 17:38:00 INFO mapreduce.Job:  map 0% reduce 0%</div><div class="line">17/03/14 17:38:04 INFO mapreduce.Job:  map 33% reduce 0%</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job:  map 100% reduce 0%</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job: Job job_1488970234114_0044 completed successfully</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job: Counters: 30</div><div class="line">        File System Counters</div><div class="line">                FILE: Number of bytes <span class="built_in">read</span>=0</div><div class="line">                FILE: Number of bytes written=378748</div><div class="line">                FILE: Number of <span class="built_in">read</span> operations=0</div><div class="line">                FILE: Number of large <span class="built_in">read</span> operations=0</div><div class="line">                FILE: Number of write operations=0</div><div class="line">                HDFS: Number of bytes <span class="built_in">read</span>=13514456</div><div class="line">                HDFS: Number of bytes written=10824901</div><div class="line">                HDFS: Number of <span class="built_in">read</span> operations=15</div><div class="line">                HDFS: Number of large <span class="built_in">read</span> operations=0</div><div class="line">                HDFS: Number of write operations=6</div><div class="line">        Job Counters </div><div class="line">                Launched map tasks=3</div><div class="line">                Data-local map tasks=3</div><div class="line">                Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=8240</div><div class="line">                Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</div><div class="line">                Total time spent by all map tasks (ms)=8240</div><div class="line">                Total vcore-seconds taken by all map tasks=8240</div><div class="line">                Total megabyte-seconds taken by all map tasks=8437760</div><div class="line">        Map-Reduce Framework</div><div class="line">                Map input records=661590</div><div class="line">                Map output records=485478</div><div class="line">                Input split bytes=393</div><div class="line">                Spilled Records=0</div><div class="line">                Failed Shuffles=0</div><div class="line">                Merged Map outputs=0</div><div class="line">                GC time elapsed (ms)=175</div><div class="line">                CPU time spent (ms)=3730</div><div class="line">                Physical memory (bytes) snapshot=712597504</div><div class="line">                Virtual memory (bytes) snapshot=8316772352</div><div class="line">                Total committed heap usage (bytes)=814743552</div><div class="line">        File Input Format Counters </div><div class="line">                Bytes Read=13514063</div><div class="line">        File Output Format Counters </div><div class="line">                Bytes Written=10824901</div><div class="line">[hdfs@slave3 jar]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP2</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7609438 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    3215441 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup         22 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00002</div></pre></td></tr></table></figure></p>
<p>最后将数据导入到hive<br>hive&gt; create EXTERNAL table IF NOT EXISTS UP2 (uid STRING,phone STRING) row format delimited fields terminated by ‘,’ location ‘/bigdata/data/hdfs/BD/UP2/‘;<br>OK<br>Time taken: 0.026 seconds<br>hive&gt; select count(1) from UP2;<br>Query ID = hdfs_20170314174141_1b60d560-2036-44e6-ab65-19f17efc5b1b<br>Total jobs = 1<br>Launching Job 1 out of 1<br>Number of reduce tasks determined at compile time: 1<br>In order to change the average load for a reducer (in bytes):<br>  set hive.exec.reducers.bytes.per.reducer=<number><br>In order to limit the maximum number of reducers:<br>  set hive.exec.reducers.max=<number><br>In order to set a constant number of reducers:<br>  set mapreduce.job.reduces=<number><br>Starting Job = job_1488970234114_0045, Tracking URL = <a href="http://master2:8088/proxy/application_1488970234114_0045/" target="_blank" rel="external">http://master2:8088/proxy/application_1488970234114_0045/</a><br>Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0045<br>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1<br>2017-03-14 17:41:33,200 Stage-1 map = 0%,  reduce = 0%<br>2017-03-14 17:41:38,371 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.54 sec<br>2017-03-14 17:41:43,486 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.38 sec<br>MapReduce Total cumulative CPU time: 4 seconds 380 msec<br>Ended Job = job_1488970234114_0045<br>MapReduce Jobs Launched:<br>Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.38 sec   HDFS Read: 10831709 HDFS Write: 7 SUCCESS<br>Total MapReduce CPU Time Spent: 4 seconds 380 msec<br>OK<br>485478<br>Time taken: 16.267 seconds, Fetched: 1 row(s)</number></number></number></p>
<p>比清洗前数据661590少了176112条</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;解决问题：&quot;&gt;&lt;a href=&quot;#解决问题：&quot; class=&quot;headerlink&quot; title=&quot;解决问题：&quot;&gt;&lt;/a&gt;解决问题：&lt;/h5&gt;&lt;p&gt;因为数据手机号一项包含为NULL的数据，需要清洗。&lt;/p&gt;
&lt;p&gt;创建工程，添加如下依赖&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;lt;dependencies&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-common&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-hdfs&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;3.8.1&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;lt;/dependencies&amp;gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH hive 实战</title>
    <link href="http://yoursite.com/2017/03/14/CDH%20hive%20%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/14/CDH hive 实战/</id>
    <published>2017-03-14T00:30:00.000Z</published>
    <updated>2017-03-31T16:13:09.590Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">hive&gt; create database BD;</div><div class="line">OK</div><div class="line">Time taken: 0.161 seconds</div><div class="line">hive&gt; use BD;</div><div class="line">OK</div><div class="line">Time taken: 0.013 seconds</div></pre></td></tr></table></figure>
<p>创建表<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hive&gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> ;</div><div class="line">OK</div><div class="line">Time taken: 0.211 seconds</div><div class="line">hive&gt; load data inpath <span class="string">'/bigdata/data/hdfs/shuju/SO/part-m-00000'</span> into table UP;</div><div class="line">Loading data to table BD.UP</div><div class="line">Table BD.UP stats: [numFiles=1, totalSize=898611236]</div><div class="line">OK</div><div class="line">Time taken: 0.317 seconds</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>查询表数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314111111_d29152a3-56b3-42f9-b17b-1ddaf7451117</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0025, Tracking URL = http://master2:8088/proxy/application_1488970234114_0025/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0025</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 4; number of reducers: 1</div><div class="line">2017-03-14 11:11:49,267 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 11:11:55,519 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 3.31 sec</div><div class="line">2017-03-14 11:11:56,542 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.57 sec</div><div class="line">2017-03-14 11:12:01,655 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 18.41 sec</div><div class="line">MapReduce Total cumulative CPU time: 18 seconds 410 msec</div><div class="line">Ended Job = job_1488970234114_0025</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 4  Reduce: 1   Cumulative CPU: 18.41 sec   HDFS Read: 899021344 HDFS Write: 8 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 18 seconds 410 msec</div><div class="line">OK</div><div class="line">5674200</div><div class="line">Time taken: 19.429 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>删除表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.088 seconds</div></pre></td></tr></table></figure></p>
<p>查看hdfs上的数据，发现数据被删除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/SO</span></div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-13 19:16 /bigdata/data/hdfs/shuju/SO/_SUCCESS</div></pre></td></tr></table></figure></p>
<h2 id="测试2："><a href="#测试2：" class="headerlink" title="测试2："></a>测试2：</h2><p>首先查询，发现biaoming数据存在<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/biaoming</span></div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup   13514063 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/part-m-00000</div></pre></td></tr></table></figure></p>
<p>创建外部表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> ;</div><div class="line">OK</div><div class="line">Time taken: 0.033 seconds</div></pre></td></tr></table></figure></p>
<p>加载数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hive&gt; load data inpath <span class="string">'/bigdata/data/hdfs/shuju/biaoming/part-m-00000'</span> into table UP;</div><div class="line">Loading data to table BD.UP</div><div class="line">Table BD.UP stats: [numFiles=1, totalSize=13514063]</div><div class="line">OK</div><div class="line">Time taken: 0.201 seconds</div></pre></td></tr></table></figure></p>
<p>查询数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314115959_6004ef81-913b-4d38-99dc-1199cc6e73f2</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0027, Tracking URL = http://master2:8088/proxy/application_1488970234114_0027/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0027</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</div><div class="line">2017-03-14 11:59:09,331 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 11:59:14,463 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.06 sec</div><div class="line">2017-03-14 11:59:19,587 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.9 sec</div><div class="line">MapReduce Total cumulative CPU time: 4 seconds 900 msec</div><div class="line">Ended Job = job_1488970234114_0027</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.9 sec   HDFS Read: 13520683 HDFS Write: 7 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 4 seconds 900 msec</div><div class="line">OK</div><div class="line">661590</div><div class="line">Time taken: 17.707 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>删除数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.037 seconds</div></pre></td></tr></table></figure></p>
<p>再查看元数据，发现依然被删除了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/biaoming</span></div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS</div></pre></td></tr></table></figure></p>
<h2 id="测试3："><a href="#测试3：" class="headerlink" title="测试3："></a>测试3：</h2><p>重新上传数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver  --connect <span class="string">'jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=BD'</span> --table=UP --target-dir /bigdata/data/hdfs/BD/UP --split-by uid -m 3</div></pre></td></tr></table></figure></p>
<p>查看数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup    3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002</div><div class="line">-rw-r--r--   3 hdfs supergroup         22 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00003</div></pre></td></tr></table></figure></p>
<p>创建外部表数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">FAILED: SemanticException [Error 10001]: Line 1:21 Table not found <span class="string">'UP'</span></div><div class="line">hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> location <span class="string">'/bigdata/data/hdfs/BD/UP/'</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.03 seconds</div><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314141111_5be2c701-ad6a-4717-8801-513f38d64928</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0031, Tracking URL = http://master2:8088/proxy/application_1488970234114_0031/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0031</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</div><div class="line">2017-03-14 14:11:48,608 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 14:11:54,743 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.26 sec</div><div class="line">2017-03-14 14:11:58,833 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.08 sec</div><div class="line">MapReduce Total cumulative CPU time: 5 seconds 80 msec</div><div class="line">Ended Job = job_1488970234114_0031</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.08 sec   HDFS Read: 13520952 HDFS Write: 7 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 5 seconds 80 msec</div><div class="line">OK</div><div class="line">661590</div><div class="line">Time taken: 16.253 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>此时再删除表后数据还存在。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.037 seconds</div><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup    3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002</div></pre></td></tr></table></figure></p>
<p>创建普通表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; create table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> location <span class="string">'/bigdata/data/hdfs/BD/UP/'</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.029 seconds</div></pre></td></tr></table></figure></p>
<p>删除表后数据不存在<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.046 seconds</div><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">ls: `/bigdata/data/hdfs/BD/UP<span class="string">': No such file or directory</span></div></pre></td></tr></table></figure></p>
<p>总结<br>用load方式无论外部表还是内部表数据都会删除，用location方式，外部表不会删除数据，内部表会删除数据</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;创建数据库&quot;&gt;&lt;a href=&quot;#创建数据库&quot; class=&quot;headerlink&quot; title=&quot;创建数据库&quot;&gt;&lt;/a&gt;创建数据库&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; create database BD;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.161 seconds&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; use BD;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.013 seconds&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;创建表&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by &lt;span class=&quot;string&quot;&gt;&#39;,&#39;&lt;/span&gt; ;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.211 seconds&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; load data inpath &lt;span class=&quot;string&quot;&gt;&#39;/bigdata/data/hdfs/shuju/SO/part-m-00000&#39;&lt;/span&gt; into table UP;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Loading data to table BD.UP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Table BD.UP stats: [numFiles=1, totalSize=898611236]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.317 seconds&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH sqoop1 实战-带条件的导入</title>
    <link href="http://yoursite.com/2017/03/13/CDH%20sqoop1%20%E5%AE%9E%E6%88%98-%E5%B8%A6%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5/"/>
    <id>http://yoursite.com/2017/03/13/CDH sqoop1 实战-带条件的导入/</id>
    <published>2017-03-13T01:30:00.000Z</published>
    <updated>2017-06-25T00:50:06.600Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h2><p>[hdfs@slave3 lib]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query “SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1”  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming<br>17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0<br>17/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br>17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br>17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation<br>17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‘$CONDITIONS’ in WHERE clause.<br>        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332)<br>        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858)<br>        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658)<br>        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)<br>        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)<br>        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)<br>        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)<br>        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)<br>        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)</p>
<p>修改为：<br>sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming</p>
<a id="more"></a>
<p>在执行，发现报错说我database不存在，并在目录下生成了一个metastore_db，原来是执行sqoop的机器不对，换了一台机器执行没有问题，很奇怪明明是分布式的为什么会出现这个问题呢</p>
<p>最终的具体日志：</p>
<p>[hdfs@master1 root]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming –hive-database shuju<br>Warning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.<br>Please set $ACCUMULO_HOME to the root of your Accumulo installation.<br>17/03/15 14:32:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0<br>17/03/15 14:32:53 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br>17/03/15 14:32:53 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br>17/03/15 14:32:53 INFO tool.CodeGenTool: Beginning code generation<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../hadoop-mapreduce<br>Note: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.java uses or overrides a deprecated API.<br>Note: Recompile with -Xlint:deprecation for details.<br>17/03/15 14:33:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.jar<br>17/03/15 14:33:00 INFO mapreduce.ImportJobBase: Beginning query import.<br>17/03/15 14:33:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar<br>17/03/15 14:33:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps<br>17/03/15 14:33:01 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:8032<br>17/03/15 14:33:02 INFO db.DBInputFormat: Using read commited transaction isolation<br>17/03/15 14:33:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM (SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 1) ) AS t1<br>17/03/15 14:33:03 INFO db.IntegerSplitter: Split size: 7508318; Num splits: 4 from: 20079 to: 30053354<br>17/03/15 14:33:03 INFO mapreduce.JobSubmitter: number of splits:4<br>17/03/15 14:33:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_0057<br>17/03/15 14:33:03 INFO impl.YarnClientImpl: Submitted application application_1488970234114_0057<br>17/03/15 14:33:03 INFO mapreduce.Job: The url to track the job: <a href="http://master2:8088/proxy/application_1488970234114_0057/" target="_blank" rel="external">http://master2:8088/proxy/application_1488970234114_0057/</a><br>17/03/15 14:33:03 INFO mapreduce.Job: Running job: job_1488970234114_0057<br>17/03/15 14:33:08 INFO mapreduce.Job: Job job_1488970234114_0057 running in uber mode : false<br>17/03/15 14:33:08 INFO mapreduce.Job:  map 0% reduce 0%<br>17/03/15 14:33:15 INFO mapreduce.Job:  map 75% reduce 0%<br>17/03/15 14:33:16 INFO mapreduce.Job:  map 100% reduce 0%<br>17/03/15 14:33:16 INFO mapreduce.Job: Job job_1488970234114_0057 completed successfully<br>17/03/15 14:33:16 INFO mapreduce.Job: Counters: 30<br>        File System Counters<br>                FILE: Number of bytes read=0<br>                FILE: Number of bytes written=608160<br>                FILE: Number of read operations=0<br>                FILE: Number of large read operations=0<br>                FILE: Number of write operations=0<br>                HDFS: Number of bytes read=428<br>                HDFS: Number of bytes written=17645206<br>                HDFS: Number of read operations=16<br>                HDFS: Number of large read operations=0<br>                HDFS: Number of write operations=8<br>        Job Counters<br>                Launched map tasks=4<br>                Other local map tasks=4<br>                Total time spent by all maps in occupied slots (ms)=17968<br>                Total time spent by all reduces in occupied slots (ms)=0<br>                Total time spent by all map tasks (ms)=17968<br>                Total vcore-seconds taken by all map tasks=17968<br>                Total megabyte-seconds taken by all map tasks=18399232<br>        Map-Reduce Framework<br>                Map input records=283659<br>                Map output records=283659<br>                Input split bytes=428<br>                Spilled Records=0<br>                Failed Shuffles=0<br>                Merged Map outputs=0<br>                GC time elapsed (ms)=268<br>                CPU time spent (ms)=12960<br>                Physical memory (bytes) snapshot=1151381504<br>                Virtual memory (bytes) snapshot=11157348352<br>                Total committed heap usage (bytes)=1045954560<br>        File Input Format Counters<br>                Bytes Read=0<br>        File Output Format Counters<br>                Bytes Written=17645206<br>17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Transferred 16.8278 MB in 15.7965 seconds (1.0653 MB/sec)<br>17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Retrieved 283659 records.<br>17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:33:16 WARN hive.TableDefWriter: Column update_time had to be cast to a less precise type in Hive<br>17/03/15 14:33:16 INFO hive.HiveImport: Loading uploaded data into Hive</p>
<p>Logging initialized using configuration in jar:file:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.properties<br>OK<br>Time taken: 1.874 seconds<br>Loading data to table shuju.card_record<br>Table shuju.card_record stats: [numFiles=4, totalSize=17645206]<br>OK<br>Time taken: 0.367 seconds</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;导入&quot;&gt;&lt;a href=&quot;#导入&quot; class=&quot;headerlink&quot; title=&quot;导入&quot;&gt;&lt;/a&gt;导入&lt;/h2&gt;&lt;p&gt;[hdfs@slave3 lib]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query “SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1”  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming&lt;br&gt;17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0&lt;br&gt;17/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.&lt;br&gt;17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.&lt;br&gt;17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation&lt;br&gt;17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‘$CONDITIONS’ in WHERE clause.&lt;br&gt;        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332)&lt;br&gt;        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858)&lt;br&gt;        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658)&lt;br&gt;        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)&lt;br&gt;        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)&lt;br&gt;        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)&lt;br&gt;        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)&lt;br&gt;        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)&lt;br&gt;        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)&lt;br&gt;        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)&lt;br&gt;        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)&lt;br&gt;        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)&lt;/p&gt;
&lt;p&gt;修改为：&lt;br&gt;sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
</feed>
