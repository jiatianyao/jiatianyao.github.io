<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张洪铭的个人博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-03-31T13:27:02.808Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>张洪铭</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CDH-spark</title>
    <link href="http://yoursite.com/2017/03/29/CDH-spark/"/>
    <id>http://yoursite.com/2017/03/29/CDH-spark/</id>
    <published>2017-03-29T03:30:00.000Z</published>
    <updated>2017-03-31T13:27:02.808Z</updated>
    
    <content type="html"><![CDATA[<p>添加服务选择spark on yarn<br>安装的时候会选择history server，博主选择的是slave1，后续可以通过这个地址看spark的UI</p>
<p>博主安装的是：<br>scala     version 2.10.5（后续再UI上看到的）<br>spark   version 1.6.0（执行spark-shell时看到的）</p>
<p>安装完后测试：</p>
<p>su hdfs<br>spark-submit –class org.apache.spark.examples.SparkPi –executor-memory 1G –total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100</p>
<p>查看UI：<br><a href="http://slave1:18088/" target="_blank" rel="external">http://slave1:18088/</a></p>
<a id="more"></a>
<p>编写代码测试离线功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</div><div class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</div><div class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</div><div class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</div><div class="line"></div><div class="line">    &lt;groupId&gt;cn.zwjf.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;SpakrZwujf&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</div><div class="line"></div><div class="line"></div><div class="line">    &lt;properties&gt;</div><div class="line">        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;</div><div class="line">        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;</div><div class="line">        &lt;encoding&gt;UTF-8&lt;/encoding&gt;</div><div class="line">        &lt;scala.version&gt;2.10.5&lt;/scala.version&gt;</div><div class="line">        &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt;</div><div class="line">    &lt;/properties&gt;</div><div class="line"></div><div class="line">    &lt;dependencies&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;1.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;1.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">    &lt;/dependencies&gt;</div><div class="line"></div><div class="line">    &lt;build&gt;</div><div class="line">        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;</div><div class="line">        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;</div><div class="line">        &lt;plugins&gt;</div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;3.2.0&lt;/version&gt;</div><div class="line">                &lt;executions&gt;</div><div class="line">                    &lt;execution&gt;</div><div class="line">                        &lt;goals&gt;</div><div class="line">                            &lt;goal&gt;compile&lt;/goal&gt;</div><div class="line">                            &lt;goal&gt;testCompile&lt;/goal&gt;</div><div class="line">                        &lt;/goals&gt;</div><div class="line">                        &lt;configuration&gt;</div><div class="line">                            &lt;args&gt;</div><div class="line">                                &lt;arg&gt;-make:transitive&lt;/arg&gt;</div><div class="line">                                &lt;arg&gt;-dependencyfile&lt;/arg&gt;</div><div class="line">                                &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt;</div><div class="line">                            &lt;/args&gt;</div><div class="line">                        &lt;/configuration&gt;</div><div class="line">                    &lt;/execution&gt;</div><div class="line">                &lt;/executions&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;2.18.1&lt;/version&gt;</div><div class="line">                &lt;configuration&gt;</div><div class="line">                    &lt;useFile&gt;false&lt;/useFile&gt;</div><div class="line">                    &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt;</div><div class="line">                    &lt;includes&gt;</div><div class="line">                        &lt;include&gt;**/*Test.*&lt;/include&gt;</div><div class="line">                        &lt;include&gt;**/*Suite.*&lt;/include&gt;</div><div class="line">                    &lt;/includes&gt;</div><div class="line">                &lt;/configuration&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line"></div><div class="line">            &lt;plugin&gt;</div><div class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</div><div class="line">                &lt;version&gt;2.3&lt;/version&gt;</div><div class="line">                &lt;executions&gt;</div><div class="line">                    &lt;execution&gt;</div><div class="line">                        &lt;phase&gt;package&lt;/phase&gt;</div><div class="line">                        &lt;goals&gt;</div><div class="line">                            &lt;goal&gt;shade&lt;/goal&gt;</div><div class="line">                        &lt;/goals&gt;</div><div class="line">                        &lt;configuration&gt;</div><div class="line">                            &lt;filters&gt;</div><div class="line">                                &lt;filter&gt;</div><div class="line">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</div><div class="line">                                    &lt;excludes&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</div><div class="line">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</div><div class="line">                                    &lt;/excludes&gt;</div><div class="line">                                &lt;/filter&gt;</div><div class="line">                            &lt;/filters&gt;</div><div class="line">                            &lt;transformers&gt;</div><div class="line">                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</div><div class="line">                                    &lt;mainClass&gt;cn.zwjf.uidPhone&lt;/mainClass&gt;</div><div class="line">                                &lt;/transformer&gt;</div><div class="line">                            &lt;/transformers&gt;</div><div class="line">                        &lt;/configuration&gt;</div><div class="line">                    &lt;/execution&gt;</div><div class="line">                &lt;/executions&gt;</div><div class="line">            &lt;/plugin&gt;</div><div class="line">        &lt;/plugins&gt;</div><div class="line">    &lt;/build&gt;</div><div class="line">&lt;/project&gt;</div></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> cn.zwjf</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by Administrator on 2017/3/29.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">uidPhone</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"uid_phone_etl"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    sc.textFile(args(<span class="number">0</span>)).map(line =&gt; &#123;</div><div class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</div><div class="line">      <span class="keyword">if</span>(arr.length &gt; <span class="number">1</span> &amp;&amp; !arr(<span class="number">1</span>).equals(<span class="string">"null"</span>))&#123;</div><div class="line">        arr(<span class="number">0</span>)+<span class="string">","</span>+arr(<span class="number">1</span>)</div><div class="line">      &#125;<span class="keyword">else</span>&#123;</div><div class="line">        <span class="string">"null"</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">    &#125;).filter(!_.equals(<span class="string">"null"</span>)).saveAsTextFile(args(<span class="number">1</span>))</div><div class="line">    sc.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>打包提交到服务器</p>
<p>spark-submit \<br>–class cn.zwjf.uidPhone –executor-memory 2G –total-executor-cores 4 \<br>/var/lib/hadoop-hdfs/data/spark/SpakrZwujf-1.0-SNAPSHOT.jar \<br>/bigdata/data/hdfs/Baidu/uid_phone/* \<br>/bigdata/data/hdfs/Baidu/uid_phone3</p>
<p>/bigdata/data/hdfs/Baidu/uid_phone/* /bigdata/data/hdfs/Baidu/uid_phone2</p>
<p>本地调试方法：、<br><a href="http://www.jianshu.com/p/c801761ce088" target="_blank" rel="external">http://www.jianshu.com/p/c801761ce088</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;添加服务选择spark on yarn&lt;br&gt;安装的时候会选择history server，博主选择的是slave1，后续可以通过这个地址看spark的UI&lt;/p&gt;
&lt;p&gt;博主安装的是：&lt;br&gt;scala     version 2.10.5（后续再UI上看到的）&lt;br&gt;spark   version 1.6.0（执行spark-shell时看到的）&lt;/p&gt;
&lt;p&gt;安装完后测试：&lt;/p&gt;
&lt;p&gt;su hdfs&lt;br&gt;spark-submit –class org.apache.spark.examples.SparkPi –executor-memory 1G –total-executor-cores 2 /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.10.0-hadoop2.6.0-cdh5.10.0.jar 100&lt;/p&gt;
&lt;p&gt;查看UI：&lt;br&gt;&lt;a href=&quot;http://slave1:18088/&quot;&gt;http://slave1:18088/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>strom 监控</title>
    <link href="http://yoursite.com/2017/03/26/strom-%E7%9B%91%E6%8E%A7/"/>
    <id>http://yoursite.com/2017/03/26/strom-监控/</id>
    <published>2017-03-26T01:30:00.000Z</published>
    <updated>2017-03-31T13:27:13.752Z</updated>
    
    <content type="html"><![CDATA[<p>看当前GC的情况<br>jstat -gcutil 端口号 1000</p>
<p>看wait的是否过高<br>top -p 端口号</p>
<p>把某个端口当前的堆栈信息dump到某个文件<br>jstack 端口号 &gt; 文件</p>
<p>看磁盘<br>iostat -x 1</p>
<p>如果发现分配不均匀，例如FiledGrouping userid=1特别多，导致下游nolt或execute搞死了，重新启动也很快就死掉。可以对userid 做hash或md5或者组成更多条件，让下游Bolt分布更均匀</p>
<p>complete latency(这里主要针对spout)是一个tuple从发出, 到经过bolt处理完成, 最终调用spout的ack这个完整的过程所花的时间.complete latency和吞吐量可以认为是互斥的, 二者不可兼得, 要提高吞吐量, 必然会增加complete latency, 反之亦然. 所以我们需要根据具体的场景, 在二者之间找到一个平衡.<br>complete latency受到两个因素的影响:<br>bolt的处理时间<br>spout的parallelism数量<br><a id="more"></a></p>
<p>官网对并行度等的设置方式：<br>storm.apache.org/releases/1.0.3/Understanding-the-parallelism-of-a-Storm-topology.html</p>
<p>如果executors总是挂就需要关注</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看当前GC的情况&lt;br&gt;jstat -gcutil 端口号 1000&lt;/p&gt;
&lt;p&gt;看wait的是否过高&lt;br&gt;top -p 端口号&lt;/p&gt;
&lt;p&gt;把某个端口当前的堆栈信息dump到某个文件&lt;br&gt;jstack 端口号 &amp;gt; 文件&lt;/p&gt;
&lt;p&gt;看磁盘&lt;br&gt;iostat -x 1&lt;/p&gt;
&lt;p&gt;如果发现分配不均匀，例如FiledGrouping userid=1特别多，导致下游nolt或execute搞死了，重新启动也很快就死掉。可以对userid 做hash或md5或者组成更多条件，让下游Bolt分布更均匀&lt;/p&gt;
&lt;p&gt;complete latency(这里主要针对spout)是一个tuple从发出, 到经过bolt处理完成, 最终调用spout的ack这个完整的过程所花的时间.complete latency和吞吐量可以认为是互斥的, 二者不可兼得, 要提高吞吐量, 必然会增加complete latency, 反之亦然. 所以我们需要根据具体的场景, 在二者之间找到一个平衡.&lt;br&gt;complete latency受到两个因素的影响:&lt;br&gt;bolt的处理时间&lt;br&gt;spout的parallelism数量&lt;br&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>strom-Metrics</title>
    <link href="http://yoursite.com/2017/03/25/strom-Metrics/"/>
    <id>http://yoursite.com/2017/03/25/strom-Metrics/</id>
    <published>2017-03-24T16:30:00.000Z</published>
    <updated>2017-03-31T13:27:19.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。"><a href="#Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。" class="headerlink" title="Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。"></a>Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。</h2><p>使用 Metrics 只需要实现一个接口方法：getValueAndReset，在方法中可以查找汇总值、并将该值复位为初始值。例如，在 MeanReducer 中就实现了通过运行总数除以对应的运行计数的方式来求取均值，然后将两个值都重新设置为 0。</p>
<p>Storm 提供了以下几种 metric 类型：</p>
<h4 id="AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。"><a href="#AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。" class="headerlink" title="AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。"></a>AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。</h4><h4 id="CombinedMetric-–-可以对-metric-进行关联更新的通用接口。"><a href="#CombinedMetric-–-可以对-metric-进行关联更新的通用接口。" class="headerlink" title="CombinedMetric – 可以对 metric 进行关联更新的通用接口。"></a>CombinedMetric – 可以对 metric 进行关联更新的通用接口。</h4><h4 id="CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。"><a href="#CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。" class="headerlink" title="CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。"></a>CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。</h4><h4 id="MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap"><a href="#MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap" class="headerlink" title="MultiCountMetric – 返回包含一组 CountMetric 的 HashMap"></a>MultiCountMetric – 返回包含一组 CountMetric 的 HashMap</h4><h4 id="ReducedMetric"><a href="#ReducedMetric" class="headerlink" title="ReducedMetric"></a>ReducedMetric</h4><h6 id="MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。"><a href="#MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。" class="headerlink" title="MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。"></a>MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。</h6><h6 id="MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap"><a href="#MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap" class="headerlink" title="MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap"></a>MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap</h6><a id="more"></a>
<p>自定义Metric：<br>代码注册：<br>conf.registerMetricsConsumer(org.apache.storm.metric.LoggingMetricsConsumer.class, 1);<br>或者修改配置文件<br>topology.metrics.consumer.register:</p>
<ul>
<li>class: “org.apache.storm.metric.LoggingMetricsConsumer”<br>parallelism.hint: 1</li>
<li>class: “org.apache.storm.metric.HttpForwardingMetricsConsumer”<br>parallelism.hint: 1<br>argument: “<a href="http://example.com:8080/metrics/my-topology/" target="_blank" rel="external">http://example.com:8080/metrics/my-topology/</a>“</li>
</ul>
<p>构建自己的Metric<br>定义不可被序列号类型transient<br>private transient CountMetric countMetric;</p>
<p>重写prepare<br>@Override<br>public void prepare(Map conf, TopologyContext context, OutputCollector collector) {<br>    // other intialization here.<br>    countMetric = new CountMetric();<br>    context.registerMetric(“execute_count”, countMetric, 60);<br>}</p>
<p>bolt的execute</p>
<p>public void execute(Tuple input) {<br>    countMetric.incr();<br>    // handle tuple here.<br>}</p>
<p>builtin_metrics.clj 为内部的 metrics 设置了数据结构，以及其他框架组件可以用于更新的虚拟方法。metrics 本身是在回调代码中实现计算的 – 请参考 clj/b/s/daemon/daemon/executor.clj 中的 ack-spout-msg 的例子。</p>
<p>LoggingMetricsConsumer,统计指标值将输出到metric.log日志文件中。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。&quot;&gt;&lt;a href=&quot;#Storm-提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm-内部通过该接口跟踪各类统计数字：executor-和-acker-的数量、每个-bolt-的平均处理时延、worker-使用的最大堆容量等等，这些信息都可以在-Nimbus-的-UI-界面中看到。&quot; class=&quot;headerlink&quot; title=&quot;Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。&quot;&gt;&lt;/a&gt;Storm 提供了一个可以获取整个拓扑中所有的统计信息的metrics接口。Storm 内部通过该接口跟踪各类统计数字：executor 和 acker 的数量、每个 bolt 的平均处理时延、worker 使用的最大堆容量等等，这些信息都可以在 Nimbus 的 UI 界面中看到。&lt;/h2&gt;&lt;p&gt;使用 Metrics 只需要实现一个接口方法：getValueAndReset，在方法中可以查找汇总值、并将该值复位为初始值。例如，在 MeanReducer 中就实现了通过运行总数除以对应的运行计数的方式来求取均值，然后将两个值都重新设置为 0。&lt;/p&gt;
&lt;p&gt;Storm 提供了以下几种 metric 类型：&lt;/p&gt;
&lt;h4 id=&quot;AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。&quot;&gt;&lt;a href=&quot;#AssignableMetric-–-将-metric-设置为指定值。此类型在两种情况下有用：1-metric-本身为外部设置的值；2-你已经另外计算出了汇总的统计值。&quot; class=&quot;headerlink&quot; title=&quot;AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。&quot;&gt;&lt;/a&gt;AssignableMetric – 将 metric 设置为指定值。此类型在两种情况下有用：1. metric 本身为外部设置的值；2. 你已经另外计算出了汇总的统计值。&lt;/h4&gt;&lt;h4 id=&quot;CombinedMetric-–-可以对-metric-进行关联更新的通用接口。&quot;&gt;&lt;a href=&quot;#CombinedMetric-–-可以对-metric-进行关联更新的通用接口。&quot; class=&quot;headerlink&quot; title=&quot;CombinedMetric – 可以对 metric 进行关联更新的通用接口。&quot;&gt;&lt;/a&gt;CombinedMetric – 可以对 metric 进行关联更新的通用接口。&lt;/h4&gt;&lt;h4 id=&quot;CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。&quot;&gt;&lt;a href=&quot;#CountMetric-–-返回-metric-的汇总结果。可以调用-incr-方法来将结果加一；调用-incrBy-n-方法来将结果加上给定值。&quot; class=&quot;headerlink&quot; title=&quot;CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。&quot;&gt;&lt;/a&gt;CountMetric – 返回 metric 的汇总结果。可以调用 incr() 方法来将结果加一；调用 incrBy(n) 方法来将结果加上给定值。&lt;/h4&gt;&lt;h4 id=&quot;MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap&quot;&gt;&lt;a href=&quot;#MultiCountMetric-–-返回包含一组-CountMetric-的-HashMap&quot; class=&quot;headerlink&quot; title=&quot;MultiCountMetric – 返回包含一组 CountMetric 的 HashMap&quot;&gt;&lt;/a&gt;MultiCountMetric – 返回包含一组 CountMetric 的 HashMap&lt;/h4&gt;&lt;h4 id=&quot;ReducedMetric&quot;&gt;&lt;a href=&quot;#ReducedMetric&quot; class=&quot;headerlink&quot; title=&quot;ReducedMetric&quot;&gt;&lt;/a&gt;ReducedMetric&lt;/h4&gt;&lt;h6 id=&quot;MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。&quot;&gt;&lt;a href=&quot;#MeanReducer-–-跟踪由它的-reduce-方法提供的运行状态均值结果（可以接受-Double、Integer、Long-等类型，内置的均值结果是-Double-类型）。MeanReducer-确实是一个相当棒的家伙。&quot; class=&quot;headerlink&quot; title=&quot;MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。&quot;&gt;&lt;/a&gt;MeanReducer – 跟踪由它的 reduce() 方法提供的运行状态均值结果（可以接受 Double、Integer、Long 等类型，内置的均值结果是 Double 类型）。MeanReducer 确实是一个相当棒的家伙。&lt;/h6&gt;&lt;h6 id=&quot;MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap&quot;&gt;&lt;a href=&quot;#MultiReducedMetric-–-返回包含一组-ReducedMetric-的-HashMap&quot; class=&quot;headerlink&quot; title=&quot;MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap&quot;&gt;&lt;/a&gt;MultiReducedMetric – 返回包含一组 ReducedMetric 的 HashMap&lt;/h6&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>bash脚本实战</title>
    <link href="http://yoursite.com/2017/03/19/bash%E8%84%9A%E6%9C%AC%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/19/bash脚本实战/</id>
    <published>2017-03-19T15:30:00.000Z</published>
    <updated>2017-04-16T09:59:50.174Z</updated>
    
    <content type="html"><![CDATA[<h3 id="练习题目"><a href="#练习题目" class="headerlink" title="练习题目"></a>练习题目</h3><p>写一个脚本getinterface.sh，脚本可以接受参数(i,I,a)，完成以下任务：<br>   (1)使用以下形式：getinterface.sh [-i interface|-I IP|-a]<br>   (2)当用户使用-i选项时，显示其指定网卡的IP地址；<br>   (3)当用户使用-I选项时，显示其后面的IP地址所属的网络接口；（如 192.168.199.183：eth0）<br>   (4)当用户单独使用-a选项时，显示所有网络接口及其IP地址（lo除外）</p>
<p>分析<br>参数1为param1 参数2位param2<br>（1）利用CAT &lt;&lt; EOF  <em>*</em> EOF打印信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cat &lt;&lt; EOF</div><div class="line">    getinterface.sh [-i interface|-I IP|<span class="_">-a</span>]</div><div class="line">    -i interface) show ip of the interface</div><div class="line">    -I IP) show interface of the IP and IP with :;</div><div class="line">    <span class="_">-a</span>) list all interfaces and their IPs except lo;</div><div class="line">    *) quit</div><div class="line">=================================================================</div><div class="line">EOF</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>（2）校验接口是否存在（参数2）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ifconfig <span class="variable">$interface</span> &gt;/dev/null </div><div class="line">$? <span class="_">-ne</span> 1  </div><div class="line"><span class="comment">#如果存在打印网卡的IP</span></div><div class="line">ifconfig <span class="variable">$parma2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div></pre></td></tr></table></figure></p>
<p>（3）先打印所有接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;</span></div></pre></td></tr></table></figure></p>
<p>并存在一个变量中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">list=`netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;'</span>`</div></pre></td></tr></table></figure>
<p>遍历集合，查看和输入IP相同的打印IP和接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> inter <span class="keyword">in</span> <span class="variable">$list</span>;  </div><div class="line"><span class="keyword">do</span>  </div><div class="line">    ip_temp=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$IP</span> == <span class="variable">$ip_temp</span> ];<span class="keyword">then</span>  </div><div class="line">	<span class="built_in">echo</span> <span class="string">"<span class="variable">$IP</span> : <span class="variable">$inter</span>"</span>  </div><div class="line">	<span class="built_in">exit</span> 0  </div><div class="line">    <span class="keyword">fi</span>  </div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<p>（4）遍历集合，过滤接口为lo的，打印IP和接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> inter <span class="keyword">in</span> <span class="variable">$list</span>;  </div><div class="line"><span class="keyword">do</span>  </div><div class="line">    ip_temp=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span></div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$inter</span> != <span class="string">"lo"</span> ];<span class="keyword">then</span>  </div><div class="line">	<span class="built_in">echo</span> <span class="string">"<span class="variable">$IP</span> : <span class="variable">$inter</span>"</span>  </div><div class="line">	<span class="built_in">exit</span> 0  </div><div class="line">    <span class="keyword">fi</span>  </div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure></p>
<p>最终的脚本如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">cat &lt;&lt; EOF</div><div class="line">    getinterface.sh [-i interface|-I IP|<span class="_">-a</span>]</div><div class="line">    -i interface) show ip of the interface</div><div class="line">    -I IP) show interface of the IP and IP with :;</div><div class="line">    <span class="_">-a</span>) list all interfaces and their IPs except lo;</div><div class="line">    *) quit</div><div class="line">=================================================================</div><div class="line">EOF</div><div class="line"></div><div class="line"><span class="built_in">read</span> -p <span class="string">"please choice "</span> param1 param2</div><div class="line">list=`netstat -i | sed -n <span class="string">'3,65535p'</span>|awk -F<span class="string">" "</span> <span class="string">'&#123;print $1&#125;'</span>`</div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-i'</span> ]]; <span class="keyword">then</span></div><div class="line">    ifconfig <span class="variable">$interface</span> &gt;/dev/null</div><div class="line">    flag=$?</div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$flag</span> <span class="_">-ne</span> 1 ];<span class="keyword">then</span></div><div class="line">        ip=`ifconfig <span class="variable">$param2</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>`</div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$param2</span> <span class="variable">$ip</span>"</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$parma2</span> is not exist"</span> </div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-I'</span> ]]; <span class="keyword">then</span></div><div class="line"><span class="keyword">for</span> lt <span class="keyword">in</span> <span class="variable">$list</span>;</div><div class="line">  <span class="keyword">do</span></div><div class="line">     ip =$(ifconfig <span class="variable">$lt</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>)</div><div class="line">     <span class="comment">#ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;')</span></div><div class="line">     <span class="keyword">if</span> [ <span class="variable">$param2</span> == <span class="variable">$ip</span> ];<span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$lt</span> : <span class="variable">$ip</span>"</span></div><div class="line">     <span class="keyword">fi</span></div><div class="line">  <span class="keyword">done</span></div><div class="line"><span class="keyword">elif</span> [[ <span class="string">"<span class="variable">$param1</span>"</span> == <span class="string">'-a'</span> ]]; <span class="keyword">then</span></div><div class="line"> <span class="keyword">for</span> lt <span class="keyword">in</span> <span class="variable">$list</span>;</div><div class="line">  <span class="keyword">do</span></div><div class="line">     ip=$(ifconfig <span class="variable">$lt</span> | awk -F<span class="string">" "</span> <span class="string">'/inet.*netmask/&#123;print $2&#125;'</span>)</div><div class="line">     <span class="comment">#ip=$(ifconfig $lt | awk -F" " '/inet.*Mask/&#123;print $2&#125;' | awk -F: '&#123;print $2&#125;')</span></div><div class="line">     <span class="keyword">if</span> [ <span class="variable">$lt</span> != <span class="string">"lo"</span> ];<span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$lt</span> : <span class="variable">$ip</span>"</span></div><div class="line">        <span class="built_in">exit</span> 0</div><div class="line">     <span class="keyword">fi</span></div><div class="line">  <span class="keyword">done</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"quit"</span></div><div class="line">    <span class="built_in">exit</span> 0</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<h3 id="注意如上内容根据你个人机器显示情况而定，如果是显示"><a href="#注意如上内容根据你个人机器显示情况而定，如果是显示" class="headerlink" title="注意如上内容根据你个人机器显示情况而定，如果是显示"></a>注意如上内容根据你个人机器显示情况而定，如果是显示</h3><p> inet addr:127.0.0.1  Mask:255.0.0.0<br> 就要把正则改成<br> ip=$(ifconfig $lt | awk -F” “ ‘/inet.*Mask/{print $2}’ | awk -F: ‘{print $2}’)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;练习题目&quot;&gt;&lt;a href=&quot;#练习题目&quot; class=&quot;headerlink&quot; title=&quot;练习题目&quot;&gt;&lt;/a&gt;练习题目&lt;/h3&gt;&lt;p&gt;写一个脚本getinterface.sh，脚本可以接受参数(i,I,a)，完成以下任务：&lt;br&gt;   (1)使用以下形式：getinterface.sh [-i interface|-I IP|-a]&lt;br&gt;   (2)当用户使用-i选项时，显示其指定网卡的IP地址；&lt;br&gt;   (3)当用户使用-I选项时，显示其后面的IP地址所属的网络接口；（如 192.168.199.183：eth0）&lt;br&gt;   (4)当用户单独使用-a选项时，显示所有网络接口及其IP地址（lo除外）&lt;/p&gt;
&lt;p&gt;分析&lt;br&gt;参数1为param1 参数2位param2&lt;br&gt;（1）利用CAT &amp;lt;&amp;lt; EOF  &lt;em&gt;*&lt;/em&gt; EOF打印信息&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;cat &amp;lt;&amp;lt; EOF&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    getinterface.sh [-i interface|-I IP|&lt;span class=&quot;_&quot;&gt;-a&lt;/span&gt;]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    -i interface) show ip of the interface&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    -I IP) show interface of the IP and IP with :;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;_&quot;&gt;-a&lt;/span&gt;) list all interfaces and their IPs except lo;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    *) quit&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;=================================================================&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;EOF&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="bash" scheme="http://yoursite.com/categories/bash/"/>
    
    
      <category term="bash" scheme="http://yoursite.com/tags/bash/"/>
    
  </entry>
  
  <entry>
    <title>CDH hadoop实战</title>
    <link href="http://yoursite.com/2017/03/14/CDH%20hadoop%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/14/CDH hadoop实战/</id>
    <published>2017-03-14T06:30:00.000Z</published>
    <updated>2017-03-31T13:27:07.075Z</updated>
    
    <content type="html"><![CDATA[<h5 id="解决问题："><a href="#解决问题：" class="headerlink" title="解决问题："></a>解决问题：</h5><p>因为数据手机号一项包含为NULL的数据，需要清洗。</p>
<p>创建工程，添加如下依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&lt;dependencies&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.6.0&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line"></div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;junit&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;junit&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;3.8.1&lt;/version&gt;</div><div class="line">            &lt;scope&gt;test&lt;/scope&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">    &lt;/dependencies&gt;</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>编写mapper类<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> map;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by Administrator on 2017/3/14.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UidPhoneMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">        String arr[] = key.toString().split(<span class="string">","</span>);</div><div class="line">        <span class="keyword">if</span> (arr.length != <span class="number">2</span>) &#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(arr[<span class="number">1</span>] == <span class="keyword">null</span>)&#123;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        context.write(<span class="keyword">new</span> Text(arr[<span class="number">0</span>]), <span class="keyword">new</span> Text(arr[<span class="number">1</span>]));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> main;</div><div class="line"></div><div class="line"><span class="keyword">import</span> map.UidPhoneMapper;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by Administrator on 2017/3/14.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UidPhoneDropNull</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">        GenericOptionsParser parser = <span class="keyword">new</span> GenericOptionsParser(conf, args);</div><div class="line">        String[] otherArgs = parser.getRemainingArgs();</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage: NewlyJoin &lt;inpath&gt; &lt;output&gt;"</span>);</div><div class="line">            System.exit(<span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Job job = <span class="keyword">new</span> Job(conf, <span class="string">"UidPhoneDropNull"</span>);</div><div class="line">        <span class="comment">// 设置运行的job</span></div><div class="line">        job.setJarByClass(UidPhoneDropNull.class);</div><div class="line">        <span class="comment">// 设置Map相关内容</span></div><div class="line">        job.setMapperClass(UidPhoneMapper.class);</div><div class="line">        job.setMapOutputKeyClass(Text.class);</div><div class="line">        job.setMapOutputValueClass(Text.class);</div><div class="line">        job.setInputFormatClass(KeyValueTextInputFormat.class); <span class="comment">//设置文件输入格式</span></div><div class="line">        job.setNumReduceTasks(<span class="number">0</span>); <span class="comment">//设置Reduce个数为0</span></div><div class="line"></div><div class="line">        job.setOutputKeyClass(Text.class);</div><div class="line">        job.setOutputValueClass(Text.class);</div><div class="line">        <span class="comment">// 设置输入和输出的目录</span></div><div class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));</div><div class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));</div><div class="line">        <span class="comment">// 执行，直到结束就退出</span></div><div class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行上面的程序<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">[hdfs@slave3 jar]$ hadoop jar bigdataMR.jar main.UidPhoneDropNull /bigdata/data/hdfs/BD/UP/* /bigdata/data/hdfs/BD/UP2 </div><div class="line">17/03/14 17:37:54 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:8032</div><div class="line">17/03/14 17:37:55 INFO input.FileInputFormat: Total input paths to process : 3</div><div class="line">17/03/14 17:37:55 INFO mapreduce.JobSubmitter: number of splits:3</div><div class="line">17/03/14 17:37:55 INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_1488970234114_0044</div><div class="line">17/03/14 17:37:55 INFO impl.YarnClientImpl: Submitted application application_1488970234114_0044</div><div class="line">17/03/14 17:37:55 INFO mapreduce.Job: The url to track the job: http://master2:8088/proxy/application_1488970234114_0044/</div><div class="line">17/03/14 17:37:55 INFO mapreduce.Job: Running job: job_1488970234114_0044</div><div class="line">17/03/14 17:38:00 INFO mapreduce.Job: Job job_1488970234114_0044 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></div><div class="line">17/03/14 17:38:00 INFO mapreduce.Job:  map 0% reduce 0%</div><div class="line">17/03/14 17:38:04 INFO mapreduce.Job:  map 33% reduce 0%</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job:  map 100% reduce 0%</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job: Job job_1488970234114_0044 completed successfully</div><div class="line">17/03/14 17:38:05 INFO mapreduce.Job: Counters: 30</div><div class="line">        File System Counters</div><div class="line">                FILE: Number of bytes <span class="built_in">read</span>=0</div><div class="line">                FILE: Number of bytes written=378748</div><div class="line">                FILE: Number of <span class="built_in">read</span> operations=0</div><div class="line">                FILE: Number of large <span class="built_in">read</span> operations=0</div><div class="line">                FILE: Number of write operations=0</div><div class="line">                HDFS: Number of bytes <span class="built_in">read</span>=13514456</div><div class="line">                HDFS: Number of bytes written=10824901</div><div class="line">                HDFS: Number of <span class="built_in">read</span> operations=15</div><div class="line">                HDFS: Number of large <span class="built_in">read</span> operations=0</div><div class="line">                HDFS: Number of write operations=6</div><div class="line">        Job Counters </div><div class="line">                Launched map tasks=3</div><div class="line">                Data-local map tasks=3</div><div class="line">                Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=8240</div><div class="line">                Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</div><div class="line">                Total time spent by all map tasks (ms)=8240</div><div class="line">                Total vcore-seconds taken by all map tasks=8240</div><div class="line">                Total megabyte-seconds taken by all map tasks=8437760</div><div class="line">        Map-Reduce Framework</div><div class="line">                Map input records=661590</div><div class="line">                Map output records=485478</div><div class="line">                Input split bytes=393</div><div class="line">                Spilled Records=0</div><div class="line">                Failed Shuffles=0</div><div class="line">                Merged Map outputs=0</div><div class="line">                GC time elapsed (ms)=175</div><div class="line">                CPU time spent (ms)=3730</div><div class="line">                Physical memory (bytes) snapshot=712597504</div><div class="line">                Virtual memory (bytes) snapshot=8316772352</div><div class="line">                Total committed heap usage (bytes)=814743552</div><div class="line">        File Input Format Counters </div><div class="line">                Bytes Read=13514063</div><div class="line">        File Output Format Counters </div><div class="line">                Bytes Written=10824901</div><div class="line">[hdfs@slave3 jar]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP2</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7609438 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    3215441 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup         22 2017-03-14 17:38 /bigdata/data/hdfs/BD/UP2/part-m-00002</div></pre></td></tr></table></figure></p>
<p>最后将数据导入到hive<br>hive&gt; create EXTERNAL table IF NOT EXISTS UP2 (uid STRING,phone STRING) row format delimited fields terminated by ‘,’ location ‘/bigdata/data/hdfs/BD/UP2/‘;<br>OK<br>Time taken: 0.026 seconds<br>hive&gt; select count(1) from UP2;<br>Query ID = hdfs_20170314174141_1b60d560-2036-44e6-ab65-19f17efc5b1b<br>Total jobs = 1<br>Launching Job 1 out of 1<br>Number of reduce tasks determined at compile time: 1<br>In order to change the average load for a reducer (in bytes):<br>  set hive.exec.reducers.bytes.per.reducer=<number><br>In order to limit the maximum number of reducers:<br>  set hive.exec.reducers.max=<number><br>In order to set a constant number of reducers:<br>  set mapreduce.job.reduces=<number><br>Starting Job = job_1488970234114_0045, Tracking URL = <a href="http://master2:8088/proxy/application_1488970234114_0045/" target="_blank" rel="external">http://master2:8088/proxy/application_1488970234114_0045/</a><br>Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0045<br>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1<br>2017-03-14 17:41:33,200 Stage-1 map = 0%,  reduce = 0%<br>2017-03-14 17:41:38,371 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.54 sec<br>2017-03-14 17:41:43,486 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.38 sec<br>MapReduce Total cumulative CPU time: 4 seconds 380 msec<br>Ended Job = job_1488970234114_0045<br>MapReduce Jobs Launched:<br>Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.38 sec   HDFS Read: 10831709 HDFS Write: 7 SUCCESS<br>Total MapReduce CPU Time Spent: 4 seconds 380 msec<br>OK<br>485478<br>Time taken: 16.267 seconds, Fetched: 1 row(s)</number></number></number></p>
<p>比清洗前数据661590少了176112条</p>
]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;解决问题：&quot;&gt;&lt;a href=&quot;#解决问题：&quot; class=&quot;headerlink&quot; title=&quot;解决问题：&quot;&gt;&lt;/a&gt;解决问题：&lt;/h5&gt;&lt;p&gt;因为数据手机号一项包含为NULL的数据，需要清洗。&lt;/p&gt;
&lt;p&gt;创建工程，添加如下依赖&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;lt;dependencies&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-common&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-hdfs&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;2.6.0&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;version&amp;gt;3.8.1&amp;lt;/version&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &amp;lt;/dependency&amp;gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;lt;/dependencies&amp;gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH hive 实战</title>
    <link href="http://yoursite.com/2017/03/14/CDH%20hive%20%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/14/CDH hive 实战/</id>
    <published>2017-03-14T00:30:00.000Z</published>
    <updated>2017-03-31T16:13:09.590Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">hive&gt; create database BD;</div><div class="line">OK</div><div class="line">Time taken: 0.161 seconds</div><div class="line">hive&gt; use BD;</div><div class="line">OK</div><div class="line">Time taken: 0.013 seconds</div></pre></td></tr></table></figure>
<p>创建表<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hive&gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> ;</div><div class="line">OK</div><div class="line">Time taken: 0.211 seconds</div><div class="line">hive&gt; load data inpath <span class="string">'/bigdata/data/hdfs/shuju/SO/part-m-00000'</span> into table UP;</div><div class="line">Loading data to table BD.UP</div><div class="line">Table BD.UP stats: [numFiles=1, totalSize=898611236]</div><div class="line">OK</div><div class="line">Time taken: 0.317 seconds</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>查询表数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314111111_d29152a3-56b3-42f9-b17b-1ddaf7451117</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0025, Tracking URL = http://master2:8088/proxy/application_1488970234114_0025/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0025</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 4; number of reducers: 1</div><div class="line">2017-03-14 11:11:49,267 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 11:11:55,519 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 3.31 sec</div><div class="line">2017-03-14 11:11:56,542 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.57 sec</div><div class="line">2017-03-14 11:12:01,655 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 18.41 sec</div><div class="line">MapReduce Total cumulative CPU time: 18 seconds 410 msec</div><div class="line">Ended Job = job_1488970234114_0025</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 4  Reduce: 1   Cumulative CPU: 18.41 sec   HDFS Read: 899021344 HDFS Write: 8 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 18 seconds 410 msec</div><div class="line">OK</div><div class="line">5674200</div><div class="line">Time taken: 19.429 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>删除表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.088 seconds</div></pre></td></tr></table></figure></p>
<p>查看hdfs上的数据，发现数据被删除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/SO</span></div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-13 19:16 /bigdata/data/hdfs/shuju/SO/_SUCCESS</div></pre></td></tr></table></figure></p>
<h2 id="测试2："><a href="#测试2：" class="headerlink" title="测试2："></a>测试2：</h2><p>首先查询，发现biaoming数据存在<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/biaoming</span></div><div class="line">Found 2 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup   13514063 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/part-m-00000</div></pre></td></tr></table></figure></p>
<p>创建外部表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> ;</div><div class="line">OK</div><div class="line">Time taken: 0.033 seconds</div></pre></td></tr></table></figure></p>
<p>加载数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hive&gt; load data inpath <span class="string">'/bigdata/data/hdfs/shuju/biaoming/part-m-00000'</span> into table UP;</div><div class="line">Loading data to table BD.UP</div><div class="line">Table BD.UP stats: [numFiles=1, totalSize=13514063]</div><div class="line">OK</div><div class="line">Time taken: 0.201 seconds</div></pre></td></tr></table></figure></p>
<p>查询数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314115959_6004ef81-913b-4d38-99dc-1199cc6e73f2</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0027, Tracking URL = http://master2:8088/proxy/application_1488970234114_0027/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0027</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</div><div class="line">2017-03-14 11:59:09,331 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 11:59:14,463 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.06 sec</div><div class="line">2017-03-14 11:59:19,587 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.9 sec</div><div class="line">MapReduce Total cumulative CPU time: 4 seconds 900 msec</div><div class="line">Ended Job = job_1488970234114_0027</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.9 sec   HDFS Read: 13520683 HDFS Write: 7 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 4 seconds 900 msec</div><div class="line">OK</div><div class="line">661590</div><div class="line">Time taken: 17.707 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>删除数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.037 seconds</div></pre></td></tr></table></figure></p>
<p>再查看元数据，发现依然被删除了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@slave3 data]<span class="comment"># hadoop fs -ls /bigdata/data/hdfs/shuju/biaoming</span></div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 10:36 /bigdata/data/hdfs/shuju/biaoming/_SUCCESS</div></pre></td></tr></table></figure></p>
<h2 id="测试3："><a href="#测试3：" class="headerlink" title="测试3："></a>测试3：</h2><p>重新上传数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver  --connect <span class="string">'jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=BD'</span> --table=UP --target-dir /bigdata/data/hdfs/BD/UP --split-by uid -m 3</div></pre></td></tr></table></figure></p>
<p>查看数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup    3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002</div><div class="line">-rw-r--r--   3 hdfs supergroup         22 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00003</div></pre></td></tr></table></figure></p>
<p>创建外部表数据：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">hive&gt; select count(1) from UP;</div><div class="line">FAILED: SemanticException [Error 10001]: Line 1:21 Table not found <span class="string">'UP'</span></div><div class="line">hive&gt; create EXTERNAL table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> location <span class="string">'/bigdata/data/hdfs/BD/UP/'</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.03 seconds</div><div class="line">hive&gt; select count(1) from UP;</div><div class="line">Query ID = hdfs_20170314141111_5be2c701-ad6a-4717-8801-513f38d64928</div><div class="line">Total <span class="built_in">jobs</span> = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">Number of reduce tasks determined at compile time: 1</div><div class="line">In order to change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">In order to <span class="built_in">limit</span> the maximum number of reducers:</div><div class="line">  <span class="built_in">set</span> hive.exec.reducers.max=&lt;number&gt;</div><div class="line">In order to <span class="built_in">set</span> a constant number of reducers:</div><div class="line">  <span class="built_in">set</span> mapreduce.job.reduces=&lt;number&gt;</div><div class="line">Starting Job = job_1488970234114_0031, Tracking URL = http://master2:8088/proxy/application_1488970234114_0031/</div><div class="line">Kill Command = /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/bin/hadoop job  -kill job_1488970234114_0031</div><div class="line">Hadoop job information <span class="keyword">for</span> Stage-1: number of mappers: 1; number of reducers: 1</div><div class="line">2017-03-14 14:11:48,608 Stage-1 map = 0%,  reduce = 0%</div><div class="line">2017-03-14 14:11:54,743 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.26 sec</div><div class="line">2017-03-14 14:11:58,833 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.08 sec</div><div class="line">MapReduce Total cumulative CPU time: 5 seconds 80 msec</div><div class="line">Ended Job = job_1488970234114_0031</div><div class="line">MapReduce Jobs Launched: </div><div class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.08 sec   HDFS Read: 13520952 HDFS Write: 7 SUCCESS</div><div class="line">Total MapReduce CPU Time Spent: 5 seconds 80 msec</div><div class="line">OK</div><div class="line">661590</div><div class="line">Time taken: 16.253 seconds, Fetched: 1 row(s)</div></pre></td></tr></table></figure></p>
<p>此时再删除表后数据还存在。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.037 seconds</div><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">Found 5 items</div><div class="line">-rw-r--r--   3 hdfs supergroup          0 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/_SUCCESS</div><div class="line">-rw-r--r--   3 hdfs supergroup    7959628 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00000</div><div class="line">-rw-r--r--   3 hdfs supergroup    2125416 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00001</div><div class="line">-rw-r--r--   3 hdfs supergroup    3428997 2017-03-14 14:06 /bigdata/data/hdfs/BD/UP/part-m-00002</div></pre></td></tr></table></figure></p>
<p>创建普通表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hive&gt; create table IF NOT EXISTS UP (uid STRING,phone STRING) row format delimited fields terminated by <span class="string">','</span> location <span class="string">'/bigdata/data/hdfs/BD/UP/'</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.029 seconds</div></pre></td></tr></table></figure></p>
<p>删除表后数据不存在<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hive&gt; DROP TABLE IF EXISTS UP;</div><div class="line">OK</div><div class="line">Time taken: 0.046 seconds</div><div class="line">[hdfs@slave3 data]$ hadoop fs -ls /bigdata/data/hdfs/BD/UP</div><div class="line">ls: `/bigdata/data/hdfs/BD/UP<span class="string">': No such file or directory</span></div></pre></td></tr></table></figure></p>
<p>总结<br>用load方式无论外部表还是内部表数据都会删除，用location方式，外部表不会删除数据，内部表会删除数据</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;创建数据库&quot;&gt;&lt;a href=&quot;#创建数据库&quot; class=&quot;headerlink&quot; title=&quot;创建数据库&quot;&gt;&lt;/a&gt;创建数据库&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; create database BD;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.161 seconds&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; use BD;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.013 seconds&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;创建表&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; create table UP(uid STRING,phone STRING) row format delimited fields terminated by &lt;span class=&quot;string&quot;&gt;&#39;,&#39;&lt;/span&gt; ;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.211 seconds&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;hive&amp;gt; load data inpath &lt;span class=&quot;string&quot;&gt;&#39;/bigdata/data/hdfs/shuju/SO/part-m-00000&#39;&lt;/span&gt; into table UP;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Loading data to table BD.UP&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Table BD.UP stats: [numFiles=1, totalSize=898611236]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;OK&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Time taken: 0.317 seconds&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH sqoop1 实战-带条件的导入</title>
    <link href="http://yoursite.com/2017/03/13/CDH%20sqoop1%20%E5%AE%9E%E6%88%98-%E5%B8%A6%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%AF%BC%E5%85%A5/"/>
    <id>http://yoursite.com/2017/03/13/CDH sqoop1 实战-带条件的导入/</id>
    <published>2017-03-13T01:30:00.000Z</published>
    <updated>2017-03-31T13:27:41.442Z</updated>
    
    <content type="html"><![CDATA[<p>[hdfs@slave3 lib]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query “SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1”  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming<br>17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0<br>17/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br>17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br>17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation<br>17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‘$CONDITIONS’ in WHERE clause.<br>        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332)<br>        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858)<br>        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658)<br>        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)<br>        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)<br>        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)<br>        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)<br>        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)<br>        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)</p>
<p>修改为：<br>sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming</p>
<a id="more"></a>
<p>在执行，发现报错说我database不存在，并在目录下生成了一个metastore_db，原来是执行sqoop的机器不对，换了一台机器执行没有问题，很奇怪明明是分布式的为什么会出现这个问题呢</p>
<p>最终的具体日志：</p>
<p>[hdfs@master1 root]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming –hive-database shuju<br>Warning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.<br>Please set $ACCUMULO_HOME to the root of your Accumulo installation.<br>17/03/15 14:32:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0<br>17/03/15 14:32:53 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br>17/03/15 14:32:53 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br>17/03/15 14:32:53 INFO tool.CodeGenTool: Beginning code generation<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:32:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/bin/../lib/sqoop/../hadoop-mapreduce<br>Note: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.java uses or overrides a deprecated API.<br>Note: Recompile with -Xlint:deprecation for details.<br>17/03/15 14:33:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/9ff9643fb7d49c9376c7b3f92d484efe/QueryResult.jar<br>17/03/15 14:33:00 INFO mapreduce.ImportJobBase: Beginning query import.<br>17/03/15 14:33:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar<br>17/03/15 14:33:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps<br>17/03/15 14:33:01 INFO client.RMProxy: Connecting to ResourceManager at master2/10.105.10.122:8032<br>17/03/15 14:33:02 INFO db.DBInputFormat: Using read commited transaction isolation<br>17/03/15 14:33:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM (SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 1) ) AS t1<br>17/03/15 14:33:03 INFO db.IntegerSplitter: Split size: 7508318; Num splits: 4 from: 20079 to: 30053354<br>17/03/15 14:33:03 INFO mapreduce.JobSubmitter: number of splits:4<br>17/03/15 14:33:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1488970234114_0057<br>17/03/15 14:33:03 INFO impl.YarnClientImpl: Submitted application application_1488970234114_0057<br>17/03/15 14:33:03 INFO mapreduce.Job: The url to track the job: <a href="http://master2:8088/proxy/application_1488970234114_0057/" target="_blank" rel="external">http://master2:8088/proxy/application_1488970234114_0057/</a><br>17/03/15 14:33:03 INFO mapreduce.Job: Running job: job_1488970234114_0057<br>17/03/15 14:33:08 INFO mapreduce.Job: Job job_1488970234114_0057 running in uber mode : false<br>17/03/15 14:33:08 INFO mapreduce.Job:  map 0% reduce 0%<br>17/03/15 14:33:15 INFO mapreduce.Job:  map 75% reduce 0%<br>17/03/15 14:33:16 INFO mapreduce.Job:  map 100% reduce 0%<br>17/03/15 14:33:16 INFO mapreduce.Job: Job job_1488970234114_0057 completed successfully<br>17/03/15 14:33:16 INFO mapreduce.Job: Counters: 30<br>        File System Counters<br>                FILE: Number of bytes read=0<br>                FILE: Number of bytes written=608160<br>                FILE: Number of read operations=0<br>                FILE: Number of large read operations=0<br>                FILE: Number of write operations=0<br>                HDFS: Number of bytes read=428<br>                HDFS: Number of bytes written=17645206<br>                HDFS: Number of read operations=16<br>                HDFS: Number of large read operations=0<br>                HDFS: Number of write operations=8<br>        Job Counters<br>                Launched map tasks=4<br>                Other local map tasks=4<br>                Total time spent by all maps in occupied slots (ms)=17968<br>                Total time spent by all reduces in occupied slots (ms)=0<br>                Total time spent by all map tasks (ms)=17968<br>                Total vcore-seconds taken by all map tasks=17968<br>                Total megabyte-seconds taken by all map tasks=18399232<br>        Map-Reduce Framework<br>                Map input records=283659<br>                Map output records=283659<br>                Input split bytes=428<br>                Spilled Records=0<br>                Failed Shuffles=0<br>                Merged Map outputs=0<br>                GC time elapsed (ms)=268<br>                CPU time spent (ms)=12960<br>                Physical memory (bytes) snapshot=1151381504<br>                Virtual memory (bytes) snapshot=11157348352<br>                Total committed heap usage (bytes)=1045954560<br>        File Input Format Counters<br>                Bytes Read=0<br>        File Output Format Counters<br>                Bytes Written=17645206<br>17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Transferred 16.8278 MB in 15.7965 seconds (1.0653 MB/sec)<br>17/03/15 14:33:16 INFO mapreduce.ImportJobBase: Retrieved 283659 records.<br>17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:33:16 INFO manager.SqlManager: Executing SQL statement: SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE  (1 = 0)<br>17/03/15 14:33:16 WARN hive.TableDefWriter: Column update_time had to be cast to a less precise type in Hive<br>17/03/15 14:33:16 INFO hive.HiveImport: Loading uploaded data into Hive</p>
<p>Logging initialized using configuration in jar:file:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.properties<br>OK<br>Time taken: 1.874 seconds<br>Loading data to table shuju.card_record<br>Table shuju.card_record stats: [numFiles=4, totalSize=17645206]<br>OK<br>Time taken: 0.367 seconds</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[hdfs@slave3 lib]$ sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query “SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1”  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming&lt;br&gt;17/03/15 11:57:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0&lt;br&gt;17/03/15 11:57:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.&lt;br&gt;17/03/15 11:57:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.&lt;br&gt;17/03/15 11:57:44 INFO tool.CodeGenTool: Beginning code generation&lt;br&gt;17/03/15 11:57:44 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Query [SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming where 1=1] must contain ‘$CONDITIONS’ in WHERE clause.&lt;br&gt;        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:332)&lt;br&gt;        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858)&lt;br&gt;        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1658)&lt;br&gt;        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)&lt;br&gt;        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)&lt;br&gt;        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)&lt;br&gt;        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)&lt;br&gt;        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)&lt;br&gt;        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)&lt;br&gt;        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)&lt;br&gt;        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)&lt;br&gt;        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)&lt;/p&gt;
&lt;p&gt;修改为：&lt;br&gt;sqoop import –connect jdbc:mysql://10.105.10.46:3306/shuju –username username –password password –query ‘SELECT id,name,name_en,update_time,modify_user_name,first_mobile,second_mobile FROM biaoming WHERE $CONDITIONS’  –fields-terminated-by “,” –lines-terminated-by “\n” –split-by id  –hive-import  –create-hive-table –hive-table card_record –target-dir /bigdata/data/hdfs/shuju/biaoming&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH sqoop1 实战</title>
    <link href="http://yoursite.com/2017/03/13/CDH%20sqoop1%20%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2017/03/13/CDH sqoop1 实战/</id>
    <published>2017-03-13T01:30:00.000Z</published>
    <updated>2017-04-13T01:10:35.112Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载sqlserver的jdbc驱动包"><a href="#下载sqlserver的jdbc驱动包" class="headerlink" title="下载sqlserver的jdbc驱动包"></a>下载sqlserver的jdbc驱动包</h2><p><a href="https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599" target="_blank" rel="external">https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599</a><br>解压<br>将sqljdbc4.jar放在：<br>/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/lib</p>
<p>下载SQL Server-Hadoop Connector：sqoop-sqlserver-1.0.tar.gz<br>             <a href="http://www.microsoft.com/en-us/download/details.aspx?id=27584" target="_blank" rel="external">http://www.microsoft.com/en-us/download/details.aspx?id=27584</a></p>
<a id="more"></a>
<h2 id="导入数据："><a href="#导入数据：" class="headerlink" title="导入数据："></a>导入数据：</h2><p>[root@slave3 sqoop]# ./bin/sqoop import –connect ‘jdbc: server://10.105.32.246 username=sa password=123456 database=databaseName –table=tableName –target-dir /bigdata/data/hdfs<br>Warning: /soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/bin/../../accumulo does not exist! Accumulo imports will fail.<br>Please set $ACCUMULO_HOME to the root of your Accumulo installation.<br>+======================================================================+<br>|                    Error: JAVA_HOME is not set                       |<br>+———————————————————————-+<br>| Please download the latest Sun JDK from the Sun Java web site        |<br>|     &gt; <a href="http://www.oracle.com/technetwork/java/javase/downloads" target="_blank" rel="external">http://www.oracle.com/technetwork/java/javase/downloads</a>        |<br>|                                                                      |<br>| HBase requires Java 1.7 or later.                                    |<br>+======================================================================+<br>Error: JAVA_HOME is not set and could not be found.</p>
<p>报错解决方法：<br>修改bin下的<br>configure-sqoop</p>
<p>注释以下代码：</p>
<p>#if [ -z “${HCAT_HOME}” ]; then</p>
<h1 id="if-d-“-usr-lib-hive-hcatalog”-then"><a href="#if-d-“-usr-lib-hive-hcatalog”-then" class="headerlink" title="if [ -d “/usr/lib/hive-hcatalog” ]; then"></a>if [ -d “/usr/lib/hive-hcatalog” ]; then</h1><h1 id="HCAT-HOME-usr-lib-hive-hcatalog"><a href="#HCAT-HOME-usr-lib-hive-hcatalog" class="headerlink" title="HCAT_HOME=/usr/lib/hive-hcatalog"></a>HCAT_HOME=/usr/lib/hive-hcatalog</h1><h1 id="elif-d-“-usr-lib-hcatalog”-then"><a href="#elif-d-“-usr-lib-hcatalog”-then" class="headerlink" title="elif [ -d “/usr/lib/hcatalog” ]; then"></a>elif [ -d “/usr/lib/hcatalog” ]; then</h1><h1 id="HCAT-HOME-usr-lib-hcatalog"><a href="#HCAT-HOME-usr-lib-hcatalog" class="headerlink" title="HCAT_HOME=/usr/lib/hcatalog"></a>HCAT_HOME=/usr/lib/hcatalog</h1><h1 id="else"><a href="#else" class="headerlink" title="else"></a>else</h1><h1 id="HCAT-HOME-SQOOP-HOME-hive-hcatalog"><a href="#HCAT-HOME-SQOOP-HOME-hive-hcatalog" class="headerlink" title="HCAT_HOME=${SQOOP_HOME}/../hive-hcatalog"></a>HCAT_HOME=${SQOOP_HOME}/../hive-hcatalog</h1><h1 id="if-d-HCAT-HOME-then"><a href="#if-d-HCAT-HOME-then" class="headerlink" title="if [ ! -d ${HCAT_HOME} ]; then"></a>if [ ! -d ${HCAT_HOME} ]; then</h1><h1 id="HCAT-HOME-SQOOP-HOME-hcatalog"><a href="#HCAT-HOME-SQOOP-HOME-hcatalog" class="headerlink" title="HCAT_HOME=${SQOOP_HOME}/../hcatalog"></a>HCAT_HOME=${SQOOP_HOME}/../hcatalog</h1><h1 id="fi"><a href="#fi" class="headerlink" title="fi"></a>fi</h1><h1 id="fi-1"><a href="#fi-1" class="headerlink" title="fi"></a>fi</h1><p>#fi</p>
<p>#if [ -z “${ACCUMULO_HOME}” ]; then</p>
<h1 id="if-d-“-usr-lib-accumulo”-then"><a href="#if-d-“-usr-lib-accumulo”-then" class="headerlink" title="if [ -d “/usr/lib/accumulo” ]; then"></a>if [ -d “/usr/lib/accumulo” ]; then</h1><h1 id="ACCUMULO-HOME-usr-lib-accumulo"><a href="#ACCUMULO-HOME-usr-lib-accumulo" class="headerlink" title="ACCUMULO_HOME=/usr/lib/accumulo"></a>ACCUMULO_HOME=/usr/lib/accumulo</h1><h1 id="else-1"><a href="#else-1" class="headerlink" title="else"></a>else</h1><h1 id="ACCUMULO-HOME-SQOOP-HOME-accumulo"><a href="#ACCUMULO-HOME-SQOOP-HOME-accumulo" class="headerlink" title="ACCUMULO_HOME=${SQOOP_HOME}/../accumulo"></a>ACCUMULO_HOME=${SQOOP_HOME}/../accumulo</h1><h1 id="fi-2"><a href="#fi-2" class="headerlink" title="fi"></a>fi</h1><p>#fi</p>
<p>#if [ -z “${HCAT_HOME}” ]; then</p>
<h1 id="if-d-“-usr-lib-hive-hcatalog”-then-1"><a href="#if-d-“-usr-lib-hive-hcatalog”-then-1" class="headerlink" title="if [ -d “/usr/lib/hive-hcatalog” ]; then"></a>if [ -d “/usr/lib/hive-hcatalog” ]; then</h1><h1 id="HCAT-HOME-usr-lib-hive-hcatalog-1"><a href="#HCAT-HOME-usr-lib-hive-hcatalog-1" class="headerlink" title="HCAT_HOME=/usr/lib/hive-hcatalog"></a>HCAT_HOME=/usr/lib/hive-hcatalog</h1><h1 id="elif-d-“-usr-lib-hcatalog”-then-1"><a href="#elif-d-“-usr-lib-hcatalog”-then-1" class="headerlink" title="elif [ -d “/usr/lib/hcatalog” ]; then"></a>elif [ -d “/usr/lib/hcatalog” ]; then</h1><h1 id="HCAT-HOME-usr-lib-hcatalog-1"><a href="#HCAT-HOME-usr-lib-hcatalog-1" class="headerlink" title="HCAT_HOME=/usr/lib/hcatalog"></a>HCAT_HOME=/usr/lib/hcatalog</h1><h1 id="else-2"><a href="#else-2" class="headerlink" title="else"></a>else</h1><h1 id="HCAT-HOME-SQOOP-HOME-hive-hcatalog-1"><a href="#HCAT-HOME-SQOOP-HOME-hive-hcatalog-1" class="headerlink" title="HCAT_HOME=${SQOOP_HOME}/../hive-hcatalog"></a>HCAT_HOME=${SQOOP_HOME}/../hive-hcatalog</h1><h1 id="if-d-HCAT-HOME-then-1"><a href="#if-d-HCAT-HOME-then-1" class="headerlink" title="if [ ! -d ${HCAT_HOME} ]; then"></a>if [ ! -d ${HCAT_HOME} ]; then</h1><h1 id="HCAT-HOME-SQOOP-HOME-hcatalog-1"><a href="#HCAT-HOME-SQOOP-HOME-hcatalog-1" class="headerlink" title="HCAT_HOME=${SQOOP_HOME}/../hcatalog"></a>HCAT_HOME=${SQOOP_HOME}/../hcatalog</h1><h1 id="fi-3"><a href="#fi-3" class="headerlink" title="fi"></a>fi</h1><h1 id="fi-4"><a href="#fi-4" class="headerlink" title="fi"></a>fi</h1><p>#fi</p>
<p>#if [ -z “${ACCUMULO_HOME}” ]; then</p>
<h1 id="if-d-“-usr-lib-accumulo”-then-1"><a href="#if-d-“-usr-lib-accumulo”-then-1" class="headerlink" title="if [ -d “/usr/lib/accumulo” ]; then"></a>if [ -d “/usr/lib/accumulo” ]; then</h1><h1 id="ACCUMULO-HOME-usr-lib-accumulo-1"><a href="#ACCUMULO-HOME-usr-lib-accumulo-1" class="headerlink" title="ACCUMULO_HOME=/usr/lib/accumulo"></a>ACCUMULO_HOME=/usr/lib/accumulo</h1><h1 id="else-3"><a href="#else-3" class="headerlink" title="else"></a>else</h1><h1 id="ACCUMULO-HOME-SQOOP-HOME-accumulo-1"><a href="#ACCUMULO-HOME-SQOOP-HOME-accumulo-1" class="headerlink" title="ACCUMULO_HOME=${SQOOP_HOME}/../accumulo"></a>ACCUMULO_HOME=${SQOOP_HOME}/../accumulo</h1><h1 id="fi-5"><a href="#fi-5" class="headerlink" title="fi"></a>fi</h1><p>#fi</p>
<p>在执行如果提示JAVA_HOME不存在，手动执行一下export JAVA_HOME=/usr/java/jdk1.8.0_121</p>
<p>再执行提示：<br>ERROR tool.BaseSqoopTool: Error parsing arguments for import:</p>
<p>./bin/sqoop import –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’ –table=tableName –target-dir /bigdata/data/hdfs –split-by order_id –fields-terminated-by ‘\t’  –m 3</p>
<p>需要下载sqoop-sqlserver<br>下载地址已经失效，在CSDN上找到：<br><a href="https://passport.csdn.net/account/login?from=http://download.csdn.net/download/nma_123456/9405343" target="_blank" rel="external">https://passport.csdn.net/account/login?from=http://download.csdn.net/download/nma_123456/9405343</a></p>
<p>解压进入目录执行：<br>[root@slave3 sqoop-sqlserver-1.0]# export SQOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop<br>[root@slave3 sqoop-sqlserver-1.0]# ./install.sh </p>
<p>再次执行：报如下错误<br>Exception in thread “main” java.lang.NoClassDefFoundError: org/json/JSONObject<br>        at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:43)<br>        at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:767)<br>        at org.apache.sqoop.mapreduce.JobBase.putSqoopOptionsToConfiguration(JobBase.java:388)<br>        at org.apache.sqoop.mapreduce.JobBase.createJob(JobBase.java:374)<br>        at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:256)<br>        at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692)<br>        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507)<br>        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)<br>        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)<br>        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)<br>        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)<br>Caused by: java.lang.ClassNotFoundException: org.json.JSONObject<br>        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)<br>        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)<br>        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)<br>        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</p>
<p>分析原因缺少org/json/JSONObject，上网下载一个json.jar<br>地址：<a href="http://download.csdn.net/download/haixia_12/8462933" target="_blank" rel="external">http://download.csdn.net/download/haixia_12/8462933</a></p>
<p>扔进去重新执行就OK了</p>
<p>最终命令：<br>sqoop import –driver com.microsoft.sqlserver.jdbc.SQLServerDriver  –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’ –table=tableName –target-dir /bigdata/data/hdfs/cards –split-by order_id -m 3</p>
<p>查看sqlserver上有什么数据：<br> sqoop list-tables –driver com.microsoft.sqlserver.jdbc.SQLServerDriver  –connect ‘jdbc:sqlserver://10.105.32.246;username=sa;password=123456;database=databaseName’</p>
<p> sqlserver如果连接失败，要看是否开启了远程访问和TCP/UDP端口映射</p>
<p>报错<br>17/03/13 17:20:01 ERROR hive.HiveConfig: Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly.<br>17/03/13 17:20:01 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</p>
<p>增加：<br>export HADOOP_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop<br>export HIVE_HOME=/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive</p>
<p>export HIVE_CONF_DIR=/etc/hive/conf<br>export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib/*</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;下载sqlserver的jdbc驱动包&quot;&gt;&lt;a href=&quot;#下载sqlserver的jdbc驱动包&quot; class=&quot;headerlink&quot; title=&quot;下载sqlserver的jdbc驱动包&quot;&gt;&lt;/a&gt;下载sqlserver的jdbc驱动包&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599&quot;&gt;https://www.microsoft.com/zh-cn/download/confirmation.aspx?id=21599&lt;/a&gt;&lt;br&gt;解压&lt;br&gt;将sqljdbc4.jar放在：&lt;br&gt;/soft/bigdata/clouderamanager/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/lib&lt;/p&gt;
&lt;p&gt;下载SQL Server-Hadoop Connector：sqoop-sqlserver-1.0.tar.gz&lt;br&gt;             &lt;a href=&quot;http://www.microsoft.com/en-us/download/details.aspx?id=27584&quot;&gt;http://www.microsoft.com/en-us/download/details.aspx?id=27584&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH 安装</title>
    <link href="http://yoursite.com/2017/03/08/CDH%20%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/03/08/CDH 安装/</id>
    <published>2017-03-08T01:30:00.000Z</published>
    <updated>2017-03-31T13:26:29.989Z</updated>
    
    <content type="html"><![CDATA[<h2 id="运行环境："><a href="#运行环境：" class="headerlink" title="运行环境："></a>运行环境：</h2><table>
<thead>
<tr>
<th>主机IP</th>
<th>主机名</th>
<th>内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1.1.147</td>
<td>po-master1</td>
<td>16G</td>
</tr>
<tr>
<td>1.1.1.127</td>
<td>po-master2</td>
<td>8G</td>
</tr>
<tr>
<td>1.1.1.118</td>
<td>po-slave1</td>
<td>8G</td>
</tr>
<tr>
<td>1.1.1.92</td>
<td>po-slave2</td>
<td>8G</td>
</tr>
<tr>
<td>1.1.1.230</td>
<td>po-slave3</td>
<td>8G</td>
</tr>
</tbody>
</table>
<h2 id="配置主机名-分别在五台机器上执行"><a href="#配置主机名-分别在五台机器上执行" class="headerlink" title="配置主机名(分别在五台机器上执行)"></a>配置主机名(分别在五台机器上执行)</h2><p>vi /etc/sysconfig/network<br>hostname +主机名<br>例如： hostname po-master1</p>
<h2 id="配置映射关系-把以下五条命令在五台机器上执行"><a href="#配置映射关系-把以下五条命令在五台机器上执行" class="headerlink" title="配置映射关系(把以下五条命令在五台机器上执行)"></a>配置映射关系(把以下五条命令在五台机器上执行)</h2><p>echo “1.1.1.147   po-master1”&gt;&gt;/etc/hosts<br>echo “1.1.1.127   po-master2”&gt;&gt;/etc/hosts<br>echo “1.1.1.118  po-slave1”&gt;&gt;/etc/hosts<br>echo “1.1.1.92   po-slave2”&gt;&gt;/etc/hosts<br>echo “1.1.1.230  po-slave3”&gt;&gt;/etc/hosts<br><a id="more"></a></p>
<h2 id="安装JDK（在po-master1上执行）"><a href="#安装JDK（在po-master1上执行）" class="headerlink" title="安装JDK（在po-master1上执行）"></a>安装JDK（在po-master1上执行）</h2><p>1.下载JDK安装包：jdk-8u102-linux-x64.tar.gz<br>注：作者放到/soft/java具体位置可自行安排</p>
<ol>
<li><p>安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /soft/java</div><div class="line">mkdir jdk1.8.0_121</div><div class="line">rpm -ivh jdk-7u76-linux-x64.rpm --prefix=/soft/java</div></pre></td></tr></table></figure>
</li>
<li><p>创建连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ln <span class="_">-s</span> <span class="_">-f</span> jdk1.8.0_121/ jdk</div></pre></td></tr></table></figure>
</li>
</ol>
<!-- more -->
<h2 id="开放端口（五台机器上都需要配置）"><a href="#开放端口（五台机器上都需要配置）" class="headerlink" title="开放端口（五台机器上都需要配置）"></a>开放端口（五台机器上都需要配置）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">/sbin/iptables -I INPUT -p tcp --dport 80 -j ACCEP</div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> 0.0.0.0/0 -p tcp --dport 22	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 22	  -j ACCEPT</div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50010	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 1004	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50075	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 1006	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50070	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50470	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50090	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 50495	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8485	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8480	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8032	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8030	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8031	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8033	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8088	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8040	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8042	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8041	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 10020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 13562	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 19888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60010	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60020	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 60030	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 3888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8080	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8085	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9090	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9095	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9090	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9083	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 10000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 16000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 2888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 3888	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 3181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 4181	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 8019	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 9010	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 11000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 11001	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 14000	  -j ACCEPT </div><div class="line">/sbin/iptables -A INPUT <span class="_">-s</span> x.x.x.x -p tcp --dport 14001	  -j ACCEPT </div><div class="line">/etc/rc.d/init.d/iptables save</div></pre></td></tr></table></figure>
<p>关闭端口详解-参考CDH官网<br><a href="https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_ports_cdh5.html" target="_blank" rel="external">https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/cdh5ig_ports_cdh5.html</a></p>
<p>测试（可忽略）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/etc/init.d/iptables status</div></pre></td></tr></table></figure></p>
<h2 id="五台机器配置互相免秘钥登陆"><a href="#五台机器配置互相免秘钥登陆" class="headerlink" title="五台机器配置互相免秘钥登陆"></a>五台机器配置互相免秘钥登陆</h2><p>1.创建ssh文件<br>如果已经创建不要覆盖<br>cat ~/.ssh/id_rsa.pub &gt;&gt;~/.ssh/authorized_keys<br>分别把五台机器的公钥加载到authorized_keys</p>
<p>2.<br>vi /etc/ssh/sshd_config<br>打开如下内容<br>HostKey /etc/ssh/ssh_host_rsa_key<br>RSAAuthentication yes<br>PubkeyAuthentication yes<br>AuthorizedKeysFile      .ssh/authorized_keys</p>
<p>3.重启<br>/etc/init.d/sshd restart</p>
<p>4.测试ssh<br>ssh po-master1<br>ssh po-master2<br>ssh po-slave1<br>ssh po-slave2<br>ssh po-slave3</p>
<h2 id="向其他机器分发jdk"><a href="#向其他机器分发jdk" class="headerlink" title="向其他机器分发jdk"></a>向其他机器分发jdk</h2><p>scp -rp /soft/java/ root@po-master2:/soft/java<br>scp -rp /soft/java/ root@po-salve1:/soft/java<br>scp -rp /soft/java/ root@po-salve2:/soft/java<br>scp -rp /soft/java/ root@po-salve3:/soft/java</p>
<h2 id="配置环境变量-分别在五台机器上执行"><a href="#配置环境变量-分别在五台机器上执行" class="headerlink" title="配置环境变量(分别在五台机器上执行)"></a>配置环境变量(分别在五台机器上执行)</h2><p>执行如下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">"export JAVA_HOME=/soft/java/jdk"</span>  &gt;&gt; /etc/profile</div><div class="line"><span class="built_in">echo</span> <span class="string">"export PATH=<span class="variable">$PATH</span>:<span class="variable">$HOME</span>/bin:<span class="variable">$JAVA_HOME</span>:<span class="variable">$JAVA_HOME</span>/bin:/usr/bin/"</span>  &gt;&gt; /etc/profile</div><div class="line"><span class="built_in">echo</span> <span class="string">"export CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib"</span>  &gt;&gt; /etc/profile</div><div class="line">. /etc/profile</div></pre></td></tr></table></figure></p>
<h2 id="测试（可忽略）"><a href="#测试（可忽略）" class="headerlink" title="测试（可忽略）"></a>测试（可忽略）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 java]<span class="comment"># java -version</span></div><div class="line">java version <span class="string">"1.8.0_121"</span></div><div class="line">Java(TM) SE Runtime Environment (build 1.8.0_121-b13)</div><div class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)</div></pre></td></tr></table></figure>
<h2 id="配置NTP服务器和客户端（因为使用阿里云此处省略）"><a href="#配置NTP服务器和客户端（因为使用阿里云此处省略）" class="headerlink" title="配置NTP服务器和客户端（因为使用阿里云此处省略）"></a>配置NTP服务器和客户端（因为使用阿里云此处省略）</h2><h2 id="配置mysql"><a href="#配置mysql" class="headerlink" title="配置mysql"></a>配置mysql</h2><p>1.上传mysql文件（博主放到/soft/mysql目录下）<br>2.解压<br>cd /soft/mysql<br>tar -zxvf mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz -C /usr/local<br>3.将目录重命名<br>cd /usr/local<br>mv mysql-5.7.17-linux-glibc2.5-x86_64/ mysql<br>4.创建data目录</p>
<p>mkdir /usr/local/mysql/data</p>
<p>5.安装插件（网上有人说不安装提示libiao错误，博主用阿里云libaio已经是最新版本，所以不用安装，也不知道没安装有什么坏处）<br> yum install libaio</p>
<p>6.安装mysql<br>cd /usr/local/mysql/bin<br>./mysql_install_db –user=root –basedir=/usr/local/mysql –datadir=//usr/local/mysql/data</p>
<p>1.官网下载yum源<br><a href="https://dev.mysql.com/downloads/repo/yum/" target="_blank" rel="external">https://dev.mysql.com/downloads/repo/yum/</a><br>2.安装yum源<br>yum localinstall  mysql57-community-release-el6-9.noarch.rpm<br>3.安装mysql<br>yum install mysql-community-server</p>
<p>4.创建组和用户<br>groupadd mysql<br>useradd mysql -g mysql</p>
<p>5.修改配置文件开启二进制日志<br>vi /etc/my.cnf  （在[mysqld]下面添加如下内容）<br>server-id=1<br>log-bin=/home/mysql/log/logbin.log</p>
<p>6.开启服务<br>service mysqld start</p>
<p>7.查看mysql默认的密码<br>grep ‘temporary password’ /var/log/mysqld.log</p>
<p>8.根据密码进入mysql<br>mysql -u root -p<br>ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘MyNewPass4!’;</p>
<p>例如：<br>ALTER USER ‘root’@’localhost’ IDENTIFIED BY ‘password’;<br>Query OK, 0 rows affected (0.01 sec)</p>
<p>注：MySQL’s validate_password plugin is installed by default. This will require that passwords contain at least one upper case letter, one lower case letter, one digit, and one special character, and that the total password length is at least 8 characters. </p>
<p>9.授权（给其他四台机器授权）<br>grant all privileges on oozie.<em> to ‘oozie’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.</em> to ‘oozie’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.<em> to ‘oozie’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.</em> to ‘oozie’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on oozie.* to ‘oozie’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>GRANT all privileges on <em>.</em> to ‘root’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>GRANT ALL PRIVILEGES ON <em>.</em> TO ‘root’@’182.48.105.23’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>grant all privileges on hive.<em> to ‘hive’@’localhost’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.</em> to ‘hive’@’10.28.92.19’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.<em> to ‘hive’@’10.28.100.108’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.</em> to ‘hive’@’10.28.100.212’ IDENTIFIED BY ‘password’ WITH GRANT OPTION;<br>grant all privileges on hive.* to ‘hive’@’10.28.100.254’ IDENTIFIED BY ‘password’ WITH GRANT OPTION; </p>
<p>flush privileges;</p>
<p>关于新版本的账户说明：<br><a href="https://dev.mysql.com/doc/refman/5.7/en/adding-users.html" target="_blank" rel="external">https://dev.mysql.com/doc/refman/5.7/en/adding-users.html</a></p>
<p>10.创建数据库<br>create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;<br>create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</p>
<p>##安装cloudera manager<br>1.下载<br>地址：<a href="http://archive-primary.cloudera.com/cm5/cm/5/" target="_blank" rel="external">http://archive-primary.cloudera.com/cm5/cm/5/</a><br>（博主下载的是cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz 放在/soft/bigdata/clouderamanager下）<br>cd /soft/bigdata/clouderamanager<br>tar -xvf cloudera-manager-wheezy-cm5.10.0_amd64.tar.gz</p>
<p>测试：（可选）<br>cat /etc/passwd</p>
<h2 id="创建用户（所有节点）"><a href="#创建用户（所有节点）" class="headerlink" title="创建用户（所有节点）"></a>创建用户（所有节点）</h2><p>useradd –system –home=/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server –no-create-home –shell=/bin/false –comment “Cloudera SCM User” cloudera-scm</p>
<p>测试（可选）<br>[root@master1 cloudera-scm-server]# cat /etc/passwd<br>….<br>cloudera-scm:x:498:498:Cloudera SCM User:/soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-server:/bin/false</p>
<h2 id="修改主机名和端口号"><a href="#修改主机名和端口号" class="headerlink" title="修改主机名和端口号"></a>修改主机名和端口号</h2><p>cd /soft/bigdata/clouderamanager/cm-5.10.0/etc/cloudera-scm-agent<br>vi config.ini </p>
<h1 id="Hostname-of-the-CM-server"><a href="#Hostname-of-the-CM-server" class="headerlink" title="Hostname of the CM server."></a>Hostname of the CM server.</h1><p>server_host=po-master1</p>
<h1 id="Port-that-the-CM-server-is-listening-on"><a href="#Port-that-the-CM-server-is-listening-on" class="headerlink" title="Port that the CM server is listening on."></a>Port that the CM server is listening on.</h1><p>server_port=7182</p>
<h2 id="下载驱动包"><a href="#下载驱动包" class="headerlink" title="下载驱动包"></a>下载驱动包</h2><p>下载mysql-connector-java-*.jar（博主下载的mysql-connector-java-5.1.7-bin.jar）放到<br>/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/lib  目录下</p>
<h2 id="为Cloudera-Manager-5建立数据库："><a href="#为Cloudera-Manager-5建立数据库：" class="headerlink" title="为Cloudera Manager 5建立数据库："></a>为Cloudera Manager 5建立数据库：</h2><p>/soft/bigdata/clouderamanager/cm-5.10.0/share/cmf/schema/scm_prepare_database.sh mysql scm -hlocalhost -uroot -ppassword –scm-host localhost scm password scm<br>格式是:scm_prepare_database.sh 数据库类型  数据库 服务器 用户名 密码  –scm-host  Cloudera_Manager_Server所在的机器，后面那三个不知道代表什么，直接照抄官网的了。 </p>
<p>开启Cloudera Manager 5 Server端：</p>
<h2 id="向其他机器分发CDH"><a href="#向其他机器分发CDH" class="headerlink" title="向其他机器分发CDH"></a>向其他机器分发CDH</h2><p>scp -rp /soft/bigdata/clouderamanager root@po-master2:/soft/bigdata<br>scp -rp /soft/bigdata/clouderamanager root@po-slave1:/soft/bigdata<br>scp -rp /soft/bigdata/clouderamanager root@po-slave2:/soft/bigdata<br>scp -rp /soft/bigdata/clouderamanager root@po-slave3:/soft/bigdata</p>
<h2 id="准备Parcels，用以安装CDH5-（博主放在-soft-bigdata-clouderamanager-cloudera-parcel-repo，路径必须包含cloudera-parcel-repo）"><a href="#准备Parcels，用以安装CDH5-（博主放在-soft-bigdata-clouderamanager-cloudera-parcel-repo，路径必须包含cloudera-parcel-repo）" class="headerlink" title="准备Parcels，用以安装CDH5 （博主放在:/soft/bigdata/clouderamanager/cloudera/parcel-repo，路径必须包含cloudera/parcel-repo）"></a>准备Parcels，用以安装CDH5 （博主放在:/soft/bigdata/clouderamanager/cloudera/parcel-repo，路径必须包含cloudera/parcel-repo）</h2><p>官方地址：<br><a href="http://archive.cloudera.com/cdh5/parcels" target="_blank" rel="external">http://archive.cloudera.com/cdh5/parcels</a><br>博主选择的<br><a href="http://archive.cloudera.com/cdh5/parcels/latest/" target="_blank" rel="external">http://archive.cloudera.com/cdh5/parcels/latest/</a></p>
<p>需要下载以下两个文件<br>•    CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel<br>•    manifest.json</p>
<p>打开 manifest.json找到CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel的hash值里的内容<br>“hash”: “52f95da433f203a05c2fd33eb0f144e6a5c9d558”<br>echo ‘52f95da433f203a05c2fd33eb0f144e6a5c9d558’ &gt;&gt; CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha</p>
<p>测试（可选）<br>[root@master1 parcel-repo]# ll<br>total 1466572<br>-rw-r–r– 1 root root 1501694035 Mar  6 14:24 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel<br>-rw-r–r– 1 root root         41 Mar 20 15:26 CDH-5.10.0-1.cdh5.10.0.p0.41-el6.parcel.sha<br>-rw-r–r– 1 root root      64807 Mar 17 17:07 manifest.json</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server start（主节点启动）<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent start（所有节点上启动）  </p>
<p>测试<br>netstat -an | grep 7182<br>netstat -an | grep 7180 </p>
<h2 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h2><p><a href="http://po-master1:7180" target="_blank" rel="external">http://po-master1:7180</a><br>默认用户密码都是admin</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CDH%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%99%BB%E9%99%86%E9%A1%B5%E9%9D%A2.png" alt="此处输入图片的描述"><br>点击继续<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%862.png" alt="此处输入图片的描述"><br>选择免费的点击继续<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%863.png" alt="此处输入图片的描述"></p>
<p>勾选机器<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%864.png" alt="此处输入图片的描述"></p>
<p>点击更多选项修改parcel路径<br>/soft/bigdata/clouderamanager/cloudera/parcel-repo<br>插入图5<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%865.png" alt="此处输入图片的描述"></p>
<p>需要重启所有节点的服务<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-server restart（主节点启动）<br>/soft/bigdata/clouderamanager/cm-5.10.0/etc/init.d/cloudera-scm-agent restart（所有节点上启动） </p>
<p>选择如下内容点击继续<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%866.png" alt="此处输入图片的描述"></p>
<p>等待安装…<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%867.png" alt="此处输入图片的描述"></p>
<p>安装完成，点击继续</p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%868.png" alt="此处输入图片的描述"><br>安装过程有个小提示：<br>已启用透明大页面压缩，可能会导致重大性能问题。请运行“echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag”以禁用此设置，然后将同一命令添加到 /etc/rc.local 等初始脚本中，以便在系统重启时予以设置。以下主机将受到影响： </p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%869.png" alt="此处输入图片的描述"><br>选择自定义服务，选择自己需要的服务<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8610.png" alt="此处输入图片的描述"></p>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8611.png" alt="节点设置"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8612.png" alt="节点设置"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8613.png" alt="数据库设置设置"></p>
<p>等待安装<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8614.png" alt="等待安装"></p>
<p>安装过程中会遇到错误：</p>
<p>是缺少jdbc驱动把文件考入到lib下即可<br><img src="http://oh6ybr0jg.bkt.clouddn.com/CHD%E7%99%BB%E9%99%8615.png" alt="此处输入图片的描述"></p>
<p>配置NameNode HA<br>进入HDFS界面，点击“启用High Availability”<br><img src="http://oh6ybr0jg.bkt.clouddn.com/HDFS%E5%90%AF%E7%94%A8HA.png" alt="此处输入图片的描述"><br>输入NameService名称，这里设置为：nameservice1，点击继续按钮。<br><img src="http://oh6ybr0jg.bkt.clouddn.com/HDFS%E5%90%AF%E7%94%A8HA2.png" alt="此处输入图片的描述"><br>配置JourNode的路径，(博主修改为/opt/dfs/jn)</p>
<p>错误整理;<br>Fatal error during KafkaServer startup. Prepare to shutdown<br>kafka.common.InconsistentBrokerIdException: Configured broker.id 52 doesn’t match stored broker.id 102 in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs).<br>    at kafka.server.KafkaServer.getBrokerId(KafkaServer.scala:648)<br>    at kafka.server.KafkaServer.startup(KafkaServer.scala:187)<br>    at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:37)<br>    at kafka.Kafka$.main(Kafka.scala:67)<br>    at com.cloudera.kafka.wrap.Kafka$.main(Kafka.scala:76)<br>    at com.cloudera.kafka.wrap.Kafka.main(Kafka.scala)</p>
<p>进入到/var/local/kafka/data目录查看meta.propertie里面的kakfa 的broker id是什么</p>
<p>[main]: Metastore Thrift Server threw an exception…<br>javax.jdo.JDOFatalInternalException: Error creating transactional connection factory<br>    at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)<br>    at java.security.AccessController.doPrivileged(Native Method)<br>    at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)<br>    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291)<br>    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)<br>    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)<br>NestedThrowablesStackTrace:<br>java.lang.reflect.InvocationTargetException<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)<br>    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)<br>    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)<br>    at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)<br>    at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)<br>    at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)<br>    at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)<br>    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)<br>    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)<br>    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)<br>    at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)<br>    at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)<br>    at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)<br>    at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)<br>    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)<br>    at java.security.AccessController.doPrivileged(Native Method)<br>    at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)<br>    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)<br>    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:411)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:440)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:335)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291)<br>    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)<br>    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)<br>Caused by: org.datanucleus.exceptions.NucleusException: Attempt to invoke the “BONECP” plugin to create a ConnectionPool gave an error : The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:259)<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:131)<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:85)<br>    … 54 more<br>Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (“com.mysql.jdbc.Driver”) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.<br>    at org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:58)<br>    at org.datanucleus.store.rdbms.connectionpool.BoneCPConnectionPoolFactory.createConnectionPool(BoneCPConnectionPoolFactory.java:54)<br>    at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:238)<br>    … 56 more</init></init></init></init></init></init></init></p>
<p>把驱动程序放在<br>/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/lib</p>
<p>SERVER[po-master1] E0103: Could not load service classes, Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>    at org.apache.oozie.service.Services.loadServices(Services.java:309)<br>    at org.apache.oozie.service.Services.init(Services.java:213)<br>    at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)<br>    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236)<br>    at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739)<br>    at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803)<br>    at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780)<br>    at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)<br>    at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944)<br>    at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779)<br>    at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505)<br>    at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322)<br>    at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325)<br>    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069)<br>    at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061)<br>    at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)<br>    at org.apache.catalina.core.StandardService.start(StandardService.java:525)<br>    at org.apache.catalina.core.StandardServer.start(StandardServer.java:759)<br>    at org.apache.catalina.startup.Catalina.start(Catalina.java:595)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)<br>    at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)<br>Caused by: <openjpa-2.2.2-r422266:1468616 fatal="" general="" error=""> org.apache.openjpa.persistence.PersistenceException: Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>    at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:102)<br>    at org.apache.openjpa.jdbc.conf.JDBCConfigurationImpl.getDBDictionaryInstance(JDBCConfigurationImpl.java:603)<br>    at org.apache.openjpa.jdbc.meta.MappingRepository.endConfiguration(MappingRepository.java:1518)<br>    at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:531)<br>    at org.apache.openjpa.lib.conf.Configurations.configureInstance(Configurations.java:456)<br>    at org.apache.openjpa.lib.conf.PluginValue.instantiate(PluginValue.java:120)<br>    at org.apache.openjpa.conf.MetaDataRepositoryValue.instantiate(MetaDataRepositoryValue.java:68)<br>    at org.apache.openjpa.lib.conf.ObjectValue.instantiate(ObjectValue.java:83)<br>    at org.apache.openjpa.conf.OpenJPAConfigurationImpl.newMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:967)<br>    at org.apache.openjpa.conf.OpenJPAConfigurationImpl.getMetaDataRepositoryInstance(OpenJPAConfigurationImpl.java:958)<br>    at org.apache.openjpa.kernel.AbstractBrokerFactory.makeReadOnly(AbstractBrokerFactory.java:644)<br>    at org.apache.openjpa.kernel.AbstractBrokerFactory.newBroker(AbstractBrokerFactory.java:203)<br>    at org.apache.openjpa.kernel.DelegatingBrokerFactory.newBroker(DelegatingBrokerFactory.java:156)<br>    at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:227)<br>    at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:154)<br>    at org.apache.openjpa.persistence.EntityManagerFactoryImpl.createEntityManager(EntityManagerFactoryImpl.java:60)<br>    at org.apache.oozie.service.JPAService.getEntityManager(JPAService.java:514)<br>    at org.apache.oozie.service.JPAService.init(JPAService.java:215)<br>    at org.apache.oozie.service.Services.setServiceInternal(Services.java:386)<br>    at org.apache.oozie.service.Services.setService(Services.java:372)<br>    at org.apache.oozie.service.Services.loadServices(Services.java:305)<br>    … 26 more<br>Caused by: org.apache.commons.dbcp.SQLNestedException: Cannot load JDBC driver class ‘com.mysql.jdbc.Driver’<br>    at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1429)<br>    at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1371)<br>    at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044)<br>    at org.apache.openjpa.lib.jdbc.DelegatingDataSource.getConnection(DelegatingDataSource.java:110)<br>    at org.apache.openjpa.lib.jdbc.DecoratingDataSource.getConnection(DecoratingDataSource.java:87)<br>    at org.apache.openjpa.jdbc.sql.DBDictionaryFactory.newDBDictionary(DBDictionaryFactory.java:91)<br>    … 46 more<br>Caused by: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver<br>    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680)<br>    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526)<br>    at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1420)<br>    … 51 more</openjpa-2.2.2-r422266:1468616></p>
<p>把mysql-connector-java.jar，mysql-connector-java-5.1.39.jar驱动程序放在：<br>/var/lib/oozie</p>
<p>[main]: Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidates<br>Required table missing : “<code>DBS</code>“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables”<br>org.datanucleus.store.rdbms.exceptions.MissingTableException: Required table missing : “<code>DBS</code>“ in Catalog “” Schema “”. DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable “datanucleus.autoCreateTables”<br>    at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.java:485)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3380)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)<br>    at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)<br>    at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)<br>    at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)<br>    at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)<br>    at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)<br>    at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)<br>    at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)<br>    at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)<br>    at org.datanucleus.store.query.Query.execute(Query.java:1654)<br>    at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)<br>    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:185)<br>    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:136)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:340)<br>    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:291)<br>    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)<br>    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)<br>    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:648)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:626)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:679)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:484)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)<br>    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5989)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5984)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:6236)<br>    at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:6161)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)<br>    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</init></init></init></p>
<pre><code>SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table &apos;oozie.VALIDATE_CONN&apos; doesn&apos;t exist)
</code></pre><p>org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist)<br>    at org.apache.oozie.service.Services.loadServices(Services.java:309)<br>    at org.apache.oozie.service.Services.init(Services.java:213)<br>    at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)<br>    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236)<br>    at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739)<br>    at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803)<br>    at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780)<br>    at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)<br>    at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944)<br>    at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779)<br>    at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505)<br>    at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322)<br>    at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325)<br>    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069)<br>    at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061)<br>    at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)<br>    at org.apache.catalina.core.StandardService.start(StandardService.java:525)<br>    at org.apache.catalina.core.StandardServer.start(StandardServer.java:759)<br>    at org.apache.catalina.startup.Catalina.start(Catalina.java:595)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)<br>    at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)</p>
<p>报这个错误需要修改hive的配置。搜索autoCreateSchema 改为true</p>
<p>SERVER[po-master1] E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist)<br>org.apache.oozie.service.ServiceException: E0103: Could not load service classes, Cannot create PoolableConnectionFactory (Table ‘oozie.VALIDATE_CONN’ doesn’t exist)<br>    at org.apache.oozie.service.Services.loadServices(Services.java:309)<br>    at org.apache.oozie.service.Services.init(Services.java:213)<br>    at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:46)<br>    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4236)<br>    at org.apache.catalina.core.StandardContext.start(StandardContext.java:4739)<br>    at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803)<br>    at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780)<br>    at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)<br>    at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:944)<br>    at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:779)<br>    at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:505)<br>    at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322)<br>    at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325)<br>    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069)<br>    at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)<br>    at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061)<br>    at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)<br>    at org.apache.catalina.core.StandardService.start(StandardService.java:525)<br>    at org.apache.catalina.core.StandardServer.start(StandardServer.java:759)<br>    at org.apache.catalina.startup.Catalina.start(Catalina.java:595)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke(Method.java:498)<br>    at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)<br>    at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)</p>
<p>点击界面上的Oozie 点击操作，创建Oozie数据库表</p>
<p>最后导入环境变量就可以测试了<br>export ZK_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/<br>export HBASE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hbase/<br>export HADOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/<br>export HIVE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hive/<br>export SQOOP_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/sqoop/<br>export OOZIE_HOME=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/oozie/<br>export PATH=$PATH:$HOME/bin:$JAVA_HOME:$JAVA_HOME/bin:/usr/bin/:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$OOZIE_HOME/bin:$ZK_HOME/bin:$HBASE_HOME/bin</p>
<h2 id="最后测试阶段，可忽略，本文完。"><a href="#最后测试阶段，可忽略，本文完。" class="headerlink" title="最后测试阶段，可忽略，本文完。"></a>最后测试阶段，可忽略，本文完。</h2><h4 id="测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）："><a href="#测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）：" class="headerlink" title="测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）："></a>测试zookeeper：在po-slave1，po-slave2，po-slave3上执行（）：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-slave1 data]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: leader</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-slave2 data]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@po-slave3 dfs]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">[root@po-slave3 dfs]<span class="comment"># zkServer.sh status</span></div><div class="line">JMX enabled by default</div><div class="line">Using config: /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf/zoo.cfg</div><div class="line">Mode: follower</div><div class="line">[root@po-slave3 dfs]<span class="comment"># zkCli.sh </span></div><div class="line">Connecting to localhost:2181</div><div class="line">2017-03-21 16:05:56,829 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.10.0--1, built on 01/20/2017 20:10 GMT</div><div class="line">2017-03-21 16:05:56,832 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=po-slave3</div><div class="line">2017-03-21 16:05:56,832 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_121</div><div class="line">2017-03-21 16:05:56,834 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</div><div class="line">2017-03-21 16:05:56,834 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/soft/java/jdk/jre</div><div class="line">2017-03-21 16:05:56,834 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/classes:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../build/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/<span class="built_in">log</span>4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../lib/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../src/java/lib/*.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/bin/../conf:.:/soft/java/jdk/lib</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-642.13.1.el6.x86_64</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=root</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/root</div><div class="line">2017-03-21 16:05:56,835 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/opt/dfs</div><div class="line">2017-03-21 16:05:56,836 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain<span class="variable">$MyWatcher</span>@506c589e</div><div class="line">Welcome to ZooKeeper!</div><div class="line">2017-03-21 16:05:56,873 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn<span class="variable">$SendThread</span>@975] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)</div><div class="line">JLine support is enabled</div><div class="line">2017-03-21 16:05:56,935 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn<span class="variable">$SendThread</span>@852] - Socket connection established, initiating session, client: /127.0.0.1:42694, server: localhost/127.0.0.1:2181</div><div class="line">2017-03-21 16:05:56,941 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn<span class="variable">$SendThread</span>@1235] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15aeb7f0edb054c, negotiated timeout = 30000</div><div class="line"></div><div class="line">WATCHER::</div><div class="line"></div><div class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</div><div class="line">[zk: localhost:2181(CONNECTED) 0] ls /</div><div class="line">[controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, config, hbase]</div><div class="line">[zk: localhost:2181(CONNECTED) 1]</div></pre></td></tr></table></figure>
<h4 id="测试hdfs"><a href="#测试hdfs" class="headerlink" title="测试hdfs"></a>测试hdfs</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 ~]<span class="comment"># hadoop dfs -ls /</span></div><div class="line">DEPRECATED: Use of this script to execute hdfs <span class="built_in">command</span> is deprecated.</div><div class="line">Instead use the hdfs <span class="built_in">command</span> <span class="keyword">for</span> it.</div><div class="line"></div><div class="line">Found 3 items</div><div class="line">drwxr-xr-x   - hbase hbase               0 2017-03-21 10:30 /hbase</div><div class="line">drwxrwxrwt   - hdfs  supergroup          0 2017-03-20 17:06 /tmp</div><div class="line">drwxr-xr-x   - hdfs  supergroup          0 2017-03-20 17:06 /user</div><div class="line"></div><div class="line">[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -mkdir /data2</div><div class="line">[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -put hdfs-audit.log /data2/hdfs-audit.log</div><div class="line">[hdfs@po-master1 hadoop-hdfs]$ hadoop fs -ls /data2</div><div class="line">Found 1 items</div><div class="line">-rw-r--r--   3 hdfs supergroup    2908825 2017-03-21 17:28 /data2/hdfs-audit.log</div></pre></td></tr></table></figure>
<p>测试网页</p>
<h4 id="测试hadoop页面"><a href="#测试hadoop页面" class="headerlink" title="测试hadoop页面"></a>测试hadoop页面</h4><p><a href="http://po-master1:50030/jobtracker.jsp" target="_blank" rel="external">http://po-master1:50030/jobtracker.jsp</a><br><img src="http://oh6ybr0jg.bkt.clouddn.com/hadoop%E9%A1%B5%E9%9D%A2.png" alt="此处输入图片的描述"></p>
<h4 id="测试hive"><a href="#测试hive" class="headerlink" title="测试hive"></a>测试hive</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 ~]<span class="comment"># hive</span></div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed <span class="keyword">in</span> 8.0</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed <span class="keyword">in</span> a future release</div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed <span class="keyword">in</span> 8.0</div><div class="line"></div><div class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar!/hive-log4j.properties</div><div class="line">WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</div><div class="line">hive&gt; show databases;</div><div class="line">OK</div><div class="line">default</div><div class="line">Time taken: 1.836 seconds, Fetched: 1 row(s)</div><div class="line">hive&gt; create database <span class="built_in">test</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.06 seconds</div><div class="line">hive&gt; drop database <span class="built_in">test</span>;</div><div class="line">OK</div><div class="line">Time taken: 0.184 seconds</div></pre></td></tr></table></figure>
<h4 id="测试hbase"><a href="#测试hbase" class="headerlink" title="测试hbase"></a>测试hbase</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@po-master1 ~]<span class="comment"># hbase shell</span></div><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed <span class="keyword">in</span> a future release</div><div class="line">17/03/21 16:20:23 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available</div><div class="line">HBase Shell; enter <span class="string">'help&lt;RETURN&gt;'</span> <span class="keyword">for</span> list of supported commands.</div><div class="line">Type <span class="string">"exit&lt;RETURN&gt;"</span> to leave the HBase Shell</div><div class="line">Version 1.2.0-cdh5.10.0, rUnknown, Fri Jan 20 12:13:18 PST 2017</div><div class="line">hbase(main):001:0&gt; list</div><div class="line">TABLE                                                                                                           0 row(s) <span class="keyword">in</span> 0.2020 seconds</div><div class="line">=&gt; []</div><div class="line">hbase(main):002:0&gt; create <span class="string">'t1'</span>,<span class="string">'id'</span>,<span class="string">'name'</span></div><div class="line">0 row(s) <span class="keyword">in</span> 2.3540 seconds</div><div class="line">=&gt; Hbase::Table - t1</div><div class="line">hbase(main):003:0&gt; list</div><div class="line">TABLE                                                                                                           t1                                                                                                              1 row(s) <span class="keyword">in</span> 0.0100 seconds</div><div class="line">=&gt; [<span class="string">"t1"</span>]</div><div class="line">hbase(main):004:0&gt;</div></pre></td></tr></table></figure>
<p>卸载安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">umount /soft/bigdata/clouderamanager/cm-5.10.0/run/cloudera-scm-agent/process</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;运行环境：&quot;&gt;&lt;a href=&quot;#运行环境：&quot; class=&quot;headerlink&quot; title=&quot;运行环境：&quot;&gt;&lt;/a&gt;运行环境：&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机IP&lt;/th&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;内存&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.147&lt;/td&gt;
&lt;td&gt;po-master1&lt;/td&gt;
&lt;td&gt;16G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.127&lt;/td&gt;
&lt;td&gt;po-master2&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.118&lt;/td&gt;
&lt;td&gt;po-slave1&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.92&lt;/td&gt;
&lt;td&gt;po-slave2&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.1.1.230&lt;/td&gt;
&lt;td&gt;po-slave3&lt;/td&gt;
&lt;td&gt;8G&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&quot;配置主机名-分别在五台机器上执行&quot;&gt;&lt;a href=&quot;#配置主机名-分别在五台机器上执行&quot; class=&quot;headerlink&quot; title=&quot;配置主机名(分别在五台机器上执行)&quot;&gt;&lt;/a&gt;配置主机名(分别在五台机器上执行)&lt;/h2&gt;&lt;p&gt;vi /etc/sysconfig/network&lt;br&gt;hostname +主机名&lt;br&gt;例如： hostname po-master1&lt;/p&gt;
&lt;h2 id=&quot;配置映射关系-把以下五条命令在五台机器上执行&quot;&gt;&lt;a href=&quot;#配置映射关系-把以下五条命令在五台机器上执行&quot; class=&quot;headerlink&quot; title=&quot;配置映射关系(把以下五条命令在五台机器上执行)&quot;&gt;&lt;/a&gt;配置映射关系(把以下五条命令在五台机器上执行)&lt;/h2&gt;&lt;p&gt;echo “1.1.1.147   po-master1”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.127   po-master2”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.118  po-slave1”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.92   po-slave2”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;echo “1.1.1.230  po-slave3”&amp;gt;&amp;gt;/etc/hosts&lt;br&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="CDH" scheme="http://yoursite.com/tags/CDH/"/>
    
  </entry>
  
  <entry>
    <title>CDH Kudu安装及介绍</title>
    <link href="http://yoursite.com/2017/03/01/CDH%20Kudu%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2017/03/01/CDH Kudu安装及介绍/</id>
    <published>2017-03-01T06:00:00.000Z</published>
    <updated>2017-04-04T15:14:40.917Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装Kudu的要求"><a href="#安装Kudu的要求" class="headerlink" title="安装Kudu的要求"></a>安装Kudu的要求</h2><p>1.操作系统和版本支持Cloudera。<br>2.通过Cloudera Manager管理Kudu，要求Cloudera Manager5.4.3或更改的版本。CDH 5.4或更高版本的要求。推荐Cloudera Manager5.4.7，因为它增加了Kudu采集的指标支持。<br>3.如果固态存储是可用的，在这种高性能的媒体存储Kudu WALs可以显著改善时的Kudu配置高耐用性。</p>
<h2 id="通过Cloudera-Manager安装Kudu"><a href="#通过Cloudera-Manager安装Kudu" class="headerlink" title="通过Cloudera Manager安装Kudu"></a>通过Cloudera Manager安装Kudu</h2><p>想要通过Cloudera Manager安装Kudu，首先下载Kudu的定制服务描述符（CSD）文件<br><a href="http://archive.cloudera.com/kudu/csd/KUDU-5.10.0.jar?_ga=1.154711740.507225699.1488251539" target="_blank" rel="external">http://archive.cloudera.com/kudu/csd/KUDU-5.10.0.jar?_ga=1.154711740.507225699.1488251539</a><br>并上传到/opt/cloudera/csd/<br>用以下操作系统命令重启Cloudera管理服务器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo service cloudera-scm-server restart</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>从<a href="http://archive.cloudera.com/kudu/parcels/网站找到相应的Kudu" target="_blank" rel="external">http://archive.cloudera.com/kudu/parcels/网站找到相应的Kudu</a> parcel，<br><img src="http://oh6ybr0jg.bkt.clouddn.com/kudu_version.png" alt="此处输入图片的描述"></p>
<p>并添加到 Parcel Settings &gt; 远程 Parcel 存储库 URL<br><img src="http://oh6ybr0jg.bkt.clouddn.com/parcel_kudu.png" alt="此处输入图片的描述"></p>
<h2 id="接下来可以通过以下两种方式安装"><a href="#接下来可以通过以下两种方式安装" class="headerlink" title="接下来可以通过以下两种方式安装"></a>接下来可以通过以下两种方式安装</h2><h3 id="Using-Parcels"><a href="#Using-Parcels" class="headerlink" title="Using Parcels"></a>Using Parcels</h3><p>1.Hosts &gt; Parcels &gt; Kudu &gt; Download<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E4%B8%8B%E8%BD%BDKudu.png" alt="此处输入图片的描述"><br>2.Locations &gt; Distribute &gt; Activate<br>3.重启集群<br>4.Actions &gt; Add a Service. &gt; Kudu (Beta) &gt; Continue<br>5.选择一个主机作为master，一些主机作为tablet 服务角色。一个主机即可以是master又可以是tablet，但是对于大集群来说会造成性能问题。Kudu的master不是一个资源密集型的，被其他相似的处理例如：HDFS的namenode，YARN的ResourceManager收集。选择完主机点击Continue<br>6.在masters 和 tablet配置Kudu的存储位置和预写日志（WAL）文件，Cloudera Manager将会创建文件夹。<br>6.1 你可以使用相同的目录存储数据和WALs<br>6.2 你不可以将WALs 目录设置为数据的子目录<br>6.3 如果你的主机及是master又是tablet，配置不同的master和tablet服务，例如 /data/kudu/master and /data/kudu/tserver.</p>
<p>6.4 如果你选择的文件系统不支持打洞技术;服务启动失败<br>6.4.1 退出配置向导，点击Cloudera Manager 接口上的Cloudera 图标<br>6.4.2 到 Kudu (Beta) 服务<br>6.4.3 Configuration &gt;  Kudu (Beta) Service Advanced Configuration Snippet (Safety Valve) &gt; gflagfile<br>5.4.4 添加如下内容并保存改变<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--block_manager=file</div></pre></td></tr></table></figure></p>
<p>Note: The file block manager does not perform well at scale and should only be used for small-scale development and testing.</p>
<p>7.如果你不需要退出，点击Continue。Kudu的master和tablet服务已经启动。否则点击<br> Kudu (Beta) &gt; Actions &gt; Start.<br>8.使用其中一种方法验证服务：<br>8.1 通过ps命令验证一个或全部kudu-master或kudu-tserver程序是否运行<br>8.2 通过打开Web浏览器中的URL访问master或者tablet。<br>master的URL： http://<_host_name_>:8051/<br>tablet的URL： http://<_host_name_>:8050/<br>9.重启监控服务并检查Kudu的图表，到Cloudera Manager服务点击Service Monitor &gt;  Actions &gt; Restart<br>10.管理角色。Kudu (Beta) 服务 使用 Actions 来停止，启动，重启或者其他管理服务</_host_name_></_host_name_></p>
<h3 id="Using-Packages"><a href="#Using-Packages" class="headerlink" title="Using Packages"></a>Using Packages</h3><p>Kudu 仓库 和 Package 链接</p>
<table>
<thead>
<tr>
<th>Operating System</th>
<th style="text-align:left">Repository Package</th>
<th style="text-align:left">Individual Packages</th>
</tr>
</thead>
<tbody>
<tr>
<td>RHEL</td>
<td style="text-align:left">RHEL 6</td>
<td style="text-align:left">RHEL 6</td>
</tr>
<tr>
<td>Ubuntu</td>
<td style="text-align:left">Trusty</td>
<td style="text-align:left">Trusty</td>
</tr>
</tbody>
</table>
<p>1.1 下载Kudu的yum源文件到：RHEL（/etc/yum.repos.d/）  或者 Ubuntu（/etc/apt/sources.list.d/）<br>1.2 如果你需要C++客户端开发库或Kudu的SDK，RHEL kudu-client-devel 包 或 Ubuntu的 libkuduclient0 and libkuduclient-dev 包<br>1.3 如果你使用Cloudera Manager，不安装 kudu-master 和 kudu-tserver包，Cloudera Manager使用Kudu 提供操作系统启动脚本。</p>
<p>2.群集上安装Kudu服务。去你想安装Kudu所在的集群。单击 Actions &gt; Add a Service &gt; Kudu &gt; Continue。</p>
<p>3.选择一个主机作为master，一些主机作为tablet 服务角色。一个主机即可以是master又可以是tablet，但是对于大集群来说会造成性能问题。Kudu的master不是一个资源密集型的，被其他相似的处理例如：HDFS的namenode，YARN的ResourceManager收集。选择完主机点击Continue<br>4.在masters 和 tablet配置Kudu的存储位置和预写日志（WAL）文件，Cloudera Manager将会创建文件夹。<br>4.1 你可以使用相同的目录存储数据和WALs<br>4.2 你不可以将WALs 目录设置为数据的子目录<br>5.3 如果你的主机及是master又是tablet，配置不同的master和tablet服务，例如 /data/kudu/master and /data/kudu/tserver.</p>
<p>5 如果你选择的文件系统不支持打洞技术;服务启动失败<br>点击 Continue &gt;  Kudu masters 和 tablet 启动。否则到Kudu的服务上点击 Actions &gt; Start.</p>
<p>6.使用其中一种方法验证服务：<br>6.1 通过ps命令验证一个或全部kudu-master或kudu-tserver程序是否运行<br>6.2 通过打开Web浏览器中的URL访问master或者tablet。<br>master的URL： http://<_host_name_>:8051/<br>tablet的URL： http://<_host_name_>:8050/ </_host_name_></_host_name_></p>
<p>7.重启监控服务并检查Kudu的图表，到Cloudera Manager服务点击Service Monitor &gt;  Actions &gt; Restart<br>8.管理角色。Kudu 服务 使用 Actions 来停止，启动，重启或者其他管理服务</p>
<p>此乃官方推荐，方法1，博主亲试没有成功，安装完不显示服务</p>
<p>本文参考页面：<br><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/kudu_install_cm.html" target="_blank" rel="external">https://www.cloudera.com/documentation/enterprise/latest/topics/kudu_install_cm.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装Kudu的要求&quot;&gt;&lt;a href=&quot;#安装Kudu的要求&quot; class=&quot;headerlink&quot; title=&quot;安装Kudu的要求&quot;&gt;&lt;/a&gt;安装Kudu的要求&lt;/h2&gt;&lt;p&gt;1.操作系统和版本支持Cloudera。&lt;br&gt;2.通过Cloudera Manager管理Kudu，要求Cloudera Manager5.4.3或更改的版本。CDH 5.4或更高版本的要求。推荐Cloudera Manager5.4.7，因为它增加了Kudu采集的指标支持。&lt;br&gt;3.如果固态存储是可用的，在这种高性能的媒体存储Kudu WALs可以显著改善时的Kudu配置高耐用性。&lt;/p&gt;
&lt;h2 id=&quot;通过Cloudera-Manager安装Kudu&quot;&gt;&lt;a href=&quot;#通过Cloudera-Manager安装Kudu&quot; class=&quot;headerlink&quot; title=&quot;通过Cloudera Manager安装Kudu&quot;&gt;&lt;/a&gt;通过Cloudera Manager安装Kudu&lt;/h2&gt;&lt;p&gt;想要通过Cloudera Manager安装Kudu，首先下载Kudu的定制服务描述符（CSD）文件&lt;br&gt;&lt;a href=&quot;http://archive.cloudera.com/kudu/csd/KUDU-5.10.0.jar?_ga=1.154711740.507225699.1488251539&quot;&gt;http://archive.cloudera.com/kudu/csd/KUDU-5.10.0.jar?_ga=1.154711740.507225699.1488251539&lt;/a&gt;&lt;br&gt;并上传到/opt/cloudera/csd/&lt;br&gt;用以下操作系统命令重启Cloudera管理服务器。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;$ sudo service cloudera-scm-server restart&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="cloudera" scheme="http://yoursite.com/categories/cloudera/"/>
    
    
      <category term="cloudera" scheme="http://yoursite.com/tags/cloudera/"/>
    
  </entry>
  
  <entry>
    <title>canal分析binlog</title>
    <link href="http://yoursite.com/2017/02/23/canal%E5%88%86%E6%9E%90binlog/"/>
    <id>http://yoursite.com/2017/02/23/canal分析binlog/</id>
    <published>2017-02-23T06:00:00.000Z</published>
    <updated>2017-04-15T13:49:47.021Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><p>linux服务器：centos7.1<br>mysql : 5.7.10<br>canal : 1.0.23</p>
<h2 id="一-centos7下安装mysql"><a href="#一-centos7下安装mysql" class="headerlink" title="一.centos7下安装mysql"></a>一.centos7下安装mysql</h2><h4 id="1-下载mysql的repo源"><a href="#1-下载mysql的repo源" class="headerlink" title="1.下载mysql的repo源"></a>1.下载mysql的repo源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</div></pre></td></tr></table></figure>
<h4 id="2-安装mysql-community-release-el7-5-noarch-rpm包"><a href="#2-安装mysql-community-release-el7-5-noarch-rpm包" class="headerlink" title="2.安装mysql-community-release-el7-5.noarch.rpm包"></a>2.安装mysql-community-release-el7-5.noarch.rpm包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -ivh mysql-community-release-el7-5.noarch.rpm</div></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="3-安装mysql"><a href="#3-安装mysql" class="headerlink" title="3.安装mysql"></a>3.安装mysql</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install mysql-server</div></pre></td></tr></table></figure>
<h4 id="4-设置用户"><a href="#4-设置用户" class="headerlink" title="4.设置用户"></a>4.设置用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">CREATE USER &apos;canal&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;canal&apos;;</div><div class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;localhost&apos; WITH GRANT OPTION;</div><div class="line">CREATE USER &apos;canal&apos;@&apos;%&apos; IDENTIFIED BY &apos;canal&apos;;</div><div class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos; WITH GRANT OPTION;</div><div class="line">flush privileges;</div></pre></td></tr></table></figure>
<p>注：canal的原理是模拟自己为mysql slave，所以这里一定需要做为mysql slave的相关权限.<br>CREATE USER canal IDENTIFIED BY ‘canal’;<br>GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON <em>.</em> TO ‘canal’@’%’;<br>– GRANT ALL PRIVILEGES ON <em>.</em> TO ‘canal’@’%’ ;<br>FLUSH PRIVILEGES;</p>
<h4 id="5-修改配置文件"><a href="#5-修改配置文件" class="headerlink" title="5.修改配置文件"></a>5.修改配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[mysqld]</div><div class="line">log-bin=mysql-bin #添加这一行就ok</div><div class="line">binlog-format=ROW #选择row模式</div><div class="line">server_id=1 #配置mysql replaction需要定义，不能和canal的slaveId重复</div></pre></td></tr></table></figure>
<h2 id="二-安装canal"><a href="#二-安装canal" class="headerlink" title="二.安装canal"></a>二.安装canal</h2><h4 id="1-下载canal"><a href="#1-下载canal" class="headerlink" title="1.下载canal"></a>1.下载canal</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget https://github.com/alibaba/canal/releases/download/v1.0.23/canal.deployer-1.0.23.tar.gz</div></pre></td></tr></table></figure>
<h4 id="2-解压缩"><a href="#2-解压缩" class="headerlink" title="2.解压缩"></a>2.解压缩</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mkdir /root/canal</div><div class="line">tar zxvf canal.deployer-1.0.23.tar.gz  -C /root/canal</div></pre></td></tr></table></figure>
<h4 id="3-修改配置文件-如果是访问本机，并且用户密码都为canal则不需要修改配置文件"><a href="#3-修改配置文件-如果是访问本机，并且用户密码都为canal则不需要修改配置文件" class="headerlink" title="3.修改配置文件(如果是访问本机，并且用户密码都为canal则不需要修改配置文件)"></a>3.修改配置文件(如果是访问本机，并且用户密码都为canal则不需要修改配置文件)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vi /root/canal/conf/example/instance.properties</div></pre></td></tr></table></figure>
<h4 id="4-启动"><a href="#4-启动" class="headerlink" title="4.启动"></a>4.启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sh /root/canal/bin/startup.sh</div></pre></td></tr></table></figure>
<h4 id="5-查看日志"><a href="#5-查看日志" class="headerlink" title="5.查看日志"></a>5.查看日志</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 ~]# cat /root/canal/logs/canal/canal.log</div><div class="line">OpenJDK 64-Bit Server VM warning: ignoring option PermSize=96m; support was removed in 8.0</div><div class="line">OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0</div><div class="line">OpenJDK 64-Bit Server VM warning: UseCMSCompactAtFullCollection is deprecated and will likely be removed in a future release.</div><div class="line">2017-02-22 17:40:08.901 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.</div><div class="line">2017-02-22 17:40:09.069 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.118.128:11111]</div><div class="line">2017-02-22 17:40:09.758 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ......</div></pre></td></tr></table></figure>
<p>具体instance的日志：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@zhm1 ~]# cat /root/canal/logs/example/example.log</div></pre></td></tr></table></figure></p>
<h4 id="6-关闭"><a href="#6-关闭" class="headerlink" title="6.关闭"></a>6.关闭</h4><p>sh /root/canal/bin/stop.sh</p>
<h2 id="三-写客户端代码"><a href="#三-写客户端代码" class="headerlink" title="三.写客户端代码"></a>三.写客户端代码</h2><h4 id="1-在maven的setting-xml加入阿里的镜像"><a href="#1-在maven的setting-xml加入阿里的镜像" class="headerlink" title="1.在maven的setting.xml加入阿里的镜像"></a>1.在maven的setting.xml加入阿里的镜像</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&lt;mirror&gt;</div><div class="line">	&lt;id&gt;alimaven&lt;/id&gt;</div><div class="line">	&lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</div><div class="line">	&lt;name&gt;aliyun maven&lt;/name&gt;</div><div class="line">	&lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;</div><div class="line">&lt;/mirror&gt;</div></pre></td></tr></table></figure>
<h4 id="2-创建初始项目"><a href="#2-创建初始项目" class="headerlink" title="2.创建初始项目"></a>2.创建初始项目</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mvn archetype:generate -DgroupId=com.alibaba.otter -DartifactId=canal.sample</div></pre></td></tr></table></figure>
<h4 id="3-pom文件增加"><a href="#3-pom文件增加" class="headerlink" title="3.pom文件增加"></a>3.pom文件增加</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;canal.client&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.0.12&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure>
<h4 id="4-ClientSample代码"><a href="#4-ClientSample代码" class="headerlink" title="4. ClientSample代码"></a>4. ClientSample代码</h4><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line">package com.alibaba.otter;</div><div class="line"></div><div class="line">/**</div><div class="line"> * Created by Administrator on 2017/2/23.</div><div class="line"> */</div><div class="line">import java.net.InetSocketAddress;</div><div class="line">import java.util.List;</div><div class="line"></div><div class="line">import com.alibaba.otter.canal.client.CanalConnector;</div><div class="line">import com.alibaba.otter.canal.client.CanalConnectors;</div><div class="line">import com.alibaba.otter.canal.common.utils.AddressUtils;</div><div class="line">import com.alibaba.otter.canal.protocol.Message;</div><div class="line">import com.alibaba.otter.canal.protocol.CanalEntry.Column;</div><div class="line">import com.alibaba.otter.canal.protocol.CanalEntry.Entry;</div><div class="line">import com.alibaba.otter.canal.protocol.CanalEntry.EntryType;</div><div class="line">import com.alibaba.otter.canal.protocol.CanalEntry.EventType;</div><div class="line">import com.alibaba.otter.canal.protocol.CanalEntry.RowChange;</div><div class="line">import com.alibaba.otter.canal.protocol.CanalEntry.RowData;</div><div class="line"></div><div class="line">public class  ClientSample &#123;</div><div class="line"></div><div class="line">    public static void main(String args[]) &#123;</div><div class="line">        // 创建链接</div><div class="line">        CanalConnector connector = CanalConnectors.newSingleConnector(</div><div class="line">            new InetSocketAddress("192.168.118.128",//AddressUtils.getHostIp(),</div><div class="line">                11111), "example", "", "");</div><div class="line">        int batchSize = 1000;</div><div class="line">        int emptyCount = 0;</div><div class="line">        try &#123;</div><div class="line">            connector.connect();</div><div class="line">            connector.subscribe(".*\\..*");</div><div class="line">            connector.rollback();</div><div class="line">            int totalEmptyCount = 120;</div><div class="line">            while (emptyCount &lt; totalEmptyCount) &#123;</div><div class="line">                Message message = connector.getWithoutAck(batchSize);                            // 获取指定数量的数据</div><div class="line">                long batchId = message.getId();</div><div class="line">                int size = message.getEntries().size();</div><div class="line">                if (batchId == -1 || size == 0) &#123;</div><div class="line">                    emptyCount++;</div><div class="line">                    System.out.println("empty count : " + emptyCount);</div><div class="line">                    try &#123;</div><div class="line">                        Thread.sleep(1000);</div><div class="line">                    &#125; catch (InterruptedException e) &#123;</div><div class="line">                    &#125;</div><div class="line">                &#125; else &#123;</div><div class="line">                    emptyCount = 0;</div><div class="line">                    // System.out.printf("message[batchId=%s,size=%s] \n", batchId, size);</div><div class="line">                    printEntry(message.getEntries());</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                connector.ack(batchId); // 提交确认</div><div class="line">                // connector.rollback(batchId); // 处理失败, 回滚数据</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            System.out.println("empty too many times, exit");</div><div class="line">        &#125; finally &#123;</div><div class="line">            connector.disconnect();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    private static void printEntry(List&lt;Entry&gt; entrys) &#123;</div><div class="line">        for (Entry entry : entrys) &#123;</div><div class="line">            if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || </div><div class="line">                entry.getEntryType() == EntryType.TRANSACTIONEND) &#123;</div><div class="line">                continue;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            RowChange rowChage = null;</div><div class="line">            try &#123;</div><div class="line">                rowChage = RowChange.parseFrom(entry.getStoreValue());</div><div class="line">            &#125; catch (Exception e) &#123;</div><div class="line">                throw new RuntimeException("ERROR ## parser of eromanga-event has an error,</div><div class="line">                                            data:" + entry.toString(),e);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            EventType eventType = rowChage.getEventType();</div><div class="line">            System.out.println(String.format("================&gt; binlog[%s:%s] ,</div><div class="line">                                name[%s,%s] , eventType : %s",</div><div class="line">                    entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(),</div><div class="line">                    entry.getHeader().getSchemaName(), entry.getHeader().getTableName(),</div><div class="line">                    eventType));</div><div class="line"></div><div class="line">            for (RowData rowData : rowChage.getRowDatasList()) &#123;</div><div class="line">                if (eventType == EventType.DELETE) &#123;</div><div class="line">                    printColumn(rowData.getBeforeColumnsList());</div><div class="line">                &#125; else if (eventType == EventType.INSERT) &#123;</div><div class="line">                    printColumn(rowData.getAfterColumnsList());</div><div class="line">                &#125; else &#123;</div><div class="line">                    System.out.println("-------&gt; before");</div><div class="line">                    printColumn(rowData.getBeforeColumnsList());</div><div class="line">                    System.out.println("-------&gt; after");</div><div class="line">                    printColumn(rowData.getAfterColumnsList());</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    private static void printColumn(List&lt;Column&gt; columns) &#123;</div><div class="line">        for (Column column : columns) &#123;</div><div class="line">            System.out.println(column.getName() + " : " + column.getValue() + "    update=" +                                       column.getUpdated());</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="5-mysql下执行操作"><a href="#5-mysql下执行操作" class="headerlink" title="5. mysql下执行操作"></a>5. mysql下执行操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">mysql&gt; use test;</div><div class="line">Database changed</div><div class="line">mysql&gt; CREATE TABLE `xdual` (</div><div class="line">    -&gt;   `ID` int(11) NOT NULL AUTO_INCREMENT,</div><div class="line">    -&gt;   `X` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,</div><div class="line">    -&gt;   PRIMARY KEY (`ID`)</div><div class="line">    -&gt; ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ;</div><div class="line">Query OK, 0 rows affected (0.06 sec)</div><div class="line"></div><div class="line">mysql&gt; insert into xdual(id,x) values(4,now());</div><div class="line">Query OK, 1 row affected (0.06 sec)</div></pre></td></tr></table></figure>
<p>可以从控制台中看到：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">empty count : 1</div><div class="line">empty count : 2</div><div class="line">empty count : 3</div><div class="line">empty count : 4</div><div class="line">================&gt; binlog[mysql-bin.001946:313661577] , name[test,xdual] , eventType : INSERT</div><div class="line">ID : 4    update=true</div><div class="line">X : 2017-02-23 14:20:00    update=true</div></pre></td></tr></table></figure></p>
<h2 id="四-canal集群搭建"><a href="#四-canal集群搭建" class="headerlink" title="四.canal集群搭建"></a>四.canal集群搭建</h2><h4 id="1-安装zookeeper"><a href="#1-安装zookeeper" class="headerlink" title="1.安装zookeeper"></a>1.安装zookeeper</h4><p>略</p>
<h4 id="2-修改配置文件"><a href="#2-修改配置文件" class="headerlink" title="2.修改配置文件"></a>2.修改配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">vi /root/canal/conf/canal.properties</div><div class="line">canal.zkServers=1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181</div><div class="line">canal.instance.global.spring.xml = classpath:spring/default-instance.xml</div><div class="line"></div><div class="line">vi /root/canal/conf/example/instance.properties</div><div class="line">canal.instance.mysql.slaveId = 1234  另外一个机器改成1235与1234不同即可</div></pre></td></tr></table></figure>
<p>分别在两台机器上启动，发现只有一台机器logs下面有example目录，并且显示启动成功</p>
<h4 id="3-进入到zkClient查看状态"><a href="#3-进入到zkClient查看状态" class="headerlink" title="3.进入到zkClient查看状态"></a>3.进入到zkClient查看状态</h4><h5 id="获取正在运行的canal-server"><a href="#获取正在运行的canal-server" class="headerlink" title="获取正在运行的canal server"></a>获取正在运行的canal server</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">get /otter/canal/destinations/example/running</div></pre></td></tr></table></figure>
<h5 id="获取正在连接的canal-client"><a href="#获取正在连接的canal-client" class="headerlink" title="获取正在连接的canal client"></a>获取正在连接的canal client</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">get /otter/canal/destinations/example/1001/running</div></pre></td></tr></table></figure>
<h5 id="获取当前最后一次消费车成功的binlog"><a href="#获取当前最后一次消费车成功的binlog" class="headerlink" title="获取当前最后一次消费车成功的binlog"></a>获取当前最后一次消费车成功的binlog</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">get /otter/canal/destinations/example/1001/cursor</div></pre></td></tr></table></figure>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/zookeeper_canal.png" alt="此处输入图片的描述"></p>
<h4 id="4-客户端代码"><a href="#4-客户端代码" class="headerlink" title="4.客户端代码"></a>4.客户端代码</h4><p>修改如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CanalConnector connector = CanalConnectors.newClusterConnector(<span class="string">"1.1.1.1:2181,1.1.1.2:2181,1.1.1.3:2181"</span>, <span class="string">"example"</span>, <span class="string">""</span>, <span class="string">""</span>);</div></pre></td></tr></table></figure></p>
<p>代码启动<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8.png" alt="此处输入图片的描述"></p>
<p>停止一个canal<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%81%9C%E6%AD%A2%E4%B8%80%E4%B8%AAcanal.png" alt="此处输入图片的描述"></p>
<p>全部停止canal<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%85%A8%E9%83%A8%E5%81%9C%E6%AD%A2.png" alt="此处输入图片的描述"></p>
<p> 查看当前canal消费到哪个position<br>[zk: localhost:2181(CONNECTED) 15] get /otter/canal/destinations/example/1001/cursor<br>{“@type”:”com.alibaba.otter.canal.protocol.position.LogPosition”,”identity”:{“slaveId”:-1,”sourceAddress”:{“address”:”po-master1”,”port”:3306}},”postion”:{“included”:false,”journalName”:”logbin.000004”,”position”:6897322,”serverId”:1,”timestamp”:1492065268000}}</p>
<p>如果提示：<br>找不到日志的位置<br>在mysql下执行(可能是设置完格式，格式不对)<br>set global binlog_checksum=’NONE’ </p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;环境介绍&quot;&gt;&lt;a href=&quot;#环境介绍&quot; class=&quot;headerlink&quot; title=&quot;环境介绍&quot;&gt;&lt;/a&gt;环境介绍&lt;/h2&gt;&lt;p&gt;linux服务器：centos7.1&lt;br&gt;mysql : 5.7.10&lt;br&gt;canal : 1.0.23&lt;/p&gt;
&lt;h2 id=&quot;一-centos7下安装mysql&quot;&gt;&lt;a href=&quot;#一-centos7下安装mysql&quot; class=&quot;headerlink&quot; title=&quot;一.centos7下安装mysql&quot;&gt;&lt;/a&gt;一.centos7下安装mysql&lt;/h2&gt;&lt;h4 id=&quot;1-下载mysql的repo源&quot;&gt;&lt;a href=&quot;#1-下载mysql的repo源&quot; class=&quot;headerlink&quot; title=&quot;1.下载mysql的repo源&quot;&gt;&lt;/a&gt;1.下载mysql的repo源&lt;/h4&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;2-安装mysql-community-release-el7-5-noarch-rpm包&quot;&gt;&lt;a href=&quot;#2-安装mysql-community-release-el7-5-noarch-rpm包&quot; class=&quot;headerlink&quot; title=&quot;2.安装mysql-community-release-el7-5.noarch.rpm包&quot;&gt;&lt;/a&gt;2.安装mysql-community-release-el7-5.noarch.rpm包&lt;/h4&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;rpm -ivh mysql-community-release-el7-5.noarch.rpm&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="canal" scheme="http://yoursite.com/categories/canal/"/>
    
    
      <category term="canal" scheme="http://yoursite.com/tags/canal/"/>
    
  </entry>
  
  <entry>
    <title>storm1.0.3集群部署</title>
    <link href="http://yoursite.com/2017/02/15/storm1.0.3%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2017/02/15/storm1.0.3集群部署/</id>
    <published>2017-02-15T12:00:00.000Z</published>
    <updated>2017-03-04T13:08:06.124Z</updated>
    
    <content type="html"><![CDATA[<p>昨日已经发布storm1.0.3版本，来玩一玩</p>
<h2 id="环境："><a href="#环境：" class="headerlink" title="环境："></a>环境：</h2><p>zookeeper3.4.9节点：storm01，storm02，storm03<br>storm1.0.3 nimbus节点storm01，supervisor节点storm02，storm03</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[storm@localhost app]$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz</div><div class="line">[storm@localhost app]$ tar zxvf zookeeper-3.4.9.tar.gz</div><div class="line">[storm@localhost app]$ ln -s -f zookeeper-3.4.9 zookeeper</div><div class="line">[storm@localhost app]$ mkdir zookeeper/data</div><div class="line">[storm@localhost app]$ cd zookeeper/conf/</div><div class="line">[storm@localhost conf]$ cp zoo_sample.cfg zoo.cfg</div><div class="line">[storm@localhost conf]$ vi zoo.cfg</div></pre></td></tr></table></figure>
<p>修改dataDir<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">dataDir=/home/storm/app/zookeeper/data</div><div class="line">server.1=storm01:2888:3888</div><div class="line">server.2=storm02:2888:3888</div><div class="line">server.3=storm03:2888:3888</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>在storm01下执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[storm@storm01 conf]# cd /home/storm/app/zookeeper/data</div><div class="line">[storm@storm01 data]# echo 1 &gt; myid</div></pre></td></tr></table></figure></p>
<p>在storm02下执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[storm@storm01 conf]# cd /home/storm/app/zookeeper/data</div><div class="line">[storm@storm01 data]# echo 2 &gt; myid</div></pre></td></tr></table></figure></p>
<p>在storm03下执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[storm@storm01 conf]# cd /home/storm/app/zookeeper/data</div><div class="line">[storm@storm01 data]# echo 3 &gt; myid</div></pre></td></tr></table></figure></p>
<h2 id="部署storm"><a href="#部署storm" class="headerlink" title="部署storm"></a>部署storm</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[storm@localhost app]$ wget https://mirrors.tuna.tsinghua.edu.cn/apache/storm/apache-storm-1.0.3/apache-storm-1.0.3.tar.gz</div><div class="line">[storm@localhost app]$ tar zxvf apache-storm-1.0.3.tar.gz</div><div class="line">[storm@localhost app]$ ln -s -f apache-storm-1.0.3 storm</div></pre></td></tr></table></figure>
<p>修改配置文件，jdk安装此处略<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[storm@localhost app]$ su</div><div class="line">[root@localhost app]# echo &quot;export JAVA_HOME=/home/storm/app/jdk&quot;&gt;&gt; /etc/profile</div><div class="line">[root@localhost app]# echo &quot;export ZOOKEEPER_HOME=/home/storm/app/zookeeper&quot;&gt;&gt; /etc/profile</div><div class="line">[root@localhost app]# echo &quot;export STORM_HOME=/home/storm/app/storm&quot;&gt;&gt; /etc/profile</div><div class="line">[root@localhost app]# source /etc/profile</div><div class="line">[root@localhost app]# echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;ZOOKEEPER_HOME&#125;/bin:$&#123;STORM_HOME&#125;/bin:$PATH&quot;&gt;&gt; /etc/profile</div><div class="line">[root@localhost app]# source /etc/profile</div><div class="line">[root@localhost app]# su storm</div></pre></td></tr></table></figure></p>
<p>修改stom配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[storm@localhost app]$ cd storm/conf</div><div class="line">[storm@localhost conf]$ vi storm.yaml</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">storm.zookeeper.servers:</div><div class="line">     - &quot;storm01&quot;</div><div class="line">     - &quot;storm02&quot;</div><div class="line">     - &quot;storm03&quot;</div><div class="line">storm.local.dir: &quot;/home/storm/app/storm/data&quot;</div><div class="line">storm.zookeeper.root: &quot;/storm&quot;  </div><div class="line">nimbus.seeds: [&quot;storm01&quot;]</div><div class="line">supervisor.slots.ports:</div><div class="line">    - 6700</div><div class="line">    - 6701</div><div class="line">    - 6702</div><div class="line">    - 6703</div></pre></td></tr></table></figure>
<h2 id="启动zookeeper，在三个节点分别执行如下命令"><a href="#启动zookeeper，在三个节点分别执行如下命令" class="headerlink" title="启动zookeeper，在三个节点分别执行如下命令"></a>启动zookeeper，在三个节点分别执行如下命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[storm@storm01 zookeeper]$ zkServer.sh start</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[storm@storm02 zookeeper]$ zkServer.sh start</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[storm@storm03 zookeeper]$ zkServer.sh start</div></pre></td></tr></table></figure>
<h2 id="启动storm集群"><a href="#启动storm集群" class="headerlink" title="启动storm集群"></a>启动storm集群</h2><p>在storm01节点上启动nimbus，ui，logviewer ：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[storm@storm01 zookeeper]$ nohup storm nimbus &amp;</div><div class="line">[storm@storm01 zookeeper]$ nohup storm ui &amp;</div><div class="line">[storm@storm01 zookeeper]$ nohup storm logviewer &amp;</div></pre></td></tr></table></figure></p>
<p>在stomr02，storm03节点上启动supervisor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[storm@storm02 zookeeper]$ nohup storm supervisor &amp;</div><div class="line">[storm@storm02 zookeeper]$ nohup storm logviewer &amp;</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[storm@storm03 zookeeper]$ nohup storm supervisor &amp;</div><div class="line">[storm@storm03 zookeeper]$ nohup storm logviewer &amp;</div></pre></td></tr></table></figure>
<p><img src="http://oh6ybr0jg.bkt.clouddn.com/storm%E9%9B%86%E7%BE%A4.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨日已经发布storm1.0.3版本，来玩一玩&lt;/p&gt;
&lt;h2 id=&quot;环境：&quot;&gt;&lt;a href=&quot;#环境：&quot; class=&quot;headerlink&quot; title=&quot;环境：&quot;&gt;&lt;/a&gt;环境：&lt;/h2&gt;&lt;p&gt;zookeeper3.4.9节点：storm01，storm02，storm03&lt;br&gt;storm1.0.3 nimbus节点storm01，supervisor节点storm02，storm03&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;[storm@localhost app]$ wget http://apache.fayea.com/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[storm@localhost app]$ tar zxvf zookeeper-3.4.9.tar.gz&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[storm@localhost app]$ ln -s -f zookeeper-3.4.9 zookeeper&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[storm@localhost app]$ mkdir zookeeper/data&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[storm@localhost app]$ cd zookeeper/conf/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[storm@localhost conf]$ cp zoo_sample.cfg zoo.cfg&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;[storm@localhost conf]$ vi zoo.cfg&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;修改dataDir&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;dataDir=/home/storm/app/zookeeper/data&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;server.1=storm01:2888:3888&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;server.2=storm02:2888:3888&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;server.3=storm03:2888:3888&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>2017学习计划</title>
    <link href="http://yoursite.com/2017/02/07/2017%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"/>
    <id>http://yoursite.com/2017/02/07/2017学习计划/</id>
    <published>2017-02-07T00:00:00.000Z</published>
    <updated>2017-03-07T14:29:26.948Z</updated>
    
    <content type="html"><![CDATA[<h2 id="大数据及机器学习学习计划"><a href="#大数据及机器学习学习计划" class="headerlink" title="大数据及机器学习学习计划"></a>大数据及机器学习学习计划</h2><ol>
<li><p>编程基础：Python<br><a href="https://cn.udacity.com/course/programming-foundations-with-python--ud036" target="_blank" rel="external">https://cn.udacity.com/course/programming-foundations-with-python--ud036</a></p>
</li>
<li><p>计算机科学导论  72小时<br><a href="https://cn.udacity.com/course/intro-to-computer-science--cs101" target="_blank" rel="external">https://cn.udacity.com/course/intro-to-computer-science--cs101</a></p>
</li>
<li>推论统计学  48小时<br><a href="https://cn.udacity.com/course/intro-to-inferential-statistics--ud201" target="_blank" rel="external">https://cn.udacity.com/course/intro-to-inferential-statistics--ud201</a></li>
<li>描述统计学  48小时<br><a href="https://cn.udacity.com/course/intro-to-inferential-statistics--ud201" target="_blank" rel="external">https://cn.udacity.com/course/intro-to-inferential-statistics--ud201</a></li>
<li>机器学习  240小时<br><a href="https://cn.udacity.com/course/machine-learning--ud262" target="_blank" rel="external">https://cn.udacity.com/course/machine-learning--ud262</a></li>
<li>统计学入门<br><a href="https://cn.udacity.com/course/intro-to-statistics--st101" target="_blank" rel="external">https://cn.udacity.com/course/intro-to-statistics--st101</a></li>
<li>基础线性代数<br><a href="https://cn.udacity.com/course/linear-algebra-refresher-course--ud953" target="_blank" rel="external">https://cn.udacity.com/course/linear-algebra-refresher-course--ud953</a></li>
<li><p>机器学习<br><a href="https://cn.udacity.com/course/machine-learning-engineer-nanodegree--nd009" target="_blank" rel="external">https://cn.udacity.com/course/machine-learning-engineer-nanodegree--nd009</a></p>
</li>
<li><p>Apache Storm 进行实时分析  48小时<br><a href="https://cn.udacity.com/course/real-time-analytics-with-apache-storm--ud381" target="_blank" rel="external">https://cn.udacity.com/course/real-time-analytics-with-apache-storm--ud381</a></p>
</li>
<li><p>Bash脚本  40+小时</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;大数据及机器学习学习计划&quot;&gt;&lt;a href=&quot;#大数据及机器学习学习计划&quot; class=&quot;headerlink&quot; title=&quot;大数据及机器学习学习计划&quot;&gt;&lt;/a&gt;大数据及机器学习学习计划&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;编程基础：Python&lt;br&gt;&lt;a hre
    
    </summary>
    
      <category term="学习计划" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="学习" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>storm1.0.2安装</title>
    <link href="http://yoursite.com/2017/01/28/storm1.0.2%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/01/28/storm1.0.2安装/</id>
    <published>2017-01-28T12:00:00.000Z</published>
    <updated>2017-03-04T13:08:03.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>配置zookeeper略</p>
<p>下载：<br>wget <a href="http://mirrors.cnnic.cn/apache/storm/apache-storm-1.0.2/apache-storm-1.0.2.tar.gz" target="_blank" rel="external">http://mirrors.cnnic.cn/apache/storm/apache-storm-1.0.2/apache-storm-1.0.2.tar.gz</a></p>
<p>解压，并创建符号链接<br>修改profile配置文件<br>修改配置文件：<br>vi storm.yaml </p>
<p>添加如下内容<br>storm.zookeeper.servers:</p>
<pre><code>- &quot;www.hadoop01.com&quot;
</code></pre><p>storm.zookeeper.root: “/storm_1.0.2”<br>nimbus.seeds: [“www.hadoop01.com”]<br>supervisor.slots.ports:</p>
<pre><code>- 6700
- 6701
- 6702
- 6703
</code></pre><a id="more"></a>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>启动zookeeper<br>启动nimbus<br>./bin/storm nimbus &amp;<br>启动ui</p>
<p>./bin/storm ui &amp;</p>
<h2 id="登录UI："><a href="#登录UI：" class="headerlink" title="登录UI："></a>登录UI：</h2><p><a href="http://www.hadoop01.com:8080/index.html" target="_blank" rel="external">http://www.hadoop01.com:8080/index.html</a><br><img src="http://oh6ybr0jg.bkt.clouddn.com/UI1.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/UI2.png" alt="此处输入图片的描述"><br>启动supervisor<br>./bin/storm supervisor &amp;<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E5%90%AF%E5%8A%A8supervisor%E5%90%8E.png" alt="此处输入图片的描述"><br>启动logviewer<br>./bin/storm logviewer &amp; </p>
<h2 id="执行demo"><a href="#执行demo" class="headerlink" title="执行demo"></a>执行demo</h2><p>cd /home/hadoop/apps/storm/storm/examples/storm-starter</p>
<p>storm jar storm-starter-topologies-1.0.2.jar org.apache.storm.starter.WordCountTopology first-topology</p>
<p>查看UI<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E6%8F%90%E4%BA%A4WordCountTopology.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E6%8F%90%E4%BA%A4WordCountTopology2.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E6%8F%90%E4%BA%A4WordCountTopology3.png" alt="此处输入图片的描述"><br>点击spout:<br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E7%82%B9%E5%87%BBspout%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E8%BF%90%E8%A1%8C%E7%9A%84%E6%9C%BA%E5%99%A8.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;配置zookeeper略&lt;/p&gt;
&lt;p&gt;下载：&lt;br&gt;wget &lt;a href=&quot;http://mirrors.cnnic.cn/apache/storm/apache-storm-1.0.2/apache-storm-1.0.2.tar.gz&quot;&gt;http://mirrors.cnnic.cn/apache/storm/apache-storm-1.0.2/apache-storm-1.0.2.tar.gz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;解压，并创建符号链接&lt;br&gt;修改profile配置文件&lt;br&gt;修改配置文件：&lt;br&gt;vi storm.yaml &lt;/p&gt;
&lt;p&gt;添加如下内容&lt;br&gt;storm.zookeeper.servers:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- &amp;quot;www.hadoop01.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;storm.zookeeper.root: “/storm_1.0.2”&lt;br&gt;nimbus.seeds: [“www.hadoop01.com”]&lt;br&gt;supervisor.slots.ports:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- 6700
- 6701
- 6702
- 6703
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>Python春节祝福语自动回复</title>
    <link href="http://yoursite.com/2017/01/27/Python%E6%98%A5%E8%8A%82%E7%A5%9D%E7%A6%8F%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%9B%9E%E5%A4%8D/"/>
    <id>http://yoursite.com/2017/01/27/Python春节祝福语自动回复/</id>
    <published>2017-01-27T12:00:00.000Z</published>
    <updated>2017-03-04T13:07:31.345Z</updated>
    
    <content type="html"><![CDATA[<p>首先安装两个库<br>pip install itchat pillow</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">环境win7，pyython3</div><div class="line">编写py文件输入以下内容</div><div class="line"><span class="keyword">import</span> itchat, time, re</div><div class="line"><span class="keyword">from</span> itchat.content <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="meta">@itchat.msg_register([TEXT])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_reply</span><span class="params">(msg)</span>:</span></div><div class="line">    match = re.search(<span class="string">'年'</span>, msg[<span class="string">'Text'</span>]).span()</div><div class="line">    <span class="keyword">if</span> match:</div><div class="line">      itchat.send((<span class="string">'鸡年大吉'</span>), msg[<span class="string">'FromUserName'</span>])</div><div class="line"></div><div class="line"><span class="meta">@itchat.msg_register([PICTURE, RECORDING, VIDEO, SHARING])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">other_reply</span><span class="params">(msg)</span>:</span></div><div class="line">    itchat.send((<span class="string">'鸡年大吉'</span>), msg[<span class="string">'FromUserName'</span>])</div><div class="line"></div><div class="line">itchat.auto_login(enableCmdQR=<span class="keyword">True</span>,hotReload=<span class="keyword">True</span>)</div><div class="line">itchat.run()</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>运行后，扫描生成的二维码即可</p>
<p>登录成功有以下提示<br><img src="http://oh6ybr0jg.bkt.clouddn.com/python%E5%8F%91%E9%80%81%E7%A5%9D%E7%A6%8F%E8%AF%AD.png" alt="此处输入图片的描述"><br><img src="http://oh6ybr0jg.bkt.clouddn.com/python%E5%8F%91%E9%80%81%E7%A5%9D%E7%A6%8F%E8%AF%AD2.png" alt="此处输入图片的描述"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先安装两个库&lt;br&gt;pip install itchat pillow&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;环境win7，pyython3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;编写py文件输入以下内容&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; itchat, time, re&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; itchat.content &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; *&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@itchat.msg_register([TEXT])&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;text_reply&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(msg)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    match = re.search(&lt;span class=&quot;string&quot;&gt;&#39;年&#39;&lt;/span&gt;, msg[&lt;span class=&quot;string&quot;&gt;&#39;Text&#39;&lt;/span&gt;]).span()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; match:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;      itchat.send((&lt;span class=&quot;string&quot;&gt;&#39;鸡年大吉&#39;&lt;/span&gt;), msg[&lt;span class=&quot;string&quot;&gt;&#39;FromUserName&#39;&lt;/span&gt;])&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;@itchat.msg_register([PICTURE, RECORDING, VIDEO, SHARING])&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;other_reply&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(msg)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    itchat.send((&lt;span class=&quot;string&quot;&gt;&#39;鸡年大吉&#39;&lt;/span&gt;), msg[&lt;span class=&quot;string&quot;&gt;&#39;FromUserName&#39;&lt;/span&gt;])&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;itchat.auto_login(enableCmdQR=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,hotReload=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;itchat.run()&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>storm基础知识</title>
    <link href="http://yoursite.com/2017/01/27/storm%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2017/01/27/storm基础知识/</id>
    <published>2017-01-27T12:00:00.000Z</published>
    <updated>2017-04-02T15:23:59.750Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Storm基本概念"><a href="#Storm基本概念" class="headerlink" title="Storm基本概念"></a>Storm基本概念</h2><p>Storm是一个分布式计算框架，主要由Clojure编程语言编写。最初是由Nathan Marz及其团队创建于BackType，该项目在被Twitter取得后开源。它使用用户创建的“管（spouts）”和“螺栓（bolts）”来定义信息源和操作来允许批量、分布式处理流式数据。最初的版本发布于2011年9月17日。<br>Storm应用被设计成为一个拓扑结构，其接口创建一个转换“流”。它提供与MapReduce作业类似的功能，当遇到异常时该拓扑结构理论上将不确定地运行，直到它被手动终止</p>
<a id="more"></a>
<p><img src="http://storm.apache.org/images/storm-flow.png" alt="此处输入图片的描述"><br>主要版本</p>
<table>
<thead>
<tr>
<th>版本</th>
<th>发布日期</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9.0.1</td>
<td>2013年12月8日</td>
</tr>
<tr>
<td>0.9.0</td>
<td>2013年12月8日</td>
</tr>
<tr>
<td>0.8.0</td>
<td>2012年8月2日</td>
</tr>
<tr>
<td>0.7.0</td>
<td>2012年2月28日</td>
</tr>
<tr>
<td>0.6.0</td>
<td>2011年12月15日</td>
</tr>
<tr>
<td>0.5.0</td>
<td>2011年9月19日</td>
</tr>
</tbody>
</table>
<h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><h3 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h3><p>一个实时计算应用程序逻辑被封装在Topology对象中， 类似Hadoop中的job， Topology会一直运行直到你显式杀死它</p>
<p>###DataSource<br>外部数据源</p>
<h3 id="Spout"><a href="#Spout" class="headerlink" title="Spout"></a>Spout</h3><p>接受外部数据源的组件，将外部数据源转化成Storm内部的数据，以Tuple为基本的传输单元下发给Bolt</p>
<h3 id="Bolt"><a href="#Bolt" class="headerlink" title="Bolt"></a>Bolt</h3><p>接受Spout发送的数据，或上游的bolt的发送的数据。根据业务逻辑进行处理。发送给下一个Bolt或者是存储到某种介质上。介质可以是Redis可以是mysql，或者其他。</p>
<h3 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h3><p>Storm内部中数据传输的基本单元，里面封装了一个List对象，用来保存数据。</p>
<h2 id="StreamGrouping"><a href="#StreamGrouping" class="headerlink" title="StreamGrouping:"></a>StreamGrouping:</h2><h4 id="数据分组策略7种："><a href="#数据分组策略7种：" class="headerlink" title="数据分组策略7种："></a>数据分组策略7种：</h4><p>shuffleGrouping(Random函数),<br>NonGrouping(Random函数,目前和shuffleGrouping一样),<br>FieldGrouping(Hash取模),<br>Local or ShuffleGrouping （本地或随机，优先本地）,<br>Fields grouping（根据Tuple中的某一个Filed或者多个Filed的是值来划分。 比如<br>Stream根据Field为user-id来grouping， 相同user-id值的Tuple会被分<br>发到相同的Task中）,<br>Global grouping（整个Stream会选择一个Task作为分发的目的地， 通常是最新的那<br>个id的Task）<br>Direct grouping（产生数据的Spout/Bolt自己明确决定这个Tuple被Bolt的那些Task所<br>消费）</p>
<h2 id="Storm优点"><a href="#Storm优点" class="headerlink" title="Storm优点"></a>Storm优点</h2><h4 id="健壮性"><a href="#健壮性" class="headerlink" title="健壮性"></a>健壮性</h4><p>当Worker失效或机器出现故障时， 自动分配新的Worker替换失<br>效Worker</p>
<h4 id="准确性"><a href="#准确性" class="headerlink" title="准确性"></a>准确性</h4><p>采用Acker机制，保证数据不丢失<br>采用事务机制，保证数据准确性</p>
<h2 id="storm架构"><a href="#storm架构" class="headerlink" title="storm架构"></a>storm架构</h2><p><img src="http://oh6ybr0jg.bkt.clouddn.com/storm%E6%9E%B6%E6%9E%84.png" alt="此处输入图片的描述"></p>
<p>Storm的主线主要包括4条：nimbus, supervisor, worker和task。</p>
<p>对于storm0.9.6的配置文件nimbus配置参数还是nimbus.hosts 现在新版本1.0.2已经修改为nimbus.seeds,已经可以支持HA<br>此处之一配置nimbus.seeds: XX。此处nimbus.seeds:后面要有一个空格，否则启动报错<br>storm启动nimbus的时候jps会有一个进程config_value然后变成nimbus。<br>其他启动同上</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Storm基本概念&quot;&gt;&lt;a href=&quot;#Storm基本概念&quot; class=&quot;headerlink&quot; title=&quot;Storm基本概念&quot;&gt;&lt;/a&gt;Storm基本概念&lt;/h2&gt;&lt;p&gt;Storm是一个分布式计算框架，主要由Clojure编程语言编写。最初是由Nathan Marz及其团队创建于BackType，该项目在被Twitter取得后开源。它使用用户创建的“管（spouts）”和“螺栓（bolts）”来定义信息源和操作来允许批量、分布式处理流式数据。最初的版本发布于2011年9月17日。&lt;br&gt;Storm应用被设计成为一个拓扑结构，其接口创建一个转换“流”。它提供与MapReduce作业类似的功能，当遇到异常时该拓扑结构理论上将不确定地运行，直到它被手动终止&lt;/p&gt;
    
    </summary>
    
      <category term="storm" scheme="http://yoursite.com/categories/storm/"/>
    
    
      <category term="storm" scheme="http://yoursite.com/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>Python机器学习准备</title>
    <link href="http://yoursite.com/2017/01/27/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%87%86%E5%A4%87/"/>
    <id>http://yoursite.com/2017/01/27/Python机器学习准备/</id>
    <published>2017-01-27T12:00:00.000Z</published>
    <updated>2017-04-03T15:49:35.522Z</updated>
    
    <content type="html"><![CDATA[<p>下载NumPy<br><a href="http://download.csdn.net/download/z1137730824/8384347（numpy" target="_blank" rel="external">http://download.csdn.net/download/z1137730824/8384347（numpy</a> 64位）<br><a href="http://download.csdn.net/detail/u010156024/9302649（numpy" target="_blank" rel="external">http://download.csdn.net/detail/u010156024/9302649（numpy</a> 32位）</p>
<p>下载matplotlib<br><a href="https://pypi.python.org/pypi/matplotlib/" target="_blank" rel="external">https://pypi.python.org/pypi/matplotlib/</a></p>
<p>博主选择<br>matplotlib-2.0.0-cp27-cp27m-win_amd64.whl (md5)<br>下载完把matplotlib-2.0.0-cp27-cp27m-win_amd64.whl改成matplotlib-2.0.0-cp27-cp27m-win_amd64.zip<br>解压到Python目录下的Lib文件夹下的site-packages目录</p>
<p>安装dateutil<br>直接用pip install python-dateutil<br>或者去网上下载<br><a href="https://pypi.python.org/pypi/python-dateutil" target="_blank" rel="external">https://pypi.python.org/pypi/python-dateutil</a></p>
<p>安装pyparsing<br>pip install pyparsing<br>或者去网上下载<br><a href="https://pypi.python.org/pypi/pyparsing/2.0.2" target="_blank" rel="external">https://pypi.python.org/pypi/pyparsing/2.0.2</a><br><a href="http://pyparsing.wikispaces.com/Download+and+Installation" target="_blank" rel="external">http://pyparsing.wikispaces.com/Download+and+Installation</a></p>
<p>最终安装如下安装包：numpy, setuptools, python-dateutil, pytz, pyparsing, and cycler，functools32</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下载NumPy&lt;br&gt;&lt;a href=&quot;http://download.csdn.net/download/z1137730824/8384347（numpy&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://download.csdn.net/
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Python网络爬虫</title>
    <link href="http://yoursite.com/2017/01/26/Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    <id>http://yoursite.com/2017/01/26/Python网络爬虫/</id>
    <published>2017-01-26T09:42:30.000Z</published>
    <updated>2017-01-28T17:00:27.546Z</updated>
    
    <content type="html"><![CDATA[<p>python官方提供的网页下载器是urllib2<br>第三方有更强大的下载器是requests</p>
<p>在python2.x里我们可以使用<br>urllib2.urlopen(“<a href="http://www.baidu.com&quot;)打开网页" target="_blank" rel="external">http://www.baidu.com&quot;)打开网页</a><br>但是在python3.x里 urllib2 需要改成urllib.request</p>
<p>在python2.x里可以使用urllib.urlencode：例如：<br>values = {“username”:”Python爬虫”,”password”:”123456789”}<br>data = urllib.urlencode(values)<br>但是在python3.x里 urllib2 需要改成urllib.request<br>data = urllib.parse.urlencode(values)<br>而且还需要把data格式转换<br>data = data.encode(‘utf-8’)或<br>data = data.encode(encoding=’UTF8’)<br>否则会提示<br>TypeError: POST data should be bytes or an iterable of bytes. It cannot be of type str.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python官方提供的网页下载器是urllib2&lt;br&gt;第三方有更强大的下载器是requests&lt;/p&gt;
&lt;p&gt;在python2.x里我们可以使用&lt;br&gt;urllib2.urlopen(“&lt;a href=&quot;http://www.baidu.com&amp;quot;)打开网页&quot; t
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>kNN实现手写数字识别</title>
    <link href="http://yoursite.com/2017/01/25/kNN%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2017/01/25/kNN实现手写数字识别/</id>
    <published>2017-01-25T10:42:30.000Z</published>
    <updated>2017-04-02T15:23:35.133Z</updated>
    
    <content type="html"><![CDATA[<p>需求<br>利用一个手写数字“先验数据”集，使用knn算法来实现对手写数字的自动识别；<br>先验数据（训练数据）集：<br>    数据维度比较大，样本数比较多。<br>    数据集包括数字0-9的手写体。<br>    每个数字大约有200个样本。<br>    每个样本保持在一个txt文件中。<br>    手写体图像本身的大小是32x32的二值图，转换到txt文件保存后，内容也是32x32个数字，0或者1，如下：<br><a id="more"></a><br><img src="http://oh6ybr0jg.bkt.clouddn.com/%E6%95%B0%E5%AD%97012.png" alt="此处输入图片的描述"></p>
<p>首先准备测试文件:<br>1934个训练数据<br>946个测试数据</p>
<p>分析：<br>1、手写体因为每个人，甚至每次写的字都不会完全精确一致，所以，识别手写体的关键是“相似度”<br>2、既然是要求样本之间的相似度，那么，首先需要将样本进行抽象，将每个样本变成一系列特征数据（即特征向量）<br>3、手写体在直观上就是一个个的图片，而图片是由上述图示中的像素点来描述的，样本的相似度其实就是像素的位置和颜色之间的组合的相似度<br>4、因此，将图片的像素按照固定顺序读取到一个个的向量中，即可很好地表示手写体样本<br>5、抽象出了样本向量，及相似度计算模型，即可应用KNN来实现</p>
<p>代码：<br>1)    一个用来生成将每个样本的txt文件转换为对应的一个向量，<br>2)    一个用来加载整个数据集，<br>3)    一个实现kNN分类算法。<br>4)    最后就是实现加载、测试的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#########################################</span></div><div class="line"><span class="comment"># kNN: k Nearest Neighbors</span></div><div class="line"></div><div class="line"><span class="comment"># 参数:        inX: vector to compare to existing dataset (1xN)</span></div><div class="line"><span class="comment">#             dataSet: size m data set of known vectors (NxM)</span></div><div class="line"><span class="comment">#             labels: data set labels (1xM vector)</span></div><div class="line"><span class="comment">#             k: number of neighbors to use for comparison </span></div><div class="line">            </div><div class="line"><span class="comment"># 输出:     多数类</span></div><div class="line"><span class="comment">#########################################</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</div><div class="line"><span class="keyword">import</span> operator</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># KNN分类核心方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNNClassify</span><span class="params">(newInput, dataSet, labels, k)</span>:</span></div><div class="line">	numSamples = dataSet.shape[<span class="number">0</span>]  <span class="comment"># shape[0]代表行数</span></div><div class="line"></div><div class="line">	<span class="comment">## step 1: 计算欧式距离</span></div><div class="line">	<span class="comment"># tile(A, reps): 将A重复reps次来构造一个矩阵</span></div><div class="line">	<span class="comment"># the following copy numSamples rows for dataSet</span></div><div class="line">	diff = tile(newInput, (numSamples, <span class="number">1</span>)) - dataSet  <span class="comment"># Subtract element-wise</span></div><div class="line">	squaredDiff = diff ** <span class="number">2</span> <span class="comment"># squared for the subtract</span></div><div class="line">	squaredDist = sum(squaredDiff, axis = <span class="number">1</span>)  <span class="comment"># sum is performed by row</span></div><div class="line">	distance = squaredDist ** <span class="number">0.5</span></div><div class="line"></div><div class="line">	<span class="comment">## step 2: 对距离排序</span></div><div class="line">	<span class="comment"># argsort()返回排序后的索引</span></div><div class="line">	sortedDistIndices = argsort(distance)</div><div class="line"></div><div class="line">	classCount = &#123;&#125;  <span class="comment"># 定义一个空的字典</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(k):</div><div class="line">		<span class="comment">## step 3: 选择k个最小距离</span></div><div class="line">		voteLabel = labels[sortedDistIndices[i]]</div><div class="line"></div><div class="line">		<span class="comment">## step 4: 计算类别的出现次数</span></div><div class="line">		<span class="comment"># when the key voteLabel is not in dictionary classCount, get()</span></div><div class="line">		<span class="comment"># will return 0</span></div><div class="line">		classCount[voteLabel] = classCount.get(voteLabel, <span class="number">0</span>) + <span class="number">1</span></div><div class="line"></div><div class="line">	<span class="comment">## step 5: 返回出现次数最多的类别作为分类结果</span></div><div class="line">	maxCount = <span class="number">0</span></div><div class="line">	<span class="keyword">for</span> key, value <span class="keyword">in</span> classCount.items():</div><div class="line">		<span class="keyword">if</span> value &gt; maxCount:</div><div class="line">			maxCount = value</div><div class="line">			maxIndex = key</div><div class="line"></div><div class="line">	<span class="keyword">return</span> maxIndex	</div><div class="line"></div><div class="line"><span class="comment"># 将图片转换为向量</span></div><div class="line"><span class="function"><span class="keyword">def</span>  <span class="title">img2vector</span><span class="params">(filename)</span>:</span></div><div class="line"> 	rows = <span class="number">32</span></div><div class="line"> 	cols = <span class="number">32</span></div><div class="line"> 	imgVector = zeros((<span class="number">1</span>, rows * cols)) </div><div class="line"> 	fileIn = open(filename)</div><div class="line"> 	<span class="keyword">for</span> row <span class="keyword">in</span> xrange(rows):</div><div class="line"> 		lineStr = fileIn.readline()</div><div class="line"> 		<span class="keyword">for</span> col <span class="keyword">in</span> xrange(cols):</div><div class="line"> 			imgVector[<span class="number">0</span>, row * <span class="number">32</span> + col] = int(lineStr[col])</div><div class="line"></div><div class="line"> 	<span class="keyword">return</span> imgVector</div><div class="line"></div><div class="line"><span class="comment"># 加载数据集</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment">## step 1: 读取训练数据集</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"---Getting training set..."</span></div><div class="line">	dataSetDir = <span class="string">'E:/Python/ml/knn/'</span></div><div class="line">	trainingFileList = os.listdir(dataSetDir + <span class="string">'trainingDigits'</span>)  <span class="comment"># 加载测试数据</span></div><div class="line">	numSamples = len(trainingFileList)</div><div class="line"></div><div class="line">	train_x = zeros((numSamples, <span class="number">1024</span>))</div><div class="line">	train_y = []</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(numSamples):</div><div class="line">		filename = trainingFileList[i]</div><div class="line"></div><div class="line">		<span class="comment"># get train_x</span></div><div class="line">		train_x[i, :] = img2vector(dataSetDir + <span class="string">'trainingDigits/%s'</span> % filename) </div><div class="line"></div><div class="line">		<span class="comment"># get label from file name such as "1_18.txt"</span></div><div class="line">		label = int(filename.split(<span class="string">'_'</span>)[<span class="number">0</span>]) <span class="comment"># return 1</span></div><div class="line">		train_y.append(label)</div><div class="line"></div><div class="line">	<span class="comment">## step 2:读取测试数据集</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"---Getting testing set..."</span></div><div class="line">	testingFileList = os.listdir(dataSetDir + <span class="string">'testDigits'</span>) <span class="comment"># load the testing set</span></div><div class="line">	numSamples = len(testingFileList)</div><div class="line">	test_x = zeros((numSamples, <span class="number">1024</span>))</div><div class="line">	test_y = []</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(numSamples):</div><div class="line">		filename = testingFileList[i]</div><div class="line"></div><div class="line">		<span class="comment"># get train_x</span></div><div class="line">		test_x[i, :] = img2vector(dataSetDir + <span class="string">'testDigits/%s'</span> % filename) </div><div class="line"></div><div class="line">		<span class="comment"># get label from file name such as "1_18.txt"</span></div><div class="line">		label = int(filename.split(<span class="string">'_'</span>)[<span class="number">0</span>]) <span class="comment"># return 1</span></div><div class="line">		test_y.append(label)</div><div class="line"></div><div class="line">	<span class="keyword">return</span> train_x, train_y, test_x, test_y</div><div class="line"></div><div class="line"><span class="comment"># 手写识别主流程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">testHandWritingClass</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment">## step 1: 加载数据</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"step 1: load data..."</span></div><div class="line">	train_x, train_y, test_x, test_y = loadDataSet()</div><div class="line"></div><div class="line">	<span class="comment">## step 2: 模型训练.</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"step 2: training..."</span></div><div class="line">	<span class="keyword">pass</span></div><div class="line"></div><div class="line">	<span class="comment">## step 3: 测试</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"step 3: testing..."</span></div><div class="line">	numTestSamples = test_x.shape[<span class="number">0</span>]</div><div class="line">	matchCount = <span class="number">0</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(numTestSamples):</div><div class="line">		predict = kNNClassify(test_x[i], train_x, train_y, <span class="number">3</span>)</div><div class="line">		<span class="keyword">if</span> predict == test_y[i]:</div><div class="line">			matchCount += <span class="number">1</span></div><div class="line">	accuracy = float(matchCount) / numTestSamples</div><div class="line"></div><div class="line">	<span class="comment">## step 4: 输出结果</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"step 4: show the result..."</span></div><div class="line">	<span class="keyword">print</span> <span class="string">'The classify accuracy is: %.2f%%'</span> % (accuracy * <span class="number">100</span>)</div></pre></td></tr></table></figure>
<p>测试<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> kNN</div><div class="line">kNN.testHandWritingClass()</div></pre></td></tr></table></figure></p>
<p>执行结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tep <span class="number">1</span>: load data...</div><div class="line">---Getting training set...</div><div class="line">---Getting testing set...</div><div class="line">step <span class="number">2</span>: training...</div><div class="line">step <span class="number">3</span>: testing...</div><div class="line">step <span class="number">4</span>: show the result...</div><div class="line">The classify accuracy <span class="keyword">is</span>: <span class="number">98.84</span>%</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;需求&lt;br&gt;利用一个手写数字“先验数据”集，使用knn算法来实现对手写数字的自动识别；&lt;br&gt;先验数据（训练数据）集：&lt;br&gt;    数据维度比较大，样本数比较多。&lt;br&gt;    数据集包括数字0-9的手写体。&lt;br&gt;    每个数字大约有200个样本。&lt;br&gt;    每个样本保持在一个txt文件中。&lt;br&gt;    手写体图像本身的大小是32x32的二值图，转换到txt文件保存后，内容也是32x32个数字，0或者1，如下：&lt;br&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之KNN算法推演</title>
    <link href="http://yoursite.com/2017/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BKNN%E7%AE%97%E6%B3%95%E6%8E%A8%E6%BC%94/"/>
    <id>http://yoursite.com/2017/01/21/机器学习之KNN算法推演/</id>
    <published>2017-01-21T13:06:30.000Z</published>
    <updated>2017-03-04T13:22:46.475Z</updated>
    
    <content type="html"><![CDATA[<h4 id="从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。"><a href="#从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。" class="headerlink" title="从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。"></a>从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。</h4><h5 id="算法涉及3个主要因素："><a href="#算法涉及3个主要因素：" class="headerlink" title="算法涉及3个主要因素："></a>算法涉及3个主要因素：</h5><p>1)    训练数据集<br>2)    距离或相似度的计算衡量<br>3)    k的大小<br><img src="http://oh6ybr0jg.bkt.clouddn.com/KNN%E7%AE%97%E6%B3%95.jpg" alt="此处输入图片的描述"><br>绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。</p>
<h2 id="KNN分类算法Python实战"><a href="#KNN分类算法Python实战" class="headerlink" title="KNN分类算法Python实战"></a>KNN分类算法Python实战</h2><p>有以下先验数据，使用knn算法对未知类别数据分类</p>
<table>
<thead>
<tr>
<th>x轴</th>
<th style="text-align:center">y轴</th>
<th style="text-align:right">类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td style="text-align:center">0.9</td>
<td style="text-align:right">A</td>
</tr>
<tr>
<td>1.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:right">A</td>
</tr>
<tr>
<td>0.0</td>
<td style="text-align:center">0.1</td>
<td style="text-align:right">B</td>
</tr>
<tr>
<td>0.1</td>
<td style="text-align:center">0.2</td>
<td style="text-align:right">B</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<p>未知类别数据</p>
<table>
<thead>
<tr>
<th>x轴</th>
<th style="text-align:center">y轴</th>
<th style="text-align:right">类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.2</td>
<td style="text-align:center">1.0</td>
<td style="text-align:right">？</td>
</tr>
<tr>
<td>0.1</td>
<td style="text-align:center">0.3</td>
<td style="text-align:right">？</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#########################################</span></div><div class="line"><span class="comment"># kNN: k Nearest Neighbors</span></div><div class="line"></div><div class="line"><span class="comment"># 输入:      newInput:  (1xN)的待分类向量</span></div><div class="line"><span class="comment">#             dataSet:   (NxM)的训练数据集</span></div><div class="line"><span class="comment">#             labels: 	训练数据集的类别标签向量</span></div><div class="line"><span class="comment">#             k: 		近邻数 </span></div><div class="line">            </div><div class="line"><span class="comment"># 输出:     可能性最大的分类标签</span></div><div class="line"><span class="comment">#########################################</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</div><div class="line"><span class="keyword">import</span> operator</div><div class="line"></div><div class="line"><span class="comment">#创建一个数据集，包含2个类别共4个样本</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># 生成一个矩阵，每行表示一个样本</span></div><div class="line">	group = array([[<span class="number">1.0</span>, <span class="number">0.9</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.0</span>, <span class="number">0.1</span>]])</div><div class="line">	<span class="comment"># 4个样本分别所属的类别</span></div><div class="line">	labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</div><div class="line">	<span class="keyword">return</span> group, labels</div><div class="line"></div><div class="line"><span class="comment"># KNN分类算法函数定义</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNNClassify</span><span class="params">(newInput, dataSet, labels, k)</span>:</span></div><div class="line">	numSamples = dataSet.shape[<span class="number">0</span>]   <span class="comment"># shape[0]表示行数</span></div><div class="line"></div><div class="line">	<span class="comment">## step 1: 计算距离</span></div><div class="line">	<span class="comment"># tile(A, reps): 构造一个矩阵，通过A重复reps次得到(tile(A, （repsX，repsY）)此处在行的方向重复repsX次，在Y的方向重复repsY次)</span></div><div class="line">	<span class="comment"># the following copy numSamples rows for dataSet</span></div><div class="line">	diff = tile(newInput, (numSamples, <span class="number">1</span>)) - dataSet  <span class="comment"># 按元素求差值</span></div><div class="line">	squaredDiff = diff ** <span class="number">2</span>  <span class="comment">#将差值平方</span></div><div class="line">	squaredDist = sum(squaredDiff, axis = <span class="number">1</span>)   <span class="comment"># 按行累加</span></div><div class="line">	distance = squaredDist ** <span class="number">0.5</span>  <span class="comment">#将差值平方和求开方，即得距离</span></div><div class="line"></div><div class="line">	<span class="comment">## step 2: 对距离排序</span></div><div class="line">	<span class="comment">## 此处排序需要注意样本标签的顺序。排序后存的是角标号。后续直接从排序后的标签序列找下标是排序内容的数据</span></div><div class="line">	<span class="comment"># argsort() 返回排序后的索引值</span></div><div class="line">	sortedDistIndices = argsort(distance)  <span class="comment">#或distance.argsort()</span></div><div class="line">	classCount = &#123;&#125; <span class="comment"># define a dictionary (can be append element)</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> xrange(k):</div><div class="line">		<span class="comment">## step 3: 选择k个最近邻</span></div><div class="line">		voteLabel = labels[sortedDistIndices[i]]</div><div class="line"></div><div class="line">		<span class="comment">## step 4: 计算k个最近邻中各类别出现的次数</span></div><div class="line">		<span class="comment"># when the key voteLabel is not in dictionary classCount, get()</span></div><div class="line">		<span class="comment"># will return 0</span></div><div class="line">		classCount[voteLabel] = classCount.get(voteLabel, <span class="number">0</span>) + <span class="number">1</span></div><div class="line"></div><div class="line">	<span class="comment">## step 5: 返回出现次数最多的类别标签</span></div><div class="line">	maxCount = <span class="number">0</span></div><div class="line">	<span class="keyword">for</span> key, value <span class="keyword">in</span> classCount.items():</div><div class="line">		<span class="keyword">if</span> value &gt; maxCount:</div><div class="line">			maxCount = value</div><div class="line">			maxIndex = key</div><div class="line"></div><div class="line">	<span class="keyword">return</span> maxIndex	</div><div class="line">	</div><div class="line">	<span class="comment">##step 5 可以改成：</span></div><div class="line">	<span class="comment"># sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)</span></div><div class="line">	<span class="comment">#return sortedClassCount[0][0]</span></div></pre></td></tr></table></figure>
<p>然后调用算法进行测试：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> kNN</div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> * </div><div class="line"><span class="comment">#生成数据集和类别标签</span></div><div class="line">dataSet, labels = kNN.createDataSet()</div><div class="line"><span class="comment">#定义一个未知类别的数据</span></div><div class="line">testX = array([<span class="number">1.2</span>, <span class="number">1.0</span>])</div><div class="line">k = <span class="number">3</span></div><div class="line"><span class="comment">#调用分类函数对未知数据分类</span></div><div class="line">outputLabel = kNN.kNNClassify(testX, dataSet, labels, <span class="number">3</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"Your input is:"</span>, testX, <span class="string">"and classified to class: "</span>, outputLabel</div><div class="line"></div><div class="line">testX = array([<span class="number">0.1</span>, <span class="number">0.3</span>])</div><div class="line">outputLabel = kNN.kNNClassify(testX, dataSet, labels, <span class="number">3</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"Your input is:"</span>, testX, <span class="string">"and classified to class: "</span>, outputLabel</div></pre></td></tr></table></figure></p>
<p>这时候会输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Your input <span class="keyword">is</span>: [ <span class="number">1.2</span>  <span class="number">1.0</span>] <span class="keyword">and</span> classified to <span class="class"><span class="keyword">class</span>:</span>  A</div><div class="line">Your input <span class="keyword">is</span>: [ <span class="number">0.1</span>  <span class="number">0.3</span>] <span class="keyword">and</span> classified to <span class="class"><span class="keyword">class</span>:</span>  B</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。&quot;&gt;&lt;a href=&quot;#从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。&quot; class=&quot;headerlink&quot; title=&quot;从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。&quot;&gt;&lt;/a&gt;从训练集中找到和新数据最接近的k条记录，然后根据多数类来决定新数据类别。&lt;/h4&gt;&lt;h5 id=&quot;算法涉及3个主要因素：&quot;&gt;&lt;a href=&quot;#算法涉及3个主要因素：&quot; class=&quot;headerlink&quot; title=&quot;算法涉及3个主要因素：&quot;&gt;&lt;/a&gt;算法涉及3个主要因素：&lt;/h5&gt;&lt;p&gt;1)    训练数据集&lt;br&gt;2)    距离或相似度的计算衡量&lt;br&gt;3)    k的大小&lt;br&gt;&lt;img src=&quot;http://oh6ybr0jg.bkt.clouddn.com/KNN%E7%AE%97%E6%B3%95.jpg&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。&lt;/p&gt;
&lt;h2 id=&quot;KNN分类算法Python实战&quot;&gt;&lt;a href=&quot;#KNN分类算法Python实战&quot; class=&quot;headerlink&quot; title=&quot;KNN分类算法Python实战&quot;&gt;&lt;/a&gt;KNN分类算法Python实战&lt;/h2&gt;&lt;p&gt;有以下先验数据，使用knn算法对未知类别数据分类&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x轴&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;y轴&lt;/th&gt;
&lt;th style=&quot;text-align:right&quot;&gt;类型&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.9&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;1.0&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.1&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;0.2&lt;/td&gt;
&lt;td style=&quot;text-align:right&quot;&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
