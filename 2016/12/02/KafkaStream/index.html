<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  ﻿<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="张洪铭的个人博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="以下内容摘自官网，熟悉的同学可跳过
Use Kafka Streams to process dataKafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will d">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka Stream">
<meta property="og:url" content="http://yoursite.com/2016/12/02/KafkaStream/index.html">
<meta property="og:site_name" content="张洪铭的个人博客">
<meta property="og:description" content="以下内容摘自官网，熟悉的同学可跳过
Use Kafka Streams to process dataKafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will d">
<meta property="og:image" content="http://oh6ybr0jg.bkt.clouddn.com/kafka-stream%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3.png">
<meta property="og:updated_time" content="2017-03-04T13:05:50.985Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kafka Stream">
<meta name="twitter:description" content="以下内容摘自官网，熟悉的同学可跳过
Use Kafka Streams to process dataKafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will d">
<meta name="twitter:image" content="http://oh6ybr0jg.bkt.clouddn.com/kafka-stream%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: 'OFEOJRRO4R',
      apiKey: 'e45796ccb3eeae1fd05f26434d457fb4',
      indexName: 'hexo',
      hits: {"per_page":10},
      labels: {"input_placeholder":"搜索...","hits_empty":"未发现与 「${query}」相关的内容","hits_stats":"${hits} 条相关条目，使用了 ${time} 毫秒"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2016/12/02/KafkaStream/"/>





  <title> Kafka Stream | 张洪铭的个人博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?7429985a9690d626019a4aae76bf7702";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">张洪铭的个人博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
             <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-kafka " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/12/02/KafkaStream/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="张洪铭">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/agns.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="张洪铭的个人博客">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="张洪铭的个人博客" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Kafka Stream
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-02T23:33:30+08:00">
                2016-12-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2016/12/02/KafkaStream/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/12/02/KafkaStream/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>以下内容摘自官网，熟悉的同学可跳过</p>
<h2 id="Use-Kafka-Streams-to-process-data"><a href="#Use-Kafka-Streams-to-process-data" class="headerlink" title="Use Kafka Streams to process data"></a>Use Kafka Streams to process data</h2><p>Kafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers. This quickstart example will demonstrate how to run a streaming application coded in this library. Here is the gist of the WordCountDemo example code (converted to use Java 8 lambda expressions for easy reading).<br>Kafka Streams是Kafka中用于客户端的库，主要用于获取实时流处理以及分析Kafka brokers中存储的数据。这个例子将会展示如何使用这个库来运行一个流式处理应用。这里有一个WordCountDemo的主要代码（转换成Java8 lambda表达式更易读）：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">KTable wordCounts = textLines</div><div class="line">    <span class="comment">// Split each text line, by whitespace, into words.</span></div><div class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</div><div class="line"></div><div class="line">    <span class="comment">// Ensure the words are available as record keys for the next aggregate operation.</span></div><div class="line">    .map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(value, value))</div><div class="line"></div><div class="line">    <span class="comment">// Count the occurrences of each word (record key) and store the results into a table named "Counts".</span></div><div class="line">    .countByKey(<span class="string">"Counts"</span>)</div></pre></td></tr></table></figure>
<a id="more"></a>
<p> It implements the WordCount algorithm, which computes a word occurrence histogram from the input text. However, unlike other WordCount examples you might have seen before that operate on bounded data, the WordCount demo application behaves slightly differently because it is designed to operate on an infinite, unbounded stream of data. Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data.</p>
<p>它实现了WordCount算法，计算了输入文本中的词频。然而，并不像其他的WordCount的例子，都是计算固定大小的数据，这个WordCount demo应用稍微有点不同，它是基于不会终止的数据流计算的。和计算固定数据的模型比较形似的是，它也会不停的更新词频计算结果。然而，由于它是基于永不停止的数据流，所以会周期性的输出当前的计算结果，他会不停的处理更多的数据，因为它也不知道何时它处理过“所有”的输入数据。</p>
<p>We will now prepare input data to a Kafka topic, which will subsequently be processed by a Kafka Streams application.<br>现在我们将输入数据导入Kafka topic，这些数据将会被Kafka Streams应用处理</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; echo -e <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; file-input.txt</div></pre></td></tr></table></figure>
<p>Or on Windows:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; echo all streams lead to kafka&gt; file-input.txt</div><div class="line">&gt; echo hello kafka streams&gt;&gt; file-input.txt</div><div class="line">&gt; echo|set /p=join kafka summit&gt;&gt; file-input.txt</div></pre></td></tr></table></figure></p>
<p>Next, we send this input data to the input topic named streams-file-input using the console producer (in practice, stream data will likely be flowing continuously into Kafka where the application will be up and running):<br>接着，我们使用终端producer来将这些输入数据发送到名为streams-file-input的topic（在实践中，流数据会持续不断的流入kafka，当应用将会启动并运行时）：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-topics.sh --create \</div><div class="line">            --zookeeper localhost:<span class="number">2181</span> \</div><div class="line">            --replication-factor <span class="number">1</span> \</div><div class="line">            --partitions <span class="number">1</span> \</div><div class="line">            --topic streams-file-input</div></pre></td></tr></table></figure></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-<span class="built_in">console</span>-producer.sh --broker-list localhost:<span class="number">9092</span> --topic streams-file-input &lt; file-input.txt</div></pre></td></tr></table></figure>
<p>We can now run the WordCount demo application to process the input data:<br>我们可以运行WordCount demo应用来处理输入数据<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-run-<span class="class"><span class="keyword">class</span>.<span class="title">sh</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">kafka</span>.<span class="title">streams</span>.<span class="title">examples</span>.<span class="title">wordcount</span>.<span class="title">WordCountDemo</span></span></div></pre></td></tr></table></figure></p>
<p>There won’t be any STDOUT output except log entries as the results are continuously written back into another topic named streams-wordcount-output in Kafka. The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.<br>不会有任何的stdout输出除了日志条目，结果会持续不断的写回kafka中另一个名为streams-wordcount-output的topic。这个demo将会运行数秒，不会像典型的流处理应用，自动终止。</p>
<p>We can now inspect the output of the WordCount demo application by reading from its output topic:<br>我们现在通过阅读主题的数来来检查WordCount demo应用程序的输出： </p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt; bin/kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server localhost:<span class="number">9092</span> \</div><div class="line">            --topic streams-wordcount-output \</div><div class="line">            --<span class="keyword">from</span>-beginning \</div><div class="line">            --formatter kafka.tools.DefaultMessageFormatter \</div><div class="line">            --property print.key=<span class="literal">true</span> \</div><div class="line">            --property print.value=<span class="literal">true</span> \</div><div class="line">            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \</div><div class="line">            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</div></pre></td></tr></table></figure>
<p>with the following output data being printed to the console:<br>终端会打印出以下数据：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">all     <span class="number">1</span></div><div class="line">lead    <span class="number">1</span></div><div class="line">to      <span class="number">1</span></div><div class="line">hello   <span class="number">1</span></div><div class="line">streams <span class="number">2</span></div><div class="line">join    <span class="number">1</span></div><div class="line">kafka   <span class="number">3</span></div><div class="line">summit  <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>Here, the first column is the Kafka message key, and the second column is the message value, both in in java.lang.String format. Note that the output is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word, aka record key such as “kafka”. For multiple records with the same key, each later record is an update of the previous one.<br>第一列是Kafka消息的key，第二列是消息value，两者都是java.lang.String格式。注意，输出实际上应该是持续的更新数据流，数据流中的每一个记录（例如，上面输出的每一行）都是一个单独词汇的数量，或者是记录了key的数量，例如上面的“kafka”。对于多条记录的key一致这种情况，每一条后面的记录都是对前一条记录的更新。</p>
<p>Now you can write more input messages to the streams-file-input topic and observe additional messages added to streams-wordcount-output topic, reflecting updated word counts (e.g., using the console producer and the console consumer, as described above).<br>现在你可以写入更多的消息到streams-file-input这个topic，可以观察到更多的消息会发送到streams-wordcount-output这个topic，反映了更新之后的词汇数量。</p>
<p>You can stop the console consumer via Ctrl-C.<br>你可以使用Ctrl+C结束控制台的消费者</p>
<p>Stream Processing<br>Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might normalize or deduplicate this content and published the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza.<br>很多用户将kafka用作多级数据处理之间的消息管道：原始数据存放于Kafka不同的topics中，然后经过聚合、增强、或者其他的转换之后，导入Kafka新的topics中，以供后面的消费。例如，对于新闻推荐的处理流程来所：首先从RSS信息流中获取文章内容，然后导入名为“articles”的topic; 其次，后面的处理可能是对这些内容进行规范化或者精简操作，然后将经过上述处理的内容导入新的topic;最后的处理可能是试图将这些内容推荐给用户。这样的处理流程实际展现了实时流在独立的topics之间流动的流程图。从0.10.0.0开始，Apache Kafka推出了一款称为Kafka Streams的流式处理库，优点是轻量级同时性能很好，它可以完成上面所描述的多级处理。除了Kafka streams之外，还有一些开源流式处理工具可以选用，包括Apache  Storm和Samza。</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Kafka Streams is a client library for processing and analyzing data stored in Kafka and either write the resulting data back to Kafka or send the final output to an external system. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, and simple yet efficient management of application state. Kafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka’s parallelism model.<br>Kafka Streams是一个客户端程序库用于处理和分析存储在Kafka中的数据，并将得到的数据写入Kafka或发送最终输出到外部系统。它建立在如适当区分事件的时间和加工时间，窗口函数的支持，和简单而高效的应用程序状态管理。Kafka Streams有一个低门槛进入：你可以快速编写和运行一个小规模的概念证明在一台机器上，你只需要运行在多台机器上的应用程序的额外的实例扩展到高容量的生产工作负载。Kafka Streams 透明地处理相同的应用程序通过利用Kafka 的并行模型的多个实例的负载平衡。</p>
<p>Some highlights of Kafka Streams:<br>Kafka Streams的一些亮点</p>
<ul>
<li>Designed as a simple and lightweight client library, which can be<br>easily embedded in any Java application and integrated with any<br>existing packaging, deployment and operational tools that users have<br>for their streaming applications.</li>
<li>作为一个简单而轻量级的客户端库，它可以方便的嵌入任何java应用和集成任何现有的包，部署和运营工具，用户有对于他们的流应用。</li>
<li>Has no external dependencies on systems other than Apache Kafka<br>itself as the internal messaging layer; notably, it uses Kafka’s<br>partitioning model to horizontally scale processing while maintaining<br>strong ordering guarantees.</li>
<li>对其他比Apache Kafka 没有外部依赖它本身作为内部消息层，特别是，它使用Kafka的 分割模型在保持同时进行水平缩放处理的分区模型强排序保证。</li>
<li>Supports fault-tolerant local state, which enables very fast and<br>efficient stateful operations like joins and windowed aggregations.</li>
<li>支持容错的本地状态，使非常快速和高效的状态操作的加入和窗口聚集。</li>
<li>Employs one-record-at-a-time processing to achieve low processing<br>latency, and supports event-time based windowing operations.</li>
<li>采用同一时刻只有一条记录处理实现低的处理延迟，并支持基于时间事件的窗口操作。</li>
<li>Offers necessary stream processing primitives, along with a<br>high-level Streams DSL and a low-level Processor API.</li>
<li>提供必要的流处理基元，以及 high-level Streams DSL和 low-level Processor API。</li>
</ul>
<h3 id="Developer-Guide-开发者指南"><a href="#Developer-Guide-开发者指南" class="headerlink" title="Developer Guide 开发者指南"></a>Developer Guide 开发者指南</h3><p>There is a quickstart example that provides how to run a stream processing program coded in the Kafka Streams library. This section focuses on how to write, configure, and execute a Kafka Streams application.</p>
<p>有一个快速入门示例提供了如何运行一个流处理程序在卡夫卡流的库代码。本节重点介绍如何编写、配置和执行卡夫卡流应用程序。</p>
<h3 id="Core-Concepts-核心概念"><a href="#Core-Concepts-核心概念" class="headerlink" title="Core Concepts 核心概念"></a>Core Concepts 核心概念</h3><p>We first summarize the key concepts of Kafka Streams.<br>我们首先总结了Kafka Streams的关键概念。</p>
<h3 id="Stream-Processing-Topology-流处理Topology"><a href="#Stream-Processing-Topology-流处理Topology" class="headerlink" title="Stream Processing Topology 流处理Topology"></a>Stream Processing Topology 流处理Topology</h3><ul>
<li>A stream is the most important abstraction provided by Kafka Streams:<br>it represents an unbounded, continuously updating data set. A stream<br>is an ordered, replayable, and fault-tolerant sequence of immutable<br>data records, where a data record is defined as a key-value pair.</li>
<li>流是Kafka Streams提供的最重要的抽象：它表示一个无界的，不断更新的数据集。一个流是一个有序的、可重复的，和不变的容错序列数据记录，其中一个数据记录被定义为一个键值对。</li>
<li>A stream processing application written in Kafka Streams defines its<br>computational logic through one or more processor topologies, where a<br>processor topology is a graph of stream processors (nodes) that are<br>connected by streams (edges).<br>在Kafka Streams中写的流处理应用程序定义了它的计算逻辑通过一个或多个处理器的topologies，其中处理器的topology是一个流处理器（节点）的图形由流连接（边缘）。</li>
<li>A stream processor is a node in the processor topology; it represents<br>a processing step to transform data in streams by receiving one input<br>record at a time from its upstream processors in the topology,<br>applying its operation to it, and may subsequently producing one or<br>more output records to its downstream processors.</li>
<li>流处理器是处理器topology中的一个节点；它表示通过接收一个输入来变换流中的数据的处理步骤在topology中的上游处理器上记录的时间，应用它的操作，并可能随后产生一个或向下游处理器的更多输出记录。</li>
</ul>
<p>Kafka Streams offers two ways to define the stream processing topology: the Kafka Streams DSL provides the most common data transformation operations such as map and filter; the lower-level Processor API allows developers define and connect custom processors as well as to interact with state stores.<br>Kafka Streams 提供了两种方式来定义流处理topology：Kafka Streams DSL提供了最常用的数据转换操作，如map和filter；lower-level Processor API允许开发者定义和连接定制处理器以及存储交互的状态。</p>
<h3 id="Time-时间"><a href="#Time-时间" class="headerlink" title="Time 时间"></a>Time 时间</h3><p>A critical aspect in stream processing is the notion of time, and how it is modeled and integrated. For example, some operations such as windowing are defined based on time boundaries.<br>流处理中的一个关键方面是时间的概念，以及它是如何建模和集成。例如，一些操作如窗口是基于时间边界的定义。</p>
<p>Common notions of time in streams are:<br>流中的时间的共同概念是： </p>
<ul>
<li>Event time - The point in time when an event or data record occurred,<br>i.e. was originally created “at the source”.</li>
<li>事件时间 - 当发生事件或数据记录时的时间点，即最初创建的“在源头上”。</li>
<li>Processing time - The point in time when the event or data record<br>happens to be processed by the stream processing application, i.e.<br>when the record is being consumed. The processing time may be<br>milliseconds, hours, or days etc. later than the original event time.</li>
<li>处理时间 - 事件或数据记录的时间点碰巧被流处理应用程序处理，即当记录被消耗。处理时间可能是比原始事件时间晚的毫秒数、小时或数天等。</li>
<li>Ingestion time - The point in time when an event or data record is<br>stored in a topic partition by a Kafka broker. The difference to<br>event time is that this ingestion timestamp is generated when the<br>record is appended to the target topic by the Kafka broker, not when<br>the record is created “at the source”. The difference to processing<br>time is that processing time is when the stream processing<br>application processes the record. For example, if a record is never<br>processed, there is no notion of processing time for it, but it still<br>has an ingestion time.</li>
<li>摄取时间 - 当一个事件或数据记录的时间点存储在Kafka broker的主题分区中。<br>事件时间不同的是，这种摄取时间戳时产生的记录追加到目标主题由Kafka broker ，而不是当记录是在“源”创建的。处理差异时间是处理时间的时候是流处理的应用程序处理记录。例如，如果一个记录是从来没有处理，没有处理时间的概念，但它仍然有一个摄取时间</li>
</ul>
<p>The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka’s configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps.<br>事件的时间和摄取时间之间的选择实际上是通过Kafka的配置（不是Kafka Streams）：从Kafka0.10.x起，时间戳被自动嵌入到Kafka的消息中。根据Kafka的配置这些时间戳表示事件时间或摄取时间。各自的Kafka配置设置可以在broker级别或每个主题上指定。在Kafka Streams的默认时间戳提取器将检索这些嵌入时间戳as-is。因此，您的应用程序的有效时间语义依赖于有效的Kafka配置这些嵌入时间戳。</p>
<p>Kafka Streams assigns a timestamp to every data record via the TimestampExtractor interface. Concrete implementations of this interface may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce different notions of time depending on their business needs. For example, per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins.<br>Kafka Streams分配一个时间戳的每一个数据记录通过TimestampExtractor接口。这个接口的具体实现可以检索或计算基于数据记录如嵌入时间戳字段提供事件时间语义内容的时间戳，或使用任何其他的方法，如加工时返回当前时钟时间，从而产生语义流处理应用程序的处理时间。因此，开发人员可以执行不同的时间概念，这取决于他们的业务需求。例如，每个记录时间戳的描述关于流时间的进展（虽然记录可能会超出流）和促使时间依赖操作例如 joins。</p>
<p>Finally, whenever a Kafka Streams application writes records to Kafka, then it will also assign timestamps to these new records. The way the timestamps are assigned depends on the context:<br>最后，当Kafka记录写入到Kafka Streams应用，那么它也会对新纪录指定时间戳。时间戳是分配方式取决于context： </p>
<ul>
<li>When new output records are generated via processing some input<br>record, for example, context.forward() triggered in the process()<br>function call, output record timestamps are inherited from input<br>record timestamps directly.</li>
<li>当通过处理一些输入而产生新的输出记录时记录，例如，context.forward()引发的process()函数调用，输出记录的时间戳是继承自输入直接记录时间戳。</li>
<li>When new output records are generated via periodic functions such as<br>punctuate(), the output record timestamp is defined as the current<br>internal time (obtained through context.timestamp()) of the stream<br>task.</li>
<li>当新的输出记录通过诸如punctuate()周期函数生成的输出记录时间戳定义为当前的内部时间（context.timestamp()获得）的流任务。</li>
<li>For aggregations, the timestamp of a resulting aggregate update<br>record will be that of the latest arrived input record that triggered<br>the update.</li>
<li>对于一个聚合，形成的聚合更新记录的时间戳将最新到达的输入记录触发更新。</li>
</ul>
<h3 id="States-状态"><a href="#States-状态" class="headerlink" title="States 状态"></a>States 状态</h3><p>Some stream processing applications don’t require state, which means the processing of a message is independent from the processing of all other messages. However, being able to maintain state opens up many possibilities for sophisticated stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.<br>一些流处理应用程序不需要状态，这意味着消息的处理是独立于所有其他消息的处理。然而，能够保持状态为复杂的流处理应用程序打开了许多可能性：您可以加入输入流，或组和汇总数据记录。许多这样的状态的操作，由Kafka Streams DSL提供。</p>
<p>Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data. This is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a persistent key-value store, an in-memory hashmap, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.<br>Kafka Streams提供了所谓的状态存储，它可以用于流处理应用程序来存储和查询数据。当实施状态操作时这是一个重要的能力。在Kafka Streams的每一项任务的一个或多个状态存储将可以通过API来存储和查询处理所需的数据访问。这些状态存储可以是一个持续的键值存储，内存中的HashMap，或另一个方便的数据结构。Kafka Streams提供了容错和本地状态存储的自动恢复。</p>
<p>Kafka Streams allows direct read-only queries of the state stores by methods, threads, processes or applications external to the stream processing application that created the state stores. This is provided through a feature called Interactive Queries. All stores are named and Interactive Queries exposes only the read operations of the underlying implementation.<br>Kafka Streams允许通过方法，线程，进程或应用程序的外部的流处理应用程序创建的状态存储的状态存储的直接只读查询。这是通过一个被称为交互式查询的功能。所有的存储都被命名和交互查询只公开底层实现的读操作。</p>
<p>As we have mentioned above, the computational logic of a Kafka Streams application is defined as a processor topology. Currently Kafka Streams provides two sets of APIs to define the processor topology, which will be described in the subsequent sections.<br>正如我们上文所提到的，Kafka Streams应用程序的计算逻辑被定义为一个处理topology。目前，Kafka Streams提供了两组的接口来定义处理的topology，这将在随后的章节中描述。 </p>
<h2 id="Low-Level-Processor-API"><a href="#Low-Level-Processor-API" class="headerlink" title="Low-Level Processor API"></a>Low-Level Processor API</h2><h3 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a>Processor</h3><p>Developers can define their customized processing logic by implementing the Processor interface, which provides process and punctuate methods. The process method is performed on each of the received record; and the punctuate method is performed periodically based on elapsed time. In addition, the processor can maintain the current ProcessorContext instance variable initialized in the init method, and use the context to schedule the punctuation period (context().schedule), to forward the modified / new key-value pair to downstream processors (context().forward), to commit the current processing progress (context().commit), etc.<br>开发者可以通过处理器接口定义自己的定制的处理逻辑，它提供了方法和标点的方法。process方法是对每个接收的记录执行；和标点法是基于时间进行定期。此外，该处理器可以维持目前的ProcessorContext实例变量在init方法初始化，并使用context安排标点符号周期（context().schedule），提出修改/新的键值对下游处理器（context().forward），把当前的处理进度（context().commit），等。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MyProcessor</span> <span class="keyword">extends</span> <span class="title">Processor</span> </span>&#123;</div><div class="line">        private ProcessorContext context;</div><div class="line">        private KeyValueStore kvStore;</div><div class="line">        @Override</div><div class="line">        @SuppressWarnings(<span class="string">"unchecked"</span>)</div><div class="line">        public <span class="keyword">void</span> init(ProcessorContext context) &#123;</div><div class="line">            <span class="keyword">this</span>.context = context;</div><div class="line">            <span class="keyword">this</span>.context.schedule(<span class="number">1000</span>);</div><div class="line">            <span class="keyword">this</span>.kvStore = (KeyValueStore) context.getStateStore(<span class="string">"Counts"</span>);</div><div class="line">        &#125;</div><div class="line">        @Override</div><div class="line">        public <span class="keyword">void</span> process(<span class="built_in">String</span> dummy, <span class="built_in">String</span> line) &#123;</div><div class="line">            <span class="built_in">String</span>[] words = line.toLowerCase().split(<span class="string">" "</span>);</div><div class="line">            <span class="keyword">for</span> (<span class="built_in">String</span> word : words) &#123;</div><div class="line">                Integer oldValue = <span class="keyword">this</span>.kvStore.get(word);</div><div class="line">                <span class="keyword">if</span> (oldValue == <span class="literal">null</span>) &#123;</div><div class="line">                    <span class="keyword">this</span>.kvStore.put(word, <span class="number">1</span>);</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    <span class="keyword">this</span>.kvStore.put(word, oldValue + <span class="number">1</span>);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        @Override</div><div class="line">        public <span class="keyword">void</span> punctuate(long timestamp) &#123;</div><div class="line">            KeyValueIterator iter = <span class="keyword">this</span>.kvStore.all();</div><div class="line"></div><div class="line">            <span class="keyword">while</span> (iter.hasNext()) &#123;</div><div class="line">                KeyValue entry = iter.next();</div><div class="line">                context.forward(entry.key, entry.value.toString());</div><div class="line">            &#125;</div><div class="line">            iter.close();</div><div class="line">            context.commit();</div><div class="line">        &#125;</div><div class="line">        @Override</div><div class="line">        public <span class="keyword">void</span> close() &#123;</div><div class="line">            <span class="keyword">this</span>.kvStore.close();</div><div class="line">        &#125;</div><div class="line">    &#125;;</div></pre></td></tr></table></figure>
<p>In the above implementation, the following actions are performed:<br>在上面的实现中，执行以下操作：</p>
<ul>
<li>In the init method, schedule the punctuation every 1 second and<br>retrieve the local state store by its name “Counts”.</li>
<li>在init方法，schedule每1秒和标点符号检索本地状态存储由它的名称“计数”。</li>
<li>In the process method, upon each received record, split the value<br>string into words, and update their counts into the state store (we<br>will talk about this feature later in the section).</li>
<li>在处理方法中，在每个接收到的记录中，将值字符串分割成单词，并更新他们的计数到状态存储区（我们将在本节中讨论这个功能）。</li>
<li>In the punctuate method, iterate the local state store and send the<br>aggregated counts to the downstream processor, and commit the current<br>stream state.</li>
<li>在标点法，迭代局部状态存储和发送汇总计数到下游的处理器，并承诺目前流状态。</li>
</ul>
<h3 id="Processor-Topology"><a href="#Processor-Topology" class="headerlink" title="Processor Topology"></a>Processor Topology</h3><p>With the customized processors defined in the Processor API, developers can use the TopologyBuilder to build a processor topology by connecting these processors together:<br>与定制的处理器在Processor API，开发者可以使用TopologyBuilder来连接这些处理器一起建立一个topology：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</div><div class="line">builder.addSource(<span class="string">"SOURCE"</span>, <span class="string">"src-topic"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS1"</span>, <span class="attr">MyProcessor1</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor1 */</span>, <span class="string">"SOURCE"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS2"</span>, <span class="attr">MyProcessor2</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor2 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS3"</span>, <span class="attr">MyProcessor3</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor3 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addSink(<span class="string">"SINK1"</span>, <span class="string">"sink-topic1"</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addSink(<span class="string">"SINK2"</span>, <span class="string">"sink-topic2"</span>, <span class="string">"PROCESS2"</span>)</div><div class="line">        .addSink(<span class="string">"SINK3"</span>, <span class="string">"sink-topic3"</span>, <span class="string">"PROCESS3"</span>);</div></pre></td></tr></table></figure></p>
<p>There are several steps in the above code to build the topology, and here is a quick walk through:<br>在上面的代码中有几个步骤来构建topology，这里是一个快速的步行通过：</p>
<ul>
<li>First of all a source node named “SOURCE” is added to the topology<br>using the addSource method, with one Kafka topic “src-topic” fed to<br>it.</li>
<li>首先，源节点命名为“源”添加到topology使用addSource方法，使用“src-topic”这个Kafka 主题。</li>
<li>Three processor nodes are then added using the addProcessor method;<br>here the first processor is a child of the “SOURCE” node, but is the<br>parent of the other two processors.</li>
<li>三个processor节点，然后使用addProcessor方法添加；这里第一个processor是一个“源”节点的子节点，但是其他两处processors</li>
<li>Finally three sink nodes are added to complete the topology using the<br>addSink method, each piping from a different parent processor node<br>and writing to a separate topic.</li>
<li>最后，添加三个汇聚节点来完成使用的topology addSink方法，每个管道从不同的父节点的processor和写到一个单独的主题。</li>
</ul>
<h3 id="Local-State-Store本地状态存储"><a href="#Local-State-Store本地状态存储" class="headerlink" title="Local State Store本地状态存储"></a>Local State Store本地状态存储</h3><p>Note that the Processor API is not limited to only accessing the current records as they arrive, but can also maintain local state stores that keep recently arrived records to use in stateful processing operations such as aggregation or windowed joins. To take advantage of this local states, developers can use the TopologyBuilder.addStateStore method when building the processor topology to create the local state and associate it with the processor nodes that needs to access it; or they can connect a created local state store with the existing processor nodes through TopologyBuilder.connectProcessorAndStateStores.<br>注意Processor API不仅限于访问当前记录，还可以维持本地状态村粗，使记录使用状态的处理操作，如聚集或窗口的加入。利用局部状态，开发人员可以使用TopologyBuilder.addStateStore方法当搭建processor topology创建本地状态，它与processor节点需要访问它的联想；或者他们可以连接创建的局部状态存储与现有的处理器节点通过TopologyBuilder.connectProcessorAndStateStores.<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</div><div class="line">    builder.addSource(<span class="string">"SOURCE"</span>, <span class="string">"src-topic"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS1"</span>, <span class="attr">MyProcessor1</span>::<span class="keyword">new</span>, <span class="string">"SOURCE"</span>)</div><div class="line">        <span class="comment">// create the in-memory state store "COUNTS" associated with processor "PROCESS1"</span></div><div class="line">        .addStateStore(Stores.create(<span class="string">"COUNTS"</span>).withStringKeys().withStringValues().inMemory().build(), <span class="string">"PROCESS1"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS2"</span>, <span class="attr">MyProcessor3</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor3 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addProcessor(<span class="string">"PROCESS3"</span>, <span class="attr">MyProcessor3</span>::<span class="keyword">new</span> <span class="comment">/* the ProcessorSupplier that can generate MyProcessor3 */</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        <span class="comment">// connect the state store "COUNTS" with processor "PROCESS2"</span></div><div class="line">        .connectProcessorAndStateStores(<span class="string">"PROCESS2"</span>, <span class="string">"COUNTS"</span>);</div><div class="line">        .addSink(<span class="string">"SINK1"</span>, <span class="string">"sink-topic1"</span>, <span class="string">"PROCESS1"</span>)</div><div class="line">        .addSink(<span class="string">"SINK2"</span>, <span class="string">"sink-topic2"</span>, <span class="string">"PROCESS2"</span>)</div><div class="line">        .addSink(<span class="string">"SINK3"</span>, <span class="string">"sink-topic3"</span>, <span class="string">"PROCESS3"</span>);</div></pre></td></tr></table></figure></p>
<p>例子：<br>搭建kafka_2.11-0.10.1.0 集群：<br>设置server.properties<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">#broker的全局唯一编号，不能重复</div><div class="line">broker.id=0</div><div class="line"></div><div class="line">#用来监听链接的端口，producer或consumer将在此端口建立连接</div><div class="line">port=9092</div><div class="line"></div><div class="line">#处理网络请求的线程数量</div><div class="line">num.network.threads=3</div><div class="line"></div><div class="line">#用来处理磁盘IO的线程数量</div><div class="line">num.io.threads=8</div><div class="line"></div><div class="line">#发送套接字的缓冲区大小</div><div class="line">socket.send.buffer.bytes=102400</div><div class="line"></div><div class="line">#接受套接字的缓冲区大小</div><div class="line">socket.receive.buffer.bytes=102400</div><div class="line"></div><div class="line">#请求套接字的缓冲区大小</div><div class="line">socket.request.max.bytes=104857600</div><div class="line"></div><div class="line">#kafka运行日志存放的路径</div><div class="line">log.dirs=/home/hadoop/apps/kafka_2.11-0.10.1.0/logs/kafka</div><div class="line"></div><div class="line">#topic在当前broker上的分片个数</div><div class="line">num.partitions=2</div><div class="line"></div><div class="line">#用来恢复和清理data下数据的线程数量</div><div class="line">num.recovery.threads.per.data.dir=1</div><div class="line"></div><div class="line">#segment文件保留的最长时间，超时将被删除</div><div class="line">log.retention.hours=168</div><div class="line"></div><div class="line">#滚动生成新的segment文件的最大时间</div><div class="line">log.roll.hours=168</div><div class="line"></div><div class="line">#日志文件中每个segment的大小，默认为1G</div><div class="line">log.segment.bytes=1073741824</div><div class="line"></div><div class="line">#周期性检查文件大小的时间</div><div class="line">log.retention.check.interval.ms=300000</div><div class="line"></div><div class="line">#日志清理是否打开</div><div class="line">log.cleaner.enable=true</div><div class="line"></div><div class="line">#broker需要使用zookeeper保存meta数据</div><div class="line">zookeeper.connect=www.hadoop01.com:2181,www.hadoop02.com:2181,www.hadoop03.com:2181/kafka0.10.1.0</div><div class="line"></div><div class="line">#zookeeper链接超时时间</div><div class="line">zookeeper.connection.timeout.ms=6000</div><div class="line"></div><div class="line">#partion buffer中，消息的条数达到阈值，将触发flush到磁盘</div><div class="line">log.flush.interval.messages=10000</div><div class="line"></div><div class="line">#消息buffer的时间，达到阈值，将触发flush到磁盘</div><div class="line">log.flush.interval.ms=3000</div><div class="line"></div><div class="line">#删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除</div><div class="line">delete.topic.enable=true</div><div class="line"></div><div class="line">#此处的host.name为本机IP(重要),如果不改,则客户端会抛出:Producer connection to localhost:9092 unsuccessful 错误!</div><div class="line">host.name=www.hadoop01.com</div></pre></td></tr></table></figure></p>
<p>分发到其他机器<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scp -rp kafka_2<span class="number">.11</span><span class="number">-0.10</span><span class="number">.1</span><span class="number">.0</span>/ hadoop@www.hadoop02.com:<span class="regexp">/home/</span>hadoop/apps/</div><div class="line">scp -rp kafka_2<span class="number">.11</span><span class="number">-0.10</span><span class="number">.1</span><span class="number">.0</span>/ hadoop@www.hadoop03.com:<span class="regexp">/home/</span>hadoop/apps/</div></pre></td></tr></table></figure></p>
<p>修改server.properties 以下两个参数<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">broker.id=<span class="number">0</span></div><div class="line">host.name=www.hadoop01.com</div></pre></td></tr></table></figure></p>
<p>zookeeper设置，启动略。</p>
<p>创建主题</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --replication-factor <span class="number">1</span> --partitions <span class="number">1</span> --topic words</div><div class="line">Created topic <span class="string">"words"</span>.</div><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh --create --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --replication-factor <span class="number">1</span> --partitions <span class="number">1</span> --topic counts</div><div class="line">Created topic <span class="string">"counts"</span>.</div><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh  --describe --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --topic words</div><div class="line">        Topic:words     PartitionCount:<span class="number">1</span>        ReplicationFactor:<span class="number">1</span>     Configs:</div><div class="line">        Topic: words    Partition: <span class="number">0</span>    Leader: <span class="number">2</span>       Replicas: <span class="number">2</span>     Isr: <span class="number">2</span></div><div class="line">[hadoop@www.hadoop02.com bin]$./kafka-topics.sh  --describe --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --topic counts</div><div class="line">        Topic:counts    PartitionCount:<span class="number">1</span>        ReplicationFactor:<span class="number">1</span>     Configs:</div><div class="line">        Topic: counts   Partition: <span class="number">0</span>    Leader: <span class="number">2</span>       Replicas: <span class="number">2</span>     Isr: <span class="number">2</span></div></pre></td></tr></table></figure>
<p>依次启动：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicLong;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.DoubleDeserializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.IntegerDeserializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</div><div class="line"></div><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">DemoConsumerManualCommit</span> </span>&#123;</div><div class="line"></div><div class="line">	public <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) throws Exception &#123;</div><div class="line">		args = <span class="keyword">new</span> <span class="built_in">String</span>[] &#123; <span class="string">"www.hadoop01.com:9092"</span>, <span class="string">"gender-amount"</span>, <span class="string">"group4"</span>, <span class="string">"consumer2"</span> &#125;;</div><div class="line">		<span class="keyword">if</span> (args == <span class="literal">null</span> || args.length != <span class="number">4</span>) &#123;</div><div class="line">			System.err.println(</div><div class="line">					<span class="string">"Usage:\n\tjava -jar kafka_consumer.jar $&#123;bootstrap_server&#125; $&#123;topic_name&#125; $&#123;group_name&#125; $&#123;client_id&#125;"</span>);</div><div class="line">			System.exit(<span class="number">1</span>);</div><div class="line">		&#125;</div><div class="line">		<span class="built_in">String</span> bootstrap = args[<span class="number">0</span>];</div><div class="line">		<span class="built_in">String</span> topic = args[<span class="number">1</span>];</div><div class="line">		<span class="built_in">String</span> groupid = args[<span class="number">2</span>];</div><div class="line">		<span class="built_in">String</span> clientid = args[<span class="number">3</span>];</div><div class="line">		</div><div class="line">		Properties props = <span class="keyword">new</span> Properties();</div><div class="line">		props.put(<span class="string">"bootstrap.servers"</span>, bootstrap);</div><div class="line">		props.put(<span class="string">"group.id"</span>, groupid);</div><div class="line">		props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</div><div class="line">		props.put(<span class="string">"key.deserializer"</span>, StringDeserializer.class.getName());</div><div class="line">		<span class="comment">//props.put("value.deserializer", DoubleDeserializer.class.getName());</span></div><div class="line">		props.put(<span class="string">"value.deserializer"</span>, IntegerDeserializer.class.getName());</div><div class="line">		props.put(<span class="string">"max.poll.interval.ms"</span>, <span class="string">"300000"</span>);</div><div class="line">		props.put(<span class="string">"max.poll.records"</span>, <span class="string">"500"</span>);</div><div class="line">		props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"earliest"</span>);</div><div class="line">		KafkaConsumer&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</div><div class="line">		consumer.subscribe(Arrays.asList(topic));</div><div class="line">		AtomicLong atomicLong = <span class="keyword">new</span> AtomicLong();</div><div class="line">		<span class="keyword">while</span> (<span class="literal">true</span>) &#123;</div><div class="line">			ConsumerRecords&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; records = consumer.poll(<span class="number">100</span>);</div><div class="line">			records.forEach(record -&gt; &#123;</div><div class="line">				System.out.printf(<span class="string">"client : %s , topic: %s , partition: %d , offset = %d, key = %s, value = %s%n"</span>,</div><div class="line">						clientid, record.topic(), record.partition(), record.offset(), record.key(), record.value());</div><div class="line">				<span class="keyword">if</span> (atomicLong.get() % <span class="number">10</span> == <span class="number">0</span>) &#123;</div><div class="line"><span class="comment">//					consumer.commitSync();</span></div><div class="line">				&#125;</div><div class="line">			&#125;);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.IntegerSerializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TopologyBuilder;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.Stores;</div><div class="line"></div><div class="line">public <span class="class"><span class="keyword">class</span> <span class="title">WordCountTopology</span> </span>&#123;</div><div class="line"></div><div class="line">	public <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) throws IOException &#123;</div><div class="line">		Properties props = <span class="keyword">new</span> Properties();</div><div class="line">        props.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"streams-wordcount-processor"</span>);</div><div class="line">        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"www.hadoop01.com:9092"</span>);</div><div class="line">        props.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, <span class="string">"www.hadoop01.com:2181/kafka0.10.1.0"</span>);</div><div class="line">        props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</div><div class="line">        props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());</div><div class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</div><div class="line">		</div><div class="line">		TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</div><div class="line">		builder.addSource(<span class="string">"SOURCE"</span>, <span class="keyword">new</span> StringDeserializer(), <span class="keyword">new</span> StringDeserializer(), <span class="string">"words"</span>)</div><div class="line">				.addProcessor(<span class="string">"WordCountProcessor"</span>, <span class="attr">WordCountProcessor</span>::<span class="keyword">new</span>, <span class="string">"SOURCE"</span>)</div><div class="line">				.addStateStore(Stores.create(<span class="string">"Counts"</span>).withStringKeys().withIntegerValues().inMemory().build(), <span class="string">"WordCountProcessor"</span>)</div><div class="line"><span class="comment">//				.connectProcessorAndStateStores("WordCountProcessor", "Counts")</span></div><div class="line">				.addSink(<span class="string">"SINK"</span>, <span class="string">"count"</span>, <span class="keyword">new</span> StringSerializer(), <span class="keyword">new</span> IntegerSerializer(), <span class="string">"WordCountProcessor"</span>);</div><div class="line">		</div><div class="line">        KafkaStreams stream = <span class="keyword">new</span> KafkaStreams(builder, props);</div><div class="line">        stream.start();</div><div class="line">        System.in.read();</div><div class="line">        stream.close();</div><div class="line">        stream.cleanUp();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>启动producer<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kafka-<span class="built_in">console</span>-producer.sh --broker-list www.hadoop01.com:<span class="number">9092</span> --topic words</div><div class="line">hello apache hello kafka</div></pre></td></tr></table></figure></p>
<p>报错：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Exception <span class="keyword">in</span> thread <span class="string">"StreamThread-1"</span> org.apache.kafka.streams.errors.StreamsException: Extracted timestamp value is negative, which is not allowed.</div><div class="line">	at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:<span class="number">111</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:<span class="number">117</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:<span class="number">144</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:<span class="number">415</span>)</div><div class="line">	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:<span class="number">242</span>)</div></pre></td></tr></table></figure></p>
<p>为时间戳的原因<br>kafka 18May的时候何如了时间戳的东东<br><img src="http://oh6ybr0jg.bkt.clouddn.com/kafka-stream%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3.png" alt="image"><br>后来更改了主题：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bin/kafka-topics.sh --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --create --topic word --partitions <span class="number">1</span> --replication-factor <span class="number">1</span> --config message.timestamp.type=LogAppendTime</div><div class="line">bin/kafka-topics.sh --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span> --create --topic count --partitions <span class="number">1</span> --replication-factor <span class="number">1</span> --config message.timestamp.type=LogAppendTime</div></pre></td></tr></table></figure></p>
<p>先启动DemoConsumerManualCommit再启动WordCountTopology</p>
<p>命令行输入<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kafka-<span class="built_in">console</span>-producer.sh --broker-list www.hadoop01.com:<span class="number">9092</span> --topic word_test</div><div class="line">hello apache kafka hello apache spark hello storm</div></pre></td></tr></table></figure></p>
<p>最后控制台输出</p>
<p>应该是<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">0</span>, key = apache, value = <span class="number">2</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">1</span>, key = hello, value = <span class="number">3</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">2</span>, key = kafka, value = <span class="number">1</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">3</span>, key = spark, value = <span class="number">1</span></div><div class="line">client : consumer2 , <span class="attr">topic</span>: count , <span class="attr">partition</span>: <span class="number">0</span> , offset = <span class="number">4</span>, key = storm, value = <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>查看所有主题<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/kafka-topics.sh --list --zookeeper www.hadoop01.com:<span class="number">2181</span>/kafka0<span class="number">.10</span><span class="number">.1</span><span class="number">.0</span></div></pre></td></tr></table></figure></p>
<p>发现多了一个<br>streams-wordcount-processor-Counts-changelog</p>
<p>In the next section we present another way to build the processor topology: the Kafka Streams DSL. </p>
<h2 id="High-Level-Streams-DSL"><a href="#High-Level-Streams-DSL" class="headerlink" title="High-Level Streams DSL"></a>High-Level Streams DSL</h2><p>To build a processor topology using the Streams DSL, developers can apply the KStreamBuilder class, which is extended from the TopologyBuilder. A simple example is included with the source code for Kafka in the streams/examples package. The rest of this section will walk through some code to demonstrate the key steps in creating a topology using the Streams DSL, but we recommend developers to read the full example source codes for details.<br>使用DSL Streams创建topology，开发人员可以应用KStreamBuilder类，这是从TopologyBuilder延伸。一个简单的例子是包含了streams/examples package的源代码。本节的其余部分将通过一些代码来演示使用流DSL创建topology的关键步骤，但我们建议开发者阅读完整的示例源代码的细节。</p>
<p>KStream and KTable</p>
<p>The DSL uses two main abstractions. A KStream is an abstraction of a record stream, where each data record represents a self-contained datum in the unbounded data set. A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered a create). To illustrate the difference between KStreams and KTables, let’s imagine the following two data records are being sent to the stream: (“alice”, 1) –&gt; (“alice”, 3). If these records a KStream and the stream processing application were to sum the values it would return 4. If these records were a KTable, the return would be 3, since the last record would be considered as an update.</p>
<p>DSL使用的两种主要的抽象。一个KStream是记录流的一个抽象的概念，其中每个数据记录代表一个独立的数据的数据集。一个KTable是一个变更的流的一个抽象的概念，其中每个数据记录的更新。更精确地说，数据记录中的值被认为是同一个记录键的最后一个值的更新，如果有（如果一个相应的key不存在，则该更新将被视为一个创建）。说明kstreams和KTables之间的差异，让我们想象一下以下两个数据记录被发送到流：(“alice”, 1)——&gt;(“alice”, 3)。如果这些记录KStream和流处理应用进行总结的值将返回4。如果这些记录是一个KTable，返回的返回的将是3，因为过去的记录将被视为一种更新。</p>
<p>Create Source Streams from Kafka<br>创建Kafka Streams</p>
<p>Either a record stream (defined as KStream) or a changelog stream (defined as KTable) can be created as a source stream from one or more Kafka topics (for KTable you can only create the source stream from a single topic).<br>一个记录流（定义为KStream）或更新流（定义为KTable）可以创建从一个或多个Kafka主题的源流（为KTable你只能从一个单一的主题创建源流）。 </p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</div><div class="line">KStream source1 = builder.stream(<span class="string">"topic1"</span>, <span class="string">"topic2"</span>);</div><div class="line">KTable source2 = builder.table(<span class="string">"topic3"</span>, <span class="string">"stateStoreName"</span>);</div></pre></td></tr></table></figure>
<p>Windowing a stream 窗口流<br>A stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for join and aggregation operations, etc. Kafka Streams currently defines the following types of windows:</p>
<p>流处理器可能需要将数据记录划分成时间桶，即通过时间窗口的流。这通常是需要的连接和聚合操作等。Kafka流目前定义了以下类型的窗口： </p>
<ul>
<li><p>Hopping time windows are windows based on time intervals. They model<br>fixed-sized, (possibly) overlapping windows. A hopping window is<br>defined by two properties: the window’s size and its advance interval<br>(aka “hop”). The advance interval specifies by how much a window<br>moves forward relative to the previous one. For example, you can<br>configure a hopping window with a size 5 minutes and an advance<br>interval of 1 minute. Since hopping windows can overlap a data record<br>may belong to more than one such windows.<br>跳跃的时间（跳时?）窗口是基于时间间隔的窗口。他们模型固定大小的，（可能）重叠的窗口。跳跃窗口是由两个属性定义的：窗口的大小和它的提前间隔（又名”hop”）。提前间隔由一个窗口相对于前一个窗口移动的多少来指定。例如，您可以配置一个具有大小5分钟和一个提前间隔1分钟的跳跃窗口。由于跳跃窗口可以重叠数据记录，可能属于多个这样的窗口。</p>
</li>
<li><p>Tumbling time windows are a special case of hopping time windows and,<br>like the latter, are windows based on time intervals. They model<br>fixed-size, non-overlapping, gap-less windows. A tumbling window is<br>defined by a single property: the window’s size. A tumbling window is<br>a hopping window whose window size is equal to its advance interval.<br>Since tumbling windows never overlap, a data record will belong to<br>one and only one window.<br>翻滚时间窗口是一个特殊的情况下的跳跃时间窗口，和后者一样，是基于时间间隔的窗口。他们模型固定的大小，不重叠，无缝隙的窗口。一个翻滚窗口是由一个单一属性定义的：窗口的大小。一个翻滚窗口是一个跳跃的窗口，它的窗口大小等于它的预先间隔。由于翻滚的窗口永远不会重叠，数据记录将属于一个并且只有一个窗口。 </p>
</li>
<li><p>Sliding windows model a fixed-size window that slides continuously<br>over the time axis; here, two data records are said to be included in<br>the same window if the difference of their timestamps is within the<br>window size. Thus, sliding windows are not aligned to the epoch, but<br>on the data record timestamps. In Kafka Streams, sliding windows are<br>used only for join operations, and can be specified through the<br>JoinWindows class.<br>滑动窗口模型一个固定大小的窗口，幻灯片不断在时间轴上；在这里，两个数据记录，说是如果他们的时间差异是在窗口的大小，包括在同一个窗口。因此，滑动窗口不一致的时代，但在数据记录的时间戳。Kafka流，滑动窗口只用于连接操作，并可以通过JoinWindows类指定。</p>
</li>
</ul>
<h3 id="Joins"><a href="#Joins" class="headerlink" title="Joins"></a>Joins</h3><p>A join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely. In Kafka Streams, you may perform the following join operations:<br>一个连接操作基于它们的数据记录的键合并两个流，并产生一个新的流。在记录流的加入通常需要执行在视窗基础否则记录必须保持履行加入的数量可以无限增长。在Kafka Streams中，您可以执行以下连接操作：</p>
<ul>
<li>KStream-to-KStream Joins are always windowed joins, since otherwise<br>the memory and state required to compute the join would grow<br>infinitely in size. Here, a newly received record from one of the<br>streams is joined with the other stream’s records within the<br>specified window interval to produce one result for each matching<br>pair based on user-provided ValueJoiner. A new KStream instance<br>representing the result stream of the join is returned from this<br>operator.<br>KStream-to-KStream Joins总是窗口连接，否则内存和计算所需的join会无限增长的大小。在这里，新接收的记录从一条数据流与其他流的记录在指定的窗口间隔为对每一个匹配的基于用户提供的ValueJoiner产生的一个结果。一个新的KStream实例表示的加入导致流从这个操作符返回。</li>
<li>KTable-to-KTable Joins are join operations designed to be consistent<br>with the ones in relational databases. Here, both changelog streams<br>are materialized into local state stores first. When a new record is<br>received from one of the streams, it is joined with the other<br>stream’s materialized state stores to produce one result for each<br>matching pair based on user-provided ValueJoiner. A new KTable<br>instance representing the result stream of the join, which is also a<br>changelog stream of the represented table, is returned from this<br>operator.<br>KTable-to-KTable Joins 操作设计与关系数据库中的一致行动。在这里，无论是修改流物化在当地商店。当一个新的记录从一个流的接收，它与其他流的物化状态存储为对每一个匹配的基于用户提供的ValueJoiner产生的一个结果。一个新的KTable实例代表了连接流，这也是一个代表表更新流，是从这个操作符返回。 </li>
<li>KStream-to-KTable Joins allow you to perform table lookups against a<br>changelog stream (KTable) upon receiving a new record from another<br>record stream (KStream). An example use case would be to enrich a<br>stream of user activities (KStream) with the latest user profile<br>information (KTable). Only records received from the record stream<br>will trigger the join and produce results via ValueJoiner, not vice<br>versa (i.e., records received from the changelog stream will be used<br>only to update the materialized state store). A new KStream instance<br>representing the result stream of the join is returned from this<br>operator.<br>KStream-to-KTable Joins允许你执行表查找和修改流（ktable）在从另一个记录流接收一个新的记录（KStream）。一个例子使用案例将丰富用户活动流（KStream）最新的用户配置文件信息（KTable）。只记录收到的记录流会触发连接并通过ValueJoiner产生结果，反之亦然（即记录收到更新流只会被用来更新物化状态存储）。一个新的KStream实例表示的加入导致流从这个操作符返回。</li>
</ul>
<p>Depending on the operands the following join operations are supported: inner joins, outer joins and left joins. Their semantics are similar to the corresponding operators in relational databases. a<br>Transform a stream</p>
<p>There is a list of transformation operations provided for KStream and KTable respectively. Each of these operations may generate either one or more KStream and KTable objects and can be translated into one or more connected processors into the underlying processor topology. All these transformation methods can be chained together to compose a complex processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types.</p>
<p>Among these transformations, filter, map, mapValues, etc, are stateless transformation operations and can be applied to both KStream and KTable, where users can usually pass a customized function to these functions as a parameter, such as Predicate for filter, KeyValueMapper for map, etc:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// written in Java 8+, using lambda expressions</span></div><div class="line">KStream mapped = source1.mapValue(record -&gt; record.get(<span class="string">"category"</span>));</div></pre></td></tr></table></figure></p>
<p>Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor; Stateful transformations, on the other hand, require accessing an associated state for processing and producing outputs. For example, in join and aggregate operations, a windowing state is usually used to store all the received records within the defined window boundary so far. The operators can then access these accumulated records in the store and compute based on them.<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// written in Java 8+, using lambda expressions</span></div><div class="line">KTable, Long&gt; counts = source1.groupByKey().aggregate(</div><div class="line">    () -&gt; <span class="number">0</span>L,  <span class="comment">// initial value</span></div><div class="line">    (aggKey, value, aggregate) -&gt; aggregate + <span class="number">1</span>L,   <span class="comment">// aggregating value</span></div><div class="line">    TimeWindows.of(<span class="string">"counts"</span>, <span class="number">5000</span>L).advanceBy(<span class="number">1000</span>L), <span class="comment">// intervals in milliseconds</span></div><div class="line">    Serdes.Long() <span class="comment">// serde for aggregated value</span></div><div class="line">);</div><div class="line"></div><div class="line">KStream joined = source1.leftJoin(source2,</div><div class="line">    (record1, record2) -&gt; record1.get(<span class="string">"user"</span>) + <span class="string">"-"</span> + record2.get(<span class="string">"region"</span>);</div><div class="line">);</div></pre></td></tr></table></figure></p>
<p>Write streams back to Kafka</p>
<p>At the end of the processing, users can choose to (continuously) write the final resulted streams back to a Kafka topic through KStream.to and KTable.to.<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">joined.to(<span class="string">"topic4"</span>);</div></pre></td></tr></table></figure></p>
<p>If your application needs to continue reading and processing the records after they have been materialized to a topic via to above, one option is to construct a new stream that reads from the output topic; Kafka Streams provides a convenience method called through:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// equivalent to</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">// joined.to("topic4");</span></div><div class="line"><span class="comment">// materialized = builder.stream("topic4");</span></div><div class="line">KStream materialized = joined.through(<span class="string">"topic4"</span>);</div></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/wechat-reward-image.jpg" alt="张洪铭 WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/alipay-reward-image.jpg" alt="张洪铭 Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>


    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/12/02/redis集群/" rel="next" title="Redis 集群">
                <i class="fa fa-chevron-left"></i> Redis 集群
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/12/02/codis 集群/" rel="prev" title="codis 集群">
                codis 集群 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        
<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/12/02/KafkaStream/"
           data-title="Kafka Stream" data-url="http://yoursite.com/2016/12/02/KafkaStream/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/agns.gif"
               alt="张洪铭" />
          <p class="site-author-name" itemprop="name">张洪铭</p>
          <p class="site-description motion-element" itemprop="description">人生在勤 勤则不匮</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">61</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jiatianyao" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
	<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=459442631&auto=1&height=66"></iframe> 
        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Use-Kafka-Streams-to-process-data"><span class="nav-number">1.</span> <span class="nav-text">Use Kafka Streams to process data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Overview"><span class="nav-number">2.</span> <span class="nav-text">Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Developer-Guide-开发者指南"><span class="nav-number">2.1.</span> <span class="nav-text">Developer Guide 开发者指南</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Core-Concepts-核心概念"><span class="nav-number">2.2.</span> <span class="nav-text">Core Concepts 核心概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stream-Processing-Topology-流处理Topology"><span class="nav-number">2.3.</span> <span class="nav-text">Stream Processing Topology 流处理Topology</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Time-时间"><span class="nav-number">2.4.</span> <span class="nav-text">Time 时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#States-状态"><span class="nav-number">2.5.</span> <span class="nav-text">States 状态</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Low-Level-Processor-API"><span class="nav-number">3.</span> <span class="nav-text">Low-Level Processor API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Processor"><span class="nav-number">3.1.</span> <span class="nav-text">Processor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Processor-Topology"><span class="nav-number">3.2.</span> <span class="nav-text">Processor Topology</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Local-State-Store本地状态存储"><span class="nav-number">3.3.</span> <span class="nav-text">Local State Store本地状态存储</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#High-Level-Streams-DSL"><span class="nav-number">4.</span> <span class="nav-text">High-Level Streams DSL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Joins"><span class="nav-number">4.1.</span> <span class="nav-text">Joins</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
      
    </main>
	
    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张洪铭</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"zhm8"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  








  
  

  

  

  

  
    

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script type="text/javascript" src="/js/src/algolia.js?v=5.1.0"></script>


  
	

</body>
</html>
